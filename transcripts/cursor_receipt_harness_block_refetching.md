# Receipt harness block refetching progress
_Exported on 1/16/2026 at 22:05:43 GMT+1 from Cursor (2.3.41)_

---

**User**

We've just added retrying of the errored blocks in a couple of ways to our harness (see md files in the root). But it seems to take a lot of time @receipt-harness (275-298) - as you can see we are struggling with something for 5 minutes after finishing our run already, and I don't even know what. We should add some separate progress-bar after the main run has finished - just for refetching the missing blocks. And some information at which state we are right now (I see "status escalating" tho - but don't see how many blocks are left and how many nodes are we sending them too, etc etc). This would help us to debug stuff.

---

**Cursor**

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            if let NetworkEvent::ActivePeerSession { info, messages } = event {
                let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                if !matches_genesis {
                    warn!(
                        peer_id = ?info.peer_id,
                        genesis = ?info.status.genesis,
                        "peer genesis mismatch"
                    );
                    continue;
                }

                info!(
                    peer_id = ?info.peer_id,
                    client_version = %info.client_version,
                    remote_addr = ?info.remote_addr,
                    eth_version = ?info.version,
                    chain = ?info.status.chain,
                    head_hash = ?info.status.blockhash,
                    "peer session established"
                );

                context_for_events
                    .stats
                    .lock()
                    .await
                    .record_peer(info.client_version.as_ref());

                let peer_id = info.peer_id;
                let peer_key = format!("{:?}", peer_id);
                let head_hash = info.status.blockhash;
                let messages = messages.clone();
                let context = Arc::clone(&context_for_events);
                let ready_tx = ready_tx_for_events.clone();

                tokio::spawn(async move {
                    let head_start = Instant::now();
                    let head_number = match request_head_number(
                        peer_id,
                        info.version,
                        BlockHashOrNumber::Hash(head_hash),
                        &messages,
                        context.as_ref(),
                    )
                    .await
                    {
                        Ok(number) => {
                            info!(
                                peer_id = ?peer_id,
                                head_number = number,
                                head_ms = head_start.elapsed().as_millis(),
                                "peer head resolved"
                            );
                            number
                        }
                        Err(err) => {
                            warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                            return;
                        }
                    };

                    context.record_active_peer(peer_key.clone()).await;
                    let _ = ready_tx.send(PeerHandle {
                        peer_id,
                        peer_key,
                        eth_version: info.version,
                        head_number,
                        messages,
                    });
                });
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];

    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        let known = self.known_blocks.lock().await;
        let mut attempts = self.escalation_attempts.lock().await;
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let mut iterations = queue.len();
        while iterations > 0 {
            iterations -= 1;
            if let Some(block) = queue.pop_front() {
                queued.remove(&block);
                if known.contains(&block) {
                    continue;
                }
                let entry = attempts.entry(block).or_default();
                if entry.contains(peer_key) {
                    queue.push_back(block);
                    queued.insert(block);
                    continue;
                }
                if sessions > 0 && entry.len() >= sessions {
                    continue;
                }
                entry.insert(peer_key.to_string());
                return vec![block];
            }
        }
        Vec::new()
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if sessions > 0 && tried >= sessions {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

Updated the harness progress reporting to switch to a dedicated retry bar after the main run completes, and expanded the status message to include queue/inflight/failed counts for easier debugging. This is all in `spawn_progress_bar`, which now tracks missing-block totals during escalation and renders a second progress bar with remaining counts and peer activity.

```1969:2078:harness/src/main.rs
fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
```

Tests not run (CLI progress change only). If you want, run the harness and watch for the `retrying_missing` bar once the main bar clears.

```plaintext
fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
```

---

**User**

My previous run is still running - I want to know which blocks were missing. If I Ctrl-C it - would it save data and exit gracefully? Or it already saved them somewhere to jsons?

---

**Cursor**

Short answer: yes, CtrlC goes through the normal shutdown path and flushes the JSON logs. The logs are already being written during the run.

- CtrlC is explicitly handled; after it fires the harness sends shutdown and then writes the summary and flushes all log files (`requests.jsonl`, `probes.jsonl`, `known_blocks.jsonl`, `stats.jsonl`).  
```293:315:harness/src/main.rs
    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
```

- Known blocks are persisted as they arrive, so you wont lose them even if you stop now:  
```1180:1187:harness/src/main.rs
    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }
```

Whats *not* persisted today is the full **list of missing blocks**. Only counts/reasons are in `stats.jsonl`, and the successful blocks are in `known_blocks.jsonl` (`/Users/vicnaum/github/stateless-history-node/harness/output/known_blocks.jsonl`). To get the missing list youd need to diff the target set against `known_blocks.jsonl`. If you want, I can add a small dump on shutdown (e.g., `missing_blocks.jsonl`) or a script to compute it from the existing files.

```plaintext
    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
```

```plaintext
    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }
```

---

**User**

Ctrl+C doesn't seem to work. Neither is Ctrl-D. What should I do? Just kill the process?

---

**Cursor**

CtrlD wont stop it (thats just EOF on stdin). If CtrlC isnt working, the process likely isnt attached to the terminal youre in. Do this:

1) Find the actual `receipt-harness` PID (not just `cargo`):
```
pgrep -af receipt-harness
```

2) Send SIGINT (same as CtrlC):
```
kill -2 <pid>
```
Give it ~10s. If its still alive, try SIGINT again.

3) Last resort: terminate it (not graceful) and then hard kill:
```
kill -15 <pid>   # SIGTERM (no shutdown hook)
kill -9 <pid>    # SIGKILL if it still wont die
```

Data safety: logs are already flushed continuously (defaults are every 1s or 200 lines), so `harness/output/*.jsonl` should be mostly complete even if you kill it. A hard kill will skip the final summary and any last buffered lines.

If you want, I can add a quick script or a shutdown dump to list missing blocks explicitly.

```plaintext
pgrep -af receipt-harness
```

```plaintext
kill -2 <pid>
```

```plaintext
kill -15 <pid>   # SIGTERM (no shutdown hook)
kill -9 <pid>    # SIGKILL if it still wont die
```

---

**User**

@zsh (416-520) 
Okay, only SIGTERM could kill it. Probably something we want to fix. It was in the escalating state - maybe that's the reason?
Help me to investigate the missing blocks. I can run it again, but I need to know beforehand which blocks were missing (you can parse the logs - I've ran it with 200000 anchor window. And also look that it received the errores from the same 3 peers all the time during escalation - and it had 54 peers before that. Why did it repeatedly send to the same peers? It should somehow note which peers it asked already and then when it runs out of peers - it can stop or something and decide it's not possible to fetch these remaining missing blocks. Help me to improve it.
Also please improve handling Ctrl-C. And also add missing blocks json as we mentioned above if such still remain.

---

**Cursor**

```rust
*** Begin Patch
*** Update File: /Users/vicnaum/github/stateless-history-node/harness/src/main.rs
@@
-use tokio::sync::{mpsc, oneshot, Mutex};
+use tokio::sync::{mpsc, oneshot, Mutex};
+#[cfg(unix)]
+use tokio::signal::unix::{signal, SignalKind};
@@
-use reth_network_api::{
-    test_utils::{PeersHandle, PeersHandleProvider},
-    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerId,
-    PeerRequest, PeerRequestSender,
-};
+use reth_network_api::{
+    test_utils::{PeersHandle, PeersHandleProvider},
+    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
+    PeerRequest, PeerRequestSender,
+};
*** End Patch
```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            if let NetworkEvent::ActivePeerSession { info, messages } = event {
                let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                if !matches_genesis {
                    warn!(
                        peer_id = ?info.peer_id,
                        genesis = ?info.status.genesis,
                        "peer genesis mismatch"
                    );
                    continue;
                }

                info!(
                    peer_id = ?info.peer_id,
                    client_version = %info.client_version,
                    remote_addr = ?info.remote_addr,
                    eth_version = ?info.version,
                    chain = ?info.status.chain,
                    head_hash = ?info.status.blockhash,
                    "peer session established"
                );

                context_for_events
                    .stats
                    .lock()
                    .await
                    .record_peer(info.client_version.as_ref());

                let peer_id = info.peer_id;
                let peer_key = format!("{:?}", peer_id);
                let head_hash = info.status.blockhash;
                let messages = messages.clone();
                let context = Arc::clone(&context_for_events);
                let ready_tx = ready_tx_for_events.clone();

                tokio::spawn(async move {
                    let head_start = Instant::now();
                    let head_number = match request_head_number(
                        peer_id,
                        info.version,
                        BlockHashOrNumber::Hash(head_hash),
                        &messages,
                        context.as_ref(),
                    )
                    .await
                    {
                        Ok(number) => {
                            info!(
                                peer_id = ?peer_id,
                                head_number = number,
                                head_ms = head_start.elapsed().as_millis(),
                                "peer head resolved"
                            );
                            number
                        }
                        Err(err) => {
                            warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                            return;
                        }
                    };

                    context.record_active_peer(peer_key.clone()).await;
                    let _ = ready_tx.send(PeerHandle {
                        peer_id,
                        peer_key,
                        eth_version: info.version,
                        head_number,
                        messages,
                    });
                });
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];

    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        let known = self.known_blocks.lock().await;
        let mut attempts = self.escalation_attempts.lock().await;
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let mut iterations = queue.len();
        while iterations > 0 {
            iterations -= 1;
            if let Some(block) = queue.pop_front() {
                queued.remove(&block);
                if known.contains(&block) {
                    continue;
                }
                let entry = attempts.entry(block).or_default();
                if entry.contains(peer_key) {
                    queue.push_back(block);
                    queued.insert(block);
                    continue;
                }
                if sessions > 0 && entry.len() >= sessions {
                    continue;
                }
                entry.insert(peer_key.to_string());
                return vec![block];
            }
        }
        Vec::new()
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if sessions > 0 && tried >= sessions {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            if let NetworkEvent::ActivePeerSession { info, messages } = event {
                let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                if !matches_genesis {
                    warn!(
                        peer_id = ?info.peer_id,
                        genesis = ?info.status.genesis,
                        "peer genesis mismatch"
                    );
                    continue;
                }

                info!(
                    peer_id = ?info.peer_id,
                    client_version = %info.client_version,
                    remote_addr = ?info.remote_addr,
                    eth_version = ?info.version,
                    chain = ?info.status.chain,
                    head_hash = ?info.status.blockhash,
                    "peer session established"
                );

                context_for_events
                    .stats
                    .lock()
                    .await
                    .record_peer(info.client_version.as_ref());

                let peer_id = info.peer_id;
                let peer_key = format!("{:?}", peer_id);
                let head_hash = info.status.blockhash;
                let messages = messages.clone();
                let context = Arc::clone(&context_for_events);
                let ready_tx = ready_tx_for_events.clone();

                tokio::spawn(async move {
                    let head_start = Instant::now();
                    let head_number = match request_head_number(
                        peer_id,
                        info.version,
                        BlockHashOrNumber::Hash(head_hash),
                        &messages,
                        context.as_ref(),
                    )
                    .await
                    {
                        Ok(number) => {
                            info!(
                                peer_id = ?peer_id,
                                head_number = number,
                                head_ms = head_start.elapsed().as_millis(),
                                "peer head resolved"
                            );
                            number
                        }
                        Err(err) => {
                            warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                            return;
                        }
                    };

                    context.record_active_peer(peer_key.clone()).await;
                    let _ = ready_tx.send(PeerHandle {
                        peer_id,
                        peer_key,
                        eth_version: info.version,
                        head_number,
                        messages,
                    });
                });
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];

    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        let known = self.known_blocks.lock().await;
        let mut attempts = self.escalation_attempts.lock().await;
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let mut iterations = queue.len();
        while iterations > 0 {
            iterations -= 1;
            if let Some(block) = queue.pop_front() {
                queued.remove(&block);
                if known.contains(&block) {
                    continue;
                }
                let entry = attempts.entry(block).or_default();
                if entry.contains(peer_key) {
                    queue.push_back(block);
                    queued.insert(block);
                    continue;
                }
                if sessions > 0 && entry.len() >= sessions {
                    continue;
                }
                entry.insert(peer_key.to_string());
                return vec![block];
            }
        }
        Vec::new()
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if sessions > 0 && tried >= sessions {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            if let NetworkEvent::ActivePeerSession { info, messages } = event {
                let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                if !matches_genesis {
                    warn!(
                        peer_id = ?info.peer_id,
                        genesis = ?info.status.genesis,
                        "peer genesis mismatch"
                    );
                    continue;
                }

                info!(
                    peer_id = ?info.peer_id,
                    client_version = %info.client_version,
                    remote_addr = ?info.remote_addr,
                    eth_version = ?info.version,
                    chain = ?info.status.chain,
                    head_hash = ?info.status.blockhash,
                    "peer session established"
                );

                context_for_events
                    .stats
                    .lock()
                    .await
                    .record_peer(info.client_version.as_ref());

                let peer_id = info.peer_id;
                let peer_key = format!("{:?}", peer_id);
                let head_hash = info.status.blockhash;
                let messages = messages.clone();
                let context = Arc::clone(&context_for_events);
                let ready_tx = ready_tx_for_events.clone();

                tokio::spawn(async move {
                    let head_start = Instant::now();
                    let head_number = match request_head_number(
                        peer_id,
                        info.version,
                        BlockHashOrNumber::Hash(head_hash),
                        &messages,
                        context.as_ref(),
                    )
                    .await
                    {
                        Ok(number) => {
                            info!(
                                peer_id = ?peer_id,
                                head_number = number,
                                head_ms = head_start.elapsed().as_millis(),
                                "peer head resolved"
                            );
                            number
                        }
                        Err(err) => {
                            warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                            return;
                        }
                    };

                    context.record_active_peer(peer_key.clone()).await;
                    let _ = ready_tx.send(PeerHandle {
                        peer_id,
                        peer_key,
                        eth_version: info.version,
                        head_number,
                        messages,
                    });
                });
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        let known = self.known_blocks.lock().await;
        let mut attempts = self.escalation_attempts.lock().await;
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let mut iterations = queue.len();
        while iterations > 0 {
            iterations -= 1;
            if let Some(block) = queue.pop_front() {
                queued.remove(&block);
                if known.contains(&block) {
                    continue;
                }
                let entry = attempts.entry(block).or_default();
                if entry.contains(peer_key) {
                    queue.push_back(block);
                    queued.insert(block);
                    continue;
                }
                if sessions > 0 && entry.len() >= sessions {
                    continue;
                }
                entry.insert(peer_key.to_string());
                return vec![block];
            }
        }
        Vec::new()
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if sessions > 0 && tried >= sessions {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            if let NetworkEvent::ActivePeerSession { info, messages } = event {
                let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                if !matches_genesis {
                    warn!(
                        peer_id = ?info.peer_id,
                        genesis = ?info.status.genesis,
                        "peer genesis mismatch"
                    );
                    continue;
                }

                info!(
                    peer_id = ?info.peer_id,
                    client_version = %info.client_version,
                    remote_addr = ?info.remote_addr,
                    eth_version = ?info.version,
                    chain = ?info.status.chain,
                    head_hash = ?info.status.blockhash,
                    "peer session established"
                );

                context_for_events
                    .stats
                    .lock()
                    .await
                    .record_peer(info.client_version.as_ref());

                let peer_id = info.peer_id;
                let peer_key = format!("{:?}", peer_id);
                let head_hash = info.status.blockhash;
                let messages = messages.clone();
                let context = Arc::clone(&context_for_events);
                let ready_tx = ready_tx_for_events.clone();

                tokio::spawn(async move {
                    let head_start = Instant::now();
                    let head_number = match request_head_number(
                        peer_id,
                        info.version,
                        BlockHashOrNumber::Hash(head_hash),
                        &messages,
                        context.as_ref(),
                    )
                    .await
                    {
                        Ok(number) => {
                            info!(
                                peer_id = ?peer_id,
                                head_number = number,
                                head_ms = head_start.elapsed().as_millis(),
                                "peer head resolved"
                            );
                            number
                        }
                        Err(err) => {
                            warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                            return;
                        }
                    };

                    context.record_active_peer(peer_key.clone()).await;
                    let _ = ready_tx.send(PeerHandle {
                        peer_id,
                        peer_key,
                        eth_version: info.version,
                        head_number,
                        messages,
                    });
                });
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        let known = self.known_blocks.lock().await;
        let mut attempts = self.escalation_attempts.lock().await;
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let mut iterations = queue.len();
        while iterations > 0 {
            iterations -= 1;
            if let Some(block) = queue.pop_front() {
                queued.remove(&block);
                if known.contains(&block) {
                    continue;
                }
                let entry = attempts.entry(block).or_default();
                if entry.contains(peer_key) {
                    queue.push_back(block);
                    queued.insert(block);
                    continue;
                }
                if sessions > 0 && entry.len() >= sessions {
                    continue;
                }
                entry.insert(peer_key.to_string());
                return vec![block];
            }
        }
        Vec::new()
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if sessions > 0 && tried >= sessions {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            if let NetworkEvent::ActivePeerSession { info, messages } = event {
                let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                if !matches_genesis {
                    warn!(
                        peer_id = ?info.peer_id,
                        genesis = ?info.status.genesis,
                        "peer genesis mismatch"
                    );
                    continue;
                }

                info!(
                    peer_id = ?info.peer_id,
                    client_version = %info.client_version,
                    remote_addr = ?info.remote_addr,
                    eth_version = ?info.version,
                    chain = ?info.status.chain,
                    head_hash = ?info.status.blockhash,
                    "peer session established"
                );

                context_for_events
                    .stats
                    .lock()
                    .await
                    .record_peer(info.client_version.as_ref());

                let peer_id = info.peer_id;
                let peer_key = format!("{:?}", peer_id);
                let head_hash = info.status.blockhash;
                let messages = messages.clone();
                let context = Arc::clone(&context_for_events);
                let ready_tx = ready_tx_for_events.clone();

                tokio::spawn(async move {
                    let head_start = Instant::now();
                    let head_number = match request_head_number(
                        peer_id,
                        info.version,
                        BlockHashOrNumber::Hash(head_hash),
                        &messages,
                        context.as_ref(),
                    )
                    .await
                    {
                        Ok(number) => {
                            info!(
                                peer_id = ?peer_id,
                                head_number = number,
                                head_ms = head_start.elapsed().as_millis(),
                                "peer head resolved"
                            );
                            number
                        }
                        Err(err) => {
                            warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                            return;
                        }
                    };

                    context.record_active_peer(peer_key.clone()).await;
                    let _ = ready_tx.send(PeerHandle {
                        peer_id,
                        peer_key,
                        eth_version: info.version,
                        head_number,
                        messages,
                    });
                });
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        let known = self.known_blocks.lock().await;
        let mut attempts = self.escalation_attempts.lock().await;
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let mut iterations = queue.len();
        while iterations > 0 {
            iterations -= 1;
            if let Some(block) = queue.pop_front() {
                queued.remove(&block);
                if known.contains(&block) {
                    continue;
                }
                let entry = attempts.entry(block).or_default();
                if entry.contains(peer_key) {
                    queue.push_back(block);
                    queued.insert(block);
                    continue;
                }
                if sessions > 0 && entry.len() >= sessions {
                    continue;
                }
                entry.insert(peer_key.to_string());
                return vec![block];
            }
        }
        Vec::new()
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if sessions > 0 && tried >= sessions {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        let known = self.known_blocks.lock().await;
        let mut attempts = self.escalation_attempts.lock().await;
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let mut iterations = queue.len();
        while iterations > 0 {
            iterations -= 1;
            if let Some(block) = queue.pop_front() {
                queued.remove(&block);
                if known.contains(&block) {
                    continue;
                }
                let entry = attempts.entry(block).or_default();
                if entry.contains(peer_key) {
                    queue.push_back(block);
                    queued.insert(block);
                    continue;
                }
                if sessions > 0 && entry.len() >= sessions {
                    continue;
                }
                entry.insert(peer_key.to_string());
                return vec![block];
            }
        }
        Vec::new()
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let sessions = self.sessions.load(Ordering::SeqCst) as usize;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if sessions > 0 && tried >= sessions {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    #[cfg(unix)]
    let mut sigint = signal(SignalKind::interrupt())?;
    #[cfg(unix)]
    let mut sigterm = signal(SignalKind::terminate())?;

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    #[cfg(unix)]
    let mut sigint = signal(SignalKind::interrupt())?;
    #[cfg(unix)]
    let mut sigterm = signal(SignalKind::terminate())?;

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    let missing_blocks = {
        let known = context.known_blocks.lock().await;
        match write_missing_blocks(&out_dir, context.config.anchor_window, &known) {
            Ok(count) => count,
            Err(err) => {
                warn!(error = ?err, "failed to write missing blocks");
                0
            }
        }
    };
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
        if missing_blocks > 0 {
            println!(
                "Missing blocks: {} (saved to output/missing_blocks.jsonl).",
                missing_blocks
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else {
        "warn"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                let _ = context
                                    .handle_peer_failure(peer_id, &peer_key, "head")
                                    .await;
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    #[cfg(unix)]
    let mut sigint = signal(SignalKind::interrupt())?;
    #[cfg(unix)]
    let mut sigterm = signal(SignalKind::terminate())?;

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    let missing_blocks = {
        let known = context.known_blocks.lock().await;
        match write_missing_blocks(&out_dir, context.config.anchor_window, &known) {
            Ok(count) => count,
            Err(err) => {
                warn!(error = ?err, "failed to write missing blocks");
                0
            }
        }
    };
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
        if missing_blocks > 0 {
            println!(
                "Missing blocks: {} (saved to output/missing_blocks.jsonl).",
                missing_blocks
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

Got it. Heres what I found from the logs and what I changed.

## Missing Blocks (from your 200000 anchor-window run)
- **Missing count:** 10,722 blocks  
- **File created:** `harness/output/missing_blocks.jsonl` (one block per line)  
- **Range summary:** 934 contiguous ranges  
  - First ranges: `1004255610042587`, `1006378810063803`, `1006410810064123`  
  - Last ranges: `2188812121888136`, `2188815321888168`, `2188818421888264`

If you want the full list, its already in `harness/output/missing_blocks.jsonl`.

## Why the same peers kept showing errors
Those repeated warnings are **head header request failures**, not escalation probes. In `probes.jsonl`, escalation only had **9 probes across 8 peers**, so the repeating errors were from new peer sessions failing to resolve head headers. Two causes:

1) We never removed peers from `active_peers`, so peer counts were stale and escalation wouldnt exhaust.  
2) Headheader failures werent counted toward peer bans, so the same peers kept retrying.

I fixed both.

## Code Changes (all in `harness/src/main.rs`)

- **Persist missing blocks on shutdown** (and remove the file if none are missing)  
```698:721:harness/src/main.rs
fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}
```

- **Disconnect tracking + headfailure bans**  
```213:296:harness/src/main.rs
NetworkEvent::ActivePeerSession { info, messages } => { /* ... */ }
NetworkEvent::Peer(peer_event) => match peer_event {
    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
        let peer_key = format!("{:?}", peer_id);
        context_for_events.record_peer_disconnect(&peer_key).await;
    }
    _ => {}
}
```

```273:276:harness/src/main.rs
Err(err) => {
    let _ = context
        .handle_peer_failure(peer_id, &peer_key, "head")
        .await;
    warn!(peer_id = ?peer_id, error = %err, "head header request failed");
    return;
}
```

- **Escalation stops after all active peers tried**  
```1370:1426:harness/src/main.rs
if active_peers > 0 && entry.len() >= active_peers { /* exhausted */ }
...
if active_peers == 0 { /* no peers left */ }
if tried >= active_peers { /* exhausted */ }
```

- **CtrlC + SIGTERM are now handled explicitly**  
```303:354:harness/src/main.rs
#[cfg(unix)]
let mut sigint = signal(SignalKind::interrupt())?;
#[cfg(unix)]
let mut sigterm = signal(SignalKind::terminate())?;
...
tokio::select! {
    #[cfg(unix)] _ = sigint.recv() => {},
    #[cfg(unix)] _ = sigterm.recv() => {},
    #[cfg(not(unix))] _ = tokio::signal::ctrl_c() => {},
    ...
}
```

## What to do next
- Re-run the harness; **CtrlC or SIGTERM should now shut down cleanly**, write summary, and emit `missing_blocks.jsonl` automatically.
- If you want, I can also add a CLI flag to **dump missing ranges** or **stop escalation after N seconds of no new peers**.

Want me to add that timeout or a no new peers detector?

```plaintext
fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}
```

```plaintext
NetworkEvent::ActivePeerSession { info, messages } => { /* ... */ }
NetworkEvent::Peer(peer_event) => match peer_event {
    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
        let peer_key = format!("{:?}", peer_id);
        context_for_events.record_peer_disconnect(&peer_key).await;
    }
    _ => {}
}
```

```plaintext
Err(err) => {
    let _ = context
        .handle_peer_failure(peer_id, &peer_key, "head")
        .await;
    warn!(peer_id = ?peer_id, error = %err, "head header request failed");
    return;
}
```

```plaintext
if active_peers > 0 && entry.len() >= active_peers { /* exhausted */ }
...
if active_peers == 0 { /* no peers left */ }
if tried >= active_peers { /* exhausted */ }
```

```plaintext
#[cfg(unix)]
let mut sigint = signal(SignalKind::interrupt())?;
#[cfg(unix)]
let mut sigterm = signal(SignalKind::terminate())?;
...
tokio::select! {
    #[cfg(unix)] _ = sigint.recv() => {},
    #[cfg(unix)] _ = sigterm.recv() => {},
    #[cfg(not(unix))] _ = tokio::signal::ctrl_c() => {},
    ...
}
```

---

**User**

@zsh (419-421) 

1. Why this shows 999228/1000000 blocks then? I thought only handful (772) were missing. Maybe they weren't saved? Or maybe they weren't counted at the end of the queue?
2. Why are we still fetching head headers after we fethed all the blocks? How is our fetching strategy look anyway? Do we first fetch blocks, and then based on that we fetch receipts one by one? What happens if we can never fetch some block headers but fetch all the receipts? Need to think about that, cause maybe our progress bar is misleading.

I don't want you to change anything, but just answer the questions and help to brainstorm

---

**Cursor**

Heres the reasoning, based on how the harness counts and schedules:

### 1) Why `999228/1000000` when only ~772 missing?
- The progress bar position is **`known_blocks + failed_blocks`**, not still missing.
- At that moment:  
  - `999,228 / 1,000,000`  **772 unresolved** *at that instant* (still inflight / queued).  
  - But blocks already **marked failed** are counted as processed, even though theyre still missing in the output set.
- Thats why the final missing list from `known_blocks.jsonl` is **10,722**:  
  **missing = failed + stillpending**, not just the 772 gap.
- Also, the percent is integerrounded, so `99.92%` shows as `100%`.

### 2) Why are head headers still being fetched after done?
Those warnings are from **peer head resolution**, not the target block fetches.  
Every new peer session triggers `request_head_number()` to know its head height **before scheduling work**. That happens even if the queue is already empty because the network still accepts peers until shutdown. So youll still see headheader logs during escalation or after the main queue drains.

### Fetching strategy (current)
- **No block bodies at all**.  
- For each batch (contiguous blocks):  
  1) Request **headers** by range.  
  2) Use header hashes to request **receipts** in chunks (`receipts_per_request`, default 16).  
- Blocks are marked known **only when receipts succeed**.  
- If a header cant be fetched, receipts cant be fetched either (no hash), so that block can never become known.

So the progress bar is effectively **blocks with receipts OR failed**, not headers fetched.

---

### Brainstorming ideas (no changes requested)
- Show **three numbers**: `known`, `failed`, `pending` (queue + escalation + inflight).  
- Track an **inflight blocks counter** (batch size added on schedule, subtracted on completion) to make progress less misleading.
- Gate `request_head_number()` calls once no work remains (or only during escalation).  
- Separate progress for **headers vs receipts** if you want to diagnose headeronly failures.

If you want, I can also summarize the missing list into ranges or extract just the pending vs failed counts from logs.

---

**User**

Let's add "Failed" blocks as another number after the progress bar (in the end maybe). And when they're resolved (during processing after requeue for example) - you can remove them from Failed until just those left which are non-solvable without escalation.
Then for escalation let's have a separate red progress bar with numbers showing how many failed blocks we're escalating, and other useful information about HOW we're escalating them.
Also I believe these WARN messages belong to -vv, and not -v. Please filter them out of default -v view.

---

**Cursor**

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else if config.verbosity >= Verbosity::V2 {
        "warn"
    } else {
        "error"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                let _ = context
                                    .handle_peer_failure(peer_id, &peer_key, "head")
                                    .await;
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    #[cfg(unix)]
    let mut sigint = signal(SignalKind::interrupt())?;
    #[cfg(unix)]
    let mut sigterm = signal(SignalKind::terminate())?;

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    let missing_blocks = {
        let known = context.known_blocks.lock().await;
        match write_missing_blocks(&out_dir, context.config.anchor_window, &known) {
            Ok(count) => count,
            Err(err) => {
                warn!(error = ?err, "failed to write missing blocks");
                0
            }
        }
    };
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
        if missing_blocks > 0 {
            println!(
                "Missing blocks: {} (saved to output/missing_blocks.jsonl).",
                missing_blocks
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else if config.verbosity >= Verbosity::V2 {
        "warn"
    } else {
        "error"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                let _ = context
                                    .handle_peer_failure(peer_id, &peer_key, "head")
                                    .await;
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    #[cfg(unix)]
    let mut sigint = signal(SignalKind::interrupt())?;
    #[cfg(unix)]
    let mut sigterm = signal(SignalKind::terminate())?;

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    let missing_blocks = {
        let known = context.known_blocks.lock().await;
        match write_missing_blocks(&out_dir, context.config.anchor_window, &known) {
            Ok(count) => count,
            Err(err) => {
                warn!(error = ?err, "failed to write missing blocks");
                0
            }
        }
    };
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
        if missing_blocks > 0 {
            println!(
                "Missing blocks: {} (saved to output/missing_blocks.jsonl).",
                missing_blocks
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn escalation_attempted_blocks(&self) -> usize {
        self.escalation_attempts.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status retrying_missing | peers 0/0 | inflight 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let msg = format!(
                                    "missing {remaining}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else if config.verbosity >= Verbosity::V2 {
        "warn"
    } else {
        "error"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                let _ = context
                                    .handle_peer_failure(peer_id, &peer_key, "head")
                                    .await;
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    #[cfg(unix)]
    let mut sigint = signal(SignalKind::interrupt())?;
    #[cfg(unix)]
    let mut sigterm = signal(SignalKind::terminate())?;

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            #[cfg(unix)]
            _ = sigint.recv() => {},
            #[cfg(unix)]
            _ = sigterm.recv() => {},
            #[cfg(not(unix))]
            _ = tokio::signal::ctrl_c() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    let missing_blocks = {
        let known = context.known_blocks.lock().await;
        match write_missing_blocks(&out_dir, context.config.anchor_window, &known) {
            Ok(count) => count,
            Err(err) => {
                warn!(error = ?err, "failed to write missing blocks");
                0
            }
        }
    };
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
        if missing_blocks > 0 {
            println!(
                "Missing blocks: {} (saved to output/missing_blocks.jsonl).",
                missing_blocks
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn escalation_attempted_blocks(&self) -> usize {
        self.escalation_attempts.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | Failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | Failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.red/black} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status escalating_failed | failed 0/0 | tried 0/0 | peers 0/0 | inflight 0 | queue 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let attempted = context.escalation_attempted_blocks().await;
                                let msg = format!(
                                    "failed {remaining}/{escalation_total} | tried {attempted}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

Updated progress output and logging to match your request.

- Main progress bar now ends with `Failed X` (counts drop when blocks are recovered, since `clear_failed_block` already removes them).
- Escalation now uses a **red** bar and shows `failed remaining/total`, `tried attempted/total`, plus peers/inflight/queue.
- WARN logs are now hidden for `-v`; they start at `-vv` (V2) and above.

Code refs:
```2080:2166:harness/src/main.rs
pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | Failed 0");
...
let msg = format!(
    "status {} | peers {}/{} | queue {} | inflight {} | Failed {}",
    status, active, sessions, queue_len, in_flight, failed
);
...
let esc_style = ProgressStyle::with_template(
    "{bar:40.red/black} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
)
...
let msg = format!(
    "failed {remaining}/{escalation_total} | tried {attempted}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
);
```

Logging change:
```35:45:harness/src/main.rs
let default_level = if config.quiet {
    "error"
} else if config.verbosity >= Verbosity::V3 {
    "info"
} else if config.verbosity >= Verbosity::V2 {
    "warn"
} else {
    "error"
};
```

No tests run.

```plaintext
pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | Failed 0");
...
let msg = format!(
    "status {} | peers {}/{} | queue {} | inflight {} | Failed {}",
    status, active, sessions, queue_len, in_flight, failed
);
...
let esc_style = ProgressStyle::with_template(
    "{bar:40.red/black} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
)
...
let msg = format!(
    "failed {remaining}/{escalation_total} | tried {attempted}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
);
```

```plaintext
let default_level = if config.quiet {
    "error"
} else if config.verbosity >= Verbosity::V3 {
    "info"
} else if config.verbosity >= Verbosity::V2 {
    "warn"
} else {
    "error"
};
```

---

**User**

@zsh (524-550) please fix

---

**Cursor**

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else if config.verbosity >= Verbosity::V2 {
        "warn"
    } else {
        "error"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                let _ = context
                                    .handle_peer_failure(peer_id, &peer_key, "head")
                                    .await;
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = wait_for_shutdown_signal() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = wait_for_shutdown_signal() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    let missing_blocks = {
        let known = context.known_blocks.lock().await;
        match write_missing_blocks(&out_dir, context.config.anchor_window, &known) {
            Ok(count) => count,
            Err(err) => {
                warn!(error = ?err, "failed to write missing blocks");
                0
            }
        }
    };
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
        if missing_blocks > 0 {
            println!(
                "Missing blocks: {} (saved to output/missing_blocks.jsonl).",
                missing_blocks
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn escalation_attempted_blocks(&self) -> usize {
        self.escalation_attempts.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | Failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | Failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.red/black} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status escalating_failed | failed 0/0 | tried 0/0 | peers 0/0 | inflight 0 | queue 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let attempted = context.escalation_attempted_blocks().await;
                                let msg = format!(
                                    "failed {remaining}/{escalation_total} | tried {attempted}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerEvent, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else if config.verbosity >= Verbosity::V2 {
        "warn"
    } else {
        "error"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                let _ = context
                                    .handle_peer_failure(peer_id, &peer_key, "head")
                                    .await;
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = wait_for_shutdown_signal() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = wait_for_shutdown_signal() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    let missing_blocks = {
        let known = context.known_blocks.lock().await;
        match write_missing_blocks(&out_dir, context.config.anchor_window, &known) {
            Ok(count) => count,
            Err(err) => {
                warn!(error = ?err, "failed to write missing blocks");
                0
            }
        }
    };
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
        if missing_blocks > 0 {
            println!(
                "Missing blocks: {} (saved to output/missing_blocks.jsonl).",
                missing_blocks
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

async fn wait_for_shutdown_signal() {
    #[cfg(unix)]
    {
        let mut sigint =
            signal(SignalKind::interrupt()).expect("failed to register SIGINT handler");
        let mut sigterm =
            signal(SignalKind::terminate()).expect("failed to register SIGTERM handler");
        tokio::select! {
            _ = sigint.recv() => {},
            _ = sigterm.recv() => {},
        }
    }
    #[cfg(not(unix))]
    {
        let _ = tokio::signal::ctrl_c().await;
    }
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn escalation_attempted_blocks(&self) -> usize {
        self.escalation_attempts.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | Failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | Failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.red/black} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status escalating_failed | failed 0/0 | tried 0/0 | peers 0/0 | inflight 0 | queue 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let attempted = context.escalation_attempted_blocks().await;
                                let msg = format!(
                                    "failed {remaining}/{escalation_total} | tried {attempted}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

Fixed the `tokio::select!` compile error by moving the platformspecific signal handling into a helper future (no `#[cfg]` inside the macro). Rerun should compile now.

```685:710:harness/src/main.rs
async fn wait_for_shutdown_signal() {
    #[cfg(unix)]
    {
        let mut sigint =
            signal(SignalKind::interrupt()).expect("failed to register SIGINT handler");
        let mut sigterm =
            signal(SignalKind::terminate()).expect("failed to register SIGTERM handler");
        tokio::select! {
            _ = sigint.recv() => {},
            _ = sigterm.recv() => {},
        }
    }
    #[cfg(not(unix))]
    {
        let _ = tokio::signal::ctrl_c().await;
    }
}
```

```310:334:harness/src/main.rs
tokio::select! {
    _ = wait_for_shutdown_signal() => {},
    _ = tokio::time::sleep(Duration::from_secs(seconds)) => { /* ... */ },
    _ = shutdown_rx.changed() => {},
}
```

Want me to run `cargo run -- --anchor-window 200000` now?

```plaintext
async fn wait_for_shutdown_signal() {
    #[cfg(unix)]
    {
        let mut sigint =
            signal(SignalKind::interrupt()).expect("failed to register SIGINT handler");
        let mut sigterm =
            signal(SignalKind::terminate()).expect("failed to register SIGTERM handler");
        tokio::select! {
            _ = sigint.recv() => {},
            _ = sigterm.recv() => {},
        }
    }
    #[cfg(not(unix))]
    {
        let _ = tokio::signal::ctrl_c().await;
    }
}
```

```plaintext
tokio::select! {
    _ = wait_for_shutdown_signal() => {},
    _ = tokio::time::sleep(Duration::from_secs(seconds)) => { /* ... */ },
    _ = shutdown_rx.changed() => {},
}
```

---

**User**

@zsh (551-564) is there a way for you to compile it to fix all the errors before finishing?

---

**Cursor**

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network_api::events::PeerEvent;
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else if config.verbosity >= Verbosity::V2 {
        "warn"
    } else {
        "error"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                let _ = context
                                    .handle_peer_failure(peer_id, &peer_key, "head")
                                    .await;
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = wait_for_shutdown_signal() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = wait_for_shutdown_signal() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    let missing_blocks = {
        let known = context.known_blocks.lock().await;
        match write_missing_blocks(&out_dir, context.config.anchor_window, &known) {
            Ok(count) => count,
            Err(err) => {
                warn!(error = ?err, "failed to write missing blocks");
                0
            }
        }
    };
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
        if missing_blocks > 0 {
            println!(
                "Missing blocks: {} (saved to output/missing_blocks.jsonl).",
                missing_blocks
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

async fn wait_for_shutdown_signal() {
    #[cfg(unix)]
    {
        let mut sigint =
            signal(SignalKind::interrupt()).expect("failed to register SIGINT handler");
        let mut sigterm =
            signal(SignalKind::terminate()).expect("failed to register SIGTERM handler");
        tokio::select! {
            _ = sigint.recv() => {},
            _ = sigterm.recv() => {},
        }
    }
    #[cfg(not(unix))]
    {
        let _ = tokio::signal::ctrl_c().await;
    }
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn escalation_attempted_blocks(&self) -> usize {
        self.escalation_attempts.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | Failed 0");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | Failed {}",
                            status, active, sessions, queue_len, in_flight, failed
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                    "{bar:40.red/black} {percent:>3}% {pos}/{len} | {elapsed_precise} | ETA {eta_precise} | speed {per_sec} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status escalating_failed | failed 0/0 | tried 0/0 | peers 0/0 | inflight 0 | queue 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                            if let Some(ref pb) = escalation_pb {
                                let attempted = context.escalation_attempted_blocks().await;
                                let msg = format!(
                                    "failed {remaining}/{escalation_total} | tried {attempted}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len}"
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

I fixed the import (`PeerEvent` now from `reth_network_api::events`), but I cant compile here because `cargo` isnt available in the sandbox (`command not found`).  

If you want me to compile to catch any remaining errors, please allow running **without sandbox**. Otherwise, you can run this locally:

```
cd /Users/vicnaum/github/stateless-history-node/harness
cargo build
```

Let me know if you want me to proceed outside the sandbox.

```plaintext
cd /Users/vicnaum/github/stateless-history-node/harness
cargo build
```

---

**User**

We need to fix the speed issue if resuming - cause 990000 blocks were already processed and load instantly - speed is in billions/s. We need to account for that when calculating speed (and also calculate it based on some kind of small window, cause I feel it averages over a lot of time - 1 second averaging window would be more than enough)

---

**Cursor**

```rust
use alloy_primitives::B256;
use futures::StreamExt;
use indicatif::{ProgressBar, ProgressDrawTarget, ProgressStyle};
use reth_chainspec::MAINNET;
use reth_eth_wire::EthNetworkPrimitives;
use reth_eth_wire_types::{
    BlockHashOrNumber, EthVersion, GetBlockHeaders, GetReceipts, GetReceipts70, HeadersDirection,
    Receipts69, Receipts70,
};
use reth_network::config::{rng_secret_key, NetworkConfigBuilder};
use reth_network::import::ProofOfStakeBlockImport;
use reth_network_api::{
    test_utils::{PeersHandle, PeersHandleProvider},
    DiscoveredEvent, DiscoveryEvent, NetworkEvent, NetworkEventListenerProvider, PeerId,
    PeerRequest, PeerRequestSender,
};
use reth_network_api::events::PeerEvent;
use reth_network::PeersConfig;
use reth_primitives_traits::{Header, SealedHeader};
use serde_json::json;
use std::cmp::Reverse;
use std::collections::{BinaryHeap, HashMap, HashSet, VecDeque};
use std::fs::{File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use tokio::sync::{mpsc, oneshot, Mutex};
#[cfg(unix)]
use tokio::signal::unix::{signal, SignalKind};
use tracing::{info, warn};
use tracing_subscriber::EnvFilter;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = CliConfig::parse();
    let default_level = if config.quiet {
        "error"
    } else if config.verbosity >= Verbosity::V3 {
        "info"
    } else if config.verbosity >= Verbosity::V2 {
        "warn"
    } else {
        "error"
    };
    let filter = if std::env::var("RUST_LOG").is_ok() {
        EnvFilter::from_default_env()
    } else {
        EnvFilter::new(default_level)
    };
    tracing_subscriber::fmt().with_env_filter(filter).init();
    let log_flush_lines = config.log_flush_lines;
    let log_flush_ms = config.log_flush_ms;
    let run_start = Instant::now();

    let out_dir = PathBuf::from("output");
    std::fs::create_dir_all(&out_dir)?;
    let known_path = out_dir.join("known_blocks.jsonl");
    let known_blocks = load_known_blocks(&known_path);
    let targets = build_anchor_targets(config.anchor_window);
    let total_targets = targets.len();
    let pending_blocks: Vec<u64> = targets
        .into_iter()
        .filter(|block| !known_blocks.contains(block))
        .collect();
    let queued_blocks: HashSet<u64> = pending_blocks.iter().copied().collect();
    let pending_targets: BinaryHeap<Reverse<u64>> =
        pending_blocks.into_iter().map(Reverse).collect();

    let secret_key = rng_secret_key();
    let mut peers_config = PeersConfig::default()
        .with_max_outbound(config.max_outbound)
        .with_max_concurrent_dials(config.max_concurrent_dials)
        .with_refill_slots_interval(Duration::from_millis(config.refill_interval_ms));
    if let Some(max_inbound) = config.max_inbound {
        peers_config = peers_config.with_max_inbound(max_inbound);
    }

    let net_config = NetworkConfigBuilder::<EthNetworkPrimitives>::new(secret_key)
        .mainnet_boot_nodes()
        .with_unused_ports()
        .peer_config(peers_config)
        .disable_tx_gossip(true)
        .block_import(Box::new(ProofOfStakeBlockImport::default()))
        .build_with_noop_provider(MAINNET.clone());

    let handle = net_config.start_network().await?;
    let peers_handle = handle.peers_handle().clone();
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::watch::channel(false);

    let context = Arc::new(RunContext {
        stats: Mutex::new(Stats::default()),
        queue: Mutex::new(pending_targets),
        known_blocks: Mutex::new(known_blocks),
        failed_blocks: Mutex::new(HashSet::new()),
        escalation_queue: Mutex::new(VecDeque::new()),
        escalation_queued: Mutex::new(HashSet::new()),
        escalation_attempts: Mutex::new(HashMap::new()),
        escalation_active: AtomicBool::new(false),
        queued_blocks: Mutex::new(queued_blocks),
        attempts: Mutex::new(HashMap::new()),
        peer_health: Mutex::new(HashMap::new()),
        active_peers: Mutex::new(HashSet::new()),
        peers_with_jobs: Mutex::new(HashSet::new()),
        window: Mutex::new(WindowStats::default()),
        config,
        peers_handle,
        logger: Logger::new(&out_dir, log_flush_lines)?,
        request_id: AtomicU64::new(1),
        total_targets,
        discovered: AtomicU64::new(0),
        sessions: AtomicU64::new(0),
        jobs_assigned: AtomicU64::new(0),
        in_flight: AtomicU64::new(0),
        run_start,
        shutdown_tx,
    });
    if log_flush_ms > 0 {
        context
            .logger
            .spawn_flush_tasks(Duration::from_millis(log_flush_ms));
    }

    let (ready_tx, mut ready_rx) = mpsc::unbounded_channel::<PeerHandle>();

    let scheduler_context = Arc::clone(&context);
    let scheduler_ready_tx = ready_tx.clone();
    tokio::spawn(async move {
        let mut warmed_up = false;
        while let Some(peer) = ready_rx.recv().await {
            if !warmed_up {
                if WARMUP_SECS > 0 {
                    tokio::time::sleep(Duration::from_secs(WARMUP_SECS)).await;
                }
                warmed_up = true;
            }

            if let Some(remaining) = scheduler_context.peer_ban_remaining(&peer.peer_key).await {
                let ready_tx = scheduler_ready_tx.clone();
                tokio::spawn(async move {
                    tokio::time::sleep(remaining).await;
                    let _ = ready_tx.send(peer);
                });
                continue;
            }

            let batch = scheduler_context
                .next_batch_for_peer(&peer.peer_key, peer.head_number)
                .await;
            if batch.blocks.is_empty() {
                continue;
            }
            scheduler_context.record_peer_job(&peer.peer_key).await;
            scheduler_context.inc_in_flight();

            let context = Arc::clone(&scheduler_context);
            let ready_tx = scheduler_ready_tx.clone();
            tokio::spawn(async move {
                let peer_for_probe = peer.clone();
                let should_requeue = probe_block_batch(
                    peer_for_probe.peer_id,
                    peer_for_probe.peer_key,
                    peer_for_probe.messages,
                    peer_for_probe.eth_version,
                    batch.blocks,
                    batch.mode,
                    Arc::clone(&context),
                )
                .await;
                context.dec_in_flight();
                if should_requeue {
                    let _ = ready_tx.send(peer);
                }
            });
        }
    });

    info!(peer_id = ?handle.peer_id(), "network started");
    println!("receipt-harness: network started");

    let mut discovery_events = handle.discovery_listener();
    let context_for_discovery = Arc::clone(&context);
    tokio::spawn(async move {
        let mut discovered = 0usize;
        while let Some(event) = discovery_events.next().await {
            match event {
                DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued {
                    peer_id,
                    addr,
                    fork_id,
                }) => {
                    discovered += 1;
                    context_for_discovery
                        .discovered
                        .fetch_add(1, Ordering::SeqCst);
                    info!(
                        peer_id = ?peer_id,
                        addr = ?addr,
                        fork_id = ?fork_id,
                        discovered,
                        "peer discovered"
                    );
                }
                DiscoveryEvent::EnrForkId(peer_id, fork_id) => {
                    info!(
                        peer_id = ?peer_id,
                        fork_id = ?fork_id,
                        "peer forkid from ENR"
                    );
                }
            }
        }
    });

    let mut event_listener = handle.event_listener();
    let context_for_events = Arc::clone(&context);
    let ready_tx_for_events = ready_tx.clone();
    tokio::spawn(async move {
        while let Some(event) = event_listener.next().await {
            match event {
                NetworkEvent::ActivePeerSession { info, messages } => {
                    let matches_genesis = info.status.genesis == MAINNET.genesis_hash();
                    if !matches_genesis {
                        warn!(
                            peer_id = ?info.peer_id,
                            genesis = ?info.status.genesis,
                            "peer genesis mismatch"
                        );
                        continue;
                    }

                    info!(
                        peer_id = ?info.peer_id,
                        client_version = %info.client_version,
                        remote_addr = ?info.remote_addr,
                        eth_version = ?info.version,
                        chain = ?info.status.chain,
                        head_hash = ?info.status.blockhash,
                        "peer session established"
                    );

                    context_for_events
                        .stats
                        .lock()
                        .await
                        .record_peer(info.client_version.as_ref());

                    let peer_id = info.peer_id;
                    let peer_key = format!("{:?}", peer_id);
                    let head_hash = info.status.blockhash;
                    let messages = messages.clone();
                    let context = Arc::clone(&context_for_events);
                    let ready_tx = ready_tx_for_events.clone();

                    tokio::spawn(async move {
                        let head_start = Instant::now();
                        let head_number = match request_head_number(
                            peer_id,
                            info.version,
                            BlockHashOrNumber::Hash(head_hash),
                            &messages,
                            context.as_ref(),
                        )
                        .await
                        {
                            Ok(number) => {
                                info!(
                                    peer_id = ?peer_id,
                                    head_number = number,
                                    head_ms = head_start.elapsed().as_millis(),
                                    "peer head resolved"
                                );
                                number
                            }
                            Err(err) => {
                                let _ = context
                                    .handle_peer_failure(peer_id, &peer_key, "head")
                                    .await;
                                warn!(peer_id = ?peer_id, error = %err, "head header request failed");
                                return;
                            }
                        };

                        context.record_active_peer(peer_key.clone()).await;
                        let _ = ready_tx.send(PeerHandle {
                            peer_id,
                            peer_key,
                            eth_version: info.version,
                            head_number,
                            messages,
                        });
                    });
                }
                NetworkEvent::Peer(peer_event) => match peer_event {
                    PeerEvent::SessionClosed { peer_id, .. } | PeerEvent::PeerRemoved(peer_id) => {
                        let peer_key = format!("{:?}", peer_id);
                        context_for_events.record_peer_disconnect(&peer_key).await;
                    }
                    _ => {}
                },
            }
        }
    });

    let progress_rx = shutdown_rx.clone();
    spawn_progress_bar(Arc::clone(&context), progress_rx);
    spawn_window_stats(Arc::clone(&context));

    if let Some(seconds) = context.config.run_secs {
        info!(run_secs = seconds, "auto-shutdown enabled");
        tokio::select! {
            _ = wait_for_shutdown_signal() => {},
            _ = tokio::time::sleep(Duration::from_secs(seconds)) => {
                info!("auto-shutdown reached");
            },
            _ = shutdown_rx.changed() => {},
        }
    } else {
        tokio::select! {
            _ = wait_for_shutdown_signal() => {},
            _ = shutdown_rx.changed() => {},
        }
    }
    let _ = context.shutdown_tx.send(true);

    let completed_blocks = context.completed_blocks().await;
    let summary =
        summarize_stats(&context.stats, run_start.elapsed(), context.total_targets, completed_blocks)
            .await;
    context.logger.log_stats(summary).await;
    context.logger.flush_all().await;
    let missing_blocks = {
        let known = context.known_blocks.lock().await;
        match write_missing_blocks(&out_dir, context.config.anchor_window, &known) {
            Ok(count) => count,
            Err(err) => {
                warn!(error = ?err, "failed to write missing blocks");
                0
            }
        }
    };
    if !context.config.quiet {
        let (elapsed_secs, fetched_blocks, avg_speed, avg_window, max_speed, failed_blocks) = {
            let stats = context.stats.lock().await;
            let elapsed_secs = run_start.elapsed().as_secs_f64().max(0.001);
            let fetched_blocks = stats.receipts_ok_total;
            let avg_speed = fetched_blocks as f64 / elapsed_secs;
            let avg_window = if stats.window_count > 0 {
                stats.window_blocks_per_sec_sum / stats.window_count as f64
            } else {
                0.0
            };
            (
                elapsed_secs,
                fetched_blocks,
                avg_speed,
                avg_window,
                stats.window_blocks_per_sec_max,
                stats.failed_blocks_total,
            )
        };
        if failed_blocks > 0 {
            println!(
                "All done! Fetched {} blocks in {:.1}s ({} failed). Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                failed_blocks,
                avg_speed,
                avg_window,
                max_speed
            );
        } else {
            println!(
                "All done! Fetched {} blocks in {:.1}s. Avg {:.2} blocks/s, window avg {:.2}, max {:.2}.",
                fetched_blocks,
                elapsed_secs,
                avg_speed,
                avg_window,
                max_speed
            );
        }
        if missing_blocks > 0 {
            println!(
                "Missing blocks: {} (saved to output/missing_blocks.jsonl).",
                missing_blocks
            );
        }
    }
    Ok(())
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]
enum Verbosity {
    Minimal,
    V1,
    V2,
    V3,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq)]
enum ProbeMode {
    Normal,
    Escalation,
}

struct WorkBatch {
    blocks: Vec<u64>,
    mode: ProbeMode,
}

impl Verbosity {
    fn from_flag(flag: &str) -> Option<Self> {
        match flag {
            "-v" => Some(Self::V1),
            "-vv" => Some(Self::V2),
            "-vvv" => Some(Self::V3),
            _ => None,
        }
    }
}

#[derive(Clone, Debug)]
struct CliConfig {
    run_secs: Option<u64>,
    max_outbound: usize,
    max_concurrent_dials: usize,
    refill_interval_ms: u64,
    max_inbound: Option<usize>,
    anchor_window: u64,
    blocks_per_assignment: usize,
    receipts_per_request: usize,
    request_timeout_ms: u64,
    log_flush_ms: u64,
    log_flush_lines: usize,
    verbosity: Verbosity,
    quiet: bool,
    peer_failure_threshold: u32,
    peer_ban_secs: u64,
    max_attempts: u32,
    stats_interval_secs: u64,
}

impl Default for CliConfig {
    fn default() -> Self {
        Self {
            run_secs: None,
            max_outbound: 400,
            max_concurrent_dials: 100,
            refill_interval_ms: 500,
            max_inbound: None,
            anchor_window: 10_000,
            blocks_per_assignment: 32,
            receipts_per_request: 16,
            request_timeout_ms: 4000,
            log_flush_ms: 1000,
            log_flush_lines: 200,
            verbosity: Verbosity::Minimal,
            quiet: false,
            peer_failure_threshold: 5,
            peer_ban_secs: 120,
            max_attempts: 3,
            stats_interval_secs: 10,
        }
    }
}

impl CliConfig {
    fn parse() -> Self {
        let mut config = Self::default();
        let mut args = std::env::args().skip(1);
        while let Some(arg) = args.next() {
            if let Some(level) = Verbosity::from_flag(&arg) {
                if level > config.verbosity {
                    config.verbosity = level;
                }
                continue;
            }
            match arg.as_str() {
                "--run-secs" => config.run_secs = parse_u64_arg(&mut args),
                "--max-outbound" => config.max_outbound = parse_usize_arg(&mut args, config.max_outbound),
                "--max-concurrent-dials" => {
                    config.max_concurrent_dials = parse_usize_arg(&mut args, config.max_concurrent_dials)
                }
                "--refill-interval-ms" => {
                    config.refill_interval_ms = parse_u64_arg(&mut args).unwrap_or(config.refill_interval_ms)
                }
                "--max-inbound" => config.max_inbound = parse_optional_usize_arg(&mut args),
                "--anchor-window" => {
                    config.anchor_window = parse_u64_arg(&mut args).unwrap_or(config.anchor_window)
                }
                "--blocks-per-assignment" => {
                    config.blocks_per_assignment = parse_usize_arg(&mut args, config.blocks_per_assignment)
                }
                "--receipts-per-request" => {
                    config.receipts_per_request = parse_usize_arg(&mut args, config.receipts_per_request)
                }
                "--request-timeout-ms" => {
                    config.request_timeout_ms = parse_u64_arg(&mut args).unwrap_or(config.request_timeout_ms)
                }
                "--log-flush-ms" => {
                    config.log_flush_ms = parse_u64_arg(&mut args).unwrap_or(config.log_flush_ms)
                }
                "--log-flush-lines" => {
                    config.log_flush_lines = parse_usize_arg(&mut args, config.log_flush_lines)
                }
                "--peer-failure-threshold" => {
                    config.peer_failure_threshold = parse_u32_arg(&mut args, config.peer_failure_threshold)
                }
                "--peer-ban-secs" => {
                    config.peer_ban_secs = parse_u64_arg(&mut args).unwrap_or(config.peer_ban_secs)
                }
                "--max-attempts" => config.max_attempts = parse_u32_arg(&mut args, config.max_attempts),
                "--stats-interval-secs" => {
                    config.stats_interval_secs = parse_u64_arg(&mut args).unwrap_or(config.stats_interval_secs)
                }
                "--quiet" => config.quiet = true,
                _ => {
                    if let Some(value) = arg.strip_prefix("--run-secs=") {
                        config.run_secs = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--max-outbound=") {
                        config.max_outbound = value.parse().unwrap_or(config.max_outbound);
                    } else if let Some(value) = arg.strip_prefix("--max-concurrent-dials=") {
                        config.max_concurrent_dials = value.parse().unwrap_or(config.max_concurrent_dials);
                    } else if let Some(value) = arg.strip_prefix("--refill-interval-ms=") {
                        config.refill_interval_ms = value.parse().unwrap_or(config.refill_interval_ms);
                    } else if let Some(value) = arg.strip_prefix("--max-inbound=") {
                        config.max_inbound = value.parse().ok();
                    } else if let Some(value) = arg.strip_prefix("--anchor-window=") {
                        config.anchor_window = value.parse().unwrap_or(config.anchor_window);
                    } else if let Some(value) = arg.strip_prefix("--blocks-per-assignment=") {
                        config.blocks_per_assignment = value.parse().unwrap_or(config.blocks_per_assignment);
                    } else if let Some(value) = arg.strip_prefix("--receipts-per-request=") {
                        config.receipts_per_request = value.parse().unwrap_or(config.receipts_per_request);
                    } else if let Some(value) = arg.strip_prefix("--request-timeout-ms=") {
                        config.request_timeout_ms = value.parse().unwrap_or(config.request_timeout_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-ms=") {
                        config.log_flush_ms = value.parse().unwrap_or(config.log_flush_ms);
                    } else if let Some(value) = arg.strip_prefix("--log-flush-lines=") {
                        config.log_flush_lines = value.parse().unwrap_or(config.log_flush_lines);
                    } else if let Some(value) = arg.strip_prefix("--peer-failure-threshold=") {
                        config.peer_failure_threshold =
                            value.parse().unwrap_or(config.peer_failure_threshold);
                    } else if let Some(value) = arg.strip_prefix("--peer-ban-secs=") {
                        config.peer_ban_secs = value.parse().unwrap_or(config.peer_ban_secs);
                    } else if let Some(value) = arg.strip_prefix("--max-attempts=") {
                        config.max_attempts = value.parse().unwrap_or(config.max_attempts);
                    } else if let Some(value) = arg.strip_prefix("--stats-interval-secs=") {
                        config.stats_interval_secs = value.parse().unwrap_or(config.stats_interval_secs);
                    }
                }
            }
        }
        config
    }
}

fn parse_u64_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<u64> {
    args.next().and_then(|value| value.parse().ok())
}

fn parse_usize_arg<I: Iterator<Item = String>>(args: &mut I, fallback: usize) -> usize {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_u32_arg<I: Iterator<Item = String>>(args: &mut I, fallback: u32) -> u32 {
    args.next().and_then(|value| value.parse().ok()).unwrap_or(fallback)
}

fn parse_optional_usize_arg<I: Iterator<Item = String>>(args: &mut I) -> Option<usize> {
    args.next().and_then(|value| value.parse().ok())
}

async fn request_head_number(
    peer_id: PeerId,
    eth_version: EthVersion,
    start_block: BlockHashOrNumber,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    context: &RunContext,
) -> Result<u64, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block": format!("{start_block:?}"),
            "kind": "head_header",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block,
        limit: 1,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "kind": "head_header",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            let head_number = headers
                .0
                .first()
                .map(|header| header.number)
                .ok_or_else(|| "empty head header response".to_string())?;
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                    "head_number": head_number,
                }))
                .await;
            Ok(head_number)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "kind": "head_header",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

fn build_anchor_targets(anchor_window: u64) -> Vec<u64> {
    let mut targets = Vec::new();
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            targets.push(anchor + offset);
        }
    }

    targets.sort_unstable();
    targets.dedup();
    targets
}

async fn wait_for_shutdown_signal() {
    #[cfg(unix)]
    {
        let mut sigint =
            signal(SignalKind::interrupt()).expect("failed to register SIGINT handler");
        let mut sigterm =
            signal(SignalKind::terminate()).expect("failed to register SIGTERM handler");
        tokio::select! {
            _ = sigint.recv() => {},
            _ = sigterm.recv() => {},
        }
    }
    #[cfg(not(unix))]
    {
        let _ = tokio::signal::ctrl_c().await;
    }
}

fn write_missing_blocks(
    out_dir: &PathBuf,
    anchor_window: u64,
    known_blocks: &HashSet<u64>,
) -> std::io::Result<usize> {
    let path = out_dir.join("missing_blocks.jsonl");
    let file = File::create(&path)?;
    let mut writer = BufWriter::new(file);
    let mut missing = 0usize;
    for anchor in ANCHORS {
        for offset in 0..anchor_window {
            let block = anchor + offset;
            if !known_blocks.contains(&block) {
                writeln!(writer, "{}", json!({ "block": block }))?;
                missing += 1;
            }
        }
    }
    writer.flush()?;
    if missing == 0 {
        drop(writer);
        let _ = std::fs::remove_file(&path);
    }
    Ok(missing)
}

async fn probe_block_batch(
    peer_id: PeerId,
    peer_key: String,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    eth_version: EthVersion,
    blocks: Vec<u64>,
    mode: ProbeMode,
    context: Arc<RunContext>,
) -> bool {
    if blocks.is_empty() {
        return true;
    }
    let is_escalation = mode == ProbeMode::Escalation;
    let header_start = Instant::now();
    let header_result = request_headers_batch(
        peer_id,
        eth_version,
        &messages,
        blocks[0],
        blocks.len(),
        context.as_ref(),
    )
    .await;
    let header_ms = header_start.elapsed().as_millis();

    let headers = match header_result {
        Ok(headers) => {
            context.record_peer_success(&peer_key).await;
            headers
        }
        Err(err) => {
            let peer_banned = context
                .handle_peer_failure(peer_id, &peer_key, "header")
                .await;
            warn!(
                peer_id = ?peer_id,
                block_start = blocks[0],
                block_count = blocks.len(),
                header_ms,
                error = %err,
                "probe header batch failed"
            );
            for block in &blocks {
                let (attempt, retry) = if is_escalation {
                    let failure_reason = if err == "timeout" {
                        "escalation_header_timeout"
                    } else {
                        "escalation_header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    context.requeue_escalation_block(*block).await;
                    (None, false)
                } else {
                    let attempt = context.record_attempt(*block).await;
                    let retry = attempt <= context.config.max_attempts;
                    if retry {
                        context.requeue_blocks(&[*block]).await;
                        context.record_retry().await;
                    } else {
                        context.mark_failed_block(*block).await;
                    }
                    let failure_reason = if err == "timeout" {
                        "header_timeout"
                    } else {
                        "header_batch"
                    };
                    context.record_failure_reason(failure_reason).await;
                    (Some(attempt), retry)
                };
                context
                    .stats
                    .lock()
                    .await
                    .record_probe(&peer_key, *block, false, false, header_ms, None);
                context
                    .update_window(&peer_key, header_ms, None, false)
                    .await;
                let payload = json!({
                    "event": "probe",
                    "peer_id": format!("{:?}", peer_id),
                    "block": block,
                    "eth_version": format!("{eth_version}"),
                    "header_ok": false,
                    "receipts_ok": false,
                    "header_ms": header_ms,
                    "attempt": attempt,
                    "will_retry": retry,
                    "mode": if is_escalation { "escalation" } else { "normal" },
                    "error": err,
                });
                if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                    println!("{}", payload);
                }
                context.log_probe(payload).await;
            }
            return !peer_banned;
        }
    };

    let mut headers_by_number = HashMap::new();
    for header in headers {
        headers_by_number.insert(header.number, header);
    }

    let mut hashes = Vec::new();
    let mut missing = Vec::new();
    for block in &blocks {
        match headers_by_number.remove(block) {
            Some(header) => {
                if header.number != *block {
                    warn!(
                        peer_id = ?peer_id,
                        requested = block,
                        received = header.number,
                        "probe header mismatch"
                    );
                }
                let hash = SealedHeader::seal_slow(header).hash();
                hashes.push((*block, hash));
            }
            None => missing.push(*block),
        }
    }

    if !missing.is_empty() {
        warn!(
            peer_id = ?peer_id,
            missing = missing.len(),
            "probe header missing in batch"
        );
        for block in &missing {
            let (attempt, retry) = if is_escalation {
                context.record_failure_reason("escalation_missing_header").await;
                context.requeue_escalation_block(*block).await;
                (None, false)
            } else {
                let attempt = context.record_attempt(*block).await;
                let retry = attempt <= context.config.max_attempts;
                if retry {
                    context.requeue_blocks(&[*block]).await;
                    context.record_retry().await;
                } else {
                    context.mark_failed_block(*block).await;
                }
                context.record_failure_reason("missing_header").await;
                (Some(attempt), retry)
            };
            context
                .stats
                .lock()
                .await
                .record_probe(&peer_key, *block, false, false, header_ms, None);
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            context
                .update_window(&peer_key, header_ms, None, false)
                .await;
            let payload = json!({
                "event": "probe",
                "peer_id": format!("{:?}", peer_id),
                "block": block,
                "eth_version": format!("{eth_version}"),
                "header_ok": false,
                "receipts_ok": false,
                "header_ms": header_ms,
                "attempt": attempt,
                "will_retry": retry,
                "mode": if is_escalation { "escalation" } else { "normal" },
                "error": "missing_header",
            });
            if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                println!("{}", payload);
            }
            context.log_probe(payload).await;
        }
    }

    for chunk in hashes.chunks(context.config.receipts_per_request.max(1)) {
        let chunk_blocks: Vec<u64> = chunk.iter().map(|(block, _)| *block).collect();
        let chunk_hashes: Vec<B256> = chunk.iter().map(|(_, hash)| *hash).collect();
        let receipts_start = Instant::now();
        let receipts_result = match eth_version {
            EthVersion::Eth69 => {
                request_receipts69(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            EthVersion::Eth70 => {
                request_receipts70(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
            _ => {
                request_receipts_legacy(
                    peer_id,
                    eth_version,
                    &messages,
                    &chunk_blocks,
                    &chunk_hashes,
                    context.as_ref(),
                )
                .await
            }
        };
        let receipts_ms = receipts_start.elapsed().as_millis();

        match receipts_result {
            Ok(counts) => {
                context.record_peer_success(&peer_key).await;
                for (idx, block) in chunk_blocks.iter().enumerate() {
                    if let Some(receipt_count) = counts.get(idx).copied() {
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, true, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), true)
                            .await;
                        context.mark_known_block(*block).await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": true,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "receipts": receipt_count,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    } else {
                        let (attempt, retry) = if is_escalation {
                            context
                                .record_failure_reason("escalation_receipt_count_missing")
                                .await;
                            context.requeue_escalation_block(*block).await;
                            (None, false)
                        } else {
                            let attempt = context.record_attempt(*block).await;
                            let retry = attempt <= context.config.max_attempts;
                            if retry {
                                context.requeue_blocks(&[*block]).await;
                                context.record_retry().await;
                            } else {
                                context.mark_failed_block(*block).await;
                            }
                            context.record_failure_reason("receipt_count_missing").await;
                            (Some(attempt), retry)
                        };
                        context
                            .stats
                            .lock()
                            .await
                            .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                        context
                            .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                            .await;
                        let payload = json!({
                            "event": "probe",
                            "peer_id": format!("{:?}", peer_id),
                            "block": block,
                            "eth_version": format!("{eth_version}"),
                            "header_ok": true,
                            "receipts_ok": false,
                            "header_ms": header_ms,
                            "receipts_ms": receipts_ms,
                            "attempt": attempt,
                            "will_retry": retry,
                            "mode": if is_escalation { "escalation" } else { "normal" },
                            "error": "receipt_count_missing",
                        });
                        if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                            println!("{}", payload);
                        }
                        context.log_probe(payload).await;
                    }
                }
            }
            Err(err) => {
                let peer_banned = context
                    .handle_peer_failure(peer_id, &peer_key, "receipts")
                    .await;
                warn!(
                    peer_id = ?peer_id,
                    blocks = chunk_blocks.len(),
                    header_ms,
                    receipts_ms,
                    error = %err,
                    "receipts batch failed"
                );
                for block in &chunk_blocks {
                    let (attempt, retry) = if is_escalation {
                        let failure_reason = if err == "timeout" {
                            "escalation_receipts_timeout"
                        } else {
                            "escalation_receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        context.requeue_escalation_block(*block).await;
                        (None, false)
                    } else {
                        let attempt = context.record_attempt(*block).await;
                        let retry = attempt <= context.config.max_attempts;
                        if retry {
                            context.requeue_blocks(&[*block]).await;
                            context.record_retry().await;
                        } else {
                            context.mark_failed_block(*block).await;
                        }
                        let failure_reason = if err == "timeout" {
                            "receipts_timeout"
                        } else {
                            "receipts_batch"
                        };
                        context.record_failure_reason(failure_reason).await;
                        (Some(attempt), retry)
                    };
                    context
                        .stats
                        .lock()
                        .await
                        .record_probe(&peer_key, *block, true, false, header_ms, Some(receipts_ms));
                    context
                        .update_window(&peer_key, header_ms, Some(receipts_ms), false)
                        .await;
                    let payload = json!({
                        "event": "probe",
                        "peer_id": format!("{:?}", peer_id),
                        "block": block,
                        "eth_version": format!("{eth_version}"),
                        "header_ok": true,
                        "receipts_ok": false,
                        "header_ms": header_ms,
                        "receipts_ms": receipts_ms,
                        "attempt": attempt,
                        "will_retry": retry,
                        "mode": if is_escalation { "escalation" } else { "normal" },
                        "error": err,
                    });
                    if !context.config.quiet && context.config.verbosity >= Verbosity::V3 {
                        println!("{}", payload);
                    }
                    context.log_probe(payload).await;
                }
                if peer_banned {
                    return false;
                }
            }
        }
    }
    true
}

async fn request_headers_batch(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    start_block: u64,
    limit: usize,
    context: &RunContext,
) -> Result<Vec<Header>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": start_block,
            "block_count": limit,
            "kind": "headers",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetBlockHeaders {
        start_block: BlockHashOrNumber::Number(start_block),
        limit: limit as u64,
        skip: 0,
        direction: HeadersDirection::Rising,
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetBlockHeaders { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": start_block,
                        "block_count": limit,
                        "kind": "headers",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(headers) => {
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": headers.0.len(),
                }))
                .await;
            Ok(headers.0)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": start_block,
                    "block_count": limit,
                    "kind": "headers",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

struct LogWriter {
    writer: BufWriter<File>,
    lines_since_flush: usize,
}

type SharedLogWriter = Arc<Mutex<LogWriter>>;

struct Logger {
    requests: SharedLogWriter,
    probes: SharedLogWriter,
    known: SharedLogWriter,
    stats: SharedLogWriter,
    flush_every_lines: usize,
}

impl Logger {
    fn new(out_dir: &PathBuf, flush_every_lines: usize) -> std::io::Result<Self> {
        let requests = open_log(out_dir.join("requests.jsonl"))?;
        let probes = open_log(out_dir.join("probes.jsonl"))?;
        let known = open_log(out_dir.join("known_blocks.jsonl"))?;
        let stats = open_log(out_dir.join("stats.jsonl"))?;
        Ok(Self {
            requests,
            probes,
            known,
            stats,
            flush_every_lines: flush_every_lines.max(1),
        })
    }

    fn spawn_flush_tasks(&self, interval: Duration) {
        spawn_flush_task(self.requests.clone(), interval);
        spawn_flush_task(self.probes.clone(), interval);
        spawn_flush_task(self.known.clone(), interval);
        spawn_flush_task(self.stats.clone(), interval);
    }

    async fn flush_all(&self) {
        flush_writer(&self.requests).await;
        flush_writer(&self.probes).await;
        flush_writer(&self.known).await;
        flush_writer(&self.stats).await;
    }

    async fn log_request(&self, value: serde_json::Value) {
        write_json_line(&self.requests, self.flush_every_lines, value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        write_json_line(&self.probes, self.flush_every_lines, value).await;
    }

    async fn log_known(&self, block: u64) {
        write_json_line(
            &self.known,
            self.flush_every_lines,
            json!({
                "block": block,
                "at_ms": now_ms(),
            }),
        )
        .await;
    }

    async fn log_stats(&self, value: serde_json::Value) {
        write_json_line(&self.stats, self.flush_every_lines, value).await;
    }
}

struct RunContext {
    stats: Mutex<Stats>,
    queue: Mutex<BinaryHeap<Reverse<u64>>>,
    queued_blocks: Mutex<HashSet<u64>>,
    known_blocks: Mutex<HashSet<u64>>,
    failed_blocks: Mutex<HashSet<u64>>,
    escalation_queue: Mutex<VecDeque<u64>>,
    escalation_queued: Mutex<HashSet<u64>>,
    escalation_attempts: Mutex<HashMap<u64, HashSet<String>>>,
    escalation_active: AtomicBool,
    attempts: Mutex<HashMap<u64, u32>>,
    peer_health: Mutex<HashMap<String, PeerHealth>>,
    active_peers: Mutex<HashSet<String>>,
    peers_with_jobs: Mutex<HashSet<String>>,
    window: Mutex<WindowStats>,
    config: CliConfig,
    peers_handle: PeersHandle,
    logger: Logger,
    request_id: AtomicU64,
    total_targets: usize,
    discovered: AtomicU64,
    sessions: AtomicU64,
    jobs_assigned: AtomicU64,
    in_flight: AtomicU64,
    run_start: Instant,
    shutdown_tx: tokio::sync::watch::Sender<bool>,
}

impl RunContext {
    fn next_request_id(&self) -> u64 {
        self.request_id.fetch_add(1, Ordering::SeqCst)
    }

    async fn pop_next_batch_for_head(&self, head_number: u64) -> Vec<u64> {
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        let mut batch = Vec::new();
        let mut last = None;
        while let Some(Reverse(next)) = queue.peek().copied() {
            if next > head_number {
                break;
            }
            if batch.len() >= self.config.blocks_per_assignment {
                break;
            }
            if let Some(prev) = last {
                if next != prev + 1 {
                    break;
                }
            }
            queue.pop();
            queued.remove(&next);
            batch.push(next);
            last = Some(next);
        }
        batch
    }

    async fn next_batch_for_peer(&self, peer_key: &str, head_number: u64) -> WorkBatch {
        let batch = self.pop_next_batch_for_head(head_number).await;
        if !batch.is_empty() {
            return WorkBatch {
                blocks: batch,
                mode: ProbeMode::Normal,
            };
        }
        self.maybe_start_escalation().await;
        let escalation = self.pop_escalation_for_peer(peer_key).await;
        WorkBatch {
            blocks: escalation,
            mode: ProbeMode::Escalation,
        }
    }

    async fn maybe_start_escalation(&self) {
        if self.escalation_active.load(Ordering::SeqCst) {
            return;
        }
        if self.queue_len().await > 0 {
            return;
        }
        let failed_blocks: Vec<u64> = {
            let failed = self.failed_blocks.lock().await;
            failed.iter().copied().collect()
        };
        if failed_blocks.is_empty() {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        for block in failed_blocks {
            if queued.insert(block) {
                queue.push_back(block);
            }
        }
        self.escalation_active.store(true, Ordering::SeqCst);
    }

    async fn pop_escalation_for_peer(&self, peer_key: &str) -> Vec<u64> {
        let active_peers = self.active_peers_count().await;
        let mut exhausted_count = 0usize;
        let result = {
            let mut queue = self.escalation_queue.lock().await;
            let mut queued = self.escalation_queued.lock().await;
            let known = self.known_blocks.lock().await;
            let mut attempts = self.escalation_attempts.lock().await;
            let mut iterations = queue.len();
            while iterations > 0 {
                iterations -= 1;
                if let Some(block) = queue.pop_front() {
                    queued.remove(&block);
                    if known.contains(&block) {
                        continue;
                    }
                    let entry = attempts.entry(block).or_default();
                    if entry.contains(peer_key) {
                        queue.push_back(block);
                        queued.insert(block);
                        continue;
                    }
                    if active_peers > 0 && entry.len() >= active_peers {
                        exhausted_count += 1;
                        continue;
                    }
                    entry.insert(peer_key.to_string());
                    return vec![block];
                }
            }
            Vec::new()
        };
        if exhausted_count > 0 {
            for _ in 0..exhausted_count {
                self.record_failure_reason("escalation_exhausted").await;
            }
        }
        result
    }

    async fn requeue_escalation_block(&self, block: u64) {
        let active_peers = self.active_peers_count().await;
        let tried = {
            let attempts = self.escalation_attempts.lock().await;
            attempts.get(&block).map(|s| s.len()).unwrap_or(0)
        };
        if active_peers == 0 {
            self.record_failure_reason("escalation_no_peers").await;
            return;
        }
        if tried >= active_peers {
            self.record_failure_reason("escalation_exhausted").await;
            return;
        }
        let known = self.known_blocks.lock().await;
        if known.contains(&block) {
            return;
        }
        let mut queue = self.escalation_queue.lock().await;
        let mut queued = self.escalation_queued.lock().await;
        if queued.insert(block) {
            queue.push_back(block);
        }
    }

    async fn escalation_len(&self) -> usize {
        self.escalation_queue.lock().await.len()
    }

    async fn queue_len(&self) -> usize {
        self.queue.lock().await.len()
    }

    async fn log_request(&self, value: serde_json::Value) {
        if !self.config.quiet && self.config.verbosity >= Verbosity::V3 {
            println!("{value}");
        }
        self.logger.log_request(value).await;
    }

    async fn log_probe(&self, value: serde_json::Value) {
        self.logger.log_probe(value).await;
    }

    async fn mark_known_block(&self, block: u64) {
        let inserted = {
            let mut known = self.known_blocks.lock().await;
            known.insert(block)
        };
        if inserted {
            self.logger.log_known(block).await;
        }
        self.clear_failed_block(block).await;
        let mut attempts = self.attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_active_peer(&self, peer_key: String) {
        let mut active = self.active_peers.lock().await;
        if active.insert(peer_key) {
            self.sessions.fetch_add(1, Ordering::SeqCst);
        }
    }

    async fn record_peer_disconnect(&self, peer_key: &str) {
        let mut active = self.active_peers.lock().await;
        if active.remove(peer_key) {
            let current = self.sessions.load(Ordering::SeqCst);
            if current > 0 {
                self.sessions.fetch_sub(1, Ordering::SeqCst);
            }
        }
    }

    async fn active_peers_count(&self) -> usize {
        self.active_peers.lock().await.len()
    }

    async fn escalation_attempted_blocks(&self) -> usize {
        self.escalation_attempts.lock().await.len()
    }

    async fn record_peer_job(&self, peer_key: &str) {
        self.jobs_assigned.fetch_add(1, Ordering::SeqCst);
        let mut peers = self.peers_with_jobs.lock().await;
        peers.insert(peer_key.to_string());
    }

    fn inc_in_flight(&self) {
        self.in_flight.fetch_add(1, Ordering::SeqCst);
    }

    fn dec_in_flight(&self) {
        self.in_flight.fetch_sub(1, Ordering::SeqCst);
    }

    async fn completed_blocks(&self) -> usize {
        self.known_blocks.lock().await.len()
    }

    async fn update_window(
        &self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        let mut window = self.window.lock().await;
        window.record(peer_key, header_ms, receipts_ms, receipts_ok);
    }

    async fn peer_ban_remaining(&self, peer_key: &str) -> Option<Duration> {
        let mut health = self.peer_health.lock().await;
        let entry = health.get_mut(peer_key)?;
        if let Some(until) = entry.ban_until {
            let remaining = until.saturating_duration_since(Instant::now());
            if remaining.is_zero() {
                entry.ban_until = None;
                None
            } else {
                Some(remaining)
            }
        } else {
            None
        }
    }

    async fn record_peer_failure(&self, peer_key: &str) -> Option<PeerBan> {
        let mut health = self.peer_health.lock().await;
        let entry = health.entry(peer_key.to_string()).or_default();
        entry.consecutive_failures += 1;
        if entry.consecutive_failures >= self.config.peer_failure_threshold {
            let failures = entry.consecutive_failures;
            entry.consecutive_failures = 0;
            entry.ban_until = Some(Instant::now() + Duration::from_secs(self.config.peer_ban_secs));
            return Some(PeerBan {
                ban_secs: self.config.peer_ban_secs,
                failures,
            });
        }
        None
    }

    async fn record_peer_success(&self, peer_key: &str) {
        let mut health = self.peer_health.lock().await;
        if let Some(entry) = health.get_mut(peer_key) {
            entry.consecutive_failures = 0;
            entry.ban_until = None;
        }
    }

    async fn record_failure_reason(&self, reason: &str) {
        self.stats
            .lock()
            .await
            .record_failure_reason(reason);
    }

    async fn record_retry(&self) {
        self.stats.lock().await.record_retry();
    }

    async fn failed_blocks_total(&self) -> u64 {
        self.failed_blocks.lock().await.len() as u64
    }

    async fn mark_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.insert(block) {
            self.stats.lock().await.record_failed_block();
        }
    }

    async fn clear_failed_block(&self, block: u64) {
        let mut failed = self.failed_blocks.lock().await;
        if failed.remove(&block) {
            self.stats.lock().await.record_failed_block_recovered();
        }
        let mut attempts = self.escalation_attempts.lock().await;
        attempts.remove(&block);
    }

    async fn record_window(&self, blocks_per_sec: f64) {
        self.stats.lock().await.record_window(blocks_per_sec);
    }

    async fn record_drain(&self, secs: f64) {
        self.stats.lock().await.record_drain(secs);
    }

    async fn handle_peer_failure(&self, peer_id: PeerId, peer_key: &str, reason: &str) -> bool {
        if let Some(ban) = self.record_peer_failure(peer_key).await {
            self.log_request(json!({
                "event": "peer_ban",
                "peer_id": peer_key,
                "reason": reason,
                "ban_secs": ban.ban_secs,
                "failures": ban.failures,
                "at_ms": now_ms(),
            }))
            .await;
            self.peers_handle.remove_peer(peer_id);
            warn!(
                peer_id = ?peer_id,
                ban_secs = ban.ban_secs,
                failures = ban.failures,
                reason = reason,
                "peer temporarily banned"
            );
            return true;
        }
        false
    }

    async fn record_attempt(&self, block: u64) -> u32 {
        let mut attempts = self.attempts.lock().await;
        let entry = attempts.entry(block).or_insert(0);
        *entry += 1;
        *entry
    }

    async fn requeue_blocks(&self, blocks: &[u64]) {
        let known = self.known_blocks.lock().await;
        let mut queued = self.queued_blocks.lock().await;
        let mut queue = self.queue.lock().await;
        for block in blocks {
            if !known.contains(block) && queued.insert(*block) {
                queue.push(Reverse(*block));
            }
        }
    }
}

fn now_ms() -> u64 {
    SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .map(|d| d.as_millis() as u64)
        .unwrap_or_default()
}

fn open_log(path: PathBuf) -> std::io::Result<SharedLogWriter> {
    let file = OpenOptions::new().create(true).append(true).open(path)?;
    Ok(Arc::new(Mutex::new(LogWriter {
        writer: BufWriter::new(file),
        lines_since_flush: 0,
    })))
}

async fn write_json_line(
    writer: &SharedLogWriter,
    flush_every_lines: usize,
    value: serde_json::Value,
) {
    let mut guard = writer.lock().await;
    if let Err(err) = writeln!(guard.writer, "{value}") {
        warn!(error = ?err, "failed to write log line");
    }
    guard.lines_since_flush += 1;
    if guard.lines_since_flush >= flush_every_lines {
        if let Err(err) = guard.writer.flush() {
            warn!(error = ?err, "failed to flush log");
        }
        guard.lines_since_flush = 0;
    }
}

async fn flush_writer(writer: &SharedLogWriter) {
    let mut guard = writer.lock().await;
    if let Err(err) = guard.writer.flush() {
        warn!(error = ?err, "failed to flush log");
    }
    guard.lines_since_flush = 0;
}

fn spawn_flush_task(writer: SharedLogWriter, interval: Duration) {
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;
            flush_writer(&writer).await;
        }
    });
}

fn load_known_blocks(path: &PathBuf) -> HashSet<u64> {
    let file = match File::open(path) {
        Ok(file) => file,
        Err(_) => return HashSet::new(),
    };
    let reader = BufReader::new(file);
    let mut blocks = HashSet::new();
    for line in reader.lines().flatten() {
        if let Ok(value) = serde_json::from_str::<serde_json::Value>(&line) {
            if let Some(block) = value.get("block").and_then(|v| v.as_u64()) {
                blocks.insert(block);
            }
        }
    }
    blocks
}

#[derive(Clone)]
struct PeerHandle {
    peer_id: PeerId,
    peer_key: String,
    eth_version: EthVersion,
    head_number: u64,
    messages: PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
}

const ANCHORS: [u64; 5] = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329];
const WARMUP_SECS: u64 = 3;

#[derive(Default)]
struct PeerHealth {
    consecutive_failures: u32,
    ban_until: Option<Instant>,
}

struct PeerBan {
    ban_secs: u64,
    failures: u32,
}

#[derive(Default)]
struct WindowStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

impl WindowStats {
    fn record(
        &mut self,
        peer_key: &str,
        header_ms: u128,
        receipts_ms: Option<u128>,
        receipts_ok: bool,
    ) {
        self.blocks_total += 1;
        if receipts_ok {
            self.blocks_ok += 1;
        }
        self.header_ms.push(header_ms);
        if let Some(ms) = receipts_ms {
            self.receipts_ms.push(ms);
        }
        self.peers.insert(peer_key.to_string());
    }

    fn take_snapshot(&mut self) -> WindowSnapshot {
        let snapshot = WindowSnapshot {
            blocks_total: self.blocks_total,
            blocks_ok: self.blocks_ok,
            header_ms: std::mem::take(&mut self.header_ms),
            receipts_ms: std::mem::take(&mut self.receipts_ms),
            peers: std::mem::take(&mut self.peers),
        };
        self.blocks_total = 0;
        self.blocks_ok = 0;
        snapshot
    }
}

struct WindowSnapshot {
    blocks_total: u64,
    blocks_ok: u64,
    header_ms: Vec<u128>,
    receipts_ms: Vec<u128>,
    peers: HashSet<String>,
}

#[derive(Default)]
struct BlockStats {
    total: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct PeerStats {
    blocks_total: u64,
    blocks_ok: u64,
    header_ok: u64,
    receipts_ok: u64,
    header_ms_sum: u128,
    receipts_ms_sum: u128,
}

#[derive(Default)]
struct Stats {
    by_block: HashMap<u64, BlockStats>,
    clients: HashMap<String, u64>,
    peer_stats: HashMap<String, PeerStats>,
    failure_reasons: HashMap<String, u64>,
    total_probes: u64,
    receipts_ok_total: u64,
    retries_total: u64,
    failed_blocks_total: u64,
    header_ms_all: Vec<u128>,
    receipts_ms_all: Vec<u128>,
    window_blocks_per_sec_sum: f64,
    window_blocks_per_sec_max: f64,
    window_count: u64,
    drain_time_secs: Option<f64>,
}

impl Stats {
    fn record_peer(&mut self, client_version: &str) {
        *self.clients.entry(client_version.to_string()).or_insert(0) += 1;
    }

    fn record_probe(
        &mut self,
        peer_key: &str,
        block: u64,
        header_ok: bool,
        receipts_ok: bool,
        header_ms: u128,
        receipts_ms: Option<u128>,
    ) {
        let stats = self.by_block.entry(block).or_default();
        stats.total += 1;
        self.total_probes += 1;
        let peer = self.peer_stats.entry(peer_key.to_string()).or_default();
        peer.blocks_total += 1;
        if header_ok {
            stats.header_ok += 1;
            stats.header_ms_sum += header_ms;
            peer.header_ok += 1;
            peer.header_ms_sum += header_ms;
            self.header_ms_all.push(header_ms);
        }
        if receipts_ok {
            stats.receipts_ok += 1;
            self.receipts_ok_total += 1;
            peer.blocks_ok += 1;
            peer.receipts_ok += 1;
            if let Some(ms) = receipts_ms {
                stats.receipts_ms_sum += ms;
                peer.receipts_ms_sum += ms;
                self.receipts_ms_all.push(ms);
            }
        }
    }

    fn record_retry(&mut self) {
        self.retries_total += 1;
    }

    fn record_failed_block(&mut self) {
        self.failed_blocks_total += 1;
    }

    fn record_failed_block_recovered(&mut self) {
        if self.failed_blocks_total > 0 {
            self.failed_blocks_total -= 1;
        }
    }

    fn record_failure_reason(&mut self, reason: &str) {
        *self.failure_reasons.entry(reason.to_string()).or_insert(0) += 1;
    }

    fn record_window(&mut self, blocks_per_sec: f64) {
        self.window_blocks_per_sec_sum += blocks_per_sec;
        if blocks_per_sec > self.window_blocks_per_sec_max {
            self.window_blocks_per_sec_max = blocks_per_sec;
        }
        self.window_count += 1;
    }

    fn record_drain(&mut self, secs: f64) {
        if self.drain_time_secs.is_none() {
            self.drain_time_secs = Some(secs);
        }
    }
}

async fn summarize_stats(
    stats: &Mutex<Stats>,
    elapsed: Duration,
    total_targets: usize,
    completed_blocks: usize,
) -> serde_json::Value {
    let stats = stats.lock().await;
    let elapsed_secs = elapsed.as_secs_f64().max(0.001);
    let throughput_blocks = stats.receipts_ok_total as f64 / elapsed_secs;
    let coverage = if total_targets > 0 {
        completed_blocks as f64 / total_targets as f64
    } else {
        0.0
    };
    let avg_window_blocks_per_sec = if stats.window_count > 0 {
        stats.window_blocks_per_sec_sum / stats.window_count as f64
    } else {
        0.0
    };
    let mut blocks: Vec<_> = stats.by_block.iter().map(|(block, stats)| {
        let header_rate = if stats.total > 0 {
            stats.header_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let receipts_rate = if stats.total > 0 {
            stats.receipts_ok as f64 / stats.total as f64
        } else {
            0.0
        };
        let avg_header_ms = if stats.header_ok > 0 {
            stats.header_ms_sum as f64 / stats.header_ok as f64
        } else {
            0.0
        };
        let avg_receipts_ms = if stats.receipts_ok > 0 {
            stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
        } else {
            0.0
        };
        json!({
            "block": block,
            "total": stats.total,
            "header_ok": stats.header_ok,
            "receipts_ok": stats.receipts_ok,
            "header_rate": header_rate,
            "receipts_rate": receipts_rate,
            "avg_header_ms": avg_header_ms,
            "avg_receipts_ms": avg_receipts_ms,
        })
    }).collect();

    blocks.sort_by_key(|entry| entry["block"].as_u64().unwrap_or_default());

    let header_p50 = percentile_from(&stats.header_ms_all, 0.50);
    let header_p95 = percentile_from(&stats.header_ms_all, 0.95);
    let header_p99 = percentile_from(&stats.header_ms_all, 0.99);
    let receipts_p50 = percentile_from(&stats.receipts_ms_all, 0.50);
    let receipts_p95 = percentile_from(&stats.receipts_ms_all, 0.95);
    let receipts_p99 = percentile_from(&stats.receipts_ms_all, 0.99);

    let mut peer_rows: Vec<_> = stats
        .peer_stats
        .iter()
        .map(|(peer, stats)| {
            let avg_header_ms = if stats.header_ok > 0 {
                stats.header_ms_sum as f64 / stats.header_ok as f64
            } else {
                0.0
            };
            let avg_receipts_ms = if stats.receipts_ok > 0 {
                stats.receipts_ms_sum as f64 / stats.receipts_ok as f64
            } else {
                0.0
            };
            json!({
                "peer_id": peer,
                "blocks_total": stats.blocks_total,
                "blocks_ok": stats.blocks_ok,
                "avg_header_ms": avg_header_ms,
                "avg_receipts_ms": avg_receipts_ms,
            })
        })
        .collect();
    peer_rows.sort_by(|a, b| {
        b["blocks_ok"]
            .as_u64()
            .unwrap_or(0)
            .cmp(&a["blocks_ok"].as_u64().unwrap_or(0))
    });
    if peer_rows.len() > 10 {
        peer_rows.truncate(10);
    }

    json!({
        "event": "summary",
        "duration_secs": elapsed_secs,
        "total_probes": stats.total_probes,
        "receipts_ok_total": stats.receipts_ok_total,
        "blocks_ok_total": stats.receipts_ok_total,
        "throughput_blocks_per_sec": throughput_blocks,
        "avg_window_blocks_per_sec": avg_window_blocks_per_sec,
        "max_window_blocks_per_sec": stats.window_blocks_per_sec_max,
        "total_targets": total_targets,
        "completed_blocks": completed_blocks,
        "coverage": coverage,
        "failed_blocks_total": stats.failed_blocks_total,
        "retries_total": stats.retries_total,
        "failure_reasons": stats.failure_reasons,
        "header_p50_ms": header_p50,
        "header_p95_ms": header_p95,
        "header_p99_ms": header_p99,
        "receipts_p50_ms": receipts_p50,
        "receipts_p95_ms": receipts_p95,
        "receipts_p99_ms": receipts_p99,
        "peer_leaderboard": peer_rows,
        "drain_time_secs": stats.drain_time_secs,
        "peers": stats.clients,
        "by_block": blocks,
    })
}

fn spawn_window_stats(context: Arc<RunContext>) {
    if context.config.stats_interval_secs == 0 {
        return;
    }
    let interval = Duration::from_secs(context.config.stats_interval_secs);
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(interval);
        let mut drain_streak = 0u8;
        loop {
            ticker.tick().await;
            let queue_len = context.queue_len().await;
            let escalation_len = context.escalation_len().await;
            let in_flight = context.in_flight.load(Ordering::SeqCst);
            let mut window = context.window.lock().await;
            let snapshot = window.take_snapshot();
            drop(window);
            let mut header_ms = snapshot.header_ms;
            let mut receipts_ms = snapshot.receipts_ms;
            let header_p50 = percentile(&mut header_ms, 0.50);
            let header_p95 = percentile(&mut header_ms, 0.95);
            let receipts_p50 = percentile(&mut receipts_ms, 0.50);
            let receipts_p95 = percentile(&mut receipts_ms, 0.95);
            let window_secs = interval.as_secs_f64().max(0.001);
            let probes_per_sec = snapshot.blocks_total as f64 / window_secs;
            let blocks_per_sec = snapshot.blocks_ok as f64 / window_secs;
            context.record_window(blocks_per_sec).await;
            let payload = json!({
                "event": "window_stats",
                "interval_secs": interval.as_secs(),
                "queue_len": queue_len,
                "active_peers_total": context.active_peers.lock().await.len(),
                "active_peers_window": snapshot.peers.len(),
                "probes_per_sec": probes_per_sec,
                "blocks_per_sec": blocks_per_sec,
                "header_p50_ms": header_p50,
                "header_p95_ms": header_p95,
                "receipts_p50_ms": receipts_p50,
                "receipts_p95_ms": receipts_p95,
                "at_ms": now_ms(),
            });
            context.logger.log_stats(payload).await;

            if queue_len == 0
                && escalation_len == 0
                && in_flight == 0
                && context.completed_blocks().await + context.failed_blocks_total().await as usize
                    >= context.total_targets
            {
                drain_streak = drain_streak.saturating_add(1);
                context
                    .record_drain(context.run_start.elapsed().as_secs_f64())
                    .await;
                if drain_streak >= 2 {
                    let _ = context.shutdown_tx.send(true);
                }
            } else {
                drain_streak = 0;
            }
        }
    });
}

fn spawn_progress_bar(
    context: Arc<RunContext>,
    mut shutdown_rx: tokio::sync::watch::Receiver<bool>,
) {
    if context.config.quiet {
        return;
    }
    let total = context.total_targets as u64;
    let pb = ProgressBar::with_draw_target(
        Some(total),
        ProgressDrawTarget::stderr_with_hz(10),
    );
    let style = ProgressStyle::with_template(
        "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | {msg}",
    )
    .unwrap()
    .progress_chars("=>-");
    pb.set_style(style);
    pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | Failed 0 | speed 0.0/s | eta --");
    tokio::spawn(async move {
        let mut ticker = tokio::time::interval(Duration::from_millis(100));
        let mut sent_shutdown = false;
        let mut main_finished = false;
        let mut escalation_total = 0u64;
        let mut escalation_pb: Option<ProgressBar> = None;
        let mut main_window: VecDeque<(Instant, u64)> = VecDeque::new();
        let mut escalation_window: VecDeque<(Instant, u64)> = VecDeque::new();
        loop {
            tokio::select! {
                _ = ticker.tick() => {
                    let completed = context.completed_blocks().await;
                    let failed = context.failed_blocks_total().await as usize;
                    let processed = completed + failed;
                    let active = context.active_peers.lock().await.len();
                    let sessions = context.sessions.load(Ordering::SeqCst);
                    let queue_len = context.queue_len().await;
                    let in_flight = context.in_flight.load(Ordering::SeqCst);
                    let escalation_active = context.escalation_active.load(Ordering::SeqCst);
                    let escalation_len = context.escalation_len().await;
                    let failed_u64 = failed as u64;
                    if failed_u64 > escalation_total {
                        escalation_total = failed_u64;
                        if let Some(ref pb) = escalation_pb {
                            pb.set_length(escalation_total);
                        }
                    }
                    let status = if sessions == 0 {
                        "looking_for_peers"
                    } else if escalation_active && escalation_len > 0 {
                        "escalating"
                    } else if processed >= context.total_targets && queue_len == 0 && in_flight == 0 {
                        "finalizing"
                    } else {
                        "fetching"
                    };
                    if !main_finished {
                        let now = Instant::now();
                        main_window.push_back((now, processed as u64));
                        while let Some((t, _)) = main_window.front() {
                            if now.duration_since(*t) > Duration::from_secs(1) && main_window.len() > 1 {
                                main_window.pop_front();
                            } else {
                                break;
                            }
                        }
                        let speed = if let (Some((t0, v0)), Some((t1, v1))) =
                            (main_window.front(), main_window.back())
                        {
                            let dt = t1.duration_since(*t0).as_secs_f64();
                            if dt > 0.0 && v1 >= v0 {
                                (v1 - v0) as f64 / dt
                            } else {
                                0.0
                            }
                        } else {
                            0.0
                        };
                        let remaining = context.total_targets.saturating_sub(processed) as f64;
                        let eta = if speed > 0.0 {
                            format!("{:.0}s", remaining / speed)
                        } else {
                            "--".to_string()
                        };
                        let msg = format!(
                            "status {} | peers {}/{} | queue {} | inflight {} | Failed {} | speed {:.1}/s | eta {}",
                            status, active, sessions, queue_len, in_flight, failed, speed, eta
                        );
                        pb.set_message(msg);
                        pb.set_position(processed.min(context.total_targets) as u64);
                    }

                    if !main_finished && processed >= context.total_targets && queue_len == 0 {
                        main_finished = true;
                        pb.finish_and_clear();
                    }

                    if main_finished {
                        if escalation_total == 0 && failed_u64 > 0 {
                            escalation_total = failed_u64;
                        }
                        if escalation_total > 0 {
                            if escalation_pb.is_none() {
                                let esc_pb = ProgressBar::with_draw_target(
                                    Some(escalation_total),
                                    ProgressDrawTarget::stderr_with_hz(10),
                                );
                                let esc_style = ProgressStyle::with_template(
                                        "{bar:40.red/black} {percent:>3}% {pos}/{len} | {elapsed_precise} | {msg}",
                                )
                                .unwrap()
                                .progress_chars("=>-");
                                esc_pb.set_style(esc_style);
                                esc_pb.set_message("status escalating_failed | failed 0/0 | tried 0/0 | peers 0/0 | inflight 0 | queue 0");
                                escalation_pb = Some(esc_pb);
                            }
                            let remaining = failed_u64;
                            let done = escalation_total.saturating_sub(remaining);
                                let now = Instant::now();
                                escalation_window.push_back((now, done));
                                while let Some((t, _)) = escalation_window.front() {
                                    if now.duration_since(*t) > Duration::from_secs(1)
                                        && escalation_window.len() > 1
                                    {
                                        escalation_window.pop_front();
                                    } else {
                                        break;
                                    }
                                }
                                let esc_speed =
                                    if let (Some((t0, v0)), Some((t1, v1))) = (
                                        escalation_window.front(),
                                        escalation_window.back(),
                                    ) {
                                        let dt = t1.duration_since(*t0).as_secs_f64();
                                        if dt > 0.0 && v1 >= v0 {
                                            (v1 - v0) as f64 / dt
                                        } else {
                                            0.0
                                        }
                                    } else {
                                        0.0
                                    };
                                let esc_eta = if esc_speed > 0.0 {
                                    format!("{:.0}s", remaining as f64 / esc_speed)
                                } else {
                                    "--".to_string()
                                };
                            if let Some(ref pb) = escalation_pb {
                                let attempted = context.escalation_attempted_blocks().await;
                                let msg = format!(
                                        "failed {remaining}/{escalation_total} | tried {attempted}/{escalation_total} | peers {active}/{sessions} | inflight {in_flight} | queue {escalation_len} | speed {:.1}/s | eta {}",
                                        esc_speed,
                                        esc_eta
                                );
                                pb.set_message(msg);
                                pb.set_position(done);
                            }
                        }
                    }
                    if !sent_shutdown
                        && processed >= context.total_targets
                        && queue_len == 0
                        && escalation_len == 0
                        && in_flight == 0
                    {
                        let _ = context.shutdown_tx.send(true);
                        sent_shutdown = true;
                    }
                }
                _ = shutdown_rx.changed() => {
                    break;
                }
            }
        }
        if !main_finished {
            pb.finish_and_clear();
        }
        if let Some(pb) = escalation_pb {
            pb.finish_and_clear();
        }
    });
}

fn percentile(values: &mut Vec<u128>, percentile: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    values.sort_unstable();
    let idx = ((values.len() - 1) as f64 * percentile).round() as usize;
    values[idx]
}

fn percentile_from(values: &[u128], pct: f64) -> u128 {
    if values.is_empty() {
        return 0;
    }
    let mut copy = values.to_vec();
    percentile(&mut copy, pct)
}

async fn request_receipts_legacy(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "legacy",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "legacy",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(receipts) => {
            let counts: Vec<usize> = receipts.0.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "legacy",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts69(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth69",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts(block_hashes.to_vec());
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts69 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth69",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts69(receipts)) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth69",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}

async fn request_receipts70(
    peer_id: PeerId,
    eth_version: EthVersion,
    messages: &PeerRequestSender<PeerRequest<EthNetworkPrimitives>>,
    block_numbers: &[u64],
    block_hashes: &[B256],
    context: &RunContext,
) -> Result<Vec<usize>, String> {
    let request_id = context.next_request_id();
    let sent_at = now_ms();
    context
        .log_request(json!({
            "event": "request_sent",
            "request_id": request_id,
            "peer_id": format!("{:?}", peer_id),
            "block_start": block_numbers.first().copied().unwrap_or_default(),
            "block_count": block_numbers.len(),
            "kind": "receipts",
            "variant": "eth70",
            "eth_version": format!("{eth_version}"),
            "sent_at_ms": sent_at,
        }))
        .await;

    let request = GetReceipts70 {
        first_block_receipt_index: 0,
        block_hashes: block_hashes.to_vec(),
    };
    let (tx, rx) = oneshot::channel();
    messages
        .try_send(PeerRequest::GetReceipts70 { request, response: tx })
        .map_err(|err| format!("send error: {err:?}"))?;

    let response =
        match tokio::time::timeout(Duration::from_millis(context.config.request_timeout_ms), rx)
            .await
        {
            Ok(inner) => inner.map_err(|err| format!("response dropped: {err:?}"))?,
            Err(_) => {
                let received_at = now_ms();
                let duration_ms = received_at.saturating_sub(sent_at);
                context
                    .log_request(json!({
                        "event": "response_err",
                        "request_id": request_id,
                        "peer_id": format!("{:?}", peer_id),
                        "block_start": block_numbers.first().copied().unwrap_or_default(),
                        "block_count": block_numbers.len(),
                        "kind": "receipts",
                        "variant": "eth70",
                        "eth_version": format!("{eth_version}"),
                        "received_at_ms": received_at,
                        "duration_ms": duration_ms,
                        "error": "timeout",
                    }))
                    .await;
                return Err("timeout".to_string());
            }
        };

    let received_at = now_ms();
    let duration_ms = received_at.saturating_sub(sent_at);

    match response {
        Ok(Receipts70 { receipts, last_block_incomplete }) => {
            let counts: Vec<usize> = receipts.iter().map(|r| r.len()).collect();
            context
                .log_request(json!({
                    "event": "response_ok",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "items": counts.len(),
                    "last_block_incomplete": last_block_incomplete,
                }))
                .await;
            Ok(counts)
        }
        Err(err) => {
            context
                .log_request(json!({
                    "event": "response_err",
                    "request_id": request_id,
                    "peer_id": format!("{:?}", peer_id),
                    "block_start": block_numbers.first().copied().unwrap_or_default(),
                    "block_count": block_numbers.len(),
                    "kind": "receipts",
                    "variant": "eth70",
                    "eth_version": format!("{eth_version}"),
                    "received_at_ms": received_at,
                    "duration_ms": duration_ms,
                    "error": format!("{err:?}"),
                }))
                .await;
            Err(format!("{err:?}"))
        }
    }
}


```

Updated the progress bar speed calculation to use a 1second rolling window and avoid the billions/s spike on resume. I also moved speed/ETA into the message so its fully controlled by the windowed calculation.

Key changes in `spawn_progress_bar`:
- Removed `{per_sec}` and `{eta_precise}` from the template.
- Added 1second rolling windows for main progress and escalation.
- Message now includes `speed /s` and `eta ` computed from the window.

```2080:2167:harness/src/main.rs
let style = ProgressStyle::with_template(
    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | {msg}",
)
...
pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | Failed 0 | speed 0.0/s | eta --");
...
let speed = /* 1s rolling window */;
let eta = if speed > 0.0 { format!("{:.0}s", remaining / speed) } else { "--".to_string() };
let msg = format!(
    "status {} | peers {}/{} | queue {} | inflight {} | Failed {} | speed {:.1}/s | eta {}",
    status, active, sessions, queue_len, in_flight, failed, speed, eta
);
```

Escalation bar (red) now has its own rolling speed/ETA based on failed resolved per second.

No tests run. If you want, I can run `cargo run -- --anchor-window 200000` after you allow running without sandbox.

```plaintext
let style = ProgressStyle::with_template(
    "{bar:40.cyan/blue} {percent:>3}% {pos}/{len} | {elapsed_precise} | {msg}",
)
...
pb.set_message("status looking_for_peers | peers 0/0 | queue 0 | inflight 0 | Failed 0 | speed 0.0/s | eta --");
...
let speed = /* 1s rolling window */;
let eta = if speed > 0.0 { format!("{:.0}s", remaining / speed) } else { "--".to_string() };
let msg = format!(
    "status {} | peers {}/{} | queue {} | inflight {} | Failed {} | speed {:.1}/s | eta {}",
    status, active, sessions, queue_len, in_flight, failed, speed, eta
);
```

---

**User**

Allright! Amazing! This works. Now please describe me in very detail how exactly do we fetch things from the nodes - what do we fetch, what do we ask for, etc etc. I want to know the whole flow. Cause I don't think block headers contain receipts hashes? Do we fetch bodies and then fetch receipts one by one? Please describe the whole process to me

---

**Cursor**

Let me trace through the code to give you the complete picture.

## Complete Fetching Flow

### 1. Target Selection
```
ANCHORS = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329]
targets = for each anchor: [anchor, anchor+1, ..., anchor+window-1]
```
With `--anchor-window 200000`, that's **1,000,000 total blocks** (5 anchors  200k each).

### 2. Resume from Disk
- Load `known_blocks.jsonl`  blocks we've already successfully fetched
- Build queue = `targets - known_blocks`

### 3. Peer Discovery & Head Resolution
When a peer connects via devp2p:
1. Check genesis hash matches Mainnet
2. Their **status message** includes their **head block hash**
3. We send `GetBlockHeaders` for that hash (limit=1) to get the **head block number**
4. This tells us: "this peer can serve blocks up to #X"

This is why you see `head header request failed` warnings  they're not for target blocks, they're for peer head resolution.

### 4. Work Assignment
For each ready peer:
1. Pop next contiguous batch from queue (default: 32 blocks)
2. Only assign blocks  peer's head number
3. If queue empty and failed blocks exist  escalation mode

### 5. The Actual Fetch (per batch)

```

  probe_block_batch(peer, blocks=[N, N+1, ..., N+31])            

                                                                 
  STEP 1: Request Headers                                        
                                        
  Send: GetBlockHeaders {                                        
      start_block: Number(N),                                    
      limit: 32,                                                 
      skip: 0,                                                   
      direction: Rising                                          
  }                                                              
  Receive: Vec<Header>                                           
                                                                 
  STEP 2: Compute Block Hashes                                   
                                     
  For each header:                                               
      hash = SealedHeader::seal_slow(header).hash()              
      // This RLP-encodes header and keccak256's it              
                                                                 
  STEP 3: Request Receipts (in chunks of 16)                     
                       
  Send: GetReceipts([hash1, hash2, ..., hash16])                 
  Receive: Vec<Vec<Receipt>>  // receipts per block              
                                                                 
  STEP 4: Mark Success                                           
                                             
  If receipts array has entry for block  mark_known_block()     
                                                                 

```

### 6. What We DON'T Fetch
- **Block bodies** (transactions, uncles)  not needed
- **State**  we're not executing anything
- We don't verify `receipts_root`  we just count what we get

### 7. Protocol Variants
Depending on peer's eth version:
- **Eth68 and below**: `GetReceipts(Vec<B256>)`  `Receipts(Vec<Vec<Receipt>>)`
- **Eth69**: Same request, different response encoding (`Receipts69`)
- **Eth70**: `GetReceipts70 { first_block_receipt_index, block_hashes }`  `Receipts70`

### 8. Why Block Hash is Needed

The `GetReceipts` message requires **block hashes**, not block numbers. The only way to get a block's hash is:
1. Fetch the header
2. RLP-encode it
3. keccak256 the encoding

That's what `SealedHeader::seal_slow()` does.

### 9. What "known" means

A block is marked "known" **only when receipts are successfully received**. If:
- Header fetch fails  block stays in queue (or escalation)
- Header received but receipts fail  block stays pending
- Both succeed  `known_blocks.jsonl` gets the entry

### Progress Bar Meaning

- **pos** = `known_blocks.len() + failed_blocks.len()`
- **Failed X** = blocks that exhausted retries (will go to escalation)
- **speed** = new blocks marked known per second (1s window)

---

**Key insight**: You're right that headers don't "contain" receipts. Headers contain `receipts_root` (a Merkle trie root), but we never verify it. We just:
1. Get header  compute hash
2. Ask for receipts by hash  count them

If a node lies about receipts, we wouldn't detect it without re-executing transactions. This harness is measuring **receipt availability**, not **receipt validity**.

```plaintext
ANCHORS = [10_000_835, 11_362_579, 12_369_621, 16_291_127, 21_688_329]
targets = for each anchor: [anchor, anchor+1, ..., anchor+window-1]
```

```plaintext

  probe_block_batch(peer, blocks=[N, N+1, ..., N+31])            

                                                                 
  STEP 1: Request Headers                                        
                                        
  Send: GetBlockHeaders {                                        
      start_block: Number(N),                                    
      limit: 32,                                                 
      skip: 0,                                                   
      direction: Rising                                          
  }                                                              
  Receive: Vec<Header>                                           
                                                                 
  STEP 2: Compute Block Hashes                                   
                                     
  For each header:                                               
      hash = SealedHeader::seal_slow(header).hash()              
      // This RLP-encodes header and keccak256's it              
                                                                 
  STEP 3: Request Receipts (in chunks of 16)                     
                       
  Send: GetReceipts([hash1, hash2, ..., hash16])                 
  Receive: Vec<Vec<Receipt>>  // receipts per block              
                                                                 
  STEP 4: Mark Success                                           
                                             
  If receipts array has entry for block  mark_known_block()     
                                                                 

```

---

**User**

Please update the @SPEC.md with this. Also note in the table the hash is blockHash, and we ask for receipts via blockHashes, as GetReceipts accepts blocks and returns whole block receipts array for all blocks given.

---

**Cursor**

```markdown
# Receipt Availability Harness Spec (Updated)

## Goal
Measure the availability and latency of Ethereum L1 block headers and receipts
served over devp2p `eth` without executing transactions or maintaining state.
Persist only what is needed for receipts availability analysis and indexing.

## Non-goals
- Full execution / state verification.
- Archive-style state or EVM traces.
- Receipts root validation (explicitly skipped in v1).

## High-level architecture
1. **P2P network layer (Reth as library)**
   - Discovery, dialing, and `eth` handshake.
   - `PeerRequestSender` for headers/receipts.
2. **Scheduler**
   - Shared work queue of target blocks.
   - Per-peer assignment and batching.
   - Soft ban on peers with repeated failures.
3. **Probing pipeline**
   - Header batch  receipt batch per block hash.
   - Eth protocol version-aware receipts requests (`eth/68`, `eth/69`, `eth/70`).
4. **Persistence**
   - JSONL logs for requests, probes, stats, and known blocks.

## Fetch flow

### Peer head resolution
When a peer connects, their `status` message includes their head block hash. We
send `GetBlockHeaders` for that hash (limit=1) to resolve the head block number.
This determines the maximum block number we can request from that peer.

### Block batch probing
For each assigned batch of blocks (default 32 contiguous):

```

  1. Request Headers                                             
     GetBlockHeaders { start: Number(N), limit: 32, skip: 0 }    
      Vec<Header>                                               
                                                                 
  2. Compute Block Hashes                                        
     For each header: blockHash = keccak256(RLP(header))         
     (via SealedHeader::seal_slow)                               
                                                                 
  3. Request Receipts (in chunks of 16)                          
     GetReceipts([blockHash1, blockHash2, ...])                  
      Vec<Vec<Receipt>>  (all receipts for each block)          
                                                                 
  4. Mark Success                                                
     If receipts returned for block  mark_known_block()         

```

### What we fetch
| Request           | Input                | Output                          |
|-------------------|----------------------|---------------------------------|
| GetBlockHeaders   | block number + limit | Vec\<Header\>                   |
| GetReceipts       | Vec\<blockHash\>     | Vec\<Vec\<Receipt\>\> per block |

### What we DON'T fetch
- **Block bodies** (transactions, uncles)  not needed for receipts.
- **State**  no execution.
- We do NOT verify `receipts_root`  we only count receipts received.

### Protocol variants
- **Eth68 and below**: `GetReceipts(Vec<B256>)`  `Receipts(Vec<Vec<Receipt>>)`
- **Eth69**: Same request, different response encoding (`Receipts69`)
- **Eth70**: `GetReceipts70 { first_block_receipt_index, block_hashes }`  `Receipts70`

### Why block hash is required
The `GetReceipts` message requires **block hashes**, not block numbers. The only
way to obtain a block's hash is to fetch its header and compute
`keccak256(RLP(header))`. This is why we must fetch headers first.

### Definition of "known"
A block is marked "known" **only when receipts are successfully received**:
- Header fetch fails  block stays in queue (or escalation)
- Header OK but receipts fail  block stays pending
- Both succeed  entry written to `known_blocks.jsonl`

## Target selection
Targets are constructed from a fixed set of DeFi deployment anchors plus a
contiguous window:
- Uniswap V2: 10000835
- Aave V2: 11362579
- Uniswap V3: 12369621
- Aave V3: 16291127
- Uniswap V4: 21688329

The default window is 10,000 blocks per anchor. Set with `--anchor-window`.

## Data stored
Minimal data to enable downstream analysis:
- **Headers:** only for probing (not persisted as canonical chain state).
- **Receipts:** only counts and timing in probe logs.
- **JSONL output:**
  - `requests.jsonl`: request events + timing.
  - `probes.jsonl`: per-block probe results.
  - `known_blocks.jsonl`: blocks proven available.
  - `stats.jsonl`: window stats + summary.

## Retry and error handling
Two-tier retry strategy:
1. **Normal retries**
   - Failed blocks are requeued up to `--max-attempts`.
2. **Escalation pass**
   - Once the main queue is drained, blocks that failed all retries are tried
     again across distinct peers (one attempt per peer).
   - If every known peer fails, the block is marked unavailable.

All failures are recorded with reason tags (`header_timeout`,
`receipts_batch`, `escalation_*`, etc).

## Peer handling
- Track peer health and ban temporarily after consecutive failures.
- Ban duration and failure threshold are configurable.
- Sessions are counted; peer assignment is based on availability and queue.

## Completion criteria
The harness exits when:
1. All targets are completed or failed, and
2. Both normal and escalation queues are empty, and
3. No in-flight work remains.

`--run-secs` provides a hard time limit override.

## CLI behavior
Console output is a real-time progress bar (10Hz) showing:
- % complete and processed/total
- elapsed time, ETA
- speed (blocks/sec)
- peer status

Verbose levels:
- `-v`: progress + periodic stats
- `-vv`: adds per-peer summaries
- `-vvv`: includes per-request and per-probe JSON logs

## Known limitations
- No receipts root validation.
- Network-only data availability is impacted by EIP-4444 pruning.
- No L2 support yet (Base/Optimism planned).

```

```markdown
# Receipt Availability Harness Spec (Updated)

## Goal
Measure the availability and latency of Ethereum L1 block headers and receipts
served over devp2p `eth` without executing transactions or maintaining state.
Persist only what is needed for receipts availability analysis and indexing.

## Non-goals
- Full execution / state verification.
- Archive-style state or EVM traces.
- Receipts root validation (explicitly skipped in v1).

## High-level architecture
1. **P2P network layer (Reth as library)**
   - Discovery, dialing, and `eth` handshake.
   - `PeerRequestSender` for headers/receipts.
2. **Scheduler**
   - Shared work queue of target blocks.
   - Per-peer assignment and batching.
   - Soft ban on peers with repeated failures.
3. **Probing pipeline**
   - Header batch  receipt batch per block hash.
   - Eth protocol version-aware receipts requests (`eth/68`, `eth/69`, `eth/70`).
4. **Persistence**
   - JSONL logs for requests, probes, stats, and known blocks.

## Fetch flow

### Peer head resolution
When a peer connects, their `status` message includes their head block hash. We
send `GetBlockHeaders` for that hash (limit=1) to resolve the head block number.
This determines the maximum block number we can request from that peer.

### Block batch probing
For each assigned batch of blocks (default 32 contiguous):

```

  1. Request Headers                                             
     GetBlockHeaders { start: Number(N), limit: 32, skip: 0 }    
      Vec<Header>                                               
                                                                 
  2. Compute Block Hashes                                        
     For each header: blockHash = keccak256(RLP(header))         
     (via SealedHeader::seal_slow)                               
                                                                 
  3. Request Receipts (in chunks of 16)                          
     GetReceipts([blockHash1, blockHash2, ...])                  
      Vec<Vec<Receipt>>  (all receipts for each block)          
                                                                 
  4. Mark Success                                                
     If receipts returned for block  mark_known_block()         

```

### What we fetch
| Request           | Input                | Output                          |
|-------------------|----------------------|---------------------------------|
| GetBlockHeaders   | block number + limit | Vec\<Header\>                   |
| GetReceipts       | Vec\<blockHash\>     | Vec\<Vec\<Receipt\>\> per block |

### What we DON'T fetch
- **Block bodies** (transactions, uncles)  not needed for receipts.
- **State**  no execution.
- We do NOT verify `receipts_root`  we only count receipts received.

### Protocol variants
- **Eth68 and below**: `GetReceipts(Vec<B256>)`  `Receipts(Vec<Vec<Receipt>>)`
- **Eth69**: Same request, different response encoding (`Receipts69`)
- **Eth70**: `GetReceipts70 { first_block_receipt_index, block_hashes }`  `Receipts70`

### Why block hash is required
The `GetReceipts` message requires **block hashes**, not block numbers. The only
way to obtain a block's hash is to fetch its header and compute
`keccak256(RLP(header))`. This is why we must fetch headers first.

### Definition of "known"
A block is marked "known" **only when receipts are successfully received**:
- Header fetch fails  block stays in queue (or escalation)
- Header OK but receipts fail  block stays pending
- Both succeed  entry written to `known_blocks.jsonl`

## Target selection
Targets are constructed from a fixed set of DeFi deployment anchors plus a
contiguous window:
- Uniswap V2: 10000835
- Aave V2: 11362579
- Uniswap V3: 12369621
- Aave V3: 16291127
- Uniswap V4: 21688329

The default window is 10,000 blocks per anchor. Set with `--anchor-window`.

## Data stored
Minimal data to enable downstream analysis:
- **Headers:** only for probing (not persisted as canonical chain state).
- **Receipts:** only counts and timing in probe logs.
- **JSONL output:**
  - `requests.jsonl`: request events + timing.
  - `probes.jsonl`: per-block probe results.
  - `known_blocks.jsonl`: blocks proven available.
  - `stats.jsonl`: window stats + summary.

## Retry and error handling
Two-tier retry strategy:
1. **Normal retries**
   - Failed blocks are requeued up to `--max-attempts`.
2. **Escalation pass**
   - Once the main queue is drained, blocks that failed all retries are tried
     again across distinct peers (one attempt per peer).
   - If every known peer fails, the block is marked unavailable.

All failures are recorded with reason tags (`header_timeout`,
`receipts_batch`, `escalation_*`, etc).

## Peer handling
- Track peer health and ban temporarily after consecutive failures.
- Ban duration and failure threshold are configurable.
- Sessions are counted; peer assignment is based on availability and queue.

## Completion criteria
The harness exits when:
1. All targets are completed or failed, and
2. Both normal and escalation queues are empty, and
3. No in-flight work remains.

`--run-secs` provides a hard time limit override.

## CLI behavior
Console output is a real-time progress bar (10Hz) showing:
- % complete and processed/total
- elapsed time, ETA (1-second rolling window)
- speed (blocks/sec, 1-second rolling window)
- peer status (active/total sessions)
- queue length, in-flight count
- failed blocks count

During escalation, a separate red progress bar shows:
- failed remaining/total
- peers tried/total
- escalation queue length

Verbose levels:
- (default): progress bar only, no log output
- `-v`: progress + periodic stats to logs
- `-vv`: adds WARN-level logs (peer failures, timeouts)
- `-vvv`: includes per-request and per-probe JSON logs

## Known limitations
- No receipts root validation.
- Network-only data availability is impacted by EIP-4444 pruning.
- No L2 support yet (Base/Optimism planned).

```

```markdown
# Receipt Availability Harness Spec (Updated)

## Goal
Measure the availability and latency of Ethereum L1 block headers and receipts
served over devp2p `eth` without executing transactions or maintaining state.
Persist only what is needed for receipts availability analysis and indexing.

## Non-goals
- Full execution / state verification.
- Archive-style state or EVM traces.
- Receipts root validation (explicitly skipped in v1).

## High-level architecture
1. **P2P network layer (Reth as library)**
   - Discovery, dialing, and `eth` handshake.
   - `PeerRequestSender` for headers/receipts.
2. **Scheduler**
   - Shared work queue of target blocks.
   - Per-peer assignment and batching.
   - Soft ban on peers with repeated failures.
3. **Probing pipeline**
   - Header batch  receipt batch per block hash.
   - Eth protocol version-aware receipts requests (`eth/68`, `eth/69`, `eth/70`).
4. **Persistence**
   - JSONL logs for requests, probes, stats, and known blocks.

## Fetch flow

### Peer head resolution
When a peer connects, their `status` message includes their head block hash. We
send `GetBlockHeaders` for that hash (limit=1) to resolve the head block number.
This determines the maximum block number we can request from that peer.

### Block batch probing
For each assigned batch of blocks (default 32 contiguous):

```

  1. Request Headers                                             
     GetBlockHeaders { start: Number(N), limit: 32, skip: 0 }    
      Vec<Header>                                               
                                                                 
  2. Compute Block Hashes                                        
     For each header: blockHash = keccak256(RLP(header))         
     (via SealedHeader::seal_slow)                               
                                                                 
  3. Request Receipts (in chunks of 16)                          
     GetReceipts([blockHash1, blockHash2, ...])                  
      Vec<Vec<Receipt>>  (all receipts for each block)          
                                                                 
  4. Mark Success                                                
     If receipts returned for block  mark_known_block()         

```

### What we fetch
| Request           | Input                | Output                          |
|-------------------|----------------------|---------------------------------|
| GetBlockHeaders   | block number + limit | Vec\<Header\>                   |
| GetReceipts       | Vec\<blockHash\>     | Vec\<Vec\<Receipt\>\> per block |

### What we DON'T fetch
- **Block bodies** (transactions, uncles)  not needed for receipts.
- **State**  no execution.
- We do NOT verify `receipts_root`  we only count receipts received.

### Protocol variants
- **Eth68 and below**: `GetReceipts(Vec<B256>)`  `Receipts(Vec<Vec<Receipt>>)`
- **Eth69**: Same request, different response encoding (`Receipts69`)
- **Eth70**: `GetReceipts70 { first_block_receipt_index, block_hashes }`  `Receipts70`

### Why block hash is required
The `GetReceipts` message requires **block hashes**, not block numbers. The only
way to obtain a block's hash is to fetch its header and compute
`keccak256(RLP(header))`. This is why we must fetch headers first.

### Definition of "known"
A block is marked "known" **only when receipts are successfully received**:
- Header fetch fails  block stays in queue (or escalation)
- Header OK but receipts fail  block stays pending
- Both succeed  entry written to `known_blocks.jsonl`

## Target selection
Targets are constructed from a fixed set of DeFi deployment anchors plus a
contiguous window:
- Uniswap V2: 10000835
- Aave V2: 11362579
- Uniswap V3: 12369621
- Aave V3: 16291127
- Uniswap V4: 21688329

The default window is 10,000 blocks per anchor. Set with `--anchor-window`.

## Data stored
Minimal data to enable downstream analysis:
- **Headers:** only for probing (not persisted as canonical chain state).
- **Receipts:** only counts and timing in probe logs.
- **JSONL output:**
  - `requests.jsonl`: request events + timing.
  - `probes.jsonl`: per-block probe results.
  - `known_blocks.jsonl`: blocks proven available.
  - `missing_blocks.jsonl`: blocks not fetched (written on shutdown if any remain).
  - `stats.jsonl`: window stats + summary.

## Retry and error handling
Two-tier retry strategy:
1. **Normal retries**
   - Failed blocks are requeued up to `--max-attempts`.
2. **Escalation pass**
   - Once the main queue is drained, blocks that failed all retries are tried
     again across distinct peers (one attempt per peer).
   - If every known peer fails, the block is marked unavailable.

All failures are recorded with reason tags (`header_timeout`,
`receipts_batch`, `escalation_*`, etc).

## Peer handling
- Track peer health and ban temporarily after consecutive failures.
- Ban duration and failure threshold are configurable.
- Sessions are counted; peer assignment is based on availability and queue.

## Completion criteria
The harness exits when:
1. All targets are completed or failed, and
2. Both normal and escalation queues are empty, and
3. No in-flight work remains.

`--run-secs` provides a hard time limit override.

## CLI behavior
Console output is a real-time progress bar (10Hz) showing:
- % complete and processed/total
- elapsed time, ETA (1-second rolling window)
- speed (blocks/sec, 1-second rolling window)
- peer status (active/total sessions)
- queue length, in-flight count
- failed blocks count

During escalation, a separate red progress bar shows:
- failed remaining/total
- peers tried/total
- escalation queue length

Verbose levels:
- (default): progress bar only, no log output
- `-v`: progress + periodic stats to logs
- `-vv`: adds WARN-level logs (peer failures, timeouts)
- `-vvv`: includes per-request and per-probe JSON logs

## Known limitations
- No receipts root validation.
- Network-only data availability is impacted by EIP-4444 pruning.
- No L2 support yet (Base/Optimism planned).

```

Done! Updated `SPEC.md` with:

1. **New "Fetch flow" section** covering:
   - Peer head resolution (why we request head headers)
   - Block batch probing flow diagram
   - Table showing GetBlockHeaders and GetReceipts inputs/outputs (noting blockHash)
   - What we DON'T fetch (bodies, state, no receipts_root verification)
   - Protocol variants (eth68/69/70)
   - Why block hash is required
   - Definition of "known"

2. **Updated "Data stored"**  added `missing_blocks.jsonl`

3. **Updated "CLI behavior"**  added escalation bar details and corrected verbose levels (`-vv` for WARN logs)

