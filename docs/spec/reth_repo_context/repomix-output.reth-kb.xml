This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
questions/
  _template.md
  Q001-repo-architecture-map.md
  Q002-network-entrypoints.md
  Q003-sync-entrypoints.md
  Q004-storage-entrypoints.md
  Q005-rpc-entrypoints.md
  Q006-mdbx-lifecycle.md
  Q007-db-abstraction-layer.md
  Q008-table-definitions-codecs.md
  Q009-static-files-nippyjar.md
  Q010-storage-migrations-versioning.md
  Q011-pipeline-orchestration.md
  Q012-reverse-headers-downloader.md
  Q013-bodies-downloader.md
  Q014-missing-ranges-holes.md
  Q015-peers-manager-reputation.md
  Q016-connection-limits-dial-backoff.md
  Q017-peer-persistence.md
  Q018-canonical-chain-reorgs.md
  Q019-engine-tree-backfill-handoff.md
  Q020-rollback-semantics-reorgs.md
  Q021-rpc-server-wiring.md
  Q022-eth-getlogs.md
  Q023-rpc-safety-knobs.md
  Q024-eth-request-handler.md
  Q025-config-plumbing-network-sync.md
  Q026-metrics-tracing-network-sync.md
INDEX.md
PROMPT_TEMPLATE.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="questions/_template.md">
# Q### - <Title>

Status: pending

## Question
<write the question here>

## Context Packs
- <pack1.xml>
- <pack2.xml>

## Gemini Prompt

```text
<paste prompt here>
```

## Gemini Answer
TBD

## Verification
TBD

## Actionable Pointers
- <file/function pointers once verified>
</file>

<file path="questions/Q001-repo-architecture-map.md">
# Q001 - Repo Architecture Map (High-Level)

Status: verified

## Question
What are the major crates/modules in Reth, and how do they fit together at a high level?

## Context Packs
- `reth/stateless-history-context.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- reth/stateless-history-context.xml

Question:
Provide a high-level architecture map of Reth: major crates/modules and how they fit together.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Use the workspace Cargo manifest to list crate groups not present in this pack and label them
   as “not in pack (details unavailable)”.
3. If something is not in the pack, say so explicitly.
4. Keep explanations plain-English and grounded in the pack.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Architecture map (crate groups, responsibilities, and connections)
3. Key data flows (if any are visible in the pack)
4. Key abstractions/types (with file paths)
5. Entry points or “where to start” for each included group
6. Suggested follow-up questions
```

## Gemini Answer
Based on the provided source code and documentation in the `stateless-history-context.xml` file, here is a high-level architecture map of Reth.

### 1. Summary
*   **Modular "Hub-and-Spoke" Design**: Reth is built as a collection of specialized crates. The `reth-network` crate acts as a central hub for P2P activity, coordinating discovery, session management, and data fetching.
*   **Trait-Driven Abstraction**: Core components are generic over traits (e.g., `Block`, `BlockHeader`, `SignedTransaction`) defined in `reth-primitives-traits`, allowing the same logic to support Ethereum and Optimism.
*   **Staged Synchronization**: The architecture is designed for a "Pipeline" sync where different stages (Headers, Bodies, Execution) request data via a `FetchClient`.
*   **P2P Protocol Layering**: Communication moves from raw TCP to ECIES encryption, then to RLPx P2P framing, and finally to sub-protocols like `eth` and `snap`.
*   **Historical Data Portability**: Through the `era` crates, Reth supports importing and exporting historical block data via specialized flat files, bypassing the P2P network for initial seeding.
*   **Lazy Hashing/Recovery**: Computationally expensive tasks like block hashing (`SealedBlock`) and sender recovery (`RecoveredBlock`) are performed lazily and cached to maximize performance.

---

### 2. Architecture Map

#### A. Networking Group (Core of this Pack)
Responsible for all peer interactions, discovery, and message routing.
*   **`reth-network`**: The primary orchestrator. Contains the `NetworkManager`, which manages the `Swarm` (connection listener + network state) and routes messages between peers and internal tasks (`crates/net/network/src/manager.rs`).
*   **`reth-eth-wire` & `reth-eth-wire-types`**: Implements the Ethereum Wire Protocol. Handles the `Hello` and `Status` handshakes and defines the RLP-encodable message structures (`crates/net/eth-wire/src/lib.rs`).
*   **`reth-network-p2p`**: Defines the consumer-facing traits for P2P interaction, such as `HeadersClient` and `BodiesClient` (`crates/net/p2p/src/lib.rs`).
*   **`reth-downloaders`**: Implementation of algorithms for batch-downloading headers (reverse) and bodies (concurrent) (`crates/net/downloaders/src/lib.rs`).
*   **`reth-discovery` (discv4, discv5, dns)**: Specialized crates for finding peers via different protocols. Aggregated by the `Discovery` struct (`crates/net/network/src/discovery.rs`).

#### B. Primitives & Traits Group
The "Common Language" of the node.
*   **`reth-primitives-traits`**: Defines the foundational traits for blocks and transactions. This allows the core logic to remain chain-agnostic (`crates/primitives-traits/src/lib.rs`).
*   **`reth-ethereum-primitives`**: Concrete implementations of the traits specifically for Ethereum (`crates/ethereum/primitives/src/lib.rs`).
*   **`reth-primitives`**: High-level wrapper crate that re-exports core types like `SealedBlock` and `Recovered` (`crates/primitives/src/lib.rs`).

#### C. Chainspec Group
*   **`reth-chainspec`**: Defines network-specific constants, genesis headers, and hardfork activation logic (Mainnet, Sepolia, etc.) (`crates/chainspec/src/lib.rs`).
*   **`reth-optimism-chainspec`**: Extension for OP-Stack specific chain configurations and base fee calculations (`crates/optimism/chainspec/src/lib.rs`).

#### D. Era / Historical Storage
*   **`reth-era`**: Implements the `E2Store` format for long-term storage of Ethereum history in `.era` and `.era1` files (`crates/era/src/lib.rs`).
*   **`reth-era-downloader`**: Async stream for downloading these historical files via HTTP (`crates/era-downloader/src/lib.rs`).

#### E. Crates Not in Pack (Details Unavailable)
According to the workspace `Cargo.toml`, the following major groups are missing from this pack:
*   **`reth-rpc-*`**: All JSON-RPC server implementations.
*   **`reth-revm` & `reth-evm-*`**: The execution engine and state transition logic.
*   **`reth-db` & `reth-storage-*`**: The actual MDBX database implementation and storage providers.
*   **`reth-stages`**: The pipeline stage logic (though mentioned in docs).
*   **`reth-transaction-pool`**: The mempool logic for sorting and validating pending transactions.
*   **`reth-engine-api` & `reth-beacon-consensus`**: Proof-of-Stake consensus engine and CL communication.

---

### 3. Key Data Flows

#### Outbound Data Request (P2P Fetching)
1.  **Pipeline Stage** (e.g., `BodyStage`) calls `get_block_bodies` on a `FetchClient` (`crates/net/network/src/fetch/client.rs`).
2.  The `FetchClient` sends a `DownloadRequest` to the **`StateFetcher`** via a channel.
3.  `StateFetcher` selects the **"Next Best Peer"** based on idle status and RTT (`crates/net/network/src/fetch/mod.rs`).
4.  The request is routed through the peer's `ActiveSession` and sent over the wire via **`EthStream`** (`crates/net/network/src/session/active.rs`).

#### Peer Discovery to Connection
1.  **`Discovery`** task receives a new node from `discv4`, `discv5`, or `DNS` (`crates/net/network/src/discovery.rs`).
2.  It notifies the **`PeersManager`**, which validates the peer (checking the `BanList` and `IpFilter`).
3.  If a slot is available, `PeersManager` issues a `PeerAction::Connect` (`crates/net/peers/src/lib.rs`).
4.  The **`SessionManager`** initiates a TCP/ECIES handshake and upgrades it to an `ActiveSession` (`crates/net/network/src/session/mod.rs`).

---

### 4. Key Abstractions & Types

| Type | File Path | Description |
| :--- | :--- | :--- |
| `NetworkManager` | `crates/net/network/src/manager.rs` | The central future that drives the entire P2P stack. |
| `SealedBlock<B>` | `crates/primitives-traits/src/block/sealed.rs` | A block that has been hashed, with the hash cached to prevent re-computation. |
| `RecoveredBlock<B>` | `crates/primitives-traits/src/block/recovered.rs` | A block that has had its transaction signers recovered (ECRecover). |
| `EthStream<S>` | `crates/net/eth-wire/src/ethstream.rs` | A wrapper that turns a stream of bytes into a stream of Ethereum protocol messages. |
| `ChainSpec` | `crates/chainspec/src/spec.rs` | The definition of a specific network's rules and history. |
| `P2PStream` | `crates/net/eth-wire/src/p2pstream.rs` | Handles P2P-level concerns like Snappy compression and Pings. |

---

### 5. Entry Points: Where to Start

*   **For P2P Logic**: Start at `NetworkManager::new` in `crates/net/network/src/manager.rs`. This shows how discovery, sessions, and the swarm are initialized.
*   **For Block/Transaction Types**: Look at `NodePrimitives` in `crates/primitives-traits/src/node.rs`. This trait maps all concrete types used by a specific node implementation.
*   **For New Chain Support**: Examine `OpChainSpecBuilder` in `crates/optimism/chainspec/src/lib.rs` to see how Reth is extended for non-Ethereum L1 chains.
*   **For Manual P2P Testing**: See the example in `examples/manual-p2p/src/main.rs`, which demonstrates a low-level manual handshake with a peer.

---

### 6. Suggested Follow-up Questions
*   "How does the `ReverseHeadersDownloader` handle peer penalization when it receives a bad header batch?"
*   "What is the difference between `Eth68` and `Eth69` status messages in the `eth-wire-types` crate?"
*   "How are Optimism's hardforks integrated into the standard `ChainSpec` logic?"
*   "Can you explain the `E2Store` entry format used in `reth-era` for historical data?"

## Verification
- Confirmed `NetworkManager` owns a `Swarm` and routes network events; `Swarm` manages listener/session/state flow and delegates fetch requests via `StateFetcher`.```90:142:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/manager.rs
///     subgraph Swarm
///         direction TB
///         B1[(Session Manager)]
///         B2[(Connection Listener)]
///         B3[(Network State)]
///     end
///  end
///  handle <--> |request response channel| NetworkManager
///  NetworkManager --> |Network events| events
///  transactions <--> |transactions| NetworkManager
///  ethrequest <--> |ETH request handing| NetworkManager
///  discovery --> |Discovered peers| NetworkManager
/// ```
#[derive(Debug)]
#[must_use = "The NetworkManager does nothing unless polled"]
pub struct NetworkManager<N: NetworkPrimitives = EthNetworkPrimitives> {
    /// The type that manages the actual network part, which includes connections.
    swarm: Swarm<N>,
    /// Underlying network handle that can be shared.
    handle: NetworkHandle<N>,
```
```25:47:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/swarm.rs
/// A swarm emits [`SwarmEvent`]s when polled.
/// ...
/// The [`NetworkState`] keeps track of all connected and discovered peers and can initiate outgoing
/// connections. For each active session, the [`NetworkState`] keeps a sender half of the ETH
/// request channel for the created session and sends requests it receives from the
/// [`StateFetcher`], which receives request objects from the client interfaces responsible for
/// downloading headers and bodies.
#[derive(Debug)]
#[must_use = "Swarm does nothing unless polled"]
pub(crate) struct Swarm<N: NetworkPrimitives = EthNetworkPrimitives> {
```
- P2P layering is explicit: TCP -> `ECIESStream` -> `P2PStream` -> `EthStream` (and optional satellite protocols).```18:36:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/session/conn.rs
/// The type of the underlying peer network connection.
pub type EthPeerConnection<N> = EthStream<P2PStream<ECIESStream<TcpStream>>, N>;
...
/// A connection that supports the ETH protocol and __at least one other__ `RLPx` protocol.
pub enum EthRlpxConnection<N: NetworkPrimitives = EthNetworkPrimitives> {
```
```1:74:/Users/vicnaum/github/stateless-history-node/reth/crates/net/eth-wire/src/eth_snap_stream.rs
//! Ethereum and snap combined protocol stream implementation.
//! A stream type for handling both eth and snap protocol messages over a single `RLPx` connection.
```
- `reth-network-p2p` does define `HeadersClient` and `BodiesClient` traits (correct).```1:50:/Users/vicnaum/github/stateless-history-node/reth/crates/net/p2p/src/headers/client.rs
pub trait HeadersClient: DownloadClient {
```
```16:50:/Users/vicnaum/github/stateless-history-node/reth/crates/net/p2p/src/bodies/client.rs
pub trait BodiesClient: DownloadClient {
```
- `reth-downloaders` contains the bodies + headers downloader modules.```19:24:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/lib.rs
/// The collection of algorithms for downloading block bodies.
pub mod bodies;
/// The collection of algorithms for downloading block headers.
pub mod headers;
```
- “Lazy hashing/recovery” is accurate for `SealedBlock` and `RecoveredBlock` (explicitly documented).```16:41:/Users/vicnaum/github/stateless-history-node/reth/crates/primitives-traits/src/block/sealed.rs
/// This type uses lazy sealing to avoid hashing the header until it is needed, see also
/// [`SealedHeader`].
pub struct SealedBlock<B: Block> {
```
```19:44:/Users/vicnaum/github/stateless-history-node/reth/crates/primitives-traits/src/block/recovered.rs
/// Sender recovery is computationally expensive. Cache recovered blocks when possible
/// to avoid repeated recovery operations.
/// ...
/// This type uses lazy sealing to avoid hashing the header until it is needed:
```
- `era` and `era-downloader` crates match the ERA/E2Store claims (ERA1 supported).```1:18:/Users/vicnaum/github/stateless-history-node/reth/crates/era/src/lib.rs
//! Era and Era1 files support for Ethereum history expiry.
pub mod era;
pub mod era1;
```
```1:8:/Users/vicnaum/github/stateless-history-node/reth/crates/era-downloader/src/lib.rs
//! An asynchronous stream interface for downloading ERA1 files.
```
- Trait-driven abstraction is accurate; `NodePrimitives` is the node-wide primitive type contract.```6:28:/Users/vicnaum/github/stateless-history-node/reth/crates/primitives-traits/src/node.rs
/// Configures all the primitive types of the node.
pub trait NodePrimitives:
```

## Corrections / Caveats
- There is no `reth-discovery` crate. Discovery is implemented inside `reth-network` via `Discovery` and uses `reth_discv4`, `reth_discv5`, and `reth_dns_discovery` as dependencies.```35:60:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/discovery.rs
/// An abstraction over the configured discovery protocol.
pub struct Discovery {
    /// Handler to interact with the Discovery v4 service
    discv4: Option<Discv4>,
    /// Handler to interact with the Discovery v5 service
    discv5: Option<Discv5>,
    /// Updates from the DNS discovery service.
    dns_discovery_updates: Option<ReceiverStream<DnsNodeRecordUpdate>>,
```
- The “next best peer based on RTT” phrasing is close, but the actual selection in `StateFetcher::next_best_peer` uses an internal `timeout()` (peer request timeout) and `last_response_likely_bad`, plus range-quality checks. It’s not explicitly labeled RTT in code.```140:167:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/fetch/mod.rs
/// Returns the _next_ idle peer that's ready to accept a request,
/// prioritizing those with the lowest timeout/latency and those that recently responded with
/// adequate data. Additionally, if full blocks are required this prioritizes peers that have
/// full history available
fn next_best_peer(&self, requirement: BestPeerRequirements) -> Option<PeerId> {
```
- The “pipeline stage calls FetchClient” flow is plausible but not in this pack. The stage logic lives in `crates/stages/**` (outside the provided pack). We can verify this in a later question using `context-sync.xml`.
- “Crates not in pack” list is directionally correct, but note the actual engine/API components are in `crates/engine/*` and `crates/rpc/rpc-engine-api` (not a crate named `reth-engine-api`). `reth-db` does exist under `crates/storage/db` and other storage crates under `crates/storage/*` per workspace members in `reth/Cargo.toml`.

## Actionable Pointers
- P2P architecture entry point: `reth/crates/net/network/src/manager.rs` and `reth/crates/net/network/src/swarm.rs`.
- Protocol framing/handshake: `reth/crates/net/eth-wire/src/p2pstream.rs` and `reth/crates/net/eth-wire/src/ethstream.rs`.
- Snap protocol support: `reth/crates/net/eth-wire/src/eth_snap_stream.rs`.
- Primitive abstractions: `reth/crates/primitives-traits/src/node.rs`, `.../block/sealed.rs`, `.../block/recovered.rs`.
- ERA history: `reth/crates/era/src/lib.rs` and `reth/crates/era-downloader/src/lib.rs`.
</file>

<file path="questions/Q002-network-entrypoints.md">
# Q002 - Networking Entry Points & Lifecycle

Status: verified

## Question
Where are the core networking entry points and how do they connect? Provide a map of the P2P stack from configuration to active peer sessions and request handling.

## Context Packs
- `spec/reth_repo_context/context-network.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-network.xml

Question:
Where are the core networking entry points and how do they connect?
I want a map of the P2P stack from configuration to active peer sessions and request handling.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Identify entry points (builders, managers, handles, public APIs).
3. Provide the lifecycle: config -> start -> discovery -> sessions -> request routing.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Lifecycle flow (step-by-step)
4. Key abstractions/types (with file paths)
5. “Where to look next” (top files to read)
6. Suggested follow-up questions
```

## Gemini Answer
Based on the provided networking context, here is the architecture map of the Reth P2P stack, ranging from configuration to active request handling.

### 1. Summary
*   **Modular Entry Points**: Users configure the stack via `NetworkConfigBuilder` and interact with it post-launch via `NetworkHandle` or `FetchClient`.
*   **Central Hub**: The `NetworkManager` acts as the primary "hub" future, orchestrating discovery, connection swarm, and sub-protocol routing.
*   **Multi-Protocol Discovery**: Discovery is handled concurrently via three protocols: Discv4 (UDP), Discv5 (UDP/ENR), and DNS (EIP-1459).
*   **Connection Layering**: Reth wraps raw TCP in ECIES encryption, then into an RLPx `P2PStream` (handling pings/compression), and finally into an `EthStream` or `EthSnapStream` for sub-protocol logic.
*   **The Swarm**: The `Swarm` struct is the internal engine that manages the `SessionManager` (connection state) and `NetworkState` (peer/data state).
*   **Task Separation**: Sub-protocols like transactions and ETH requests are offloaded to specialized spawned tasks (`TransactionsManager` and `EthRequestHandler`).

---

### 2. Entry Points

| File Path | Type | Purpose |
| :--- | :--- | :--- |
| `crates/net/network/src/config.rs` | `NetworkConfigBuilder` | The primary UI for configuring the stack (ports, secret keys, bootnodes, and discovery settings). |
| `crates/net/network/src/manager.rs` | `NetworkManager` | The core "worker" task. It is an endless future that drives the entire P2P stack forward. |
| `crates/net/network/src/network.rs` | `NetworkHandle` | A cloneable, thread-safe handle used by other parts of the node (like the RPC or Pipeline) to send commands or query network status. |
| `crates/net/network/src/fetch/client.rs` | `FetchClient` | The high-level interface used by the sync pipeline to request headers and block bodies. |
| `crates/net/network/src/builder.rs` | `NetworkBuilder` | A helper to assemble the `NetworkManager` with its sub-components like the `TransactionsManager`. |

---

### 3. Lifecycle Flow

#### Step 1: Configuration
The user uses `NetworkConfigBuilder::new(secret_key)` to define parameters. This builder eventually produces a `NetworkConfig` (`crates/net/network/src/config.rs`).

#### Step 2: Starting the Manager
`NetworkManager::new(config)` is called. This initializes the `Swarm`, which includes:
*   **`ConnectionListener`**: Binds to a TCP port for incoming peers (`crates/net/network/src/listener.rs`).
*   **`SessionManager`**: Prepares to handle handshakes (`crates/net/network/src/session/mod.rs`).
*   **`Discovery`**: Starts the UDP discovery services (`crates/net/network/src/discovery.rs`).

#### Step 3: Discovery & Peering
The `Discovery` task finds peer addresses. It sends `DiscoveryEvent::NewNode` to the `NetworkState`. If there is capacity, the `PeersManager` issues a `PeerAction::Connect` (`crates/net/network/src/peers.rs`).

#### Step 4: The Handshake (Unauthed to Authed)
1.  **TCP Connection**: Established via `TcpStream`.
2.  **ECIES Handshake**: Authenticates and encrypts the stream (`crates/net/network/src/session/mod.rs`).
3.  **P2P Hello**: `UnauthedP2PStream` exchanges `Hello` messages and negotiates capabilities (`crates/net/eth-wire/src/p2pstream.rs`).
4.  **ETH Status**: `UnauthedEthStream` exchanges `Status` messages to ensure peers are on the same chain/fork (`crates/net/eth-wire/src/ethstream.rs`).

#### Step 5: Active Session & Multiplexing
Once authed, a session becomes an `ActiveSession`. It uses `RlpxProtocolMultiplexer` to allow multiple protocols (like `eth` and `snap`) to run over the same encrypted connection (`crates/net/eth-wire/src/multiplex.rs`).

#### Step 6: Request Routing
When a message arrives:
*   **Transactions** are sent to the `TransactionsManager` via the `to_transactions_manager` channel.
*   **ETH Data Requests** (GetHeaders, GetBodies) are sent to the `EthRequestHandler` (`crates/net/network/src/eth_requests.rs`).
*   **Inbound Data** for our own requests is routed back to the `FetchClient`'s response channel.

---

### 4. Key Abstractions/Types

*   **`Swarm`** (`crates/net/network/src/swarm.rs`): Orchestrates the interaction between the TCP listener, the sessions, and the internal state.
*   **`EthStream`** (`crates/net/eth-wire/src/ethstream.rs`): A wrapper that transforms RLPx bytes into `EthMessage` types.
*   **`PeerMetadata`** (`crates/net/network/src/test_utils/transactions.rs`): Tracks what data a peer has seen to prevent redundant broadcasts.
*   **`UnifiedStatus`** (`crates/net/eth-wire-types/src/status.rs`): A version-agnostic structure that encapsulates all fields needed for `eth/66` through `eth/70` handshakes.
*   **`BlockRangeInfo`** (`crates/net/network/src/session/types.rs`): Tracks the block range a peer can serve (critical for `eth/69` protocol).

---

### 5. Where to Look Next
1.  **`crates/net/network/src/swarm.rs`**: This is the best place to see the actual logic for how Reth decides to accept a connection or dial a peer.
2.  **`crates/net/network/src/session/active.rs`**: Read this to understand the main loop of a single peer connection, including how timeouts and protocol breaches are handled.
3.  **`crates/net/eth-wire/src/multiplex.rs`**: Read this to understand how Reth runs both the `eth` and `snap` protocols simultaneously.
4.  **`crates/net/network/src/transactions.rs`** (referenced but logic partially outside): To see how transaction gossip and propagation are handled.

---

### 6. Suggested Follow-up Questions
*   "How does the `SessionManager` enforce connection limits for inbound versus outbound peers?"
*   "How are the new `eth/70` partial receipts requested and validated in `eth_requests.rs`?"
*   "What is the role of the `RequiredBlockFilter` in filtering peers for shadowforks?"
*   "How does the `Pinger` state machine in `pinger.rs` trigger a peer disconnect on timeout?"

## Verification
- `NetworkConfigBuilder` exists and is the primary config UI; `NetworkConfig::start_network` spawns the network and eth-request handler tasks via `NetworkManager::builder`.```170:182:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/config.rs
    /// Starts the networking stack given a [`NetworkConfig`] and returns a handle to the network.
    pub async fn start_network(self) -> Result<NetworkHandle<N>, NetworkError> {
        let client = self.client.clone();
        let (handle, network, _txpool, eth) = NetworkManager::builder::<C>(self)
            .await?
            .request_handler::<C>(client)
            .split_with_handle();

        tokio::task::spawn(network);
        tokio::task::spawn(eth);
        Ok(handle)
    }
```
```185:222:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/config.rs
/// Builder for [`NetworkConfig`](struct.NetworkConfig.html).
#[derive(Debug)]
pub struct NetworkConfigBuilder<N: NetworkPrimitives = EthNetworkPrimitives> {
    /// The node's secret key, from which the node's identity is derived.
    secret_key: SecretKey,
    /// How to configure discovery over DNS.
    dns_discovery_config: Option<DnsDiscoveryConfig>,
    /// How to set up discovery version 4.
    discovery_v4_builder: Option<Discv4ConfigBuilder>,
    /// How to set up discovery version 5.
    discovery_v5_builder: Option<reth_discv5::ConfigBuilder>,
    /// All boot nodes to start network discovery with.
    boot_nodes: HashSet<TrustedPeer>,
```
- `NetworkManager` is an endless future and wires up discovery, sessions, state, and swarm.```224:318:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/manager.rs
/// The [`NetworkManager`] is an endless future that needs to be polled in order to advance the
/// state of the entire network.
pub async fn new<C: BlockNumReader + 'static>(
    config: NetworkConfig<C, N>,
) -> Result<Self, NetworkError> {
    ...
    let discovery = Discovery::new(...).await?;
    let sessions = SessionManager::new(...);
    let state = NetworkState::new(...);
    let swarm = Swarm::new(incoming, sessions, state);
    ...
}
```
- Discovery uses discv4/discv5/DNS and emits `DiscoveryEvent`s.```35:67:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/discovery.rs
/// An abstraction over the configured discovery protocol.
pub struct Discovery {
    /// Handler to interact with the Discovery v4 service
    discv4: Option<Discv4>,
    /// Handler to interact with the Discovery v5 service
    discv5: Option<Discv5>,
    /// Updates from the DNS discovery service.
    dns_discovery_updates: Option<ReceiverStream<DnsNodeRecordUpdate>>,
    ...
}
```
- Connection layering is explicit: `EthStream<P2PStream<ECIESStream<TcpStream>>>`, with optional satellite protocols.```18:37:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/session/conn.rs
/// The type of the underlying peer network connection.
pub type EthPeerConnection<N> = EthStream<P2PStream<ECIESStream<TcpStream>>, N>;
...
pub enum EthRlpxConnection<N: NetworkPrimitives = EthNetworkPrimitives> {
    /// A connection that only supports the ETH protocol.
    EthOnly(Box<EthPeerConnection<N>>),
    /// A connection that supports the ETH protocol and __at least one other__ `RLPx` protocol.
    Satellite(Box<EthSatelliteConnection<N>>),
}
```
- P2P and ETH handshakes are performed by `UnauthedP2PStream` and `UnauthedEthStream`.```63:103:/Users/vicnaum/github/stateless-history-node/reth/crates/net/eth-wire/src/p2pstream.rs
/// An un-authenticated [`P2PStream`]. This is consumed and returns a [`P2PStream`] after the
/// `Hello` handshake is completed.
pub struct UnauthedP2PStream<S> {
...
    pub async fn handshake(
        mut self,
        hello: HelloMessageWithProtocols,
    ) -> Result<(P2PStream<S>, HelloMessage), P2PStreamError> {
```
```35:73:/Users/vicnaum/github/stateless-history-node/reth/crates/net/eth-wire/src/ethstream.rs
/// An un-authenticated [`EthStream`]. This is consumed and returns a [`EthStream`] after the
/// `Status` handshake is completed.
pub struct UnauthedEthStream<S> {
...
    pub async fn handshake<N: NetworkPrimitives>(
        self,
        status: UnifiedStatus,
        fork_filter: ForkFilter,
    ) -> Result<(EthStream<S, N>, UnifiedStatus), EthStreamError> {
```
- The RLPx protocol multiplexer exists and is named `RlpxProtocolMultiplexer`.```35:52:/Users/vicnaum/github/stateless-history-node/reth/crates/net/eth-wire/src/multiplex.rs
/// A Stream and Sink type that wraps a raw rlpx stream [`P2PStream`] and handles message ID
/// multiplexing.
pub struct RlpxProtocolMultiplexer<St> {
    inner: MultiplexInner<St>,
}
```
- `Swarm` is the core state machine for listener/session/state wiring.```25:57:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/swarm.rs
/// A swarm emits [`SwarmEvent`]s when polled.
#[derive(Debug)]
#[must_use = "Swarm does nothing unless polled"]
pub(crate) struct Swarm<N: NetworkPrimitives = EthNetworkPrimitives> {
    /// Listens for new incoming connections.
    incoming: ConnectionListener,
    /// All sessions.
    sessions: SessionManager<N>,
    /// Tracks the entire state of the network and handles events received from the sessions.
    state: NetworkState<N>,
}
```
- `FetchClient` is a front-end API that sends download requests over a channel.```27:43:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/fetch/client.rs
/// Front-end API for fetching data from the network.
#[derive(Debug, Clone)]
pub struct FetchClient<N: NetworkPrimitives = EthNetworkPrimitives> {
    /// Sender half of the request channel.
    pub(crate) request_tx: UnboundedSender<DownloadRequest<N>>,
```
- Discovery → state → peer actions: discovery events are queued as `StateAction::DiscoveredNode`, and peer actions can produce `PeerAction::Connect`.```325:347:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/state.rs
fn on_discovery_event(&mut self, event: DiscoveryEvent) {
    match event {
        DiscoveryEvent::NewNode(DiscoveredEvent::EventQueued { peer_id, addr, fork_id }) => {
            self.queued_messages.push_back(StateAction::DiscoveredNode { ... });
        }
        ...
    }
}
fn on_peer_action(&mut self, action: PeerAction) {
    match action {
        PeerAction::Connect { peer_id, remote_addr } => {
            self.queued_messages.push_back(StateAction::Connect { peer_id, remote_addr });
        }
        ...
    }
}
```
- `PeerMetadata` is in `crates/net/network/src/transactions/mod.rs`, not `test_utils`.```1963:1975:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/transactions/mod.rs
/// Tracks a single peer in the context of [`TransactionsManager`].
pub struct PeerMetadata<N: NetworkPrimitives = EthNetworkPrimitives> {
```
- `UnifiedStatus` and `BlockRangeInfo` live in the claimed files.```10:31:/Users/vicnaum/github/stateless-history-node/reth/crates/net/eth-wire-types/src/status.rs
/// `UnifiedStatus` is an internal superset of all ETH status fields for all `eth/` versions.
pub struct UnifiedStatus {
```
```14:42:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/session/types.rs
/// Information about the range of full blocks available from a peer.
pub struct BlockRangeInfo {
```

## Corrections / Caveats
- “EIP-1459” is not referenced in code; DNS discovery is implemented via `reth_dns_discovery`, but the EIP mention should be treated as unverified within this pack.
- `PeerMetadata` is not in `test_utils`; it’s in `crates/net/network/src/transactions/mod.rs` (corrected above).
- The “transactions task is spawned” depends on the caller. `NetworkConfig::start_network` spawns the network and eth-request handler tasks; it doesn’t spawn a `TransactionsManager` unless the caller wires and spawns it (e.g., node builder).
- “FetchClient used by the sync pipeline” is plausible but not proven by this pack alone; the pipeline lives in `crates/stages/**`.

## Actionable Pointers
- Start/configure the network: `reth/crates/net/network/src/config.rs` (`NetworkConfigBuilder`, `start_network`).
- Core runtime loop: `reth/crates/net/network/src/manager.rs` (`NetworkManager::new`, `Swarm::new`).
- P2P session lifecycle: `reth/crates/net/network/src/session/mod.rs`, `.../session/active.rs`.
- Discovery plumbing: `reth/crates/net/network/src/discovery.rs` and `.../state.rs`.
- Request routing: `reth/crates/net/network/src/fetch/client.rs` + `.../fetch/mod.rs` + `.../state.rs`.
- Transactions task wiring: `reth/crates/net/network/src/builder.rs` + `.../transactions/mod.rs`.
</file>

<file path="questions/Q003-sync-entrypoints.md">
# Q003 - Sync Pipeline Entry Points & Orchestration

Status: verified

## Question
Where are the sync pipeline entry points (stages, pipeline, downloaders), and how do they connect?

## Context Packs
- `spec/reth_repo_context/context-sync.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-sync.xml

Question:
Where are the sync pipeline entry points (stages, pipeline, downloaders), and how do they connect?
I want a map of the stage pipeline, how checkpoints/progress are tracked, and how downloaders are used.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Identify entry points (pipeline builder, stage sets, downloader interfaces).
3. Provide the lifecycle: pipeline init -> stage execution -> checkpoint update -> next stage.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Lifecycle flow (step-by-step)
4. Key abstractions/types (with file paths)
5. “Where to look next” (top files to read)
6. Suggested follow-up questions
```

## Gemini Answer
Based on the provided `context-sync.xml` file, here is a high-level architecture map of Reth’s synchronization pipeline, including entry points, stage logic, and data flow.

### 1. Summary
*   **Sequential Execution**: Reth syncs using a `Pipeline` that runs a series of `Stage` implementations serially, moving from one task (e.g., Headers) to the next (e.g., Bodies).
*   **Trait-Driven Design**: The `Stage` trait defines how a component "rolls forward" (`execute`) and "rolls back" (`unwind`).
*   **Checkpoint Persistence**: Each stage saves its progress as a `StageCheckpoint` in the database, allowing the node to resume after a restart.
*   **Decoupled Downloading**: Downloaders (for headers and bodies) are abstracted behind traits and often run as standalone tasks (`TaskDownloader`) to prevent blocking the main pipeline.
*   **Unified State Tracking**: The `ChainInfoTracker` and `InMemoryState` manage the latest canonical, safe, and finalized block information.
*   **Hybrid Storage**: Stages interact with both a database (MDBX) and "Static Files" (flat files for immutable historical data like headers and receipts).

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/stages/api/src/pipeline/builder.rs` | `PipelineBuilder` | The main UI for assembling a sync pipeline. It allows adding stages, setting a max block, and configuring tip listeners. |
| `crates/stages/stages/src/sets.rs` | `DefaultStages` | A pre-configured set of all stages required for a full sync (Headers -> Bodies -> Execution -> Merkle, etc.). |
| `crates/stages/api/src/pipeline/mod.rs` | `Pipeline::run` | The primary execution loop that drives the pipeline until the target block is reached. |
| `crates/net/downloaders/src/bodies/bodies.rs` | `BodiesDownloaderBuilder` | Entry point for configuring how block bodies are fetched from the network. |
| `crates/net/downloaders/src/headers/reverse_headers.rs` | `ReverseHeadersDownloaderBuilder` | Configures the downloader that fetches headers in reverse order (from tip to local head). |

---

### 3. Lifecycle Flow

#### Step 1: Initialization (`PipelineBuilder`)
The node starts by creating a `Pipeline` via the `PipelineBuilder`. It registers a `StaticFileProducer` and a `ProviderFactory` (which provides DB access).
*   **File**: `crates/stages/api/src/pipeline/builder.rs`
*   **Function**: `PipelineBuilder::build`

#### Step 2: Loop Initiation (`run_loop`)
The pipeline enters an infinite loop (or until a `max_block` is reached). It iterates through every registered stage in order.
*   **File**: `crates/stages/api/src/pipeline/mod.rs`
*   **Function**: `Pipeline::run_loop`

#### Step 3: Readiness Check (`poll_execute_ready`)
Before a stage executes, the pipeline calls `poll_execute_ready`. For downloader stages (like `HeaderStage`), this is when they fetch data from the network and fill internal buffers.
*   **File**: `crates/stages/api/src/stage.rs`
*   **Function**: `Stage::poll_execute_ready`

#### Step 4: Execution (`execute`)
The stage performs its logic (e.g., `ExecutionStage` runs the EVM, `SenderRecoveryStage` recovers public keys).
*   **File**: `crates/stages/api/src/stage.rs`
*   **Function**: `Stage::execute`

#### Step 5: Checkpoint & Commit
Once a stage completes a batch, it returns an `ExecOutput`. The pipeline then saves the new `StageCheckpoint` to the database and commits the transaction.
*   **File**: `crates/stages/api/src/pipeline/mod.rs`
*   **Function**: `Pipeline::execute_stage_to_completion`

#### Step 6: Control Flow (`ControlFlow`)
The result of a stage determines if the pipeline continues to the next stage, stops (no progress), or must **Unwind** due to a validation error.
*   **File**: `crates/stages/api/src/pipeline/ctrl.rs`
*   **Type**: `ControlFlow`

---

### 4. Key Abstractions/Types

*   **`Pipeline`** (`crates/stages/api/src/pipeline/mod.rs`): The orchestrator that owns the list of stages and manages transitions between them.
*   **`Stage`** (`crates/stages/api/src/stage.rs`): A trait that concrete sync tasks must implement. Note: Actual EVM execution logic (REVM) is used by `ExecutionStage` but defined in crates outside this pack.
*   **`StageCheckpoint`** (`crates/stages/types/src/checkpoints.rs`): A persistent record of how far a stage has progressed (block number + stage-specific metadata).
*   **`TaskDownloader`** (`crates/net/downloaders/src/headers/task.rs`): A wrapper that spawns a downloader onto its own Tokio task, communicating with the pipeline via channels to ensure the pipeline isn't blocked by P2P latency.
*   **`NewCanonicalChain`** (`crates/chain-state/src/in_memory.rs`): Represents a batch of executed blocks that are ready to be signaled as the new canonical head.
*   **`StaticFileSegment`** (`reth_static_file_types` - *referenced in* `crates/stages/stages/src/stages/utils.rs`): Represents different types of immutable data (Headers, Transactions, Receipts) stored outside the database.

---

### 5. Where to Look Next
1.  **`crates/stages/stages/src/stages/execution.rs`**: To see the bridge between the sync pipeline and the actual EVM execution.
2.  **`crates/stages/stages/src/stages/merkle.rs`**: To understand how the state root is calculated and verified at the end of execution.
3.  **`crates/stages/api/src/pipeline/mod.rs`**: To understand the `unwind` logic, which is critical for handling reorgs.
4.  **`docs/crates/stages.md`**: Provides a plain-English narrative of the ordering and purpose of every single stage.

---

### 6. Suggested Follow-up Questions
*   "How does the `HeaderStage` handle a `DetachedHead` error during reverse downloading?"
*   "What is the role of `external_clean_threshold` in the `ExecutionStage`'s decision to prune changesets?"
*   "How do `ExEx` (Execution Extensions) receive notifications from the `ExecutionStage` after a commit?"
*   "How does `MerkleChangeSets` maintain trie updates between the finalized block and the tip?"

## Verification
- Stage trait exposes `poll_execute_ready`, `execute`, and `unwind`; stages are executed serially via the pipeline.```229:299:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/stage.rs
/// Stages are executed as part of a pipeline where they are executed serially.
pub trait Stage<Provider>: Send {
    ...
    fn poll_execute_ready(
        &mut self,
        _cx: &mut Context<'_>,
        _input: ExecInput,
    ) -> Poll<Result<(), StageError>> { ... }
    fn execute(&mut self, provider: &Provider, input: ExecInput) -> Result<ExecOutput, StageError>;
    fn unwind(
        &mut self,
        provider: &Provider,
        input: UnwindInput,
    ) -> Result<UnwindOutput, StageError>;
}
```
- Pipeline run loop executes stages in order; it also calls `move_to_static_files()` each pass.```223:257:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/mod.rs
pub async fn run_loop(&mut self) -> Result<ControlFlow, PipelineError> {
    self.move_to_static_files()?;
    let mut previous_stage = None;
    for stage_index in 0..self.stages.len() {
        let stage = &self.stages[stage_index];
        let stage_id = stage.id();
        ...
        let next = self.execute_stage_to_completion(previous_stage, stage_index).await?;
        ...
    }
    Ok(self.progress.next_ctrl())
}
```
- Execution commits checkpoints and calls `post_execute_commit` after `save_stage_checkpoint`.```479:488:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/mod.rs
match self.stage(stage_index).execute(&provider_rw, exec_input) {
    Ok(out @ ExecOutput { checkpoint, done }) => {
        // Update stage checkpoint.
        provider_rw.save_stage_checkpoint(stage_id, checkpoint)?;
        // Commit processed data to the database.
        provider_rw.commit()?;
        // Invoke stage post commit hook.
        self.stage(stage_index).post_execute_commit()?;
        ...
    }
```
- `StageCheckpoint` persists stage progress (block number + optional per‑stage metadata).```369:381:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/types/src/checkpoints.rs
pub struct StageCheckpoint {
    /// The maximum block processed by the stage.
    pub block_number: BlockNumber,
    /// Stage-specific checkpoint. None if stage uses only block-based checkpoints.
    pub stage_checkpoint: Option<StageUnitCheckpoint>,
}
```
- Downloaders can be run as `TaskDownloader` for both headers and bodies.```24:33:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/task.rs
/// A [HeaderDownloader] that drives a spawned [HeaderDownloader] on a spawned task.
pub struct TaskDownloader<H: Sealable> { ... }
```
```25:33:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/task.rs
/// A [BodyDownloader] that drives a spawned [BodyDownloader] on a spawned task.
pub struct TaskDownloader<B: Block> { ... }
```
- Builders for headers and bodies downloaders are present.```1103:1121:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
/// The builder for [`ReverseHeadersDownloader`] with
pub struct ReverseHeadersDownloaderBuilder {
```
```508:523:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/bodies.rs
/// Builder for [`BodiesDownloader`].
pub struct BodiesDownloaderBuilder {
    /// The batch size of non-empty blocks per one request
```
- Default stage set is defined and enumerates the full stage order.```60:83:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/sets.rs
/// A set containing all stages to run a fully syncing instance of reth.
/// This expands to the following series of stages:
/// - [`EraStage`] (optional, for ERA1 import)
/// - [`HeaderStage`]
/// - [`BodyStage`]
/// - [`SenderRecoveryStage`]
/// - [`ExecutionStage`]
/// ...
pub struct DefaultStages<Provider, H, B, E>
```
- Chain state tracking types exist in `chain-state` as described.```16:24:/Users/vicnaum/github/stateless-history-node/reth/crates/chain-state/src/chain_info.rs
/// Tracks the chain info: canonical head, safe block, finalized block.
pub struct ChainInfoTracker<N: NodePrimitives> { ... }
```
```55:67:/Users/vicnaum/github/stateless-history-node/reth/crates/chain-state/src/in_memory.rs
pub(crate) struct InMemoryState<N: NodePrimitives = EthPrimitives> { ... }
```

## Corrections / Caveats
- The claim about “stages interacting with static files” is supported in the pipeline via `move_to_static_files()` and via `StaticFileProviderFactory` usage in stage helpers, but the detailed static‑file flow is outside this question’s scope.

## Actionable Pointers
- Pipeline execution and unwind: `reth/crates/stages/api/src/pipeline/mod.rs`.
- Stage trait contract: `reth/crates/stages/api/src/stage.rs`.
- Default stage ordering: `reth/crates/stages/stages/src/sets.rs`.
- Downloaders: `reth/crates/net/downloaders/src/headers/reverse_headers.rs` and `.../bodies/bodies.rs`.
- Task-based downloader execution: `reth/crates/net/downloaders/src/headers/task.rs` and `.../bodies/task.rs`.
</file>

<file path="questions/Q004-storage-entrypoints.md">
# Q004 - Storage Entry Points (DB, Providers, Static Files)

Status: verified

## Question
Where are the storage entry points (DB open, provider layer, static files), and how do they connect?

## Context Packs
- `spec/reth_repo_context/context-storage.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-storage.xml

Question:
Where are the storage entry points (DB open, provider layer, static files), and how do they connect?
I want a map of: MDBX open/init, db-api traits, provider layer, static files, and how callers use them.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Identify entry points (open env, provider factory, static file builder).
3. Explain read/write transaction flow and where migrations/versioning live.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Storage flow (open -> provider -> read/write -> commit)
4. Key abstractions/types (with file paths)
5. “Where to look next” (top files to read)
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-storage.xml` file, here is the architecture map of Reth’s storage layer.

### 1. Summary
*   **Hybrid Backend Strategy**: Reth uses **MDBX** (key-value) for "hot" data like state and trie nodes, and **Static Files** (immutable "Nippy Jars") for "cold" data like historical headers, transactions, and receipts.
*   **The Coordinator**: `ProviderFactory` is the primary entry point that holds handles to MDBX, the Static File Provider, and an auxiliary RocksDB instance.
*   **Trait-Driven API**: The `reth-storage-api` crate defines chain-agnostic traits (e.g., `HeaderProvider`, `StateWriter`) which are implemented by the `DatabaseProvider`.
*   **Transactional Integrity**: All writes occur through `DatabaseProviderRW`, which coordinates a database transaction and multiple static file writers.
*   **Smart Routing**: The `EitherWriter` and `EitherReader` abstractions decide at runtime whether to access data from MDBX or Static Files based on the node's `StorageSettings`.
*   **Immutable Snapshots**: Historical data is stored in segments (e.g., `Headers`, `Receipts`) within `.dat` and `.idx` files, managed by the `NippyJar` format.
*   **Strict Versioning**: Reth tracks a `database.version` file and prevents startup if the version on disk doesn't match the code.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/storage/db/src/mdbx.rs` | `init_db` | The low-level entry point to open MDBX, create tables, and check the version file. |
| `crates/storage/provider/src/providers/database/mod.rs` | `ProviderFactory` | The central struct used by the rest of the node to spawn read or write providers. |
| `crates/storage/provider/src/providers/database/builder.rs` | `ProviderFactoryBuilder` | A high-level builder to configure MDBX, Static Files, and RocksDB paths in one flow. |
| `crates/storage/provider/src/providers/static_file/manager.rs` | `StaticFileProviderBuilder` | Entry point for configuring the immutable data store (block ranges per file, compression). |

---

### 3. Storage Flow

#### Phase 1: Open & Initialization
1.  **Version Check**: `create_db` calls `check_db_version_file` (`crates/storage/db/src/version.rs`). If `database.version` != `DB_VERSION` (currently 2), it fails.
2.  **MDBX Startup**: `init_db` creates the environment and opens the 250+ potential tables defined in `tables!` macro (`crates/storage/db-api/src/tables/mod.rs`).
3.  **Static File Indexing**: `StaticFileProvider::initialize_index` scans the data directory for existing `.dat` files and maps their block/transaction ranges (`crates/storage/provider/src/providers/static_file/manager.rs`).
4.  **Factory Creation**: These are bundled into a `ProviderFactory`.

#### Phase 2: Provider Creation
*   **Read-Only**: `factory.provider()` returns a `DatabaseProviderRO`, which holds a long-lived MDBX read transaction.
*   **Read-Write**: `factory.provider_rw()` returns a `DatabaseProviderRW`, giving the caller access to `DbTxMut` methods.

#### Phase 3: Writing (The Save Loop)
When the node saves a block, `save_blocks` is called (`crates/storage/provider/src/providers/database/provider.rs`):
1.  **Parallel Write**: It spawns a scoped thread to write headers/transactions to **Static Files** while the main thread writes state changes to **MDBX**.
2.  **Routing**: `EitherWriter::new_receipts` checks `StorageSettings`. If `receipts_in_static_files` is true, it writes to a NippyJar; otherwise, it uses an MDBX cursor.
3.  **Consistency**: `insert_block_mdbx_only` ensures that even if data is in static files, the MDBX indices (hash-to-number) are updated.

#### Phase 4: Commit
The `commit()` function in `DatabaseProvider` (`crates/storage/provider/src/providers/database/provider.rs`) performs a "Two-Phase-ish" commit:
1.  **Static Files**: Calls `finalize()` to flush NippyJar buffers and update the file-system headers.
2.  **MDBX**: Calls `tx.commit()` to finalize the key-value changes.

---

### 4. Key Abstractions / Types

| Type | File Path | Description |
| :--- | :--- | :--- |
| `Table` | `crates/storage/db-api/src/table.rs` | A trait defining how keys (`Encode/Decode`) and values (`Compress/Decompress`) are handled. |
| `DatabaseProvider` | `crates/storage/provider/src/providers/database/provider.rs` | The implementation core that implements dozens of traits for reading/writing. |
| `NippyJar` | `crates/storage/nippy-jar/src/lib.rs` | The format for immutable columnar storage. Supports LZ4 and Zstd compression. |
| `EitherWriter` | `crates/storage/provider/src/either_writer.rs` | A runtime switch between `Database(CURSOR)` and `StaticFile(WRITER)`. |
| `StorageSettings` | `crates/storage/db-api/src/models/metadata.rs` | Persisted config that tells the node whether to act as a "Legacy" or "Edge" node. |

---

### 5. "Where to Look Next"
1.  **`crates/storage/db-api/src/tables/mod.rs`**: To see the full list of MDBX tables (Schema).
2.  **`crates/storage/provider/src/providers/database/provider.rs`**: This is the most important file for understanding how block execution results are actually persisted.
3.  **`crates/storage/nippy-jar/src/writer.rs`**: To understand the internal disk layout of the static files (offsets vs data).
4.  **`crates/storage/provider/src/providers/state/historical.rs`**: To see how Reth reconstructs the state of an account at an arbitrary block in the past by looking at changesets.

---

### 6. Suggested Follow-up Questions
*   "How does the `StaticFileAccountChangesetWalker` avoid loading an entire segment into memory during a range query?"
*   "What happens during an `unwind_trie_state_from` call if the data has already been moved to static files?"
*   "How does MDBX's `handle_slow_readers` function prevent the database from growing indefinitely due to long read transactions?"
*   "How are the `RocksDB` consistency checks used to heal indices that get out of sync with MDBX checkpoints?"

## Verification
- `create_db` checks/creates `database.version`; `init_db` creates tables via `create_and_track_tables_for`.```10:47:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/mdbx.rs
/// Creates a new database at the specified path if it doesn't exist. Does NOT create tables. Check
/// [`init_db`].
pub fn create_db<P: AsRef<Path>>(path: P, args: DatabaseArguments) -> eyre::Result<DatabaseEnv> {
    use crate::version::{check_db_version_file, create_db_version_file, DatabaseVersionError};
    ...
    Ok(DatabaseEnv::open(rpath, DatabaseEnvKind::RW, args)?)
}
...
pub fn init_db_for<P: AsRef<Path>, TS: TableSet>(
    path: P,
    args: DatabaseArguments,
) -> eyre::Result<DatabaseEnv> {
    let mut db = create_db(path, args)?;
    db.create_and_track_tables_for::<TS>()?;
    db.record_client_version(client_version)?;
    Ok(db)
}
```
```8:12:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/version.rs
pub const DB_VERSION_FILE_NAME: &str = "database.version";
pub const DB_VERSION: u64 = 2;
```
- `ProviderFactory` is the central entry point and holds MDBX, static files, storage settings, and RocksDB.```60:79:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/mod.rs
pub struct ProviderFactory<N: NodeTypesWithDB> {
    /// Database instance
    db: N::DB,
    /// Static File Provider
    static_file_provider: StaticFileProvider<N::Primitives>,
    /// Storage configuration settings for this node
    storage_settings: Arc<RwLock<StorageSettings>>,
    /// `RocksDB` provider
    rocksdb_provider: RocksDBProvider,
    ...
}
```
- `ProviderFactoryBuilder` wires DB + static files + RocksDB (read-only path shown).```99:113:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/builder.rs
    pub fn open_read_only(
        self,
        chainspec: Arc<N::ChainSpec>,
        config: impl Into<ReadOnlyConfig>,
    ) -> eyre::Result<ProviderFactory<NodeTypesWithDBAdapter<N, Arc<DatabaseEnv>>>>
    ...
        self.db(Arc::new(open_db_read_only(db_dir, db_args)?))
            .chainspec(chainspec)
            .static_file(StaticFileProvider::read_only(static_files_dir, watch_static_files)?)
            .rocksdb_provider(RocksDBProvider::builder(&rocksdb_dir).with_default_tables().build()?)
            .build_provider_factory()
```
- `StaticFileProviderBuilder` configures blocks-per-file and access mode.```120:180:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/static_file/manager.rs
pub struct StaticFileProviderBuilder<P> {
    access: StaticFileAccess,
    blocks_per_file: StaticFileMap<u64>,
    path: P,
    ...
}
...
pub fn with_blocks_per_file(mut self, blocks_per_file: u64) -> Self {
    for segment in StaticFileSegment::iter() {
        self.blocks_per_file.insert(segment, blocks_per_file);
    }
    self
}
```
- `EitherWriter::new_receipts` uses cached storage settings and prune modes to choose DB vs static files.```109:195:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/either_writer.rs
pub fn new_receipts<P>(
    provider: &'a P,
    block_number: BlockNumber,
) -> ProviderResult<...> {
    if Self::receipts_destination(provider).is_static_file() {
        Ok(EitherWriter::StaticFile(
            provider.get_static_file_writer(block_number, StaticFileSegment::Receipts)?,
        ))
    } else {
        Ok(EitherWriter::Database(
            provider.tx_ref().cursor_write::<tables::Receipts<ReceiptTy<P::Primitives>>>()?,
        ))
    }
}
...
pub fn receipts_destination<P: DBProvider + StorageSettingsCache>(provider: &P) -> EitherWriterDestination {
    let receipts_in_static_files = provider.cached_storage_settings().receipts_in_static_files;
    let prune_modes = provider.prune_modes_ref();
    ...
}
```
- `save_blocks` explicitly parallelizes static‑file writes and MDBX writes.```416:475:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/provider.rs
/// This method parallelizes static file (SF) writes with MDBX writes.
/// The SF thread writes headers, transactions, senders (if SF), and receipts (if SF, Full mode
/// only). The main thread writes MDBX data (indices, state, trie - Full mode only).
pub fn save_blocks(
    &self,
    blocks: Vec<ExecutedBlock<N::Primitives>>,
    save_mode: SaveBlocksMode,
) -> ProviderResult<()> {
    ...
    thread::scope(|s| {
        // SF writes
        let sf_handle = s.spawn(|| {
            sf_provider.write_blocks_data(&blocks, &tx_nums, sf_ctx)?;
            Ok::<_, ProviderError>(start.elapsed())
        });
        ...
        // MDBX writes
        ...
    })
}
```
- `commit` order is conditional: if unwinding, MDBX commits before static files; otherwise static files finalize first, then MDBX commit.```3454:3494:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/provider.rs
/// Commit database transaction, static files, and pending `RocksDB` batches.
fn commit(self) -> ProviderResult<()> {
    if self.static_file_provider.has_unwind_queued() {
        self.tx.commit()?;
        ...
        self.static_file_provider.commit()?;
    } else {
        // Normal path: finalize() will call sync_all() if not already synced
        self.static_file_provider.finalize()?;
        ...
        self.tx.commit()?;
    }
    Ok(())
}
```
- NippyJar supports Zstd/Lz4 compression and uses `.idx` / `.off` / `.conf` sidecar files; data file uses the base path.```54:61:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/nippy-jar/src/lib.rs
const INDEX_FILE_EXTENSION: &str = "idx";
const OFFSETS_FILE_EXTENSION: &str = "off";
pub const CONFIG_FILE_EXTENSION: &str = "conf";
```
```158:167:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/nippy-jar/src/lib.rs
/// Adds [`compression::Zstd`] compression.
pub fn with_zstd(...)
/// Adds [`compression::Lz4`] compression.
pub fn with_lz4(...)
```
- Static file names are `static_file_{segment}_{start}_{end}` (no `.dat` extension).```106:112:/Users/vicnaum/github/stateless-history-node/reth/crates/static-file/types/src/segment.rs
/// Returns the default file name for the provided segment and range.
pub fn filename(&self, block_range: &SegmentRangeInclusive) -> String {
    format!("static_file_{}_{}_{}", self.as_str(), block_range.start(), block_range.end())
}
```
- `initialize_index` scans files using `iter_static_files`, which loads `NippyJar` headers from those filenames.```21:38:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/static_file/mod.rs
/// Given the `static_files` directory path, it returns a list over the existing `static_files`
pub fn iter_static_files(path: &Path) -> Result<SortedStaticFiles, NippyJarError> {
    ...
    for entry in entries {
        if entry.metadata().is_ok_and(|metadata| metadata.is_file()) &&
            let Some((segment, _)) = StaticFileSegment::parse_filename(...)
        {
            let jar = NippyJar::<SegmentHeader>::load(&entry.path())?;
            ...
        }
    }
}
```
- `StorageSettings` flags are defined in `metadata.rs` (receipts in static files, RocksDB toggles, etc.).```13:34:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/models/metadata.rs
pub struct StorageSettings {
    pub receipts_in_static_files: bool,
    pub transaction_senders_in_static_files: bool,
    pub storages_history_in_rocksdb: bool,
    pub transaction_hash_numbers_in_rocksdb: bool,
    pub account_history_in_rocksdb: bool,
    pub account_changesets_in_static_files: bool,
}
```

## Corrections / Caveats
- Static file data files are named `static_file_{segment}_{start}_{end}` (no `.dat` extension). NippyJar uses `.idx` / `.off` / `.conf` sidecar files. The answer’s “.dat and .idx” claim is not supported by the code.
- `StaticFileProviderBuilder` does not expose compression settings directly; compression is configured per `StaticFileSegment` (default LZ4) via `SegmentConfig`.
- Commit order is conditional: for normal operation static files finalize before MDBX commit, but on unwind MDBX commits before static files.

## Actionable Pointers
- MDBX open/versioning: `reth/crates/storage/db/src/mdbx.rs` and `reth/crates/storage/db/src/version.rs`.
- ProviderFactory + provider RW/RO: `reth/crates/storage/provider/src/providers/database/mod.rs`.
- Save flow + commit ordering: `reth/crates/storage/provider/src/providers/database/provider.rs`.
- Static files: `reth/crates/storage/provider/src/providers/static_file/manager.rs` and `reth/crates/storage/db/src/static_file/mod.rs`.
- NippyJar format: `reth/crates/storage/nippy-jar/src/lib.rs` and `.../writer.rs`.
</file>

<file path="questions/Q005-rpc-entrypoints.md">
# Q005 - RPC Entry Points & Server Wiring

Status: verified

## Question
Where are the RPC entry points (module builder, server wiring, transports), and how do they connect?

## Context Packs
- `spec/reth_repo_context/context-rpc.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-rpc.xml

Question:
Where are the RPC entry points (module builder, server wiring, transports), and how do they connect?
I want a map of RPC server construction, module registration, and runtime wiring.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Identify entry points (builder types, server start, CLI wiring).
3. Explain how RPC modules are composed and how requests are routed to providers.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Lifecycle flow (step-by-step)
4. Key abstractions/types (with file paths)
5. “Where to look next” (top files to read)
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-rpc.xml` file, here is the high-level architecture map of Reth’s RPC layer, covering entry points, module construction, and runtime wiring.

### 1. Summary
*   **Trait-First Implementation**: RPC namespaces (like `eth`, `debug`, `trace`) are defined as traits in `reth-rpc-api` and implemented via helper traits in `reth-rpc-eth-api`, allowing for chain-agnostic logic.
*   **Modular Assembly**: The `RpcModuleBuilder` acts as a factory, taking core node components (Provider, Pool, Network) and producing a registry of API handlers.
*   **Transport Agnostic logic**: Handlers are written to be transport-blind; the `TransportRpcModules` container later maps these handlers to specific HTTP, WS, or IPC listeners.
*   **Asynchronous Cache Layer**: To avoid blocking the RPC server with disk I/O, the `EthStateCache` provides a frontend to an async service that manages LRU caches for blocks, headers, and receipts.
*   **The "Eth" Super-Namespace**: The `eth_` namespace is so large it is split into specialized sub-components (Blocks, Transactions, Fees, State, Filter, PubSub) coordinated by the `EthApi` struct.
*   **Middleware-Heavy Pipeline**: Reth wraps `jsonrpsee` servers in layers for JWT authentication, Prometheus metrics, and resource-based rate limiting.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/node/core/src/args/rpc_server.rs` | `RpcServerArgs` | Maps CLI flags (e.g., `--http.api`, `--rpc.gascap`) into internal configuration objects. |
| `crates/rpc/rpc-builder/src/lib.rs` | `RpcModuleBuilder` | The primary orchestrator used to initialize all RPC namespaces with node dependencies. |
| `crates/rpc/rpc-builder/src/lib.rs` | `RpcServerConfig` | Handles the physical binding to sockets (TCP/IPC) and the injection of Tower middleware. |
| `crates/rpc/rpc/src/eth/core.rs` | `EthApi` | The core implementation of the Ethereum JSON-RPC spec, delegating logic to helper traits. |
| `crates/rpc/rpc-builder/src/auth.rs` | `AuthServerConfig` | Specifically handles the Engine API server (port 8551) used by Consensus Clients. |

---

### 3. Lifecycle Flow

#### Step 1: CLI Configuration
CLI arguments are parsed into `RpcServerArgs`. The `transport_rpc_module_config()` function determines which APIs (eth, net, web3, etc.) are exposed on which transport (`crates/node/core/src/args/rpc_server.rs`).

#### Step 2: Builder Initialization
The node creates an `RpcModuleBuilder`, providing it with the `Provider` (database access), `Pool` (mempool), and `Network` (p2p info) handles (`crates/rpc/rpc-builder/src/lib.rs`).

#### Step 3: Module Registration (`RpcRegistryInner`)
The builder populates an `RpcRegistryInner`. When `register_eth()` or `register_debug()` is called, the registry instantiates the implementation (e.g., `EthApi`) and wraps it in a `jsonrpsee::Methods` object (`crates/rpc/rpc-builder/src/lib.rs`).

#### Step 4: Transport Mapping
`create_transport_rpc_modules` takes the registry and splits the methods into three potential `RpcModule` instances: `http`, `ws`, and `ipc`, based on the user's CLI selection (`crates/rpc/rpc-builder/src/lib.rs`).

#### Step 5: Server Startup
`RpcServerConfig::start` is invoked. It:
1.  Binds to the `SocketAddr`.
2.  Adds `AuthLayer` (if JWT is required).
3.  Adds `RpcRequestMetrics` for Prometheus tracking.
4.  Spawns the `jsonrpsee` server task (`crates/rpc/rpc-builder/src/lib.rs`).

#### Step 6: Request Routing (Runtime)
When an `eth_call` arrives:
1.  **Transport**: `jsonrpsee` identifies the method.
2.  **Handler**: `EthApi` receives the request.
3.  **State Lookup**: `EthApi` calls `state_at_block_id`, which queries the `EthStateCache` (`crates/rpc/rpc-eth-api/src/helpers/state.rs`).
4.  **Execution**: Logic is dispatched to the `REVM` (logic outside this pack) via the `SpawnBlocking` trait to ensure EVM execution doesn't stall the async runtime (`crates/rpc/rpc-eth-api/src/helpers/blocking_task.rs`).

---

### 4. Key Abstractions / Types

*   **`EthHandlers<EthApi>`** (`crates/rpc/rpc-builder/src/eth.rs`): A container that groups the standard API, the polling-based `EthFilter`, and the subscription-based `EthPubSub`.
*   **`EthStateCache`** (`crates/rpc/rpc-eth-types/src/cache/mod.rs`): A multi-consumer LRU cache that manages `RecoveredBlock` and `Receipt` objects to reduce database load.
*   **`RpcNodeCore`** (`crates/rpc/rpc-eth-api/src/node.rs`): A trait that abstracts the requirements for a node to support RPC (must provide Primitives, Provider, Pool, and Network).
*   **`PendingBlockKind`** (`crates/rpc/rpc-eth-types/src/builder/config.rs`): An enum (Full/Empty/None) that controls how Reth simulates the "pending" block when users query for it.
*   **`SignersForRpc`** (`crates/rpc/rpc-eth-api/src/helpers/spec.rs`): A thread-safe handle to local developer keys (e.g., `DevSigner`) used for `eth_sign` and `eth_sendTransaction`.

---

### 5. "Where to Look Next"
1.  **`crates/rpc/rpc/src/eth/helpers/call.rs`**: To see how Reth actually executes an `eth_call` or `eth_estimateGas` using overrides.
2.  **`crates/rpc/rpc-builder/src/auth.rs`**: To understand the specific security and routing requirements of the Engine API.
3.  **`crates/rpc/rpc-eth-api/src/helpers/pending_block.rs`**: To see the complex logic for building a local block from the mempool to satisfy "pending" tag queries.
4.  **`crates/rpc/rpc/src/eth/pubsub.rs`**: To see how Reth uses `tokio::sync::broadcast` to push new headers and logs to websocket clients.

---

### 6. Suggested Follow-up Questions
*   "How does the `EthStateCacheService` handle cache invalidation during a chain reorg?"
*   "What is the difference between `spawn_blocking_io` and `spawn_tracing` in the `SpawnBlocking` trait?"
*   "How is the `eth_simulateV1` method implemented to support multi-block simulations?"
*   "How does Reth enforce the `rpc-eth-proof-window` when a user calls `eth_getProof` on a very old block?"

## Verification
- `RpcModuleBuilder` is the main entrypoint; it builds `TransportRpcModules` and the registry wires modules to transports.```6:12:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
//! The [`RpcModuleBuilder`] is the main entrypoint for configuring all reth modules. It takes
//! instances of components required to start the servers, such as provider impls, network and
//! transaction pool. [`RpcModuleBuilder::build`] returns a [`TransportRpcModules`] which contains
//! the transport specific config (what APIs are available via this transport).
//! ...
//! The [`RpcServerConfig`] is used to assemble and start the http server, ws server, ipc servers,
//! it requires the [`TransportRpcModules`] so it can start the servers with the configured modules.
```
```322:403:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
pub fn build<EthApi>(
    self,
    module_config: TransportRpcModuleConfig,
    eth: EthApi,
    engine_events: EventSender<ConsensusEngineEvent<N>>,
) -> TransportRpcModules<()> {
    let mut modules = TransportRpcModules::default();
    if !module_config.is_empty() {
        let TransportRpcModuleConfig { http, ws, ipc, config } = module_config.clone();
        let mut registry = self.into_registry(config.unwrap_or_default(), eth, engine_events);
        modules.config = module_config;
        modules.http = registry.maybe_module(http.as_ref());
        modules.ws = registry.maybe_module(ws.as_ref());
        modules.ipc = registry.maybe_module(ipc.as_ref());
    }
    modules
}
```
- `TransportRpcModules` explicitly holds per‑transport `RpcModule`s (http/ws/ipc).```1629:1639:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
/// Holds installed modules per transport type.
pub struct TransportRpcModules<Context = ()> {
    /// The original config
    config: TransportRpcModuleConfig,
    /// rpcs module for http
    http: Option<RpcModule<Context>>,
    /// rpcs module for ws
    ws: Option<RpcModule<Context>>,
    /// rpcs module for ipc
    ipc: Option<RpcModule<Context>>,
}
```
- `RpcServerConfig::start` wires CORS/JWT/compression and `RpcRequestMetrics` into jsonrpsee servers.```1278:1389:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
fn maybe_jwt_layer(jwt_secret: Option<JwtSecret>) -> Option<AuthLayer<JwtAuthValidator>> { ... }
...
pub async fn start(self, modules: &TransportRpcModules) -> Result<RpcServerHandle, RpcError> {
    ...
    let server = ServerBuilder::new()
        .set_http_middleware(
            tower::ServiceBuilder::new()
                .option_layer(Self::maybe_cors_layer(cors)?)
                .option_layer(Self::maybe_jwt_layer(self.jwt_secret))
                .option_layer(Self::maybe_compression_layer(self.http_disable_compression)),
        )
        .set_rpc_middleware(
            RpcServiceBuilder::default()
                .layer(modules.http.as_ref().map(RpcRequestMetrics::http).unwrap_or_default())
                .layer(self.rpc_middleware.clone()),
        )
        .set_config(config.build())
        .build(http_socket_addr)
        .await?;
    ...
}
```
- `RpcServerArgs` maps CLI flags and transport module selection.```412:458:/Users/vicnaum/github/stateless-history-node/reth/crates/node/core/src/args/rpc_server.rs
#[derive(Debug, Clone, Args, PartialEq, Eq)]
pub struct RpcServerArgs {
    /// Enable the HTTP-RPC server
    pub http: bool,
    /// Rpc Modules to be configured for the HTTP server
    pub http_api: Option<RpcModuleSelection>,
    /// Enable the WS-RPC server
    pub ws: bool,
    /// Rpc Modules to be configured for the WS server
    pub ws_api: Option<RpcModuleSelection>,
    ...
}
```
- `EthApi` is implemented in `crates/rpc/rpc/src/eth/core.rs` and uses `EthStateCache` + `PendingBlockKind`.```54:160:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/core.rs
/// `Eth` API implementation.
pub struct EthApi<N: RpcNodeCore, Rpc: RpcConvert> { ... }
...
pub fn new(
    components: N,
    eth_cache: EthStateCache<N::Primitives>,
    ...
    pending_block_kind: PendingBlockKind,
    ...
) -> Self { ... }
```
- `EthHandlers` groups `EthFilter` and `EthPubSub` with the main `EthApi`.```6:14:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/eth.rs
pub struct EthHandlers<EthApi: EthApiTypes> {
    /// Main `eth_` request handler
    pub api: EthApi,
    /// Polling based filter handler available on all transports
    pub filter: EthFilter<EthApi>,
    /// Handler for subscriptions only available for transports that support it (ws, ipc)
    pub pubsub: EthPubSub<EthApi>,
}
```
- `EthStateCache` is a frontend for an async cache service (spawned task, LRU caches).```58:140:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-eth-types/src/cache/mod.rs
/// Provides async access to cached eth data
/// This is the frontend for the async caching service which manages cached data on a different task.
pub struct EthStateCache<N: NodePrimitives> {
    to_service: UnboundedSender<CacheAction<N::Block, N::Receipt>>,
}
...
pub fn spawn_with<Provider, Tasks>(provider: Provider, config: EthStateCacheConfig, executor: Tasks) -> Self {
    ...
    executor.spawn_critical("eth state cache", Box::pin(service));
    this
}
```
- `RpcNodeCore`, `PendingBlockKind`, and `SignersForRpc` are defined in the claimed files.```24:63:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-eth-api/src/node.rs
pub trait RpcNodeCore: Clone + Send + Sync + Unpin + 'static {
    type Primitives: NodePrimitives;
    type Provider: BlockReaderIdExt<...> + ChainSpecProvider<...> + StateProviderFactory + ...;
    type Pool: TransactionPool<...>;
    type Evm: ConfigureEvm<...>;
    type Network: NetworkInfo + Clone;
    ...
}
```
```21:31:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-eth-types/src/builder/config.rs
pub enum PendingBlockKind {
    Empty,
    None,
    #[default]
    Full,
}
```
```75:79:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-eth-api/src/helpers/spec.rs
pub type SignersForRpc<Provider, Rpc> = parking_lot::RwLock<
    Vec<Box<dyn EthSigner<<Provider as TransactionsProvider>::Transaction, RpcTxReq<Rpc>>>>,
>;
```
- Rate limiter exists as a middleware layer, but it is not automatically applied unless configured as RPC middleware.```1:37:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/rate_limiter.rs
/// Rate limiter for the RPC server.
pub struct RpcRequestRateLimiter { ... }
impl<S> Layer<S> for RpcRequestRateLimiter { ... }
```

## Corrections / Caveats
- “Prometheus metrics” is not explicitly stated; the code uses `RpcRequestMetrics` (metrics layer), but the backend exporter is not implied here.
- Rate limiting is available as a middleware layer but is not applied by default in `RpcServerConfig::start`.
- The Engine API auth server uses `AuthServerConfig`; the exact default port comes from constants, not hardcoded here.

## Actionable Pointers
- RPC server build/start: `reth/crates/rpc/rpc-builder/src/lib.rs` (build, TransportRpcModules, RpcServerConfig::start).
- Engine API auth: `reth/crates/rpc/rpc-builder/src/auth.rs`.
- RPC CLI wiring: `reth/crates/node/core/src/args/rpc_server.rs` and `reth/crates/rpc/rpc-builder/src/config.rs`.
- Eth API core: `reth/crates/rpc/rpc/src/eth/core.rs`.
- Eth cache: `reth/crates/rpc/rpc-eth-types/src/cache/mod.rs`.
</file>

<file path="questions/Q006-mdbx-lifecycle.md">
# Q006 - MDBX Connection Lifecycle (Env, Txns, Versioning)

Status: verified

## Question
How does Reth open MDBX, manage transactions, and enforce DB versioning?

## Context Packs
- `spec/reth_repo_context/context-storage.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-storage.xml

Question:
How does Reth open MDBX, manage transactions, and enforce DB versioning?
I want the lifecycle: create/open env -> check version file -> init tables -> RO/RW tx usage.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Show where the database version is defined and checked.
3. Explain the differences between init_db/open_db/open_db_read_only.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. MDBX lifecycle flow (step-by-step)
4. Key abstractions/types (with file paths)
5. “Where to look next” (top files to read)
6. Suggested follow-up questions
```

## Gemini Answer
Based on the provided `context-storage.xml` file, here is the detailed analysis of how Reth manages its MDBX environment, transactions, and versioning.

### 1. Summary
*   **Version-Locked Startup**: Reth enforces database compatibility by checking a `database.version` file before opening the MDBX environment; a mismatch prevents the node from starting.
*   **Orchestrated Initialization**: The `init_db` function is a "full-service" entry point that creates the directory, initializes the version file, opens the environment, and ensures all 30+ tables are created.
*   **Safety via "Slow Readers" Handling**: Reth implements an MDBX callback to detect and log long-lived read transactions that exceed 10GB of "gap" space, which prevents the database from growing indefinitely on disk.
*   **Trait-Abstracted Transactions**: Transactions are accessed via the `Database` trait, which separates read-only (`DbTx`) and read-write (`DbTxMut`) capabilities to ensure compile-time memory safety.
*   **Unified DBI Management**: Table handles (DBIs) are opened once during initialization and cached in an `Arc<HashMap>` inside the `DatabaseEnv` to avoid the overhead and risks of reopening tables during runtime.
*   **Automatic Metrics**: Transactions are wrapped in a `MetricsHandler` that automatically records open duration, commit latency, and operation counts for Prometheus.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/storage/db/src/version.rs` | `DB_VERSION` | Defines the current database schema version (currently `2`). |
| `crates/storage/db/src/mdbx.rs` | `init_db` | The primary entry point for a fresh node. Creates the DB, checks versioning, and initializes all tables. |
| `crates/storage/db/src/mdbx.rs` | `open_db` | Opens an existing database in Read-Write mode without attempting to create or migrate tables. |
| `crates/storage/db/src/mdbx.rs` | `open_db_read_only` | Opens an existing database in Read-Only mode for observers or secondary processes. |
| `crates/storage/db-api/src/database.rs` | `Database` (Trait) | The interface for spawning transactions (`tx()` and `tx_mut()`). |

---

### 3. MDBX Lifecycle Flow

#### Step 1: Version and Environment Check (`create_db`)
Before MDBX is touched, Reth checks the directory.
*   **Function**: `create_db` in `crates/storage/db/src/mdbx.rs`.
*   **Logic**: If the directory is empty, it calls `create_db_version_file`. If not empty, it calls `check_db_version_file`.
*   **Versioning**: `check_db_version_file` (`crates/storage/db/src/version.rs`) reads `database.version`. If the number inside does not match `DB_VERSION` (fixed at `2`), it returns a `DatabaseVersionError::VersionMismatch`.

#### Step 2: Environment Opening (`DatabaseEnv::open`)
Reth configures MDBX with specific performance and safety flags.
*   **Function**: `DatabaseEnv::open` in `crates/storage/db/src/implementation/mdbx/mod.rs`.
*   **Key Flags**: 
    *   `write_map`: Enabled for RW mode to improve performance.
    *   `no_rdahead`: Readahead is disabled to optimize for random access patterns.
    *   `set_max_readers`: Limited to `32,000` (`DEFAULT_MAX_READERS`).
*   **Slow Reader Protection**: A C-style callback `handle_slow_readers` is registered. If a read transaction causes the "gap" to exceed `MAX_SAFE_READER_SPACE` (10GB), a warning is issued to prevent disk exhaustion.

#### Step 3: Table Initialization (`create_and_track_tables_for`)
Reth ensures the schema exists.
*   **Function**: `_create_tables` in `crates/storage/db/src/implementation/mdbx/mod.rs`.
*   **Logic**: Opens a one-time `RW` transaction. It iterates through `Tables::ALL` (defined in `crates/storage/db-api/src/tables/mod.rs`), calling `tx.create_db` for each. If a table is marked as `DUPSORT` in the macro, the `DatabaseFlags::DUP_SORT` is applied.

#### Step 4: Transaction Management (`DbTx` / `DbTxMut`)
Callers interact with the DB through wrapped transactions.
*   **Read-Only**: `db.tx()` returns `Tx<RO>`. It includes a `MetricsHandler` that logs if the transaction is held longer than 60 seconds (`LONG_TRANSACTION_DURATION`).
*   **Read-Write**: `db.tx_mut()` returns `Tx<RW>`. 
*   **Implementation**: `crates/storage/db/src/implementation/mdbx/tx.rs` implements `get`, `put`, `delete`, and `commit`.

---

### 4. Key Abstractions / Types

*   **`DB_VERSION`** (`crates/storage/db/src/version.rs`): The constant `2` that serves as the hard-coded schema version.
*   **`DatabaseEnv`** (`crates/storage/db/src/implementation/mdbx/mod.rs`): The owner of the MDBX `Environment`. It holds the `dbis` (cached table handles) and the `metrics` handle.
*   **`Tx<K>`** (`crates/storage/db/src/implementation/mdbx/tx.rs`): A wrapper around `reth_libmdbx::Transaction`. It uses the `metrics_handler` to record the lifecycle of every DB interaction.
*   **`Cursor<K, T>`** (`crates/storage/db/src/implementation/mdbx/cursor.rs`): A generic wrapper around MDBX cursors. It handles the `compress_to_buf_or_ref!` logic, which avoids copying uncompressible types (like `B256` hashes) to improve performance.
*   **`TableViewer`** (`crates/storage/db-api/src/tables/mod.rs`): A trait that allows code to operate on tables generically without knowing the specific table type at compile time.

---

### 5. “Where to Look Next”
*   **`crates/storage/db-api/src/tables/mod.rs`**: To see the `tables!` macro that defines every table's key/value types and whether they use `DUPSORT`.
*   **`crates/storage/db/src/implementation/mdbx/cursor.rs`**: To understand how Reth uses internal buffers (`buf`) to minimize allocations during database reads.
*   **`crates/storage/db/src/metrics.rs`**: To see how Reth tracks every single `get`, `put`, and `commit` operation using `FxHashMap` for lock-free metric updates.

---

### 6. Suggested Follow-up Questions
*   "How does the `rp_augment_limit` setting in `DatabaseEnv::open` affect database growth versus lookup speed?"
*   "What is the difference between `SyncMode::Durable` and `SyncMode::SafeNoSync` when configuring `DatabaseArguments`?"
*   "How does the `uncompressable_ref` optimization in `Compress` trait work for fixed-size types like `Address`?"
*   "Can you explain the logic behind `unwind_history_shards` and how it manages sharded history keys during a reorg?"

## Verification
- DB version is defined in `DB_VERSION` and checked via `check_db_version_file` against `database.version`.```8:52:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/version.rs
pub const DB_VERSION_FILE_NAME: &str = "database.version";
pub const DB_VERSION: u64 = 2;
...
pub fn check_db_version_file<P: AsRef<Path>>(db_path: P) -> Result<(), DatabaseVersionError> {
    let version = get_db_version(db_path)?;
    if version != DB_VERSION {
        return Err(DatabaseVersionError::VersionMismatch { version })
    }
    Ok(())
}
```
- `create_db` creates the directory/version file when empty, or checks/creates the version file when present, then opens the environment.```10:28:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/mdbx.rs
pub fn create_db<P: AsRef<Path>>(path: P, args: DatabaseArguments) -> eyre::Result<DatabaseEnv> {
    use crate::version::{check_db_version_file, create_db_version_file, DatabaseVersionError};
    let rpath = path.as_ref();
    if is_database_empty(rpath) {
        reth_fs_util::create_dir_all(rpath)?;
        create_db_version_file(rpath)?;
    } else {
        match check_db_version_file(rpath) {
            Ok(_) => (),
            Err(DatabaseVersionError::MissingFile) => create_db_version_file(rpath)?,
            Err(err) => return Err(err.into()),
        }
    }
    Ok(DatabaseEnv::open(rpath, DatabaseEnvKind::RW, args)?)
}
```
- `init_db`/`init_db_for` wraps `create_db` and creates tables, then records client version.```31:47:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/mdbx.rs
pub fn init_db<P: AsRef<Path>>(path: P, args: DatabaseArguments) -> eyre::Result<DatabaseEnv> {
    init_db_for::<P, Tables>(path, args)
}
pub fn init_db_for<P: AsRef<Path>, TS: TableSet>(
    path: P,
    args: DatabaseArguments,
) -> eyre::Result<DatabaseEnv> {
    let client_version = args.client_version().clone();
    let mut db = create_db(path, args)?;
    db.create_and_track_tables_for::<TS>()?;
    db.record_client_version(client_version)?;
    Ok(db)
}
```
- `open_db_read_only` opens RO without table creation; `open_db` opens RW and records client version.```50:68:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/mdbx.rs
pub fn open_db_read_only(
    path: impl AsRef<Path>,
    args: DatabaseArguments,
) -> eyre::Result<DatabaseEnv> {
    let path = path.as_ref();
    DatabaseEnv::open(path, DatabaseEnvKind::RO, args)
        .with_context(|| format!("Could not open database at path: {}", path.display()))
}
pub fn open_db(path: impl AsRef<Path>, args: DatabaseArguments) -> eyre::Result<DatabaseEnv> {
    let client_version = args.client_version().clone();
    let db = DatabaseEnv::open(path, DatabaseEnvKind::RW, args)?;
    db.record_client_version(client_version)?;
    Ok(db)
}
```
- `DatabaseEnv::open` sets `write_map` for RW, disables readahead, sets max readers, and installs a slow‑reader callback that warns when space exceeds `MAX_SAFE_READER_SPACE` (10GB).```360:433:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/mod.rs
let mut inner_env = Environment::builder();
let mode = match kind {
    DatabaseEnvKind::RO => Mode::ReadOnly,
    DatabaseEnvKind::RW => {
        inner_env.write_map();
        Mode::ReadWrite { sync_mode: args.sync_mode }
    }
};
...
extern "C" fn handle_slow_readers(..., space: usize, ...) -> HandleSlowReadersReturnCode {
    if space > MAX_SAFE_READER_SPACE { ... warn!(...) }
    HandleSlowReadersReturnCode::ProceedWithoutKillingReader
}
inner_env.set_handle_slow_readers(handle_slow_readers);
inner_env.set_flags(EnvironmentFlags { no_rdahead: true, ... });
inner_env.set_max_readers(args.max_readers.unwrap_or(DEFAULT_MAX_READERS));
```
- Table creation uses a single RW tx and applies `DUP_SORT` when the table is dupsort.```542:559:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/mod.rs
let tx = self.inner.begin_rw_txn()?;
for table in TS::tables() {
    let flags = if table.is_dupsort() { DatabaseFlags::DUP_SORT } else { DatabaseFlags::default() };
    let db = tx.create_db(Some(table.name()), flags)?;
    handles.push((table.name(), db.dbi()));
}
tx.commit()?;
```
- `DatabaseEnv` caches DBIs in an `Arc<HashMap>` and warns against dynamic open/close at runtime.```224:234:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/mod.rs
/// Opened DBIs for reuse.
/// Important: Do not manually close these DBIs, like via `mdbx_dbi_close`.
/// More generally, do not dynamically create, re-open, or drop tables at runtime.
dbis: Arc<HashMap<&'static str, ffi::MDBX_dbi>>,
```
- The `Database` trait exposes `tx()` / `tx_mut()`, and `DatabaseEnv` implements them via MDBX RO/RW transactions.```8:23:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/database.rs
pub trait Database: Send + Sync + Debug {
    type TX: DbTx + Send + Sync + Debug + 'static;
    type TXMut: DbTxMut + DbTx + TableImporter + Send + Sync + Debug + 'static;
    fn tx(&self) -> Result<Self::TX, DatabaseError>;
    fn tx_mut(&self) -> Result<Self::TXMut, DatabaseError>;
}
```
```241:259:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/mod.rs
impl Database for DatabaseEnv {
    type TX = tx::Tx<RO>;
    type TXMut = tx::Tx<RW>;
    fn tx(&self) -> Result<Self::TX, DatabaseError> {
        Tx::new(self.inner.begin_ro_txn()?, self.dbis.clone(), self.metrics.clone())?
    }
    fn tx_mut(&self) -> Result<Self::TXMut, DatabaseError> {
        Tx::new(self.inner.begin_rw_txn()?, self.dbis.clone(), self.metrics.clone())?
    }
}
```
- Metrics are recorded only when `DatabaseEnv::with_metrics` is used; `Tx` uses `MetricsHandler` and logs long‑lived transactions (60s).```500:504:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/mod.rs
pub fn with_metrics(mut self) -> Self {
    self.metrics = Some(DatabaseEnvMetrics::new().into());
    self
}
```
```26:62:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/tx.rs
const LONG_TRANSACTION_DURATION: Duration = Duration::from_secs(60);
...
let metrics_handler = env_metrics.map(|env_metrics| {
    let handler = MetricsHandler::<K>::new(inner.id()?, env_metrics);
    handler.env_metrics.record_opened_transaction(handler.transaction_mode());
    handler.log_transaction_opened();
    Ok(handler)
})?;
```
```232:245:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/tx.rs
fn log_backtrace_on_long_read_transaction(&self) {
    if self.transaction_mode().is_read_only() {
        let open_duration = self.start.elapsed();
        if open_duration >= self.long_transaction_duration {
            ... warn!(...);
        }
    }
}
```
- The cursor wrapper uses `compress_to_buf_or_ref!` to avoid copying uncompressable values.```77:87:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/cursor.rs
macro_rules! compress_to_buf_or_ref {
    ($self:expr, $value:expr) => {
        if let Some(value) = $value.uncompressable_ref() {
            Some(value)
        } else {
            $self.buf.clear();
            $value.compress_to_buf(&mut $self.buf);
            None
        }
    };
}
```

## Corrections / Caveats
- The slow‑reader callback only logs and returns `ProceedWithoutKillingReader`; it does not actively prevent growth or abort readers.
- Metrics are optional (`DatabaseEnv::with_metrics`) and are recorded via the `metrics` crate; “Prometheus” is not implied by this code.
- DBIs are cached, but `Tx::get_dbi_raw` can still open a DBI on demand if it was not pre‑tracked.

## Actionable Pointers
- MDBX open + flags + slow readers: `reth/crates/storage/db/src/implementation/mdbx/mod.rs` (`DatabaseEnv::open`).
- Versioning: `reth/crates/storage/db/src/version.rs` and `reth/crates/storage/db/src/mdbx.rs`.
- Table creation + DBI tracking: `reth/crates/storage/db/src/implementation/mdbx/mod.rs` (`create_and_track_tables_for`/`_create_tables`).
- Transaction interfaces: `reth/crates/storage/db-api/src/database.rs` and `reth/crates/storage/db-api/src/transaction.rs`.
- Transaction metrics + long‑txn logging: `reth/crates/storage/db/src/implementation/mdbx/tx.rs` and `reth/crates/storage/db/src/metrics.rs`.
</file>

<file path="questions/Q007-db-abstraction-layer.md">
# Q007 - DB Abstraction Layer (db-api traits + provider layer)

Status: verified

## Question
How do the `db-api` traits (Database, DbTx, DbTxMut, Table, cursor types) interact with the provider layer, and how do providers wrap `DatabaseEnv`?

## Context Packs
- `spec/reth_repo_context/context-storage.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-storage.xml

Question:
How do the `db-api` traits (Database, DbTx, DbTxMut, Table, cursor types) interact with the provider layer, and how do providers wrap `DatabaseEnv`?
I want a map of the abstraction layers: db-api -> DatabaseEnv -> ProviderFactory/DatabaseProvider -> callers.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Identify the key `db-api` traits and where they are implemented for MDBX.
3. Explain how Provider types obtain transactions and expose read/write access.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Layer map (db-api -> env -> providers)
4. Key abstractions/types (with file paths)
5. “Where to look next” (top files to read)
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-storage.xml` pack, here is the architecture map of how the low-level `db-api` traits interact with the high-level Provider layer.

### 1. Summary
*   **Trait-Defined Interface**: The `reth-db-api` crate defines the low-level behavior (`Database`, `DbTx`, `Table`) without knowing about MDBX.
*   **Concrete Implementation**: The `reth-db` crate implements these traits via `DatabaseEnv` (Environment) and `Tx` (Transaction) using `libmdbx`.
*   **Factory Pattern**: `ProviderFactory` is the long-lived entry point. It holds the `DatabaseEnv` and spawns short-lived `DatabaseProvider` instances.
*   **Transaction Wrapping**: A `DatabaseProvider` owns a specific database transaction (`Tx`). It exposes high-level domain methods (e.g., `block_by_hash`) which internally call low-level `tx.get::<Table>()`.
*   **Typed Tables**: Tables are zero-sized marker structs implementing the `Table` trait, tying Rust types (Key/Value) to string table names at compile time.

---

### 2. Entry Points & Key Types

| Component | File Path | Type | Purpose |
| :--- | :--- | :--- | :--- |
| **API Contract** | `crates/storage/db-api/src/database.rs` | `trait Database` | Defines how to spawn transactions (`tx()`, `tx_mut()`). |
| **Transaction API** | `crates/storage/db-api/src/transaction.rs` | `trait DbTx`, `trait DbTxMut` | Defines `get`, `put`, `delete`, and `cursor` operations. |
| **MDBX Env** | `crates/storage/db/src/implementation/mdbx/mod.rs` | `struct DatabaseEnv` | The concrete MDBX implementation of `Database`. |
| **MDBX Tx** | `crates/storage/db/src/implementation/mdbx/tx.rs` | `struct Tx<K>` | The concrete MDBX implementation of `DbTx` / `DbTxMut`. |
| **Factory** | `crates/storage/provider/src/providers/database/mod.rs` | `struct ProviderFactory` | Holds `Arc<DatabaseEnv>`. Creates new Providers. |
| **Provider** | `crates/storage/provider/src/providers/database/provider.rs` | `struct DatabaseProvider` | Wraps a `Tx`. Implements high-level traits (e.g. `HeaderProvider`). |

---

### 3. Layer Map: From Raw Bytes to Block Data

This architecture moves from raw byte storage (bottom) to domain-specific logic (top).

#### Layer 1: The Schema (`db-api`)
Tables are defined using the `tables!` macro. They enforce types for keys and values but contain no data.
*   **File**: `crates/storage/db-api/src/tables/mod.rs`
*   **Concept**: `struct Headers;` implements `Table<Key=BlockNumber, Value=Header>`.

#### Layer 2: The Implementation (`db`)
`DatabaseEnv` manages the open file handle. It implements the `Database` trait.
*   **File**: `crates/storage/db/src/implementation/mdbx/mod.rs`
*   **Logic**: `DatabaseEnv::tx()` calls `self.inner.begin_ro_txn()` (libmdbx) and returns a `Tx` struct.
*   **Transaction**: The `Tx` struct (`crates/storage/db/src/implementation/mdbx/tx.rs`) implements `DbTx`. It knows how to decode bytes into the types defined in Layer 1.

#### Layer 3: The Factory (`provider`)
The `ProviderFactory` is the long-lived service. It owns the `DatabaseEnv`.
*   **File**: `crates/storage/provider/src/providers/database/mod.rs`
*   **Function**: `ProviderFactory::provider()` calls `self.db.tx()?` to create a raw transaction, then wraps it in a `DatabaseProvider`.

#### Layer 4: The Domain Provider (`provider`)
`DatabaseProvider<TX>` is a wrapper around a generic `TX`. It translates "Get Block" requests into "Get from Table X, Y, and Z".
*   **File**: `crates/storage/provider/src/providers/database/provider.rs`
*   **Read Flow**:
    1.  Caller invokes `block_body_indices(num)`.
    2.  `DatabaseProvider` calls `self.tx.get::<tables::BlockBodyIndices>(num)`.
    3.  `DbTx::get` (Layer 2) encodes `num`, queries MDBX, decodes the result, and returns `StoredBlockBodyIndices`.

#### Layer 5: The Callers (RPC/Sync)
Callers do not see `db-api`. They see traits like `BlockReader` or `HeaderProvider` implemented by `DatabaseProvider`.

---

### 4. Key Abstractions & Interactions

#### The `Table` Trait
This is the glue between raw bytes and Rust types.
*   **File**: `crates/storage/db-api/src/table.rs`
*   **Mechanism**: A struct implementing `Table` must define `Key` (implements `Encode`/`Decode`) and `Value` (implements `Compress`/`Decompress`).
*   **Usage**: `tx.get::<tables::Headers>(100)` uses `tables::Headers::Key` to encode `100` into bytes.

#### The `DatabaseProvider` Wrapper
This struct is generic over `TX`.
*   **Definition**: `pub struct DatabaseProvider<TX, N: NodeTypes>`
*   **Implication**: It can wrap a Read-Only transaction (`Tx<RO>`) or a Read-Write transaction (`Tx<RW>`).
*   **Access**: It provides `tx_ref()` to access the underlying cursor/get methods if raw access is needed, but mostly exposes trait implementations (e.g., `impl HeaderProvider for DatabaseProvider`).

#### Cursors
Cursors are exposed via `DbTx`.
*   **File**: `crates/storage/db-api/src/cursor.rs`
*   **Types**: `DbCursorRO` (Read-Only) and `DbCursorRW` (Read-Write).
*   **Interaction**: `DatabaseProvider` uses cursors for range queries. For example, `block_range` uses `cursor_read_collect` to scan headers efficiently.

---

### 5. Where to Look Next

1.  **`crates/storage/db/src/implementation/mdbx/tx.rs`**: To see how `get` and `cursor_read` actually call into the C MDBX library.
2.  **`crates/storage/provider/src/providers/database/provider.rs`**: Specifically the `BlockWriter` implementation (search for `impl BlockWriter`) to see how a high-level "insert block" operation is broken down into multiple `tx.put` calls across different tables.
3.  **`crates/storage/db-api/src/tables/mod.rs`**: To see the full list of tables and their types.

### 6. Suggested Follow-up Questions
*   "How does `DatabaseProvider` handle data that lives in Static Files instead of MDBX (e.g., `header_by_number` delegates to `static_file_provider`)?"
*   "How does the `TableImporter` trait in `db-api` facilitate copying data between transactions during pruning or stage execution?"
*   "How are `DbTxMut` traits used in `StateWriter` to update the Merkle Trie cursor?"

## Verification
- `Database` trait defines `tx()`/`tx_mut()` and is the db‑api contract for transactions.```8:23:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/database.rs
pub trait Database: Send + Sync + Debug {
    type TX: DbTx + Send + Sync + Debug + 'static;
    type TXMut: DbTxMut + DbTx + TableImporter + Send + Sync + Debug + 'static;
    fn tx(&self) -> Result<Self::TX, DatabaseError>;
    fn tx_mut(&self) -> Result<Self::TXMut, DatabaseError>;
}
```
- `DbTx`/`DbTxMut` define read/write operations and cursor types.```20:80:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/transaction.rs
pub trait DbTx: Debug + Send {
    type Cursor<T: Table>: DbCursorRO<T> + Send + Sync;
    type DupCursor<T: DupSort>: DbDupCursorRO<T> + DbCursorRO<T> + Send + Sync;
    fn get<T: Table>(&self, key: T::Key) -> Result<Option<T::Value>, DatabaseError>;
    fn cursor_read<T: Table>(&self) -> Result<Self::Cursor<T>, DatabaseError>;
    ...
}
pub trait DbTxMut: Send {
    type CursorMut<T: Table>: DbCursorRW<T> + DbCursorRO<T> + Send + Sync;
    ...
    fn put<T: Table>(&self, key: T::Key, value: T::Value) -> Result<(), DatabaseError>;
    fn cursor_write<T: Table>(&self) -> Result<Self::CursorMut<T>, DatabaseError>;
}
```
- `Table` ties key/value types to table names and codecs.```79:101:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/table.rs
pub trait Table: Send + Sync + Debug + 'static {
    const NAME: &'static str;
    const DUPSORT: bool;
    type Key: Key;
    type Value: Value;
}
```
- Table marker structs are generated as zero‑sized types with `PhantomData` by `tables!`.```129:156:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/tables/mod.rs
pub struct $name$(<$($generic $( = $default)?),*>)? {
    _private: std::marker::PhantomData<($($($generic,)*)?)>,
}
impl$(<$($generic),*>)? $crate::table::Table for $name$(<$($generic),*>)? { ... }
```
- MDBX implementation wires `DatabaseEnv` -> `Tx` for RO/RW transactions.```241:259:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/mod.rs
impl Database for DatabaseEnv {
    type TX = tx::Tx<RO>;
    type TXMut = tx::Tx<RW>;
    fn tx(&self) -> Result<Self::TX, DatabaseError> { ... begin_ro_txn ... }
    fn tx_mut(&self) -> Result<Self::TXMut, DatabaseError> { ... begin_rw_txn ... }
}
```
- `Tx` implements `DbTx` for any MDBX transaction kind, and `DbTxMut` for RW.```285:312:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/tx.rs
impl<K: TransactionKind> DbTx for Tx<K> {
    fn get<T: Table>(&self, key: T::Key) -> Result<Option<T::Value>, DatabaseError> { ... }
    fn cursor_read<T: Table>(&self) -> Result<Self::Cursor<T>, DatabaseError> { ... }
    ...
}
```
```398:440:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/tx.rs
impl DbTxMut for Tx<RW> {
    fn put<T: Table>(&self, key: T::Key, value: T::Value) -> Result<(), DatabaseError> { ... }
    fn cursor_write<T: Table>(&self) -> Result<Self::CursorMut<T>, DatabaseError> { ... }
}
```
- `ProviderFactory` holds the database and creates providers with `db.tx()` / `db.tx_mut()`.```194:230:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/mod.rs
pub fn provider(&self) -> ProviderResult<DatabaseProviderRO<N::DB, N>> {
    Ok(DatabaseProvider::new(self.db.tx()?, ...))
}
pub fn provider_rw(&self) -> ProviderResult<DatabaseProviderRW<N::DB, N>> {
    Ok(DatabaseProviderRW(DatabaseProvider::new_rw(self.db.tx_mut()?, ...)))
}
```
- `DatabaseProvider` wraps a transaction, exposes `tx_ref`, and uses `tx.get` for table lookups.```167:189:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/provider.rs
pub struct DatabaseProvider<TX, N: NodeTypes> {
    /// Database transaction.
    tx: TX,
    ...
}
```
```868:880:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/provider.rs
pub fn into_tx(self) -> TX { self.tx }
pub const fn tx_mut(&mut self) -> &mut TX { &mut self.tx }
pub const fn tx_ref(&self) -> &TX { &self.tx }
```
```1892:1894:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/provider.rs
fn block_body_indices(&self, num: u64) -> ProviderResult<Option<StoredBlockBodyIndices>> {
    Ok(self.tx.get::<tables::BlockBodyIndices>(num)?)
}
```
- `DatabaseProvider` implements domain traits like `HeaderProvider`/`BlockReader`.```1413:1426:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/provider.rs
impl<TX: DbTx + 'static, N: NodeTypesForProvider> HeaderProvider for DatabaseProvider<TX, N> {
    fn header_by_number(&self, num: BlockNumber) -> ProviderResult<Option<Self::Header>> {
        self.static_file_provider.header_by_number(num)
    }
}
```

## Corrections / Caveats
- Many read paths (e.g., `header_by_number`) delegate to the static file provider; not all reads are direct `tx.get` calls.
- `ProviderFactory` only “holds DatabaseEnv” for node types whose `DB` is `Arc<DatabaseEnv>`; the struct is generic over `N::DB`.

## Actionable Pointers
- db‑api contracts: `reth/crates/storage/db-api/src/database.rs`, `reth/crates/storage/db-api/src/transaction.rs`, `reth/crates/storage/db-api/src/table.rs`.
- MDBX implementation: `reth/crates/storage/db/src/implementation/mdbx/mod.rs` and `.../tx.rs`.
- Provider creation: `reth/crates/storage/provider/src/providers/database/mod.rs`.
- Provider behavior + trait impls: `reth/crates/storage/provider/src/providers/database/provider.rs`.
</file>

<file path="questions/Q008-table-definitions-codecs.md">
# Q008 - Table Definitions + Codecs Mapping

Status: verified

## Question
How are MDBX tables defined and how do key/value codecs map to stored data?

## Context Packs
- `spec/reth_repo_context/context-storage.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-storage.xml

Question:
How are MDBX tables defined and how do key/value codecs map to stored data?
I want a map of the `tables!` macro definitions, the `Table`/`Encode`/`Decode`/`Compress`/`Decompress` traits, and a few concrete examples of tables (Headers, BlockBodyIndices, Transactions, Accounts/Storage).

Requirements:
1. Cite file paths and type names for every major claim.
2. For each example table, specify Key/Value types and where those types live.
3. Explain how encoding/compression is applied when reading/writing (`DbTx::get`, `DbTxMut::put`).
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Table definition mechanism (files + macro + traits)
3. Example tables (key/value + codec types)
4. Read/write path (encode/compress vs decode/decompress)
5. “Where to look next”
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-storage.xml` pack, here is an analysis of how MDBX tables, keys, and values are defined and processed in Reth.

### 1. Summary
*   **Macro-Driven Definitions**: All database tables are defined centrally using the `tables!` macro, which generates zero-sized marker structs implementing the `Table` trait.
*   **Key vs. Value Codecs**: Keys use the `Encode`/`Decode` traits (preserving order for B-Tree indexing), while Values use `Compress`/`Decompress` (optimizing for storage size).
*   **The `Compact` Trait**: Most table values (e.g., Headers, Transactions) use Reth's custom `Compact` codec, which uses bitfields and variable-length encoding to pack structs tighter than RLP.
*   **DUPSORT Tables**: Specific tables (like `PlainStorageState`) are defined as `DUPSORT`, allowing multiple values per key, sorted by a specific `SubKey`.
*   **Strict Type Safety**: The `Table` trait enforces specific Rust types for `Key`, `Value`, and optional `SubKey` at compile time, preventing type mismatches during database transactions.

---

### 2. Table Definition Mechanism

The database schema is defined in a single file using a declarative macro.

#### A. The `tables!` Macro
Located in **`crates/storage/db-api/src/tables/mod.rs`**, this macro generates the structs and trait implementations.

*   **Syntax**:
    ```rust
    tables! {
        table Name {
            type Key = K;
            type Value = V;
            // Optional:
            type SubKey = S;
        }
    }
    ```
*   **Output**: For each entry, it creates a struct (e.g., `pub struct Headers;`) and implements the `Table` trait for it. It also generates an enum `Tables` containing all table variants for runtime iteration.

#### B. The Core Traits
Located in **`crates/storage/db-api/src/table.rs`**:

1.  **`trait Table`**: Defines the metadata.
    *   `const NAME`: The string name used in MDBX (e.g., "Headers").
    *   `type Key`: Must implement `Encode` + `Decode` + `Ord`.
    *   `type Value`: Must implement `Compress` + `Decompress`.
2.  **`trait DupSort`**: Extends `Table` for tables where a key maps to multiple values.
    *   `type SubKey`: Must implement `Encode` + `Decode`.
3.  **`trait Encode` / `trait Decode`**:
    *   Used for **Keys** and **SubKeys**.
    *   Crucial for preservation of ordering (e.g., BigEndian for integers).
4.  **`trait Compress` / `trait Decompress`**:
    *   Used for **Values**.
    *   Usually delegates to the `Compact` trait (bit packing) or simple serialization.

---

### 3. Example Tables

Here are specific examples found in **`crates/storage/db-api/src/tables/mod.rs`**:

#### A. `Headers` (Standard KV Table)
Stores block headers.
*   **Key**: `BlockNumber` (alias for `u64`).
    *   *Codec*: BigEndian bytes. Implemented via `impl_uints!` macro in `crates/storage/db-api/src/models/mod.rs`.
*   **Value**: `Header` (from `alloy_consensus`).
    *   *Codec*: `Compact` trait. Implemented in `crates/storage/codecs/src/alloy/header.rs`. It uses bitflags to skip empty fields (like `base_fee` in pre-London blocks).

#### B. `BlockBodyIndices` (Indices)
Maps block numbers to transaction ranges.
*   **Key**: `BlockNumber` (`u64`).
*   **Value**: `StoredBlockBodyIndices` (`struct { first_tx_num: u64, tx_count: u64 }`).
    *   *Codec*: `Compact`. Defined in `crates/storage/db-api/src/models/blocks.rs`.

#### C. `Transactions` (Canonical Data)
Stores transaction bodies.
*   **Key**: `TxNumber` (alias for `u64`, an incremental global ID).
*   **Value**: `TransactionSigned` (from `reth_ethereum_primitives`).
    *   *Codec*: `Compact`. Implemented in `crates/storage/codecs/src/alloy/transaction/mod.rs`. This handles EIP-1559 vs Legacy vs Blob formats efficiently.

#### D. `PlainStorageState` (DUPSORT Table)
Stores raw storage slots for accounts.
*   **Key**: `Address` (20 bytes).
*   **Value**: `StorageEntry` (`struct { key: B256, value: U256 }`).
*   **SubKey**: `B256` (The storage slot hash).
    *   *Mechanism*: In MDBX, the key is `Address`. The value stored is `StorageEntry`. MDBX sorts multiple entries for the same `Address` by extracting the `SubKey` (the `key` field of `StorageEntry`).

---

### 4. Read/Write Path

The mapping from Rust types to raw bytes occurs in the Transaction and Cursor implementations.

#### A. Writing (`put` / `upsert`)
Flow: `DbTxMut::put` -> `Encode`/`Compress` -> MDBX.

1.  **Interface**: `tx.put::<tables::Headers>(number, header)` is called.
2.  **Implementation**: In **`crates/storage/db/src/implementation/mdbx/tx.rs`**:
    ```rust
    fn put<T: Table>(...) {
        let key = key.encode();         // Trait Encode (Key)
        let value = value.compress();   // Trait Compress (Value)
        // ... pass raw bytes to MDBX ...
    }
    ```
3.  **Optimization**: `Compress::compress_to_buf` is often used to write directly into a reusable buffer to avoid allocation.

#### B. Reading (`get` / `cursor.next`)
Flow: MDBX -> Raw Bytes -> `Decode`/`Decompress`.

1.  **Interface**: `tx.get::<tables::Headers>(number)` or `cursor.next()`.
2.  **Implementation**: In **`crates/storage/db/src/implementation/mdbx/cursor.rs`**:
    ```rust
    pub fn decode<T>(...) -> PairResult<T> {
        // ...
        let key = T::Key::decode(k)?;       // Trait Decode
        let value = T::Value::decompress(v)?; // Trait Decompress
        Ok((key, value))
    }
    ```
3.  **Decompression**:
    *   If the type implements `Compact`, `decompress` calls `Compact::from_compact`.
    *   Ideally, `from_compact` reads directly from the MDBX pointer (zero-copy) for simple types, but complex types (like Headers) usually require deserialization/copying.

---

### 5. "Where to Look Next"

1.  **`crates/storage/db-api/src/tables/mod.rs`**: The authoritative source of truth for the database schema.
2.  **`crates/storage/codecs/src/lib.rs`**: The definition of the `Compact` trait, which explains the bitfield packing strategy.
3.  **`crates/storage/db/src/implementation/mdbx/cursor.rs`**: To see exactly how `DUPSORT` tables (`DbDupCursorRO`) handle subkeys compared to standard tables.
4.  **`crates/storage/db-api/src/models/accounts.rs`**: To see how composite keys (like `BlockNumberAddress` used in `StorageChangeSets`) are manually encoded to preserve sort order.

### 6. Suggested Follow-up Questions
*   "How does the `Compact` derive macro generate the bitflags for a struct like `Header`?"
*   "How are `IntegerList`s encoded in `AccountsHistory` to store lists of block numbers efficiently?"
*   "What is the difference between `PlainAccountState` and `HashedAccounts`, and why do we need both?"

## Verification
- `tables!` generates zero‑sized marker structs (with `PhantomData`) and implements `Table`.```129:156:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/tables/mod.rs
pub struct $name$(<$($generic $( = $default)?),*>)? {
    _private: std::marker::PhantomData<($($($generic,)*)?)>,
}
impl$(<$($generic),*>)? $crate::table::Table for $name$(<$($generic),*>)? { ... }
```
- The `Table` trait enforces `Key` and `Value` bounds; `Key` requires `Encode`/`Decode`, and `Value` requires `Compress`/`Decompress`.```69:101:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/table.rs
pub trait Key: Encode + Decode + Ord + Clone + Serialize + for<'a> Deserialize<'a> {}
pub trait Value: Compress + Decompress + Serialize {}
...
pub trait Table: Send + Sync + Debug + 'static {
    const NAME: &'static str;
    const DUPSORT: bool;
    type Key: Key;
    type Value: Value;
}
```
- Integer keys use big‑endian encoding via `impl_uints!` (e.g., `u64`).```37:58:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/models/mod.rs
impl Encode for u64 {
    type Encoded = [u8; std::mem::size_of::<u64>()];
    fn encode(self) -> Self::Encoded { self.to_be_bytes() }
}
impl Decode for u64 {
    fn decode(value: &[u8]) -> Result<Self, DatabaseError> {
        Ok(u64::from_be_bytes(value.try_into().map_err(|_| DatabaseError::Decode)?))
    }
}
```
- `Headers`, `BlockBodyIndices`, `Transactions`, and `PlainStorageState` tables define their Key/Value (and SubKey for dupsort).```327:399:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/tables/mod.rs
table Headers<H = Header> {
    type Key = BlockNumber;
    type Value = H;
}
table BlockBodyIndices {
    type Key = BlockNumber;
    type Value = StoredBlockBodyIndices;
}
table Transactions<T = TransactionSigned> {
    type Key = TxNumber;
    type Value = T;
}
table PlainStorageState {
    type Key = Address;
    type Value = StorageEntry;
    type SubKey = B256;
}
```
- `StoredBlockBodyIndices` stores `first_tx_num` and `tx_count` and derives `Compact` (via feature).```17:27:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-models/src/blocks.rs
#[cfg_attr(any(test, feature = "reth-codec"), derive(reth_codecs::Compact))]
pub struct StoredBlockBodyIndices {
    pub first_tx_num: TxNumber,
    pub tx_count: NumTransactions,
}
```
- Many value types use `Compact` for `Compress`/`Decompress` via `impl_compression_for_compact!`, including `Header`, `StorageEntry`, `StoredBlockBodyIndices`, and `TransactionSigned`.```195:237:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/models/mod.rs
impl_compression_for_compact!(
    Header,
    StorageEntry,
    StoredBlockBodyIndices,
    TransactionSigned,
    ...
);
```
- `StorageEntry` carries `(key, value)` and exposes `get_subkey()` returning the storage key (B256).```15:41:/Users/vicnaum/github/stateless-history-node/reth/crates/primitives-traits/src/storage.rs
pub struct StorageEntry {
    pub key: B256,
    pub value: U256,
}
impl ValueWithSubKey for StorageEntry {
    type SubKey = B256;
    fn get_subkey(&self) -> Self::SubKey { self.key }
}
```
- `DbTxMut::put` encodes the key and compresses the value before MDBX write.```372:385:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/tx.rs
let key = key.encode();
let value = value.compress();
...
tx.put(self.get_dbi::<T>()?, key.as_ref(), value, flags)
```
- Cursor decode path uses `Decode` + `Decompress` for `(key, value)` pairs.```64:75:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/cursor.rs
pub fn decode<T>(...) -> PairResult<T>
where T: Table, T::Key: Decode, T::Value: Decompress {
    res.map_err(|e| DatabaseError::Read(e.into()))?.map(decoder::<T>).transpose()
}
```
- `Compact` uses bitflag structs and variable‑length integer encoding to minimize bytes (no compression).```28:34:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/codecs/README.md
The general idea behind [`Compact`] is to minimize the number of bytes ...
... uses a companion bitflag struct ...
```
```149:159:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/codecs/src/lib.rs
let leading = self.leading_zeros() as usize / 8;
buf.put_slice(&self.to_be_bytes()[leading..]);
core::mem::size_of::<$name>() - leading
```

## Corrections / Caveats
- The comparison to “tighter than RLP” is not stated in code; `Compact` is described as minimizing bytes, but RLP is not referenced.
- Not all values necessarily use `Compact`; many do via `impl_compression_for_compact!`, but it’s not universal.
- DUPSORT ordering is governed by MDBX’s duplicate value ordering; Reth exposes `SubKey`/`ValueWithSubKey`, but the precise sorting mechanics aren’t explicitly documented here.

## Actionable Pointers
- Table schema: `reth/crates/storage/db-api/src/tables/mod.rs`.
- Key/value traits and codecs: `reth/crates/storage/db-api/src/table.rs` and `reth/crates/storage/db-api/src/models/mod.rs`.
- Compact codec details: `reth/crates/storage/codecs/README.md` and `reth/crates/storage/codecs/src/lib.rs`.
- Read/write encoding flow: `reth/crates/storage/db/src/implementation/mdbx/tx.rs` and `reth/crates/storage/db/src/implementation/mdbx/cursor.rs`.
</file>

<file path="questions/Q009-static-files-nippyjar.md">
# Q009 - Static Files + NippyJar Architecture and Data Split

Status: verified

## Question
How do static files and NippyJar work in Reth, and how is data split between MDBX and static files?

## Context Packs
- `spec/reth_repo_context/context-storage.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-storage.xml

Question:
How do static files and NippyJar work in Reth, and how is data split between MDBX and static files?
I want the architecture, file layout, read/write flow, and how providers decide which backend to use.

Requirements:
1. Cite file paths and type names for every major claim.
2. Identify entry points for static file initialization and read/write paths.
3. Explain how NippyJar segments are named and how indices/offsets/configs are used.
4. Describe how provider code routes reads/writes between MDBX and static files.
5. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Static files lifecycle flow (step-by-step)
4. Data split rules (what goes to MDBX vs static files)
5. Key abstractions/types (with file paths)
6. “Where to look next”
7. Suggested follow-up questions
```

## Gemini Answer
Based on the provided XML pack, here is a detailed architectural map of how Reth uses Static Files (`NippyJar`) alongside MDBX.

### 1. Summary
*   **Hybrid Storage Model**: Reth uses MDBX for "hot" random-access data (state, trie, tip of chain) and Static Files for "cold" immutable data (headers, transactions, receipts).
*   **NippyJar Format**: Custom immutable file format consisting of three parts: Data (`.dat`), Offsets (`.off`), and Configuration (`.conf`). It supports column-based compression (Zstd/LZ4).
*   **Segmented Data**: Data is split into `StaticFileSegment`s (e.g., `Headers`, `Transactions`, `Receipts`), stored in fixed-size block ranges (default 500k blocks per file).
*   **Dual-Threaded Writing**: When saving blocks, `DatabaseProvider` spawns parallel threads: one to write immutable data to Static Files and one to write state/indices to MDBX.
*   **Abstracted Access**: The `EitherReader` and `EitherWriter` enums dynamically route I/O to either MDBX or Static Files based on `StorageSettings` and data availability.
*   **Memory-Mapped Reads**: Reading from static files relies on memory-mapped files (`mmap`) managed by the `DataReader` struct for high-performance zero-copy access.

---

### 2. Entry Points

| File Path | Type | Purpose |
| :--- | :--- | :--- |
| `crates/storage/provider/src/providers/static_file/manager.rs` | `StaticFileProvider` | The central manager that holds indices of all available static files and routes queries to the correct file segment. |
| `crates/storage/nippy-jar/src/lib.rs` | `NippyJar` | The core struct representing the file format, handling serialization of configuration and file path management. |
| `crates/storage/provider/src/either_writer.rs` | `EitherWriter` | The routing logic that decides whether to write data (like Receipts) to MDBX cursors or Static File writers. |
| `crates/storage/provider/src/providers/database/provider.rs` | `DatabaseProvider::save_blocks` | The high-level function that orchestrates writing a block, splitting work between MDBX and Static Files. |
| `crates/static-file/static-file/src/static_file_producer.rs` | `StaticFileProducer` | A background process (not fully detailed in this pack, but referenced) that moves data from MDBX to static files for "legacy" node configurations. |

---

### 3. Static Files Lifecycle & Architecture

#### A. File Layout & Naming
Static files are stored in the `static_files/` directory. A single segment range produces three files.
*   **Naming Convention**: Defined in `crates/static-file/types/src/segment.rs`.
    *   Format: `static_file_{segment}_{start}_{end}`
    *   Example: `static_file_headers_0_499999`
*   **Extensions** (`crates/storage/nippy-jar/src/lib.rs`):
    1.  `.dat`: The compressed raw data (columnar).
    2.  `.off`: Offsets (64-bit integers pointing to row starts in the `.dat` file).
    3.  `.conf`: Bincode-serialized `NippyJar` struct (metadata, compression dictionaries, version).

#### B. The NippyJar Format
*   **Structure**: A `NippyJar` holds a `user_header` (metadata like `SegmentHeader`) and supports multiple columns.
*   **Writing**: Handled by `NippyJarWriter` (`crates/storage/nippy-jar/src/writer.rs`).
    *   It appends data to the `.dat` file.
    *   It keeps offsets in memory and flushes them to `.off` only on `commit()`.
    *   It ensures consistency; if a write is interrupted, it can heal (truncate) the files on the next open using `NippyJarChecker`.
*   **Reading**: Handled by `NippyJarCursor` (`crates/storage/nippy-jar/src/cursor.rs`).
    *   Uses `DataReader` to `mmap` the `.dat` and `.off` files.
    *   Reads the offset, slices the byte range from the mmap, and decompresses it using `Zstd` or `Lz4`.

#### C. Lifecycle Flow (Write -> Read)
1.  **Creation**: `StaticFileProvider::get_writer` creates a `StaticFileProviderRW`.
2.  **Append**: Data is appended (e.g., `append_header`) via `NippyJarWriter`.
3.  **Commit**: calling `commit()` flushes offsets to disk and updates the `indexes` in `StaticFileProviderInner` to make the new data visible.
4.  **Read Query**: A read request (e.g., `header_by_number`) hits `StaticFileProvider`.
5.  **Index Lookup**: `find_fixed_range` calculates which file holds the block.
6.  **Cursor Access**: A `StaticFileJarProvider` creates a cursor over the specific `.dat` file and retrieves the row.

---

### 4. Data Split & Routing (MDBX vs. Static Files)

Reth decides where to put data based on **Storage Settings** and the **Segment** type.

#### The `EitherWriter` Pattern
Located in `crates/storage/provider/src/either_writer.rs`, this enum abstracts the destination.

```rust
pub enum EitherWriter<'a, CURSOR, N> {
    Database(CURSOR),
    StaticFile(StaticFileProviderRWRefMut<'a, N>),
    // ...
}
```

*   **Receipts**: `EitherWriter::new_receipts` checks `StorageSettings`.
    *   If `receipts_in_static_files` is true: Returns `EitherWriter::StaticFile`.
    *   Otherwise: Returns `EitherWriter::Database` (MDBX).
*   **Senders**: Similar logic via `EitherWriter::new_senders`.
*   **Headers & Transactions**: These are almost exclusively written to Static Files in modern Reth configurations (Edge), bypassing MDBX storage logic for bodies entirely in `DatabaseProvider::save_blocks`.

#### The `save_blocks` Flow
Located in `crates/storage/provider/src/providers/database/provider.rs`.
When a block is inserted:
1.  **Parallel Execution**: `thread::scope` is used to spawn threads.
    *   **Thread 1 (Static Files)**: Writes Headers, Transactions, and (optionally) Receipts/Senders to static files.
    *   **Thread 2 (MDBX)**: Writes Block Indices (`HeaderNumbers`, `BlockBodyIndices`) and State (Trie updates, History) to MDBX.
2.  **Commit**: Both providers are committed. MDBX commit creates the transaction boundary; Static File commit flushes offsets.

---

### 5. Key Abstractions & Types

| Type | File Path | Description |
| :--- | :--- | :--- |
| `StaticFileSegment` | `crates/static-file/types/src/segment.rs` | Enum identifying data types (Headers, Transactions, Receipts, AccountChangeSets). |
| `SegmentHeader` | `crates/static-file/types/src/segment.rs` | Metadata stored in the `.conf` file, tracking block ranges and transaction ranges included in the jar. |
| `StaticFileProvider` | `crates/storage/provider/src/providers/static_file/manager.rs` | Thread-safe manager that maintains indices (`blocks_per_file`, `map` of loaded jars) and allows searching for data across files. |
| `NippyJar` | `crates/storage/nippy-jar/src/lib.rs` | The struct representing the file on disk. Handles loading/saving the `.conf` file. |
| `NippyJarCursor` | `crates/storage/nippy-jar/src/cursor.rs` | Reads data. Supports `row_by_number_with_cols` to read specific columns (e.g., just the Header, avoiding the Hash). |
| `StorageSettings` | `crates/storage/db-api/src/models/metadata.rs` | Config struct determining if Receipts/Senders go to MDBX or Static Files. |

---

### 6. "Where to look next"

1.  **`crates/storage/provider/src/providers/static_file/manager.rs`**: Look at `get_segment_provider_for_block`. This is the core routing logic that translates a block number into a specific file handle.
2.  **`crates/storage/nippy-jar/src/compression/zstd.rs`**: To understand how Reth uses Zstd dictionaries. Static files train a dictionary based on the first N rows to compress subsequent rows efficiently.
3.  **`crates/storage/provider/src/providers/database/provider.rs`**: Specifically the `save_blocks` method. This is the critical junction where data forks between the "Hot" path (MDBX) and "Cold" path (Static Files).

---

### 7. Suggested Follow-up Questions

*   "How does `NippyJarWriter` handle crash recovery if the node shuts down while appending to the `.dat` file but before writing the `.off` file?"
*   "How does `StaticFileProvider` handle overlapping block ranges if a static file was generated for blocks 0-1000 but MDBX still has blocks 900-1000?"
*   "How are Zstd dictionaries trained and stored within the `NippyJar` `.conf` file?"
*   "What is the specific role of `SegmentRangeInclusive` in mapping transaction numbers to block numbers within the `StaticFileProvider` indices?"

## Verification
- Static file segments and naming: `StaticFileSegment::filename` uses `static_file_{segment}_{start}_{end}`; default blocks per file is 500,000.```106:112:/Users/vicnaum/github/stateless-history-node/reth/crates/static-file/types/src/segment.rs
pub fn filename(&self, block_range: &SegmentRangeInclusive) -> String {
    format!("static_file_{}_{}_{}", self.as_str(), block_range.start(), block_range.end())
}
```
```27:28:/Users/vicnaum/github/stateless-history-node/reth/crates/static-file/types/src/lib.rs
pub const DEFAULT_BLOCKS_PER_STATIC_FILE: u64 = 500_000;
```
- `NippyJar` supports column-based compression and reads via `mmap`; `DataReader` maps data and offsets files.```82:86:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/nippy-jar/src/lib.rs
/// Data is organized into a columnar format, enabling column-based compression. Data retrieval
/// entails consulting an offset list and fetching the data from file via `mmap`.
```
```332:355:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/nippy-jar/src/lib.rs
pub struct DataReader {
    data_mmap: Mmap,
    offset_mmap: Mmap,
}
...
let data_mmap = unsafe { Mmap::map(&data_file)? };
let offset_mmap = unsafe { Mmap::map(&offset_file)? };
```
- NippyJar file layout uses base data file + `.idx`, `.off`, `.conf` sidecars (no `.dat` extension).```223:241:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/nippy-jar/src/lib.rs
pub fn data_path(&self) -> &Path { self.path.as_ref() }
pub fn index_path(&self) -> PathBuf { self.path.with_extension(INDEX_FILE_EXTENSION) }
pub fn offsets_path(&self) -> PathBuf { self.path.with_extension(OFFSETS_FILE_EXTENSION) }
pub fn config_path(&self) -> PathBuf { self.path.with_extension(CONFIG_FILE_EXTENSION) }
```
- `NippyJarWriter` keeps offsets in memory and flushes/commits them on `commit()`.```348:364:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/nippy-jar/src/writer.rs
/// Commits configuration and offsets to disk. It drains the internal offset list.
pub fn commit(&mut self) -> Result<(), NippyJarError> {
    self.sync_all()?;
    self.finalize()?;
    Ok(())
}
```
- `StaticFileProvider` manages static files and supports per‑segment `blocks_per_file`.```103:156:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/static_file/manager.rs
/// [`StaticFileProvider`] manages all existing [`StaticFileJarProvider`].
...
/// Each static file segment is stored across multiple files, and each of these files contains
/// up to the specified number of blocks of data.
```
- Static file writes are parallel per segment (headers, txs, senders, receipts, account changesets).```646:699:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/static_file/manager.rs
/// Writes all static file data for multiple blocks in parallel per-segment.
...
let h_headers = self.spawn_segment_writer(... StaticFileSegment::Headers, ...);
let h_txs = self.spawn_segment_writer(... StaticFileSegment::Transactions, ...);
let h_senders = ctx.write_senders.then(|| { ... StaticFileSegment::TransactionSenders ... });
let h_receipts = ctx.write_receipts.then(|| { ... StaticFileSegment::Receipts ... });
let h_account_changesets = ctx.write_account_changesets.then(|| { ... StaticFileSegment::AccountChangeSets ... });
```
- `DatabaseProvider::save_blocks` parallelizes static file writes with MDBX writes.```416:490:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/provider.rs
/// This method parallelizes static file (SF) writes with MDBX writes.
...
thread::scope(|s| {
    let sf_handle = s.spawn(|| { sf_provider.write_blocks_data(&blocks, &tx_nums, sf_ctx)?; ... });
    ...
    // MDBX writes happen on the main thread
})
```
- `EitherWriter` routes receipts/senders/account changesets based on `StorageSettings` and prune modes.```109:167:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/either_writer.rs
pub fn new_receipts<P>(...) -> ProviderResult<...> {
    if Self::receipts_destination(provider).is_static_file() { ... } else { ... }
}
pub fn new_senders<P>(...) -> ProviderResult<...> { ... }
pub fn new_account_changesets<P>(...) -> ProviderResult<...> { ... }
```
- `StorageSettings` defines flags for receipts/senders/account changesets in static files.```13:33:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/models/metadata.rs
pub struct StorageSettings {
    pub receipts_in_static_files: bool,
    pub transaction_senders_in_static_files: bool,
    pub account_changesets_in_static_files: bool,
    ...
}
```
- `StaticFileProducer` exists and copies data from DB to static files in parallel; it does not delete DB data.```108:116:/Users/vicnaum/github/stateless-history-node/reth/crates/static-file/static-file/src/static_file_producer.rs
/// For each target ... runs it ... All segments are run in parallel.
///
/// NOTE: it doesn't delete the data from database, and the actual deleting (aka pruning) logic
/// lives in the `prune` crate.
```

## Corrections / Caveats
- NippyJar data files do not use a `.dat` extension; the data file is the base path, with `.idx`, `.off`, `.conf` sidecars.
- The `static_files/` directory path is not hardcoded here; it depends on the configured `StaticFileProvider` path.
- “EitherReader” routing is not shown in this pack; `EitherWriter` is the proven routing logic.
- `StaticFileProducer` is a utility for copying DB data to static files; it is not explicitly described as “legacy-only”.

## Actionable Pointers
- Static file routing + indexing: `reth/crates/storage/provider/src/providers/static_file/manager.rs`.
- Static file segments and naming: `reth/crates/static-file/types/src/segment.rs` and `reth/crates/static-file/types/src/lib.rs`.
- NippyJar format + mmap readers: `reth/crates/storage/nippy-jar/src/lib.rs`, `.../writer.rs`, `.../cursor.rs`.
- Storage settings routing: `reth/crates/storage/provider/src/either_writer.rs`, `reth/crates/storage/db-api/src/models/metadata.rs`.
- Save blocks split: `reth/crates/storage/provider/src/providers/database/provider.rs` (`save_blocks`).
</file>

<file path="questions/Q010-storage-migrations-versioning.md">
# Q010 - Storage Migrations and Versioning

Status: verified

## Question
How are storage migrations and versioning handled beyond `database.version`?

## Context Packs
- `spec/reth_repo_context/context-storage.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-storage.xml

Question:
How are storage migrations and versioning handled beyond `database.version`?
I want to understand how client versions are recorded, how incompatible changes are detected, and how any migration hooks are wired.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Identify where client versions are recorded and how they are queried.
3. Note any explicit migration or compatibility checks beyond `check_db_version_file`.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Versioning/migration flow (step-by-step)
4. Key abstractions/types (with file paths)
5. “Where to look next”
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-storage.xml` pack, here is an analysis of how Reth handles versioning, client tracking, and migrations.

### 1. Summary
*   **Strict Schema Versioning**: The primary gatekeeper is a hard-coded constant (`DB_VERSION = 2`). If the file `database.version` on disk does not match this constant, startup fails immediately. There is no automatic "upcasting" or migration logic visible in this pack for the global schema version.
*   **Additive Schema Updates**: Table creation is idempotent. The `init_db` function calls `create_db` for every table defined in the `tables!` macro. If the code adds a new table, it is created automatically on the next startup; existing tables are opened.
*   **Client Version History**: Reth maintains a dedicated MDBX table (`VersionHistory`) that logs every unique client software version (e.g., "reth/v1.0.0") that has opened the database in Read-Write mode, keyed by timestamp.
*   **Configuration persistence**: "Soft migrations" or behavior changes are handled via `StorageSettings` stored in the `Metadata` table. This allows the node to persist decisions (like "store receipts in static files") across restarts without changing the global schema version.
*   **No Explicit Migration Framework**: There is no SQL-style migration runner (e.g., "apply migration 001, then 002") visible in this pack. Version mismatches result in errors, implying migration is likely manual or handled by external tools/commands not included here.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/storage/db/src/mdbx.rs` | `record_client_version` | The core logic that checks the last recorded client version and appends a new one if it has changed. |
| `crates/storage/db-api/src/tables/mod.rs` | `VersionHistory` | The MDBX table definition storing `(u64, ClientVersion)`. Key is a UNIX timestamp. |
| `crates/storage/db-api/src/models/metadata.rs` | `ClientVersion` | The struct serializing the git SHA, version string, and build timestamp. |
| `crates/storage/db/src/version.rs` | `check_db_version_file` | The "hard gate" that returns `DatabaseVersionError::VersionMismatch` if the disk version != code version. |
| `crates/storage/provider/src/providers/database/mod.rs` | `ProviderFactory::new` | Loads `StorageSettings` from the `Metadata` table to configure how data is written (e.g., Legacy vs. Edge behavior). |

---

### 3. Versioning & Migration Flow

#### A. The Hard Gate (Schema Version)
1.  **Startup**: When `init_db` is called, it invokes `check_db_version_file` (`crates/storage/db/src/version.rs`).
2.  **Check**: It reads the plain-text file `database.version` in the data directory.
3.  **Validate**: It compares the number in the file to `pub const DB_VERSION: u64 = 2`.
4.  **Result**:
    *   If equal: Proceed.
    *   If mismatch: Return `VersionMismatch { version: u64 }`. **There is no code here to resolve this mismatch automatically.**

#### B. The Audit Log (Client Version)
1.  **Open RW**: When `DatabaseEnv::open` or `init_db` creates a Read-Write transaction, it calls `record_client_version` (`crates/storage/db/src/mdbx.rs`).
2.  **Query Last**: It opens a cursor on the `VersionHistory` table and fetches the `last()` entry.
3.  **Compare**: It compares the `ClientVersion` struct passed in `DatabaseArguments` with the one on disk.
4.  **Append**: If they differ, it gets the current system time (UNIX timestamp) and inserts a new row: `tx.upsert(timestamp, &version)`.

#### C. Feature Flags (Storage Settings)
1.  **Initialization**: `ProviderFactory::new` creates a temporary provider to read the `Metadata` table (`crates/storage/provider/src/providers/database/mod.rs`).
2.  **Read**: It looks for the key `storage_settings`.
3.  **Decide**:
    *   If found: It deserializes `StorageSettings` (e.g., `receipts_in_static_files: true`).
    *   If missing: It defaults to `StorageSettings::legacy()`.
4.  **Enforce**: These settings are cached in `ProviderFactory` and dictate whether subsequent writes go to MDBX or Static Files (via `EitherWriter`).

---

### 4. Key Abstractions & Types

| Type | File Path | Description |
| :--- | :--- | :--- |
| `ClientVersion` | `crates/storage/db-api/src/models/metadata.rs` | Contains `version` (String), `commit_sha` (String), `build_timestamp` (String). Implements `Compact` for efficient storage. |
| `VersionHistory` | `crates/storage/db-api/src/tables/mod.rs` | `Table<Key=u64, Value=ClientVersion>`. The key is the timestamp of when the version was first seen. |
| `StorageSettings` | `crates/storage/db-api/src/models/metadata.rs` | Controls granular data layout features (e.g., `account_history_in_rocksdb`). This acts as a soft versioning system for feature flags. |
| `Metadata` | `crates/storage/db-api/src/tables/mod.rs` | A general-purpose `Table<Key=String, Value=Vec<u8>>` used to store `StorageSettings` and potentially other migration flags. |

---

### 5. Where to Look Next
*   **`crates/storage/db/src/mdbx.rs`**: Specifically `record_client_version` to see the exact logic for determining if a new history entry is needed.
*   **`crates/storage/db-api/src/models/metadata.rs`**: To see the specific flags available in `StorageSettings` (Legacy vs Edge) which act as the primary way Reth handles layout evolution without breaking `DB_VERSION`.
*   **Outside this pack**: `reth-node` or `reth-cli` likely contains the logic that handles the `DatabaseVersionError`—potentially prompting the user to run a separate migration command (like `reth db migrate`), as no such logic exists in the storage crates themselves.

---

### 6. Suggested Follow-up Questions
*   "How are the `StorageSettings` initially written to the `Metadata` table during a fresh node initialization?"
*   "What happens if the `VersionHistory` table contains a client version newer than the currently running code?" (The code suggests nothing happens; it just appends the current version, but verifying this behavior is useful).
*   "Since `DB_VERSION` is hardcoded to 2, where is the code that handled the migration from version 1, and is it still in the codebase?"

## Verification
- `DB_VERSION` and `check_db_version_file` enforce the `database.version` check.```9:52:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/version.rs
pub const DB_VERSION: u64 = 2;
pub fn check_db_version_file<P: AsRef<Path>>(db_path: P) -> Result<(), DatabaseVersionError> {
    let version = get_db_version(db_path)?;
    if version != DB_VERSION {
        return Err(DatabaseVersionError::VersionMismatch { version })
    }
    Ok(())
}
```
- `create_db` runs the version check (or creates the file) before opening RW MDBX.```12:28:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/mdbx.rs
if is_database_empty(rpath) { ... create_db_version_file(rpath)?; } else {
    match check_db_version_file(rpath) { ... Err(err) => return Err(err.into()) }
}
Ok(DatabaseEnv::open(rpath, DatabaseEnvKind::RW, args)?)
```
- `init_db` creates tables for the default `Tables` set and records client version.```31:47:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/mdbx.rs
pub fn init_db<P: AsRef<Path>>(path: P, args: DatabaseArguments) -> eyre::Result<DatabaseEnv> {
    init_db_for::<P, Tables>(path, args)
}
...
let mut db = create_db(path, args)?;
db.create_and_track_tables_for::<TS>()?;
db.record_client_version(client_version)?;
```
- `VersionHistory` and `Metadata` tables exist with the claimed key/value types.```535:552:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-api/src/tables/mod.rs
table VersionHistory { type Key = u64; type Value = ClientVersion; }
table Metadata { type Key = String; type Value = Vec<u8>; }
```
- `record_client_version` stores versions keyed by UNIX timestamp seconds if changed.```563:577:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db/src/implementation/mdbx/mod.rs
let last_version = version_cursor.last()?.map(|(_, v)| v);
if Some(&version) != last_version.as_ref() {
    version_cursor.upsert(SystemTime::now().duration_since(UNIX_EPOCH).unwrap_or_default().as_secs(), &version)?;
    tx.commit()?;
}
```
- `ClientVersion` is defined in `db-models` (and re-exported), not `models/metadata.rs`.```10:16:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/db-models/src/client_version.rs
pub struct ClientVersion {
    pub version: String,
    pub git_sha: String,
    pub build_timestamp: String,
}
```
- Storage settings are stored as JSON in `Metadata` under the `storage_settings` key.```8:24:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/storage-api/src/metadata.rs
pub mod keys { pub const STORAGE_SETTINGS: &str = "storage_settings"; }
fn storage_settings(&self) -> ProviderResult<Option<StorageSettings>> {
    self.get_metadata(keys::STORAGE_SETTINGS)?
        .map(|bytes| serde_json::from_slice(&bytes).map_err(ProviderError::other))
        .transpose()
}
```
- `ProviderFactory::new` loads cached storage settings via `storage_settings()` (fallback legacy).```97:113:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/mod.rs
let legacy_settings = StorageSettings::legacy();
let storage_settings = DatabaseProvider::<_, N>::new(...).storage_settings()?.unwrap_or(legacy_settings);
```

## Corrections / Caveats
- `record_client_version` is defined on `DatabaseEnv` in `implementation/mdbx/mod.rs`, not in `mdbx.rs` (though `open_db`/`init_db` call it).
- `ClientVersion` is defined in `db-models` and re-exported via `db-api` models, not in `models/metadata.rs`.
- No explicit migration runner is shown in this pack; that claim remains an inference about absence.

## Actionable Pointers
- Global DB version: `reth/crates/storage/db/src/version.rs`, `reth/crates/storage/db/src/mdbx.rs`.
- Client version history: `reth/crates/storage/db/src/implementation/mdbx/mod.rs`, `reth/crates/storage/db-api/src/tables/mod.rs`.
- Metadata + storage settings: `reth/crates/storage/storage-api/src/metadata.rs`, `reth/crates/storage/db-api/src/tables/mod.rs`.
- Provider cache load: `reth/crates/storage/provider/src/providers/database/mod.rs`.
</file>

<file path="questions/Q011-pipeline-orchestration.md">
# Q011 - Pipeline Orchestration and Checkpoints

Status: verified

## Question
How does the pipeline orchestrate stage execution and checkpoints?

## Context Packs
- `spec/reth_repo_context/context-sync.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-sync.xml

Question:
How does the pipeline orchestrate stage execution and checkpoints?
I want the flow for execute/unwind, checkpoint persistence, and how stages are scheduled/ordered.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Identify the stage entry points and where checkpoints are read/written.
3. Explain execute vs unwind control flow and how progress is persisted.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Lifecycle flow (step-by-step)
4. Key abstractions/types (with file paths)
5. “Where to look next”
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-sync.xml` pack, here is the architectural map of how the Reth pipeline orchestrates execution and checkpoints.

### 1. Summary
*   **Serial Execution Loop**: The pipeline runs stages sequentially (e.g., Headers → Bodies → Execution) in a loop. The output block number of Stage N becomes the target block number for Stage N+1.
*   **Checkpoint-Driven**: Every stage persists a `StageCheckpoint` (block number + stage-specific metrics) to the database upon completing a batch. If the node restarts, it resumes from these checkpoints.
*   **Two-Phase Execution**: Stages implement `poll_execute_ready` (for async work like downloading) and `execute` (for synchronous database writing).
*   **Reverse Unwind**: When a reorg is detected or a validation error occurs, the pipeline pauses execution and iterates through the stages in *reverse order*, calling `unwind` to rollback database changes.
*   **Transaction granularity**: Database commits happen after every successful stage execution batch, ensuring that a stage doesn't save a checkpoint without saving the data associated with it.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/stages/api/src/pipeline/mod.rs` | `Pipeline::run` | The top-level entry point that starts the infinite sync loop. |
| `crates/stages/api/src/pipeline/mod.rs` | `Pipeline::run_loop` | The logic that iterates through the list of stages once. |
| `crates/stages/api/src/pipeline/builder.rs` | `PipelineBuilder` | Constructs the pipeline, defining the order of stages. |
| `crates/stages/api/src/stage.rs` | `trait Stage` | The interface every stage (Headers, Bodies, Execution) must implement. |
| `crates/stages/stages/src/sets.rs` | `DefaultStages` | Defines the standard order of stages for a full node (Headers -> ... -> Finish). |

---

### 3. Lifecycle Flow

#### A. Pipeline Construction
The pipeline is built via `PipelineBuilder`. The order is determined by the order in which `add_stage` is called.
*   **Source**: `crates/stages/stages/src/sets.rs` (See `DefaultStages::builder`).
*   **Standard Order**: `Headers` -> `Bodies` -> `SenderRecovery` -> `Execution` -> `Merkle` -> `Hashing` -> `History` -> `Finish`.

#### B. The Run Loop (`Pipeline::run_loop`)
Located in `crates/stages/api/src/pipeline/mod.rs`, this function iterates through the registered stages list index `0..N`.

1.  **Determine Target**: For the first stage (Headers), the target is usually the network tip (communicated via `tip_tx` channel). For subsequent stages, the target is the block number reached by the *previous* stage.
2.  **Load Checkpoint**: `prev_checkpoint = provider.get_stage_checkpoint(stage_id)`.
3.  **Execute Stage**: Calls `self.execute_stage_to_completion`.

#### C. Stage Execution (`execute_stage_to_completion`)
Located in `crates/stages/api/src/pipeline/mod.rs`.

1.  **Check Readiness**: Calls `stage.poll_execute_ready(cx, input)`.
    *   This allows stages like `HeaderStage` or `BodyStage` to drive async downloaders and buffer data before the DB transaction opens.
2.  **Execute**: Calls `stage.execute(&provider_rw, input)`.
    *   This performs the actual logic (e.g., inserting headers, running EVM).
    *   Returns `ExecOutput { checkpoint, done }`.
3.  **Persist**:
    *   `provider_rw.save_stage_checkpoint(stage_id, checkpoint)` updates the progress.
    *   `provider_rw.commit()` commits the database transaction (data + checkpoint atomically).
4.  **Loop/Next**: If `done` is false, it repeats execution (for batched processing). If `done` is true, the pipeline moves to the next stage.

#### D. Unwind Flow (`Pipeline::unwind`)
Triggered if a stage returns `StageError::Validation`, `StageError::DetachedHead`, or `ControlFlow::Unwind`.

1.  **Reverse Iteration**: The pipeline iterates through stages in **reverse order** (from Finish back to Headers).
2.  **Unwind Logic**: Calls `stage.unwind(&provider_rw, input)`.
    *   The stage deletes data above the `unwind_to` block.
    *   Example: `ExecutionStage` deletes state changes and receipts.
3.  **Update Checkpoint**: The checkpoint is reset to the `unwind_to` block.
4.  **Commit**: The unwind is committed to the DB immediately.

---

### 4. Key Abstractions & Types

| Type | File Path | Description |
| :--- | :--- | :--- |
| `ExecInput` | `crates/stages/api/src/stage.rs` | Input passed to `execute`. Contains `target` (block number to reach) and `checkpoint` (where we are now). |
| `ExecOutput` | `crates/stages/api/src/stage.rs` | Output from `execute`. Contains new `checkpoint` and `done` (bool). |
| `StageCheckpoint` | `crates/stages/types/src/checkpoints.rs` | Struct stored in DB. Contains `block_number` and optional stage-specific metrics (e.g., `entities_processed`). |
| `ControlFlow` | `crates/stages/api/src/pipeline/ctrl.rs` | Enum returned by the loop: `Continue`, `NoProgress`, or `Unwind`. |
| `StageId` | `crates/stages/types/src/id.rs` | Unique identifier for each stage (e.g., `StageId::Headers`), used as the DB key for checkpoints. |

---

### 5. Where to look next

1.  **`crates/stages/api/src/pipeline/mod.rs`**: Read `run_loop` and `execute_stage_to_completion` to see the exact sequence of locking the provider, executing, and committing.
2.  **`crates/stages/stages/src/sets.rs`**: Look at `DefaultStages` to see the authoritative ordering of stages for a standard node.
3.  **`crates/stages/stages/src/stages/headers.rs`**: A good example of a stage that implements `poll_execute_ready` (to download) and `execute` (to write).
4.  **`crates/stages/stages/src/stages/execution.rs`**: Look at how `execute` handles batching (using `thresholds`) and how `unwind` uses the provider to revert state.

---

### 6. Suggested Follow-up Questions
*   "How does the `HeaderStage` specifically use `poll_execute_ready` to bridge the gap between async network downloaders and synchronous database writes?"
*   "How are `PruneCheckpoints` handled differently from standard `StageCheckpoints` in the pipeline?"
*   "What happens to the `StaticFileProducer` during a pipeline unwind?"

## Verification
- `Pipeline::run` loops, calling `run_loop` repeatedly.```182:209:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/mod.rs
pub async fn run(&mut self) -> Result<(), PipelineError> {
    loop {
        let next_action = self.run_loop().await?;
        if next_action.is_unwind() && self.fail_on_unwind {
            return Err(PipelineError::UnexpectedUnwind)
        }
        ...
    }
}
```
- `run_loop` executes stages in order and uses the previous stage checkpoint as the next stage’s target.```223:256:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/mod.rs
for stage_index in 0..self.stages.len() {
    let stage_id = stage.id();
    let next = self.execute_stage_to_completion(previous_stage, stage_index).await?;
    ...
    previous_stage = Some(
        self.provider_factory
            .provider()?
            .get_stage_checkpoint(stage_id)?
            .unwrap_or_default()
            .block_number,
    );
}
```
```420:446:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/mod.rs
let target = self.max_block.or(previous_stage);
let exec_input = ExecInput { target, checkpoint: prev_checkpoint };
```
- `execute_stage_to_completion` calls `poll_execute_ready`, then `execute`, then saves checkpoint and commits.```458:486:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/mod.rs
if let Err(err) = self.stage(stage_index).execute_ready(exec_input).await { ... }
...
match self.stage(stage_index).execute(&provider_rw, exec_input) {
    Ok(out @ ExecOutput { checkpoint, done }) => {
        provider_rw.save_stage_checkpoint(stage_id, checkpoint)?;
        provider_rw.commit()?;
        self.stage(stage_index).post_execute_commit()?;
        ...
    }
}
```
- `unwind` iterates stages in reverse order and commits after each unwind batch.```311:395:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/mod.rs
let unwind_pipeline = self.stages.iter_mut().rev();
...
let output = stage.unwind(&provider_rw, input);
...
provider_rw.save_stage_checkpoint(stage_id, checkpoint)?;
...
provider_rw.commit()?;
stage.post_unwind_commit()?;
```
- `Stage` trait defines `poll_execute_ready`, `execute`, `unwind`, and post‑commit hooks.```241:306:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/stage.rs
pub trait Stage<Provider>: Send {
    fn id(&self) -> StageId;
    fn poll_execute_ready(&mut self, _cx: &mut Context<'_>, _input: ExecInput) -> Poll<Result<(), StageError>> { ... }
    fn execute(&mut self, provider: &Provider, input: ExecInput) -> Result<ExecOutput, StageError>;
    fn unwind(&mut self, provider: &Provider, input: UnwindInput) -> Result<UnwindOutput, StageError>;
    fn post_execute_commit(&mut self) -> Result<(), StageError> { ... }
    fn post_unwind_commit(&mut self) -> Result<(), StageError> { ... }
}
```
- Checkpoint and output types are in `stage.rs`/`checkpoints.rs`.```14:18:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/stage.rs
pub struct ExecInput { pub target: Option<BlockNumber>, pub checkpoint: Option<StageCheckpoint> }
```
```201:208:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/stage.rs
pub struct ExecOutput { pub checkpoint: StageCheckpoint, pub done: bool }
```
```375:379:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/types/src/checkpoints.rs
pub struct StageCheckpoint { pub block_number: BlockNumber, pub stage_checkpoint: Option<StageUnitCheckpoint> }
```
- `ControlFlow` describes Continue/NoProgress/Unwind.```4:28:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/ctrl.rs
pub enum ControlFlow { Unwind { ... }, Continue { ... }, NoProgress { ... } }
```
- Default stage order is assembled via stage sets: Online (Headers/Bodies, optionally Era) → ExecutionStages (SenderRecovery, Execution) → HashingStages (Merkle unwind, AccountHashing, StorageHashing, Merkle execute) → HistoryIndexingStages (TransactionLookup, IndexStorageHistory, IndexAccountHistory) → optional prune sender recovery → PruneStage → Finish.```139:179:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/sets.rs
Self::add_offline_stages(self.online.builder(), ...).add_stage(FinishStage)
```
```256:287:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/sets.rs
builder
    .add_stage(HeaderStage::new(...))
    .add_stage(BodyStage::new(...))
```
```384:393:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/sets.rs
StageSetBuilder::default()
    .add_stage(SenderRecoveryStage::new(...))
    .add_stage(ExecutionStage::from_config(...))
```
```416:431:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/sets.rs
StageSetBuilder::default()
    .add_stage(MerkleStage::default_unwind())
    .add_stage(AccountHashingStage::new(...))
    .add_stage(StorageHashingStage::new(...))
    .add_stage(MerkleStage::new_execution(...))
```
```450:466:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/sets.rs
StageSetBuilder::default()
    .add_stage(TransactionLookupStage::new(...))
    .add_stage(IndexStorageHistoryStage::new(...))
    .add_stage(IndexAccountHistoryStage::new(...))
```
```335:350:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/sets.rs
ExecutionStages::new(...).builder()
    .add_stage_opt(self.prune_modes.sender_recovery.map(|prune_mode| { ... }))
    .add_set(HashingStages { ... })
    .add_set(HistoryIndexingStages { ... })
    .add_stage(PruneStage::new(...))
```

## Corrections / Caveats
- The pipeline’s `ExecInput.target` is `max_block` or the *previous stage’s checkpoint*; the network tip is used by the headers stage itself (via the downloader), not directly by the pipeline target logic.
- Stage ordering can include optional `EraStage` and optional prune sender recovery; it’s not always a fixed linear list.
- Checkpoints are persisted per batch (loop in `execute_stage_to_completion`), not only once at stage end.

## Actionable Pointers
- Pipeline loop and commit/unwind: `reth/crates/stages/api/src/pipeline/mod.rs`.
- Stage contract: `reth/crates/stages/api/src/stage.rs`.
- Stage checkpoints: `reth/crates/stages/types/src/checkpoints.rs`.
- Default stage ordering: `reth/crates/stages/stages/src/sets.rs`.
</file>

<file path="questions/Q012-reverse-headers-downloader.md">
# Q012 - ReverseHeadersDownloader Algorithm

Status: verified

## Question
How does the ReverseHeadersDownloader work?

## Context Packs
- `spec/reth_repo_context/context-sync.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-sync.xml

Question (Q012):
How does the ReverseHeadersDownloader work?
I want the algorithm: starting point selection, range requests, validation strategy, retry/backoff, and how results feed into the Headers stage.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Describe the request/response flow and how ranges are chosen.
3. Explain how errors or invalid headers are handled (retry, ban, or unwind).
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the code in `crates/net/downloaders/src/headers/reverse_headers.rs` and `crates/stages/stages/src/stages/headers.rs`, here is the architectural breakdown of the **ReverseHeadersDownloader**.

### 1. Summary
*   **Reverse Direction**: Downloads headers from the Chain Tip *down* to the Local Head. This strategy secures the chain by anchoring to a trusted tip and verifying lineage backwards, preventing long-range attacks.
*   **Concurrent Fetching**: Issues multiple `GetBlockHeaders` requests in parallel to different peers to fill the gap between the tip and local head.
*   **Response Buffering**: Responses arriving out of order are stored in a binary heap (`buffered_responses`) and processed only when they connect to the currently validated high-block.
*   **Strict Lineage Validation**: Every downloaded header is checked to ensure its `parent_hash` matches the hash of the next header in the chain (descending).
*   **Detached Head Detection**: If the downloaded chain reaches the `local_head` block number but the hashes don't match, it returns a `DetachedHead` error, triggering a pipeline unwind.
*   **Stream Output**: It acts as a Rust `Stream`, yielding batches of validated `SealedHeader`s to the `HeaderStage`.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/net/downloaders/src/headers/reverse_headers.rs` | `ReverseHeadersDownloader` | The core struct managing state, requests, and buffering. |
| `crates/net/downloaders/src/headers/reverse_headers.rs` | `Stream::poll_next` | The execution driver. Polled by the `HeaderStage`. orchestrates request submission and response processing. |
| `crates/net/downloaders/src/headers/reverse_headers.rs` | `update_local_head` | Sets the lower bound (where we are currently). |
| `crates/net/downloaders/src/headers/reverse_headers.rs` | `update_sync_target` | Sets the upper bound (where we want to go). |

---

### 3. Algorithm Flow

#### Step 1: Boundary Initialization
The `HeaderStage` sets the boundaries before polling.
*   **Lower Bound**: `update_local_head` sets the block we currently have on disk.
*   **Upper Bound**: `update_sync_target` sets the target (Hash or Number).
    *   *Refinement*: If the target is a **Hash**, the downloader first issues a single request for that hash to resolve its **Block Number**. Only once the number is known does the parallel download begin (`on_sync_target_outcome`).

#### Step 2: Request Calculation (`next_request`)
The downloader calculates the next range to fetch.
*   It tracks `next_request_block_number` (initially the Tip).
*   It calculates a **Falling** request: `start = next_request_block_number`, `limit = request_limit`.
*   It subtracts `limit` from `next_request_block_number` for the next iteration.
*   **File**: `crates/net/downloaders/src/headers/reverse_headers.rs` -> `calc_next_request`.

#### Step 3: Concurrent Dispatch (`poll_next`)
Inside the `poll_next` loop:
*   It keeps the `in_progress_queue` full up to `concurrent_request_limit`.
*   Requests are sent via the `HeadersClient` (usually `FetchClient` in `reth-network`).
*   **Priority**: Standard requests use `Priority::Normal`; Retries use `Priority::High`.

#### Step 4: Response Handling (`on_headers_outcome`)
When a peer responds:
1.  **Validation**: Checks if the response length matches the request and if the starting block number matches.
2.  **Buffering**: If the response block number is higher than the `next_chain_tip_block_number` (the next one we need to validate), it pushes the response into `buffered_responses` (a Binary Heap).
3.  **Processing**: If the response matches `next_chain_tip_block_number`, it calls `process_next_headers`.

#### Step 5: Lineage Validation (`process_next_headers`)
This is the critical security step.
1.  **Internal Consistency**: Iterates through the batch ensuring `header[i].parent_hash == header[i+1].hash`.
2.  **Connection**: Ensures the highest block in this batch connects to the lowest block of the previous batch (`queued_validated_headers`).
3.  **Local Attachment**: If the batch reaches the `local_head` number:
    *   It checks if `batch_last.parent_hash == local_head.hash`.
    *   If **Mismatch**: Returns `HeadersDownloaderError::DetachedHead`. This signals that the local chain is on a different fork than the network tip.

#### Step 6: Output to Stage
Validated headers are pushed to `queued_validated_headers`.
*   When `queued_validated_headers` reaches `stream_batch_size`, `poll_next` returns `Poll::Ready(Some(batch))`.
*   The `HeaderStage` receives these, writes them to the ETL collector, and eventually commits them to the database.

---

### 4. Error Handling & Retry Logic

*   **Peer Penalization**: If a peer returns a "Likely Bad" response (empty when not expected, or invalid lineage), `penalize_peer` is called, which reports to the `PeersHandle` to lower reputation.
*   **Retry**: `on_headers_error` re-submits the exact same request to the `in_progress_queue` with `Priority::High` to ensure it gets picked up by a different peer quickly.
*   **Channel Closure**: If the network channel closes, the stream terminates (`Poll::Ready(None)`).
*   **Detached Head**: This is a fatal error for the *stage run*, but recoverable for the *node*. It causes the pipeline to unwind the local database to a common ancestor before retrying.

---

### 5. Key Abstractions/Types

| Type | File Path | Description |
| :--- | :--- | :--- |
| `HeadersRequest` | `crates/net/network-p2p/src/headers/client.rs` (via import) | Defines a request: `start` (Hash/Num), `limit`, and `direction` (Falling). |
| `SyncTargetBlock` | `crates/net/downloaders/src/headers/reverse_headers.rs` | Enum (`Hash`, `Number`, `HashAndNumber`) tracking what we are trying to sync to. |
| `OrderedHeadersResponse` | `crates/net/downloaders/src/headers/reverse_headers.rs` | Wrapper for responses stored in the `BinaryHeap` buffer, ordered by block number. |
| `DetachedHead` | `crates/net/network-p2p/src/headers/error.rs` (via import) | The specific error returned when the download stream cannot connect to the local database head. |

---

### 6. Where to Look Next

1.  **`crates/net/downloaders/src/headers/reverse_headers.rs`**: Specifically the `process_next_headers` function. This contains the logic for validating the parent hash chain and detecting detached heads.
2.  **`crates/stages/stages/src/stages/headers.rs`**: Look at `poll_execute_ready`. This is where the stage consumes the downloader stream and handles the `DetachedHead` error by preparing an unwind.
3.  **`crates/net/network/src/fetch/client.rs`**: To see how the `HeadersClient` trait is implemented and how it actually sends the message over the P2P network. (Located in `crates/net/network/src/fetch/client.rs` in the full repo, or referenced in `fetch/mod.rs` in this pack).

---

### 7. Suggested Follow-up Questions
*   "How does the `HeaderStage` handle the `DetachedHead` error returned by the downloader to trigger an unwind?"
*   "How does the `ReverseHeadersDownloader` determine the `request_limit` dynamically based on peer count?"
*   "What happens to the `buffered_responses` heap if the `sync_target` changes mid-download?"

## Verification
- Reverse headers download direction and batch output via `Stream::poll_next`.```63:66:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
/// This downloader downloads in reverse ... falling block numbers.
```
```778:900:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
impl Stream for ReverseHeadersDownloader<H> {
    fn poll_next(...) -> Poll<Option<Self::Item>> { ... }
    ...
    if this.queued_validated_headers.len() >= this.stream_batch_size { ... return Poll::Ready(Some(Ok(next_batch))) }
}
```
- Sync target by hash is resolved via a one-header request, then updates target number.```705:737:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
SyncTarget::Tip(tip) => {
    ...
    self.sync_target_request = Some(self.request_fut(self.get_sync_target_request(tip.into()), Priority::High));
}
```
```374:437:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
let header = headers.swap_remove(0);
let target = SealedHeader::seal_slow(header);
...
let parent_block_number = target.number().saturating_sub(1);
self.on_block_number_update(target.number(), parent_block_number);
self.queued_validated_headers.push(target);
```
- Request range uses `calc_next_request` with falling direction and `limit = min(request_limit, next_request_block_number - local_head)`.```174:183:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
let request = calc_next_request(local_head, self.next_request_block_number, self.request_limit);
self.next_request_block_number -= request.limit;
```
```1235:1251:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
let diff = next_request_block_number - local_head;
let limit = diff.min(request_limit);
HeadersRequest::falling(start.into(), limit)
```
- Concurrency is controlled by `concurrent_request_limit`, derived from peer count.```153:166:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
let num_peers = self.client.num_connected_peers();
let dynamic_target = num_peers * REQUESTS_PER_PEER_MULTIPLIER;
...
max_dynamic.min(self.max_concurrent_requests)
```
- Buffered responses stored in a `BinaryHeap` and validated in order against `next_chain_tip_block_number`.```100:102:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
buffered_responses: BinaryHeap<OrderedHeadersResponse<H::Header>>,
```
```567:579:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
let next_response = this.buffered_responses.peek_mut()?;
match next_block_number.cmp(&self.next_chain_tip_block_number) {
    Ordering::Equal => { ... self.process_next_headers(...) }
    Ordering::Greater => { PeekMut::pop(next_response); }
}
```
- Lineage validation uses `validate_header_download` for each parent/child pair; detached head detected when the last header cannot attach to local head.```261:273:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
if let Err(error) = self.validate(validated_header, &parent) { ... }
```
```281:324:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
if let Some((last_header, head)) = validated.last_mut().zip(self.local_head.as_ref()).filter(|(last, head)| last.number() == head.number() + 1) {
    if let Err(error) = self.consensus.validate_header_against_parent(&*last_header, head) {
        self.reset();
        return Err(HeadersDownloaderError::DetachedHead { ... }.into())
    }
}
```
- Retry on bad responses: penalize peer and re-submit with `Priority::High`.```539:560:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
self.client.report_bad_message(peer_id);
...
self.submit_request(request, Priority::High);
```
- HeaderStage consumes the downloader stream and maps `DetachedHead` to `StageError::DetachedHead`.```192:266:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/stages/headers.rs
match ready!(self.downloader.poll_next_unpin(cx)) {
    Some(Err(HeadersDownloaderError::DetachedHead { local_head, header, error })) => {
        return Poll::Ready(Err(StageError::DetachedHead { ... }))
    }
    ...
}
```

## Corrections / Caveats
- The "long-range attack prevention" rationale is not stated in code; it's an interpretation of reverse syncing.
- The downloader issues concurrent requests, but peer selection is internal to `HeadersClient`; "different peers" is not guaranteed here.
- Detached head is detected when the last downloaded header should attach to `local_head + 1` but fails `validate_header_against_parent`, not by directly comparing hashes at the same block number.
- `EitherWriter`/pipeline unwind behavior is outside this file; the stage only surfaces `StageError::DetachedHead`.

## Actionable Pointers
- Downloader algorithm: `reth/crates/net/downloaders/src/headers/reverse_headers.rs`.
- Header stage integration: `reth/crates/stages/stages/src/stages/headers.rs`.
- Request types/errors: `reth/crates/net/network-p2p/src/headers/client.rs` and `reth/crates/net/network-p2p/src/headers/error.rs` (via imports).
</file>

<file path="questions/Q013-bodies-downloader.md">
# Q013 - Bodies Downloader Batching & Retries

Status: verified

## Question
How does the bodies downloader handle batching, retries, and missing bodies?

## Context Packs
- `spec/reth_repo_context/context-sync.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-sync.xml

Question (Q013):
How does the bodies downloader handle batching, retries, and missing bodies?
I want the request strategy, how bodies are queued/validated, how errors are handled, and how results are delivered to the Bodies stage.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Describe the request/response flow and how batches are formed.
3. Explain how errors, timeouts, or missing bodies are handled.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-sync.xml` pack, here is the architecture of the **Bodies Downloader**.

### 1. Summary
*   **Concurrent Batching**: The downloader fetches bodies in multiple concurrent batches. It reads headers from the database and groups them into requests sent to different peers.
*   **Self-Contained Request Futures**: A single `BodiesRequestFuture` is responsible for fetching a specific batch of bodies. It manages its own retries and handles partial responses internally without alerting the main downloader loop unless a fatal error occurs.
*   **Priority Escalation**: If a request fails or yields a partial response, the future re-requests the missing data with `Priority::High` to ensure fast completion.
*   **Internal Buffering**: Responses from peers arrive out of order. The downloader buffers them in a `BinaryHeap` to ensure they are yielded to the pipeline in strict block number order.
*   **Pre-Execution Validation**: Bodies are validated against their headers (e.g., transaction root, ommers hash) immediately upon download, before being passed to the stage.

---

### 2. Entry Points

| File Path | Type | Purpose |
| :--- | :--- | :--- |
| `crates/net/downloaders/src/bodies/bodies.rs` | `BodiesDownloader` | The main orchestrator struct. It implements `Stream` and manages the queue of requests and the buffer of responses. |
| `crates/net/downloaders/src/bodies/request.rs` | `BodiesRequestFuture` | A Future representing a *single* batch request. It handles the logic for retrying and filling gaps if a peer sends a partial response. |
| `crates/net/downloaders/src/bodies/queue.rs` | `BodiesRequestQueue` | A wrapper around `FuturesUnordered` that manages the collection of active `BodiesRequestFuture` tasks. |

---

### 3. Algorithm Flow

#### Step 1: Request Generation (`next_headers_request`)
The `BodiesDownloader` calculates the next range of blocks to fetch.
*   It queries the database for headers via `query_headers`.
*   It groups headers into a batch based on `request_limit` (default 200) and `stream_batch_size`.
*   **File**: `crates/net/downloaders/src/bodies/bodies.rs`

#### Step 2: Dispatch (`submit_request`)
The batch of headers is wrapped in a `BodiesRequestFuture` and pushed to the `in_progress_queue`.
*   The future initiates a P2P request via the `BodiesClient` (network interface).
*   **File**: `crates/net/downloaders/src/bodies/bodies.rs`

#### Step 3: Response & Partial Handling (`BodiesRequestFuture`)
Inside the `BodiesRequestFuture`, the network response is polled.
*   **Partial Responses**: If a peer returns fewer bodies than requested, `try_buffer_blocks` saves the valid ones. The future then calculates the remaining missing headers (`next_request`) and issues a *new* network request for the remainder immediately, staying in `Poll::Pending` state.
*   **Retries**: If a request fails (timeout, empty response), `on_error` is called. This penalizes the peer and resubmits the request with `Priority::High`.
*   **File**: `crates/net/downloaders/src/bodies/request.rs`

#### Step 4: Validation (`try_buffer_blocks`)
As bodies arrive, the future validates them against the headers using `consensus.validate_block_pre_execution`.
*   Checks calculated transaction root vs header root.
*   Checks ommers hash vs header ommers hash.
*   If invalid, the peer is penalized, and the specific header is put back into the pending queue for retry.
*   **File**: `crates/net/downloaders/src/bodies/request.rs`

#### Step 5: Ordering & Buffering (`poll_next`)
When a `BodiesRequestFuture` finally completes (has all bodies for its batch):
*   It returns `Vec<BlockResponse<B>>` to the main downloader.
*   The downloader pushes this into `buffered_responses` (a `BinaryHeap`).
*   `try_next_buffered` checks if the heap contains the *next expected block number*. If so, it moves the data to `queued_bodies`.
*   **File**: `crates/net/downloaders/src/bodies/bodies.rs`

#### Step 6: Yielding to Stage
The stream yields batches from `queued_bodies` to the `BodyStage` for database insertion.

---

### 4. Key Abstractions/Types

| Type | File Path | Description |
| :--- | :--- | :--- |
| `BodiesRequestFuture` | `crates/net/downloaders/src/bodies/request.rs` | The "worker" that refuses to die until it has fetched every body in its assigned batch of headers. |
| `OrderedBodiesResponse` | `crates/net/downloaders/src/bodies/bodies.rs` | A wrapper around a vector of bodies that implements `Ord` (reverse block number) for the `BinaryHeap`. |
| `BlockResponse<B>` | `crates/net/network-p2p/src/bodies/response.rs` (Ref) | Enum: `Full(SealedBlock)` or `Empty(SealedHeader)`. Allows handling empty blocks (no txs/ommers) gracefully. |
| `Priority` | `crates/net/network-p2p/src/priority.rs` (Ref) | `Normal` for initial requests, `High` for retries to ensure the pipeline doesn't stall on a single missing block. |

---

### 5. Where to Look Next
1.  **`crates/net/downloaders/src/bodies/request.rs`**: Specifically the `poll` method and `on_block_response`. This contains the complex logic for handling "soft limits" (receiving 2MB of data but not all requested blocks) and retrying partials.
2.  **`crates/net/downloaders/src/bodies/bodies.rs`**: The `poll_next` method demonstrates how the downloader balances submitting new requests vs. processing buffered responses to manage memory pressure (`can_submit_new_request`).

---

### 6. Suggested Follow-up Questions
*   "How does the `BodiesRequestFuture` determine if a response is 'empty' versus 'partial' regarding the Soft Response Limit?"
*   "What happens to the `in_progress_queue` if the `BodiesDownloader` receives a `clear()` command during a pipeline unwind?"
*   "How does `consensus.validate_block_pre_execution` differ from the full execution validation done in the Execution stage?"

## Verification
- Bodies downloader batches requests from DB headers and limits non-empty headers per request.```31:132:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/bodies.rs
/// Downloads bodies in batches.
/// All blocks in a batch are fetched at the same time.
...
fn next_headers_request(&self) -> DownloadResult<Option<Vec<SealedHeader<Provider::Header>>>> { ... }
...
fn query_headers(..., max_non_empty: u64) -> DownloadResult<Option<Vec<SealedHeader<B::Header>>>> { ... }
```
- Default request limit is 200 non-empty headers per request, configurable via `BodiesDownloaderBuilder`.```508:543:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/bodies.rs
pub struct BodiesDownloaderBuilder { ... pub request_limit: u64, ... }
impl Default for BodiesDownloaderBuilder {
    fn default() -> Self {
        Self { request_limit: 200, stream_batch_size: 1_000, ... }
    }
}
```
- Requests are queued as `BodiesRequestFuture` instances in a `FuturesUnordered` wrapper.```19:89:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/queue.rs
pub(crate) struct BodiesRequestQueue { inner: FuturesUnordered<BodiesRequestFuture<...>>, ... }
...
pub(crate) fn push_new_request(...) { ... BodiesRequestFuture::new(...).with_headers(request) }
```
- `BodiesRequestFuture` re-issues requests until pending headers are exhausted, and retries with `Priority::High` after errors/partial responses.```20:155:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/request.rs
If the response arrived with insufficient number of bodies, the future will issue another request until all bodies are collected.
...
if let Some(req) = self.next_request() { self.submit_request(req, Priority::High); }
...
fn on_error(...) { ... self.submit_request(self.next_request().expect(...), Priority::High); }
```
- Pre-execution body validation happens in `try_buffer_blocks` via `validate_block_pre_execution`.```159:199:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/request.rs
let block = SealedBlock::from_sealed_parts(next_header, next_body);
if let Err(error) = self.consensus.validate_block_pre_execution(&block) { ... }
```
- Out-of-order responses are buffered in a `BinaryHeap` and drained in order via `try_next_buffered`.```64:252:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/bodies.rs
buffered_responses: BinaryHeap<OrderedBodiesResponse<B>>,
...
fn try_next_buffered(&mut self) -> Option<Vec<BlockResponse<B>>> { ... }
```
- Downloader yields batches to the stage, and `BodyStage` stores them via `append_block_bodies`.```369:441:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/bodies.rs
fn poll_next(...) -> Poll<Option<Self::Item>> { ... }
```
```159:206:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/stages/bodies.rs
self.downloader.set_download_range(input.next_block_range())?;
...
provider.append_block_bodies(...)
```
- `BlockResponse` is `Full(SealedBlock)` or `Empty(SealedHeader)`.```4:11:/Users/vicnaum/github/stateless-history-node/reth/crates/net/p2p/src/bodies/response.rs
pub enum BlockResponse<B: Block> {
    Full(SealedBlock<B>),
    Empty(SealedHeader<B::Header>),
}
```

## Corrections / Caveats
- Peer selection is inside `BodiesClient`; the downloader does not guarantee "different peers."
- "Strict block number order" is enforced by buffering + `try_next_buffered`, but ordering is based on response ranges and expected next block, not a full global sort.
- The internal details of `validate_block_pre_execution` (tx root/ommers checks) are outside this pack.
- "Soft response limit (2MB)" appears only in a TODO comment, not in enforcement logic.

## Actionable Pointers
- Downloader orchestration: `reth/crates/net/downloaders/src/bodies/bodies.rs`.
- Per-request logic, retries, validation: `reth/crates/net/downloaders/src/bodies/request.rs`.
- Stage integration: `reth/crates/stages/stages/src/stages/bodies.rs`.
</file>

<file path="questions/Q014-missing-ranges-holes.md">
# Q014 - Missing Ranges (Holes) Representation

Status: verified

## Question
How are missing ranges/holes represented and handled during sync?

## Context Packs
- `spec/reth_repo_context/context-sync.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-sync.xml

Question (Q014):
How are missing ranges/holes represented and handled during sync?
I want the data structures, how gaps are detected, how they are persisted (if at all), and how they drive retries or stage logic.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Explain how gaps are tracked and updated over time.
3. Describe how gaps affect downloader behavior and pipeline stages.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-sync.xml` pack, here is the analysis of how missing ranges (holes) are represented and handled in Reth.

### 1. Summary
*   **Implicit Representation**: Reth does not persist explicit "gap" or "hole" data structures in the database. Instead, a "hole" is implicitly defined as the range between a stage's **Persistent Checkpoint** (local head) and the **Sync Target** (network tip).
*   **Sequential Filling**: The pipeline forces stages to run sequentially. A stage cannot process range `N..M` until the previous stage has successfully completed that range. This ensures holes only exist at the *tip* of the chain, never in the middle.
*   **Header Gap Strategy**: The `HeaderStage` identifies the gap between the local database head and the network tip using the `HeaderSyncGap` struct. It fills this gap by downloading in **reverse** (from tip down to local head) to cryptographically guarantee the chain links before committing.
*   **Transient Gaps (Network)**: During body downloads, if a peer returns a partial response (creating a transient hole in a requested batch), the `BodiesRequestFuture` detects the missing items and immediately issues a high-priority re-request to fill the gap before yielding results to the stage.
*   **Inconsistency Detection**: The `ensure_consistency` logic checks for "gaps" between the database (MDBX) and Static Files (e.g., if a write was interrupted). If the database is ahead of static files (missing static data), it forces an unwind to align them.

---

### 2. Entry Points

| File Path | Type | Purpose |
| :--- | :--- | :--- |
| `crates/stages/stages/src/stages/headers.rs` | `HeaderSyncGap` | In-memory struct representing the range `(local_head..target]` that needs to be filled. |
| `crates/net/downloaders/src/headers/reverse_headers.rs` | `ReverseHeadersDownloader` | The engine that closes the header gap by fetching backwards from the target to the local head. |
| `crates/net/downloaders/src/bodies/request.rs` | `BodiesRequestFuture` | Handles transient holes in network responses by re-requesting missing bodies within a batch. |
| `crates/stages/api/src/stage.rs` | `ExecInput` | Defines the current working range (`checkpoint..target`) for any stage. |
| `crates/stages/stages/src/stages/bodies.rs` | `ensure_consistency` | Detects and handles storage holes/inconsistencies between MDBX and Static Files. |

---

### 3. Algorithm Flow: Handling the "Big Hole" (Sync)

The primary "hole" is the sync gap. Reth handles this differently for Headers vs. Bodies.

#### A. Headers: The Reverse Fill (Closing the Gap)
1.  **Detection**: `HeaderStage::poll_execute_ready` compares the `input.checkpoint()` (Local Head) with `self.tip` (Network Tip). It creates a `HeaderSyncGap` struct.
2.  **Assignment**: The gap is passed to the `ReverseHeadersDownloader` via `update_sync_gap`.
3.  **Reverse Fetch**: The downloader requests headers starting from the **Target** moving backwards.
    *   *Why?* This ensures the new chain is valid and rooted in a trusted tip, preventing long-range attacks.
4.  **Connection Check**: When the downloader reaches the `local_head` block number, it verifies if `downloaded_header.parent_hash == local_head.hash`.
    *   **Match**: The hole is bridged. Headers are yielded to the stage.
    *   **Mismatch**: A `StageError::DetachedHead` is returned.
5.  **Resolution**: If `DetachedHead` occurs, the pipeline triggers an **Unwind**. The checkpoint is moved *backwards* (widening the hole) to find a common ancestor, effectively rejecting the local fork.

#### B. Bodies: The Forward Fill (Populating Content)
1.  **Range**: `BodyStage` calculates the range `checkpoint + 1 .. target`.
2.  **Request**: It calls `downloader.set_download_range(range)`.
3.  **Batching**: `BodiesDownloader` reads headers from the DB for that range and issues requests.
4.  **Partial Responses**:
    *   If a peer returns 50 bodies but 100 were requested (a "hole" in the batch), `BodiesRequestFuture::try_buffer_blocks` buffers the 50.
    *   It immediately calculates the remaining hashes and calls `submit_request` with `Priority::High` to fill the hole.
    *   The stage never sees the hole; it only receives the full batch once the future completes.

---

### 4. Key Abstractions & Data Types

*   **`HeaderSyncGap`** (`crates/stages/stages/src/stages/headers.rs`):
    *   Contains `local_head` (SealedHeader) and `target` (SyncTarget).
    *   Represents the "work to be done" for the header stage.
*   **`StageCheckpoint`** (`crates/stages/types/src/checkpoints.rs`):
    *   Persisted in the database.
    *   The `block_number` field marks the *end* of valid data. The "hole" implicitly starts at `block_number + 1`.
*   **`SyncTarget`** (`crates/net/network-p2p` - re-exported in `headers/mod.rs`):
    *   Enum: `Tip(B256)`, `Gap(BlockWithParent)`, or `TipNum(u64)`.
    *   Defines the upper bound of the hole.
*   **`DetachedHead`** (`crates/stages/api/src/error.rs`):
    *   Error variant indicating the "hole" could not be cleanly filled because the edges (Local Head and Downloaded Chain) do not link.

---

### 5. Where to Look Next

1.  **`crates/net/downloaders/src/headers/reverse_headers.rs`**: Look at `process_next_headers`. This is where the logic verifies if the downloaded stream connects to the `local_head`, effectively checking if the "hole" is perfectly filled.
2.  **`crates/stages/stages/src/stages/execution.rs`**: Look at `ensure_consistency`. This shows how Reth handles "holes" caused by data corruption or partial writes (e.g., Static Files having fewer blocks than the Database).
3.  **`crates/net/downloaders/src/bodies/request.rs`**: Look at `on_block_response`. This shows the retry logic for filling transient holes when a peer sends incomplete data.

---

### 6. Suggested Follow-up Questions
*   "How does the `HeaderStage` determine the `SyncTarget` if the network tip is unknown or conflicting?"
*   "What happens to the `BodyStage` if it encounters a header in the database that references a transaction not found in the network (data availability gap)?"
*   "How does `ensure_consistency` in `ExecutionStage` decide whether to unwind the database or the static files when their heights mismatch?"

## Verification
- A header "gap" is explicitly represented as `HeaderSyncGap { local_head, target }`, and the target is a `SyncTarget` (Tip, Gap, TipNum).```45:101:/Users/vicnaum/github/stateless-history-node/reth/crates/net/p2p/src/headers/downloader.rs
pub enum SyncTarget { Tip(B256), Gap(BlockWithParent), TipNum(u64) }
pub struct HeaderSyncGap<H: Sealable = Header> { pub local_head: SealedHeader<H>, pub target: SyncTarget }
```
- `HeaderStage::poll_execute_ready` builds the gap from the local head and network tip, and passes it to the downloader via `update_sync_gap`.```204:230:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/stages/headers.rs
let local_head = self.provider.local_tip_header(current_checkpoint.block_number)?;
let target = SyncTarget::Tip(*self.tip.borrow());
let gap = HeaderSyncGap { local_head, target };
...
self.downloader.update_sync_gap(gap.local_head, gap.target);
```
- Per-stage "gap" is represented by `ExecInput` checkpoint/target and `next_block_range()` which starts at `checkpoint.block_number + 1` and ends at `target`.```12:84:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/stage.rs
pub struct ExecInput { pub target: Option<BlockNumber>, pub checkpoint: Option<StageCheckpoint> }
...
let start = current_block.block_number + 1;
let target = self.target();
let end = min(target, current_block.block_number.saturating_add(threshold));
```
- Bodies re-requests missing items within a batch by resubmitting with `Priority::High` whenever pending headers remain after a response.```116:155:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/request.rs
// Buffer block responses
self.try_buffer_blocks(bodies)?;
// Submit next request if any
if let Some(req) = self.next_request() { self.submit_request(req, Priority::High); }
```
- `ensure_consistency` checks static file vs DB transaction heights and returns a missing-static-data error when static files are behind.```70:137:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/stages/bodies.rs
match next_static_file_tx_num.cmp(&next_tx_num) {
    Ordering::Greater => { ... prune_transactions ... }
    Ordering::Less => { return Err(missing_static_data_error(..., StaticFileSegment::Transactions)?) }
    Ordering::Equal => {}
}
```
- Detached head is surfaced as a `StageError::DetachedHead` when the downloader cannot attach to the local head.```30:64:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/error.rs
DetachedHead { local_head: Box<BlockWithParent>, header: Box<BlockWithParent>, error: Box<ConsensusError> }
```

## Corrections / Caveats
- `HeaderSyncGap` is defined in `reth/crates/net/p2p/src/headers/downloader.rs`, not in `headers.rs`.
- "Holes only exist at the tip" is not guaranteed; range resets, unwinds, or static file mismatches can create gaps outside the tip.
- The long-range attack rationale is not stated in code; it is an interpretation of reverse syncing.
- `ensure_consistency` referenced here is in `bodies.rs`, not `execution.rs`.
- `StageCheckpoint` is a struct used for progress tracking; persistence is handled elsewhere (e.g., pipeline checkpointing), not shown in this file set.

## Actionable Pointers
- Header gap representation: `reth/crates/net/p2p/src/headers/downloader.rs`.
- Header gap detection and downloader wiring: `reth/crates/stages/stages/src/stages/headers.rs`.
- Bodies transient gap handling: `reth/crates/net/downloaders/src/bodies/request.rs`.
- Static file consistency check: `reth/crates/stages/stages/src/stages/bodies.rs`.
</file>

<file path="questions/Q015-peers-manager-reputation.md">
# Q015 - PeersManager and Reputation

Status: verified

## Question
How does PeersManager work and how is peer reputation scored or updated?

## Context Packs
- `spec/reth_repo_context/context-network.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-network.xml

Question (Q015):
How does PeersManager work and how is peer reputation scored or updated?
I want the main types, event flow, the reputation model, and where penalties/rewards are applied.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Describe the event flow from network events into reputation changes.
3. Explain how bans/penalties are enforced.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-network.xml` pack, here is the architectural breakdown of the `PeersManager` and the reputation system in Reth.

### 1. Summary
*   **Central State Machine**: `PeersManager` is the state machine responsible for tracking all known peers, their connection state, and their reputation. It does not handle I/O directly but emits `PeerAction`s.
*   **Integer-Based Reputation**: Reputation is a signed integer (`i32`). Peers start at `0`. Bad behavior subtracts points; time spent connected adds points (decaying penalties).
*   **Weighted Offenses**: Different offenses (Bad Message, Timeout, Failed to Connect) have specific weights defined in `ReputationChangeWeights`.
*   **Threshold Banning**: If a peer's reputation drops below `BANNED_REPUTATION`, they are disconnected and banned.
*   **Backoff Mechanism**: For non-fatal errors (like timeouts), peers are placed in a "backoff" state (temporary timeout) rather than being immediately banned.
*   **Trusted Peers**: Trusted peers (configured via CLI/Config) are exempt from most reputation slashing and are prioritized for connection slots.
*   **BanList Integration**: The manager checks a `BanList` (IPs and PeerIds) before allowing incoming connections or dialing out.

---

### 2. Entry Points

| File Path | Type | Purpose |
| :--- | :--- | :--- |
| `crates/net/network/src/peers.rs` | `PeersManager` | The core struct managing the collection of peers, slots, and reputation logic. |
| `crates/net/network-types/src/peers/mod.rs` | `Peer` | Represents the state of a single peer (address, reputation score, connection state). |
| `crates/net/network-types/src/peers/reputation.rs` | `ReputationChangeKind` | Enum defining all possible reasons for reputation changes (e.g., `BadMessage`, `Timeout`). |
| `crates/net/network-api/src/test_utils/peers_manager.rs` | `PeersHandle` | A handle used by other parts of the system (like Sync or RPC) to report reputation changes manually. |

---

### 3. Algorithm Flow: Reputation & Banning

#### Step 1: Triggering a Reputation Change
A component (e.g., the Sync Pipeline or Session Manager) detects an issue.
*   **Example**: `StateFetcher` receives a bad block header.
*   **Action**: It calls `peers_handle.reputation_change(peer_id, ReputationChangeKind::BadMessage)`.

#### Step 2: Applying the Change
The `PeersManager` processes the change in `apply_reputation_change`.
*   **File**: `crates/net/network/src/peers.rs`
*   **Logic**:
    1.  It retrieves the `Peer` struct.
    2.  It calculates the numeric penalty using `reputation_weights.change(kind)`.
    3.  **Trusted Peer Exception**: If the peer is `Trusted` or `Static`, severe penalties (like `Dropped` or `BadAnnouncement`) are ignored, and others are capped at `MAX_TRUSTED_PEER_REPUTATION_CHANGE` to prevent banning.

#### Step 3: Updating the Score
The score is updated in `Peer::apply_reputation`.
*   **File**: `crates/net/network-types/src/peers/mod.rs`
*   **Logic**: `self.reputation = self.reputation.saturating_add(reputation_change)`.

#### Step 4: Determining the Outcome
The `apply_reputation` method returns a `ReputationChangeOutcome`.
*   If `reputation < BANNED_REPUTATION` (approx -51,200): Returns `DisconnectAndBan`.
*   If currently banned but score improved: Returns `Unban`.
*   Otherwise: Returns `None`.

#### Step 5: Enforcement (`PeersManager`)
Based on the outcome, `PeersManager` queues a `PeerAction`.
*   **Disconnect**: `PeerAction::Disconnect` is queued, which `NetworkManager` picks up to kill the session.
*   **Ban**: `ban_list.ban_peer_until` is called. Future connection attempts from this PeerID/IP will be rejected in `on_incoming_pending_session` or `add_and_connect`.

#### Step 6: Recovery (The "Tick")
`PeersManager` runs a `tick()` method on every poll.
*   **Logic**: For every peer currently connected (`is_connected()`), if their reputation is below 0, it increments the score based on seconds elapsed. This allows peers to recover from minor infractions by staying useful.

---

### 4. Key Abstractions & Types

#### `ReputationChangeWeights`
*   **File**: `crates/net/network-types/src/peers/reputation.rs`
*   **Description**: Configuration struct mapping abstract offenses to concrete integer penalties.
*   **Defaults**:
    *   `BadMessage`: -16,384 (`16 * REPUTATION_UNIT`)
    *   `Timeout`: -4,096
    *   `FailedToConnect`: -25,600

#### `PeerConnectionState`
*   **File**: `crates/net/network-types/src/peers/state.rs`
*   **Description**: Enum tracking if a peer is `Idle`, `PendingOut`, `In` (connected via inbound), or `Out` (connected via outbound). Used to manage slot limits.

#### `PeerAction`
*   **File**: `crates/net/network/src/peers.rs`
*   **Description**: Events emitted by `PeersManager::poll` to instruct the `NetworkManager` to perform I/O.
*   **Variants**: `Connect`, `Disconnect`, `BanPeer`, `DiscoveryBanIp`.

#### `BackoffKind`
*   **File**: `crates/net/network-types/src/backoff.rs`
*   **Description**: Differentiates between `Low`, `Medium`, and `High` severity backoffs. Used when a peer disconnects non-fatally (e.g., `TooManyPeers`) to determine how long to wait before redialing.

---

### 5. Where to Look Next
1.  **`crates/net/network/src/peers.rs` -> `apply_reputation_change`**: This is the central logic for deciding if a peer survives an error.
2.  **`crates/net/network/src/swarm.rs` -> `on_session_closed`**: To see how the network stack reports session drops to the `PeersManager`.
3.  **`crates/net/network/src/fetch/mod.rs`**: To see how the downloaders report `BadResponse` which eventually triggers reputation changes.

---

### 6. Suggested Follow-up Questions
*   "How does the `TrustedPeersResolver` work to keep DNS-based trusted peer addresses up to date?"
*   "What is the difference between `soft_remove_node` and `remove_node` in `Discv4Service`?"
*   "How does `NetworkState` use `PeerAction` to trigger actual socket disconnections?"

## Verification
- PeersManager buffers `PeerAction` and returns actions on `poll`, acting as a state machine rather than doing I/O directly.```41:64:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
/// Maintains the state of _all_ the peers known to the network.
/// From this type, connections to peers are established or disconnected, see [`PeerAction`].
...
queued_actions: VecDeque<PeerAction>,
```
```1038:1064:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
pub fn poll(&mut self, cx: &mut Context<'_>) -> Poll<PeerAction> {
    if let Some(action) = self.queued_actions.pop_front() { return Poll::Ready(action) }
    ...
}
```
- Reputation is `i32` with default 0, and the banned threshold is `BANNED_REPUTATION` (50 * -1024 = -51200).```3:11:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network-types/src/peers/reputation.rs
pub const DEFAULT_REPUTATION: Reputation = 0;
const REPUTATION_UNIT: i32 = -1024;
pub const BANNED_REPUTATION: i32 = 50 * REPUTATION_UNIT;
```
- Weighted reputation changes come from `ReputationChangeWeights::change`.```111:168:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network-types/src/peers/reputation.rs
pub struct ReputationChangeWeights { ... }
pub fn change(&self, kind: ReputationChangeKind) -> ReputationChange { ... }
```
- `Peer::apply_reputation` uses `saturating_add` and returns a ban/disconnect outcome.```85:110:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network-types/src/peers/mod.rs
self.reputation = previous.saturating_add(reputation);
...
return ReputationChangeOutcome::DisconnectAndBan
```
- `PeersManager::apply_reputation_change` handles trusted/static peers with exemptions and caps, then enforces Ban/Disconnect outcomes.```466:516:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
if peer.is_trusted() || peer.is_static() { ... return ... }
if reputation_change < MAX_TRUSTED_PEER_REPUTATION_CHANGE { reputation_change = MAX_TRUSTED_PEER_REPUTATION_CHANGE; }
...
match outcome { ReputationChangeOutcome::Ban => self.ban_peer(*peer_id), ... }
```
- Tick-based reputation recovery only increments connected peers whose reputation is below default.```438:457:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
for peer in self.peers.iter_mut().filter(|(_, peer)| peer.state.is_connected()) {
    if peer.1.reputation < DEFAULT_REPUTATION { peer.1.reputation += secs_since_last_tick; }
}
```
- Backoff is applied on non-fatal connection failures when `SessionError::should_backoff` returns a severity.```626:716:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
if let Some(kind) = err.should_backoff() { ... backoff_until = Some(...) }
```
```83:106:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/error.rs
fn should_backoff(&self) -> Option<BackoffKind>;
```
- Trusted/static peers are prioritized for outbound slots in `best_unconnected`.```936:969:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
/// Peers that are trusted or static are prioritized ...
if maybe_better.1.is_trusted() || maybe_better.1.is_static() { return Some(...) }
```
- BanList checks guard inbound and outbound paths.```248:260:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
if self.ban_list.is_banned_ip(&addr) { return Err(InboundConnectionError::IpBanned) }
```
```879:888:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
if self.ban_list.is_banned(&peer_id, &ip_addr) { return }
```
- PeersHandle exposes `reputation_change` to send commands to the manager.```20:50:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network-api/src/test_utils/peers_manager.rs
pub struct PeersHandle { ... }
pub fn reputation_change(&self, peer_id: PeerId, kind: ReputationChangeKind) {
    self.send(PeerCommand::ReputationChange(peer_id, kind));
}
```

## Corrections / Caveats
- Trusted/static peers are exempt only for specific change kinds (Dropped, BadAnnouncement, Timeout, AlreadySeenTransaction) and otherwise have slashing capped; not "most" reputation changes.
- "PeersHandle used by Sync/RPC" is not shown in this pack; `PeersHandle` is defined in `network-api` test_utils and provides a command channel.
- PeerAction variants listed are not exhaustive; see `PeerAction` in `peers.rs` for all actions.
- Backoff is tied to connection/session errors via `SessionError::should_backoff`; it is not applied to all reputation changes.

## Actionable Pointers
- Core reputation flow and enforcement: `reth/crates/net/network/src/peers.rs`.
- Reputation weights/constants: `reth/crates/net/network-types/src/peers/reputation.rs`.
- Peer state changes and ban outcomes: `reth/crates/net/network-types/src/peers/mod.rs`.
- Backoff severity model: `reth/crates/net/network-types/src/backoff.rs`.
</file>

<file path="questions/Q016-connection-limits-dial-backoff.md">
# Q016 - Connection Limits, Dialing, and Backoff

Status: verified

## Question
How are connection limits, dialing, and backoff controls implemented?

## Context Packs
- `spec/reth_repo_context/context-network.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-network.xml

Question (Q016):
How are connection limits, dialing, and backoff controls implemented?
I want the main types, how inbound/outbound slots are tracked, how peers are selected for dialing, and how backoff durations are computed/applied.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Describe the control flow for inbound accept, outbound dial scheduling, and backoff release.
3. Explain how trusted/static peers affect slot allocation or dialing priority.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-network.xml` pack, here is the implementation analysis of connection limits, dialing logic, and backoff controls in Reth.

### 1. Summary
*   **Central Control**: The `PeersManager` is the authoritative state machine for deciding *who* to connect to and *when*. It manages connection slots via `ConnectionInfo`.
*   **Slot Tracking**: Inbound and outbound slots are tracked separately. There are limits for "pending" (handshaking) connections and "active" (established) sessions.
*   **Dialing Strategy**: Dialing is interval-based (`refill_slots_interval`). The strategy prioritizes **Trusted** and **Static** peers first, followed by peers with the highest **Reputation** score.
*   **Backoff Logic**: Backoffs are deterministic based on error severity (`BackoffKind`). "Severe" errors increment a counter on the peer, causing the backoff duration to scale linearly (duration * counter) up to a max cap.
*   **Trusted Bypass**: Trusted peers have special privileges; they are prioritized during dialing and can sometimes bypass inbound connection limits if the node is in a specific restricted mode.
*   **Lazy Cleanup**: Backed-off peers are stored in a `HashMap` and released via a periodic tick (`release_interval`) in the `PeersManager`, rather than individual timers.

---

### 2. Entry Points

| File Path | Type | Purpose |
| :--- | :--- | :--- |
| `crates/net/network/src/peers.rs` | `PeersManager` | The core state machine managing limits, backoffs, and dial selection. |
| `crates/net/network/src/peers.rs` | `ConnectionInfo` | Tracks integer counts of In/Out/Pending connections against configured limits. |
| `crates/net/network-types/src/peers/config.rs` | `PeerBackoffDurations` | Defines base durations for Low, Medium, and High severity backoffs. |
| `crates/net/network-types/src/backoff.rs` | `BackoffKind` | Enum classifying error severity (Low, Medium, High). |
| `crates/net/network/src/session/mod.rs` | `SessionManager` | Executes the physical dial logic based on `PeersManager` commands. |

---

### 3. Algorithm Flow

#### A. Inbound Connection Acceptance
When a new TCP stream arrives at the `Swarm`, it asks the `PeersManager` if it can accept.

1.  **Trigger**: `Swarm` calls `sessions.on_incoming`.
2.  **Check**: `PeersManager::on_incoming_pending_session` is called.
3.  **Validation**:
    *   **IP Filter**: Checks `IpFilter` and `BanList`.
    *   **Capacity**: Calls `connection_info.has_in_capacity()`.
    *   **Trusted Exception**: If at capacity, it checks if there are `idle` trusted peers. If so, it allows the connection (up to a secondary limit) to ensure trusted peers aren't locked out by random peers (`crates/net/network/src/peers.rs`).
4.  **Throttling**: Applies `throttle_incoming_ip` to rate-limit the specific IP temporarily.
5.  **Result**: If successful, `inc_pending_in()` is called, and the session starts.

#### B. Outbound Dialing (Refill Slots)
The node proactively fills outbound slots via the `PeersManager::poll` loop.

1.  **Tick**: `refill_slots_interval` ticks.
2.  **Capacity Check**: Calls `fill_outbound_slots`. Checks `connection_info.has_out_capacity()` (pending + active < max).
3.  **Selection**: Calls `best_unconnected()` to pick a peer.
    *   **Filter**: Must not be `backed_off`, `banned`, or already `connected`.
    *   **Priority 1**: If a **Trusted** or **Static** peer is found, return immediately.
    *   **Priority 2**: Sort remaining peers by **Reputation** (highest first).
4.  **Action**: Updates peer state to `PendingOut`, increments `num_pending_out`, and queues a `PeerAction::Connect`.
5.  **Execution**: `NetworkManager` receives the action and calls `dial_outbound` on the `SessionManager`.

#### C. Backoff Controls
When a connection fails (e.g., handshake timeout or TCP error):

1.  **Classification**: The error is mapped to a `BackoffKind` (Low, Medium, High).
    *   *Low*: Timouts, already connected.
    *   *Medium*: Malformed messages, logic errors.
    *   *High*: Protocol breaches, useless peer.
2.  **Calculation**: `PeersManager` calculates the duration.
    *   If `kind.is_severe()` (Medium/High), increment `peer.severe_backoff_counter`.
    *   Duration = `base_duration * counter`.
    *   Max Duration is capped by `backoff_durations.max`.
3.  **Application**: The peer is marked `backed_off = true` and added to `backed_off_peers` map with an expiry timestamp.
4.  **Release**:
    *   `PeersManager::poll` checks `release_interval` (default ~30s).
    *   Iterates `backed_off_peers`. If `now > expiry`, sets `peer.backed_off = false` and removes from map.

---

### 4. Key Abstractions & Types

#### `ConnectionInfo`
*   **Location**: `crates/net/network/src/peers.rs`
*   **Purpose**: Tracks usage against `ConnectionsConfig`.
*   **Logic**: Maintains four counters: `num_inbound`, `num_outbound`, `num_pending_in`, `num_pending_out`.
    *   *Note*: Pending outbound connections count towards the total outbound limit to prevent over-dialing.

#### `PeerBackoffDurations`
*   **Location**: `crates/net/network-types/src/peers/config.rs`
*   **Defaults**:
    *   `Low`: 30s (e.g., timeout).
    *   `Medium`: 3m (e.g., logic error).
    *   `High`: 15m (e.g., protocol violation).
    *   `Max`: 1h.

#### `PeerConnectionState`
*   **Location**: `crates/net/network-types/src/peers/state.rs`
*   **States**:
    *   `Idle`: Ready to be dialed or backoff.
    *   `PendingOut`: Currently dialing.
    *   `In`/`Out`: Active session established.
    *   `DisconnectingIn`/`DisconnectingOut`: Teardown phase.

#### `ReputationChangeWeights`
*   **Location**: `crates/net/network-types/src/peers/reputation.rs`
*   **Logic**: Defines how much to penalize a peer for specific errors (e.g., `BadMessage` = -16k). Used in conjunction with backoff for connection failures.

---

### 5. Where to Look Next
1.  **`crates/net/network/src/peers.rs` - `fill_outbound_slots`**: The exact logic for prioritizing trusted peers over high-reputation peers.
2.  **`crates/net/network/src/peers.rs` - `on_connection_failure`**: How errors are converted into backoff timestamps.
3.  **`crates/net/network/src/session/mod.rs` - `dial_outbound`**: The physical act of creating the TCP socket, which consumes the "slots" allocated by `PeersManager`.

### 6. Suggested Follow-up Questions
*   "How does the `TrustedPeersResolver` update the IP addresses of trusted peers if they are defined by DNS names?"
*   "What happens to the reputation of a peer if it is backed off multiple times in a row?"
*   "How does `NetworkConnectionState` (Active vs Hibernate) affect the `fill_outbound_slots` logic?"

## Verification
- Connection limits and pending/active counts live in `ConnectionInfo`, with outbound capacity gated by both pending-dial and active limits.```1111:1138:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
pub struct ConnectionInfo { ... }
const fn has_out_capacity(&self) -> bool {
    self.num_pending_out < self.config.max_concurrent_outbound_dials &&
        self.num_outbound < self.config.max_outbound
}
```
- Inbound accept checks IP filter and ban list, then applies capacity and a special trusted-peers allowance, and increments pending inbound.```245:297:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
if !self.ip_filter.is_allowed(&addr) { ... }
if self.ban_list.is_banned_ip(&addr) { ... }
if !self.connection_info.has_in_capacity() { ... trusted_peer_ids ... }
...
self.connection_info.inc_pending_in();
```
- Dial scheduling runs on `refill_slots_interval`, using `fill_outbound_slots` and `best_unconnected` (trusted/static first, then highest reputation).```1090:1092:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
while self.refill_slots_interval.poll_tick(cx).is_ready() {
    self.fill_outbound_slots();
}
```
```936:1004:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
fn best_unconnected(&mut self) -> Option<(PeerId, &mut Peer)> { ... }
fn fill_outbound_slots(&mut self) { ... PeerAction::Connect { ... } }
```
- Backoff durations are defined by `PeerBackoffDurations` and capped by `max`, with linear scaling via the backoff counter.```30:70:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network-types/src/peers/config.rs
pub struct PeerBackoffDurations { pub low: Duration, pub medium: Duration, pub high: Duration, pub max: Duration }
pub fn backoff_until(&self, kind: BackoffKind, backoff_counter: u8) -> Instant {
    let backoff_time = self.backoff(kind);
    let backoff_time = backoff_time + backoff_time * backoff_counter as u32;
    now + backoff_time.min(self.max)
}
```
- Non-fatal connection failures apply backoff based on `SessionError::should_backoff`, with severe kinds incrementing `severe_backoff_counter`.```626:716:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
if let Some(kind) = err.should_backoff() { ... if kind.is_severe() { peer.severe_backoff_counter = ... } ... }
```
```83:106:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/error.rs
fn should_backoff(&self) -> Option<BackoffKind>;
```
- Backoff release and ban evictions happen on `release_interval` ticks.```1066:1087:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
if self.release_interval.poll_tick(cx).is_ready() { ... self.backed_off_peers.retain(...); }
```
- Session dialing happens in `SessionManager::dial_outbound`, and Swarm routes connect actions to it.```306:345:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/session/mod.rs
pub fn dial_outbound(&mut self, remote_addr: SocketAddr, remote_peer_id: PeerId) { ... }
```
```228:233:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/swarm.rs
StateAction::Connect { remote_addr, peer_id } => {
    self.dial_outbound(remote_addr, peer_id);
}
```

## Corrections / Caveats
- Outbound capacity does not use "pending + active < max"; it separately enforces `max_concurrent_outbound_dials` and `max_outbound`.
- Backoff duration is `base * (1 + counter)` capped by `max`, not `base * counter`.
- The "trusted bypass inbound limits" logic allows extra pending inbound slots when trusted peers are configured; it does not check the incoming peer's trust at that stage.
- The specific error-to-BackoffKind mapping lives in `SessionError::should_backoff` and is more detailed than the summary. See `network/src/error.rs`.
- The exact mapping from `PeerAction::Connect` to `StateAction::Connect` is handled in network state/manager code outside these snippets.

## Actionable Pointers
- Slot counters and limits: `reth/crates/net/network/src/peers.rs` (`ConnectionInfo`).
- Backoff durations and scaling: `reth/crates/net/network-types/src/peers/config.rs`.
- Dial selection priority: `reth/crates/net/network/src/peers.rs` (`best_unconnected`, `fill_outbound_slots`).
- Outbound dial execution: `reth/crates/net/network/src/session/mod.rs` and `reth/crates/net/network/src/swarm.rs`.
</file>

<file path="questions/Q017-peer-persistence.md">
# Q017 - Peer Persistence (Disk Format + Lifecycle)

Status: verified

## Question
How does peer persistence work (disk format + lifecycle)?

## Context Packs
- `spec/reth_repo_context/context-network.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-network.xml

Question (Q017):
How does peer persistence work (disk format + lifecycle)?
I want how peer records are stored, loaded on startup, updated during runtime, and any pruning/expiration behavior.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Describe the read/write flow and when persistence is triggered.
3. Explain how persisted peers interact with discovery and the in-memory peer set.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the provided `context-network.xml` pack, here is the architecture of peer persistence in Reth.

### 1. Summary
*   **Simple JSON Storage**: Peer records are persisted as a JSON list of `NodeRecord`s. There is no complex database for peer history in this layer; it relies on a flat file (usually named `known_peers.json` in the data dir, though the filename is passed in by the caller).
*   **Manual Triggering**: The `network` crate does **not** contain an internal timer loop to auto-save peers. Instead, it exposes a `write_peers_to_file` method on `NetworkManager`, expecting the node application (the binary) to trigger persistence (e.g., on shutdown or periodically).
*   **Load-at-Startup**: Peers are loaded during configuration via `PeersConfig::with_basic_nodes_from_file`. These peers are loaded into memory as `PeerKind::Basic`.
*   **Discovery Integration**: While Discovery v4/v5 have their own internal routing tables (K-Buckets), Reth persists the higher-level "Known Peers" list from `PeersManager`, which aggregates nodes found via discovery, trusted peers, and static peers.
*   **No Explicit File Pruning**: The persistence logic simply dumps the current in-memory state of `PeersManager`. Any pruning or cleanup must happen in memory before the dump occurs.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/net/network/src/manager.rs` | `NetworkManager::write_peers_to_file` | The primary function to dump all currently known peers to disk as JSON. |
| `crates/net/network-types/src/peers/config.rs` | `PeersConfig::with_basic_nodes_from_file` | The configuration hook used at startup to read the JSON file and populate the initial peer set. |
| `crates/net/network/src/peers.rs` | `PeersManager::iter_peers` | The iterator used by the export function to collect all `NodeRecord`s for persistence. |
| `crates/net/network-peers/src/lib.rs` | `NodeRecord` | The serializable struct representing a peer (ID, IP, TCP/UDP ports) stored in the file. (Deduced from usage). |

---

### 3. Peer Persistence Lifecycle

#### Step 1: Loading at Startup
When the node starts, it builds the `NetworkConfig`.
1.  The caller (node binary) calls `PeersConfig::with_basic_nodes_from_file(path)`.
2.  **File Read**: It opens the file using `std::fs::File::open`.
3.  **Deserialization**: It uses `serde_json::from_reader` to parse a `HashSet<NodeRecord>`.
4.  **Memory Population**: These records are added to the `basic_nodes` field of `PeersConfig`.
5.  **Manager Init**: When `PeersManager::new` is called, it iterates `basic_nodes` and inserts them into the `peers` HashMap with `PeerKind::Basic`.

*   **Cite**: `crates/net/network-types/src/peers/config.rs` (`with_basic_nodes_from_file`).
*   **Cite**: `crates/net/network/src/peers.rs` (`PeersManager::new`).

#### Step 2: Runtime Updates (In-Memory)
During runtime, the peer list changes, but these changes are **not** immediately written to disk.
1.  **Discovery**: When `Discovery` finds a new node (`DiscoveryEvent::NewNode`), `NetworkManager` calls `peers_mut().add_peer(...)`.
2.  **Connections**: Incoming connections create new ephemeral peer entries in `PeersManager`.
3.  **Pruning**: `PeersManager` removes peers if they are explicitly banned or if an active session disconnects and the peer was marked `remove_after_disconnect` (common for incoming connections without a known discovery record).

*   **Cite**: `crates/net/network/src/manager.rs` (`on_discovery_event`).

#### Step 3: Persisting to Disk
Persistence is an explicit action, not a background background background task within this crate.
1.  **Trigger**: The node application calls `network_manager.write_peers_to_file(&path)`.
2.  **Collection**: `NetworkManager` delegates to `self.all_peers()`, which calls `self.swarm.state().peers().iter_peers()`.
3.  **Serialization**: The collected `Vec<NodeRecord>` is serialized to JSON.
4.  **Write**: `reth_fs_util::write_json_file` writes the data to `persistent_peers_file`.

*   **Cite**: `crates/net/network/src/manager.rs` (`write_peers_to_file`).

---

### 4. Key Abstractions & Types

*   **`NodeRecord`** (re-exported in `crates/net/network/src/lib.rs`):
    The data structure serialized to disk. Contains:
    *   `Address` (IP)
    *   `tcp_port`
    *   `udp_port`
    *   `id` (PeerId/Public Key)
*   **`PeersConfig`** (`crates/net/network-types/src/peers/config.rs`):
    Holds the configuration for loading peers, including the `basic_nodes` set populated from disk.
*   **`PeersManager`** (`crates/net/network/src/peers.rs`):
    The authoritative source of truth for "Known Peers". Only peers in this struct's `peers` HashMap are persisted. Note that `Discv4` and `Discv5` have their own internal routing tables (`KBucketsTable`), but Reth chooses to persist the application-level `PeersManager` list instead of raw K-Buckets.

---

### 5. Where to Look Next
*   **`crates/net/network/src/manager.rs`**: Look at `write_peers_to_file` to see exactly how the export is orchestrated.
*   **`crates/net/network-types/src/peers/config.rs`**: Look at `with_basic_nodes_from_file` to see the loading logic and error handling (e.g., ignoring `NotFound` errors).
*   **`crates/net/network/src/peers.rs`**: Look at `iter_peers` to see precisely which data from the internal `Peer` struct is converted into the exportable `NodeRecord`.

---

### 6. Suggested Follow-up Questions
*   "Since `PeersManager` does not seem to have a maximum size limit in this code, does the `known_peers.json` file grow indefinitely if the node runs for months?"
*   "How does `Discv4` persist its specific Kademlia table logic? Or is the K-bucket structure lost on restart, re-bootstrapping purely from the flat list in `PeersManager`?"
*   "Where is `write_peers_to_file` actually called? (Likely in `reth-node` shutdown hooks, outside this pack)."

## Verification
- `NetworkManager::write_peers_to_file` collects `all_peers()` and writes JSON to the provided path.```418:441:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/manager.rs
pub fn all_peers(&self) -> impl Iterator<Item = NodeRecord> + '_ {
    self.swarm.state().peers().iter_peers()
}
pub fn write_peers_to_file(&self, persistent_peers_file: &Path) -> Result<(), FsPathError> {
    let known_peers = self.all_peers().collect::<Vec<_>>();
    persistent_peers_file.parent().map(fs::create_dir_all).transpose()?;
    reth_fs_util::write_json_file(persistent_peers_file, &known_peers)?;
    Ok(())
}
```
- `PeersConfig::with_basic_nodes_from_file` loads a JSON file into `HashSet<NodeRecord>` and ignores `NotFound`.```295:308:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network-types/src/peers/config.rs
pub fn with_basic_nodes_from_file(self, optional_file: Option<impl AsRef<Path>>) -> Result<Self, io::Error> {
    let reader = match std::fs::File::open(file_path.as_ref()) { ... };
    let nodes: HashSet<NodeRecord> = serde_json::from_reader(reader)?;
    Ok(self.with_basic_nodes(nodes))
}
```
- Default peer list path in the node is `known-peers.json` under the chain data dir.```335:340:/Users/vicnaum/github/stateless-history-node/reth/crates/node/core/src/dirs.rs
/// `<DIR>/<CHAIN_ID>/known-peers.json`
pub fn known_peers(&self) -> PathBuf { self.data_dir().join("known-peers.json") }
```
- `NodeRecord` is serialized via display/parse helpers (enode URL format), not a JSON object.```24:35:/Users/vicnaum/github/stateless-history-node/reth/crates/net/peers/src/node_record.rs
#[derive(... SerializeDisplay, DeserializeFromStr, ...)]
pub struct NodeRecord { ... }
```
```122:143:/Users/vicnaum/github/stateless-history-node/reth/crates/net/peers/src/node_record.rs
impl fmt::Display for NodeRecord { ... f.write_str("enode://")?; ... }
```
```163:198:/Users/vicnaum/github/stateless-history-node/reth/crates/net/peers/src/node_record.rs
impl FromStr for NodeRecord { ... }
```
- `PeersManager::new` seeds peers from `basic_nodes` and `trusted_nodes`.```121:142:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
for trusted_peer in &trusted_nodes { ... Peer::trusted(...) }
for NodeRecord { address, tcp_port, udp_port, id } in basic_nodes { peers.entry(id).or_insert_with(|| Peer::new(...)) }
```
- Discovery updates feed into the peer set via state actions.```325:333:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/state.rs
DiscoveryEvent::NewNode(...) => { self.queued_messages.push_back(StateAction::DiscoveredNode { ... }) }
```
```248:255:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/swarm.rs
StateAction::DiscoveredNode { peer_id, addr, fork_id } => {
    self.state_mut().peers_mut().add_peer(peer_id, addr, fork_id);
}
```
- Incoming-only peers can be marked for removal on disconnect, and are removed when sessions close.```372:378:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
let mut peer = Peer::with_state(PeerAddr::from_tcp(addr), PeerConnectionState::In);
peer.remove_after_disconnect = true;
```
```540:549:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
if entry.get().remove_after_disconnect && !entry.get().is_trusted() {
    entry.remove();
    self.queued_actions.push_back(PeerAction::PeerRemoved(peer_id));
}
```
- Persistence is triggered on graceful shutdown in the node builder, not by a timer inside `NetworkManager`.```907:924:/Users/vicnaum/github/stateless-history-node/reth/crates/node/builder/src/builder/mod.rs
if let Some(peers_file) = known_peers_file {
    ... network.write_peers_to_file(peers_file.as_path()) ...
}
```

## Corrections / Caveats
- The persisted file is `known-peers.json` (dash), not `known_peers.json`, and the path is configurable.
- The JSON format is an array of stringified `enode://...` URLs, not a JSON object per peer.
- The "Discovery K-bucket persistence" claim is outside this pack; only `PeersManager` persistence is shown here.
- `NodeRecord` is defined in `reth/crates/net/peers/src/node_record.rs`, not `network-peers/src/lib.rs`.

## Actionable Pointers
- Persistence entrypoint: `reth/crates/net/network/src/manager.rs` (`write_peers_to_file`).
- Startup loading: `reth/crates/net/network-types/src/peers/config.rs` (`with_basic_nodes_from_file`).
- Default file path and CLI hooks: `reth/crates/node/core/src/dirs.rs`, `reth/crates/node/core/src/args/network.rs`.
- Peer set updates: `reth/crates/net/network/src/state.rs`, `reth/crates/net/network/src/peers.rs`.
</file>

<file path="questions/Q018-canonical-chain-reorgs.md">
# Q018 - Canonical Chain and Reorg Detection

Status: verified

## Question
How is the canonical chain represented and how are reorgs detected?

## Context Packs
- `spec/reth_repo_context/context-engine.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-engine.xml

Question (Q018):
How is the canonical chain represented and how are reorgs detected?
I want the core data structures, how canonical head is tracked, and how reorg detection flows into pipeline or engine logic.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Describe how canonical state is updated on new blocks.
3. Explain how reorg detection is surfaced (events, errors, or unwind requests).
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-engine.xml` pack, here is the architecture of how the canonical chain is represented and how reorgs are detected in the Reth engine.

### 1. Summary
*   **In-Memory Tree Structure**: The canonical chain is tracked within `TreeState`, which maintains a collection of executed blocks (`blocks_by_hash`) and their parent-child relationships (`parent_to_child`).
*   **Head Tracking**: The engine explicitly tracks a `current_canonical_head`. Any block not strictly following this head is part of a sidechain until chosen.
*   **Reorg Detection via Walk-Back**: Reorgs are detected in `on_new_head` by walking backwards from the proposed new head and the current canonical head simultaneously until a common ancestor (fork point) is found.
*   **Classification**: A chain update is classified as either a `Commit` (simple extension) or a `Reorg` (switching to a different fork) via the `NewCanonicalChain` enum.
*   **Preservation of History**: When a reorg occurs, blocks from the "old" canonical chain are re-inserted into the tree state as sidechains (`reinsert_reorged_blocks`), ensuring they are available if the chain reorgs back.
*   **Pipeline Integration**: Deep reorgs or invalid blocks detected during backfill trigger a `ControlFlow::Unwind` in the pipeline, which the engine handles by resetting the tree state to the unwind target.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/engine/tree/src/tree/state.rs` | `TreeState` | The central data structure holding all known executed blocks and the pointer to the `current_canonical_head`. |
| `crates/engine/tree/src/tree/mod.rs` | `EngineApiTreeHandler::on_new_head` | The logic core that calculates the path from the current head to a proposed head, detecting reorgs. |
| `crates/engine/tree/src/tree/mod.rs` | `EngineApiTreeHandler::on_canonical_chain_update` | The function that actually applies the reorg, updates metrics, and emits events. |
| `crates/engine/tree/src/tree/mod.rs` | `EngineApiTreeHandler::on_backfill_sync_finished` | Handles reorgs/unwinds triggered by the historical sync pipeline (e.g., bad blocks found during backfill). |

---

### 3. Algorithm Flow: Reorg Detection (`on_new_head`)

The core reorg detection logic resides in `on_new_head` within `crates/engine/tree/src/tree/mod.rs`.

#### Step 1: Identify New Head
The engine receives a request to make a specific block hash canonical (e.g., via Forkchoice Updated). It retrieves the `ExecutedBlock` from `TreeState`.

#### Step 2: Connect to Tree
The function walks backwards from the `new_head` until it finds a block hash that exists in the current `TreeState` or reaches the canonical head.

#### Step 3: Determine Relationship
*   **Extension (Commit)**: If the walk-back leads directly to `current_canonical_head`, it is a simple append. Returns `NewCanonicalChain::Commit`.
*   **Reorg**: If the walk-back hits a block that is *not* the current head, or if the current head is at a higher block number than the intersection point, it is a reorg.

#### Step 4: Find Common Ancestor (The Fork Point)
In the reorg case:
1.  The logic walks back the **Current Canonical Chain** (`old_chain`) and the **New Chain** (`new_chain`) simultaneously.
2.  It identifies the **Common Ancestor** (where `old_hash == current_hash`).
3.  It collects all blocks from the Old Chain that are being discarded and all blocks from the New Chain that are being enacted.

#### Step 5: Construct Notification
It constructs a `NewCanonicalChain::Reorg { new, old }` struct containing the vectors of added and removed blocks.

#### Step 6: Application (`on_canonical_chain_update`)
1.  **Update Head**: Sets `tree_state.current_canonical_head` to the new tip.
2.  **Reinsert Old**: Calls `reinsert_reorged_blocks` to move the `old` blocks back into the general `blocks_by_hash` pool (effectively turning them into sidechains).
3.  **Emit**: Emits `ConsensusEngineEvent::CanonicalChainCommitted`.

---

### 4. Key Abstractions & Data Types

*   **`TreeState`** (`crates/engine/tree/src/tree/state.rs`):
    *   Holds `blocks_by_hash: HashMap<B256, ExecutedBlock<N>>`.
    *   Holds `parent_to_child` to track tree topology.
    *   Tracks `current_canonical_head`.
*   **`NewCanonicalChain`** (`crates/chain-state/src/in_memory.rs`):
    *   Variants: `Commit { new }` and `Reorg { new, old }`.
*   **`TreeAction`** (`crates/engine/tree/src/tree/mod.rs`):
    *   Enum used to signal internal intent, specifically `TreeAction::MakeCanonical`.
*   **`ForkchoiceStateTracker`** (`crates/engine/primitives/src/forkchoice.rs`):
    *   Tracks the `latest`, `last_valid`, and `last_syncing` forkchoice states received from the Beacon node to determine if we are in sync or need to trigger downloads/reorgs.

---

### 5. Where to Look Next

1.  **`crates/engine/tree/src/tree/mod.rs`**: Specifically `on_new_head` and `on_canonical_chain_update`, which implement reorg detection and canonical updates.
2.  **`crates/engine/tree/src/tree/state.rs`**: The `TreeState` struct definition and its invariants for what is kept in memory.
3.  **`crates/engine/tree/src/tree/metrics.rs`**: `TreeMetrics` and reorg counters such as `reorgs` and `latest_reorg_depth`.

---

### 6. Suggested Follow-up Questions

*   "How does `EngineApiTreeHandler` decide to trigger a backfill sync instead of handling a reorg in-memory if the gap is too large?"
*   "How does `PersistenceService` handle disk reorgs (`find_disk_reorg`) differently from in-memory reorgs?"
*   "What happens to the `InvalidHeaderCache` if a reorg switches to a chain that contains a block previously marked invalid?"

## Verification
- TreeState stores executed blocks by hash/number, parent-child edges, and a canonical head pointer.```24:39:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/state.rs
pub struct TreeState<N: NodePrimitives = EthPrimitives> {
    pub(crate) blocks_by_hash: HashMap<B256, ExecutedBlock<N>>,
    pub(crate) blocks_by_number: BTreeMap<BlockNumber, Vec<ExecutedBlock<N>>>,
    pub(crate) parent_to_child: HashMap<B256, HashSet<B256>>,
    pub(crate) current_canonical_head: BlockNumHash,
}
```
- on_new_head walks back the new chain to the canonical height, then either commits or walks both chains to produce a reorg.```714:803:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
fn on_new_head(&self, new_head: B256) -> ProviderResult<Option<NewCanonicalChain<N>>> {
    ... walk back new chain ...
    if current_hash == self.state.tree_state.current_canonical_head.hash { return Ok(Some(NewCanonicalChain::Commit { new: new_chain })) }
    ... walk old and new chains until common ancestor ...
    Ok(Some(NewCanonicalChain::Reorg { new: new_chain, old: old_chain }))
}
```
- NewCanonicalChain defines Commit/Reorg variants and is in chain-state.```907:922:/Users/vicnaum/github/stateless-history-node/reth/crates/chain-state/src/in_memory.rs
pub enum NewCanonicalChain<N: NodePrimitives = EthPrimitives> {
    Commit { new: Vec<ExecutedBlock<N>> },
    Reorg { new: Vec<ExecutedBlock<N>>, old: Vec<ExecutedBlock<N>> },
}
```
- Canonical updates set the tree's canonical head, reinsert reorged blocks, update in-memory state, and emit CanonicalChainCommitted.```2317:2355:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
fn on_canonical_chain_update(&mut self, chain_update: NewCanonicalChain<N>) {
    self.state.tree_state.set_canonical_head(chain_update.tip().num_hash());
    if let NewCanonicalChain::Reorg { new, old } = &chain_update {
        self.reinsert_reorged_blocks(new.clone());
        self.reinsert_reorged_blocks(old.clone());
    }
    self.canonical_in_memory_state.update_chain(chain_update);
    self.emit_event(ConsensusEngineEvent::CanonicalChainCommitted(...));
}
```
- Backfill unwind resets the tree state to the unwind target and updates canonical head tracking.```1558:1617:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
if ctrl.is_unwind() {
    self.state.tree_state.reset(backfill_num_hash)
} else {
    self.state.tree_state.remove_until(...)
}
...
self.state.tree_state.set_canonical_head(new_head.num_hash());
```
- Canonical chain events are emitted via ConsensusEngineEvent.```20:35:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/primitives/src/event.rs
pub enum ConsensusEngineEvent { ... CanonicalChainCommitted(...), ... }
```

## Corrections / Caveats
- TreeState also tracks `blocks_by_number`, not only `blocks_by_hash` and `parent_to_child`.
- The "sidechain until chosen" statement is implicit; TreeState only stores executed blocks connected to the canonical chain (including forks).
- The backfill unwind handling is in `EngineApiTreeHandler::on_backfill_sync_finished`, not a direct pipeline hook.
- ForkchoiceStateTracker usage is present in tree state, but its internal logic is outside this pack.

## Actionable Pointers
- Reorg detection: `reth/crates/engine/tree/src/tree/mod.rs` (`on_new_head`).
- Canonical update and reorg handling: `reth/crates/engine/tree/src/tree/mod.rs` (`on_canonical_chain_update`, `reinsert_reorged_blocks`).
- Tree storage invariants: `reth/crates/engine/tree/src/tree/state.rs`.
- Canonical chain type: `reth/crates/chain-state/src/in_memory.rs` (`NewCanonicalChain`).
</file>

<file path="questions/Q019-engine-tree-backfill-handoff.md">
# Q019 - Engine Tree Backfill Handoff

Status: verified

## Question
How does the engine tree hand off to the pipeline and handle backfill?

## Context Packs
- `spec/reth_repo_context/context-engine.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-engine.xml

Question (Q019):
How does the engine tree hand off to the pipeline and handle backfill?
I want the decision criteria, event flow, and how backfill results are applied to the tree state.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Describe the control flow between EngineApiTreeHandler and pipeline backfill.
3. Explain how tree state changes on backfill start/finish, including unwind cases.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-engine.xml` pack, here is the architecture of how the Engine Tree hands off synchronization to the Pipeline and handles the backfill process.

### 1. Summary
*   **Gap-Based Trigger**: The engine switches to backfill mode when the gap between the local tip and the target block (from a Forkchoice Update or downloaded block) exceeds `MIN_BLOCKS_FOR_PIPELINE_RUN` (32 blocks).
*   **Orchestrated Handoff**: The `ChainOrchestrator` sits above both the `EngineHandler` and the `BackfillSync` (Pipeline). The engine requests a backfill, and the orchestrator activates the pipeline.
*   **Mutually Exclusive**: When backfill is active (`BackfillSyncState::Active`), the engine pauses processing new Forkchoice Updates, returning `SYNCING` to the Consensus Layer.
*   **State Reset**: Upon backfill completion, the engine purges in-memory blocks that are now persisted in the database and resets its in-memory canonical state to match the new database tip.
*   **Unwind Handling**: If the pipeline detects a bad block, it returns a `ControlFlow::Unwind`. The engine handles this by marking the block invalid and resetting the in-memory tree to the unwind point.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/engine/tree/src/chain.rs` | `ChainOrchestrator` | The top-level state machine that polls both the `handler` (Engine) and `backfill_sync` (Pipeline). |
| `crates/engine/tree/src/tree/mod.rs` | `EngineApiTreeHandler::backfill_sync_target` | Calculates the distance to a target and decides if backfill is needed. |
| `crates/engine/tree/src/backfill.rs` | `PipelineSync` | Wraps the actual stages pipeline and spawns it on a background task. |
| `crates/engine/tree/src/tree/mod.rs` | `on_backfill_sync_finished` | The callback logic that updates the tree state after the pipeline finishes. |

---

### 3. Algorithm Flow

#### Step 1: Detection (Engine Tree)
When `on_forkchoice_updated` or `on_downloaded_block` runs, the engine checks the distance between the local head and the new block.
*   **Function**: `backfill_sync_target` in `crates/engine/tree/src/tree/mod.rs`.
*   **Logic**: It calls `exceeds_backfill_run_threshold`. If `target_block - local_tip > MIN_BLOCKS_FOR_PIPELINE_RUN` (32), it returns `Some(target_hash)`.

#### Step 2: Handoff (Tree -> Orchestrator)
The tree emits an event to request backfill.
*   **Action**: `emit_event(EngineApiEvent::BackfillAction(BackfillAction::Start(target)))`.
*   **State Change**: `backfill_sync_state` is set to `Pending`.
*   **Routing**: The event bubbles up through `EngineApiRequestHandler` -> `EngineHandler` -> `ChainOrchestrator`.

#### Step 3: Execution (Orchestrator -> Pipeline)
The `ChainOrchestrator` receives the request and commands the pipeline.
*   **File**: `crates/engine/tree/src/chain.rs`.
*   **Logic**: `poll_next_event` receives `HandlerEvent::BackfillAction`. It calls `self.backfill_sync.on_action(action)`.
*   **Spawn**: `PipelineSync::try_spawn_pipeline` (`crates/engine/tree/src/backfill.rs`) spawns the pipeline on a generic `TaskSpawner`.

#### Step 4: Active Syncing (Engine Wait)
While the pipeline runs, `BackfillSyncState` is `Active`.
*   **Impact**: In `validate_forkchoice_state` (`tree/mod.rs`), if `backfill_sync_state` is not idle, it immediately returns `PayloadStatus::Syncing` to the Consensus Layer, preventing the engine from touching the database while the pipeline writes to it.

#### Step 5: Completion & State Update
The pipeline finishes and returns a `ControlFlow`.
*   **Orchestrator**: Receives `BackfillEvent::Finished`. Calls `handler.on_event(FromOrchestrator::BackfillSyncFinished)`.
*   **Tree Handler**: `on_backfill_sync_finished` (`tree/mod.rs`) executes:
    1.  **State Cleanup**: Calls `tree_state.remove_until` to delete in-memory blocks that the pipeline just persisted to disk.
    2.  **Canonical Reset**: `canonical_in_memory_state.clear_state()` is called because the database is now the authority.
    3.  **Head Update**: `tree_state.set_canonical_head` updates the local tip to the pipeline's result.

#### Step 6: Handling Unwinds
If the pipeline returns `ControlFlow::Unwind` (e.g., due to a bad block found during sync):
*   **Logic**: `on_backfill_sync_finished` detects `ctrl.is_unwind()`.
*   **Action**: It inserts the bad block into `invalid_headers`.
*   **Reset**: Calls `tree_state.reset(backfill_num_hash)` to completely clear the in-memory tree above the unwind point, forcing the node to re-sync from the valid ancestor.

---

### 4. Key Abstractions & Types

*   **`BackfillSync` (Trait)**: Abstract interface for the backfill process. Implemented by `PipelineSync`.
*   **`PipelineSync`** (`crates/engine/tree/src/backfill.rs`): Wraps the `Pipeline`. It uses a `oneshot` channel to receive the result (`PipelineWithResult`) from the spawned task.
*   **`BackfillSyncState`**: Enum (`Idle`, `Pending`, `Active`) stored in `EngineApiTreeHandler`. Used to guard database access.
*   **`ControlFlow`**: Return type from the pipeline. Variants: `Continue` (sync success), `Unwind` (reorg/bad block), `NoProgress`.
*   **`MIN_BLOCKS_FOR_PIPELINE_RUN`**: Constant (`32`) defining the boundary between "live sync" (downloading single blocks via `BlockDownloader`) and "backfill sync" (running the Pipeline).

---

### 5. Where to Look Next

1.  **`crates/engine/tree/src/tree/mod.rs`**: Specifically `on_backfill_sync_finished`. This is the critical convergence point where the "old" world (in-memory tree) reconciles with the "new" world (database state after pipeline).
2.  **`crates/engine/tree/src/chain.rs`**: The `poll_next_event` loop shows the priority inversion: the orchestrator polls the backfill sync *before* the handler, ensuring the pipeline drives the process when active.
3.  **`crates/engine/tree/src/backfill.rs`**: `try_spawn_pipeline` shows how the `TaskSpawner` is used to run the pipeline without blocking the engine's event loop.

---

### 6. Suggested Follow-up Questions

*   "How does `PipelineSync` ensure exclusive write access to the database, given that `EngineApiTreeHandler` also writes to the DB via `PersistenceHandle`?"
*   "What happens to `pending_events` in the `BlockDownloader` when a backfill action triggers `DownloadAction::Clear`?"
*   "How does the `CanonicalInMemoryState` populate itself after a backfill finishes and clears the previous state?"

## Verification
- Backfill threshold uses `MIN_BLOCKS_FOR_PIPELINE_RUN = EPOCH_SLOTS` and is checked via `exceeds_backfill_run_threshold`.```76:85:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
pub(crate) const MIN_BLOCKS_FOR_PIPELINE_RUN: u64 = EPOCH_SLOTS;
```
```2170:2177:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
const fn exceeds_backfill_run_threshold(&self, local_tip: u64, block: u64) -> bool {
    block > local_tip && block - local_tip > MIN_BLOCKS_FOR_PIPELINE_RUN
}
```
- Backfill target selection uses forkchoice sync target state and may select the finalized hash or head (optimistic) when exceeding the threshold.```2190:2261:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
fn backfill_sync_target(&self, canonical_tip_num: u64, target_block_number: u64, downloaded_block: Option<BlockNumHash>) -> Option<B256> { ... }
```
- Backfill actions are emitted by the tree, set `backfill_sync_state` to Pending, and are blocked if persistence is in progress.```1726:1746:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
if event.is_backfill_action() {
    if self.persistence_state.in_progress() { ... return }
    self.backfill_sync_state = BackfillSyncState::Pending;
}
```
- Forkchoice updates return Syncing while backfill is active.```1035:1055:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
if !self.backfill_sync_state.is_idle() {
    return Ok(Some(OnForkChoiceUpdated::syncing()));
}
```
- ChainOrchestrator polls backfill sync first, then handler; it forwards backfill actions and start/finish events.```74:127:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/chain.rs
match this.backfill_sync.poll(cx) { ... BackfillEvent::Started/Finished ... }
...
match handler_event { HandlerEvent::BackfillAction(action) => this.backfill_sync.on_action(action); }
```
- PipelineSync spawns the pipeline with a oneshot result and marks it active.```132:176:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/backfill.rs
fn try_spawn_pipeline(&mut self) -> Option<BackfillEvent> { ... spawn_critical_blocking ... pipeline.run_as_fut(Some(target)) ... }
```
- EngineHandler forwards backfill actions and clears the downloader before handoff.```82:93:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/engine.rs
HandlerEvent::BackfillAction(target) => {
    self.downloader.on_action(DownloadAction::Clear);
    Poll::Ready(HandlerEvent::BackfillAction(target))
}
```
- Backfill finish handling clears/updates tree state; unwind resets the tree to the backfill height and marks invalid headers.```1551:1617:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
if let ControlFlow::Unwind { bad_block, target } = &ctrl { ... self.state.invalid_headers.insert(**bad_block); }
...
if ctrl.is_unwind() { self.state.tree_state.reset(backfill_num_hash) } else { self.state.tree_state.remove_until(...) }
...
self.canonical_in_memory_state.clear_state();
self.state.tree_state.set_canonical_head(new_head.num_hash());
```

## Corrections / Caveats
- The threshold is `> MIN_BLOCKS_FOR_PIPELINE_RUN` (strictly greater), not `>=`.
- Backfill is tied to forkchoice state and finalized hash handling, not just "tip vs target"; optimistic mode can pick the head hash when finalized is unknown.
- The orchestrator does not directly poll the tree; it polls the handler, which emits backfill actions via `EngineApiEvent`.
- PipelineSync notes that running the pipeline acquires exclusive DB write access; the handler skips writes during backfill to avoid deadlocks.

## Actionable Pointers
- Backfill decision logic: `reth/crates/engine/tree/src/tree/mod.rs` (`backfill_sync_target`, `exceeds_backfill_run_threshold`).
- Orchestration flow: `reth/crates/engine/tree/src/chain.rs` and `reth/crates/engine/tree/src/engine.rs`.
- Pipeline execution wrapper: `reth/crates/engine/tree/src/backfill.rs`.
</file>

<file path="questions/Q020-rollback-semantics-reorgs.md">
# Q020 - Rollback Semantics on Reorgs

Status: verified

## Question
What are rollback semantics on reorgs across engine + persistence?

## Context Packs
- `spec/reth_repo_context/context-engine.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-engine.xml

Question (Q020):
What are rollback semantics on reorgs across engine + persistence?
I want how in-memory state and persisted blocks are rolled back, and which paths trigger disk reorg handling.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Describe how reorgs affect TreeState vs CanonicalInMemoryState.
3. Explain how on-disk reorg detection works (find_disk_reorg / remove_blocks).
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-engine.xml` pack, here is the analysis of rollback semantics and reorg handling in Reth.

### 1. Summary
*   **Two-Layer Reorgs**: Reth handles reorgs in two distinct layers: **In-Memory** (instant, frequent) and **Persistence** (background, infrequent).
*   **In-Memory Swapping**: The `TreeState` keeps all executed blocks (canonical and sidechains). When an in-memory reorg occurs, the engine simply updates the `current_canonical_head` pointer and swaps the relevant blocks in `CanonicalInMemoryState`. No re-execution is needed if the blocks are already in the tree.
*   **Preservation of History**: When a reorg happens, the "old" canonical blocks are not deleted; they are re-inserted into `TreeState` as sidechains (`reinsert_reorged_blocks`), ensuring they are available if the chain reorgs back.
*   **Async Disk Reorgs**: Because persistence happens in the background, the database might contain blocks that are no longer canonical. The engine detects this via `find_disk_reorg`.
*   **Destructive Disk Rollback**: If a disk reorg is detected, the engine issues a `PersistenceAction::RemoveBlocksAbove` command. The persistence service physically deletes the invalid blocks and execution data from the database to align it with the canonical in-memory state.
*   **Unwind on Backfill**: If a bad block is detected during backfill/pipeline sync, the engine triggers a "Reset," clearing the in-memory tree state entirely above the valid ancestor.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/engine/tree/src/tree/mod.rs` | `on_new_head` | Core logic to detect if a new head requires a `Commit` (extend) or `Reorg` (switch fork). |
| `crates/engine/tree/src/tree/mod.rs` | `find_disk_reorg` | Determines if the block persisted on disk diverges from the current in-memory canonical chain. |
| `crates/engine/tree/src/tree/mod.rs` | `advance_persistence` | The loop that checks for disk reorgs and triggers removal if found. |
| `crates/engine/tree/src/persistence.rs` | `PersistenceService::on_remove_blocks_above` | The handler that executes the physical deletion of blocks from the DB. |
| `crates/engine/tree/src/tree/state.rs` | `TreeState::reinsert_reorged_blocks` | Ensures blocks removed from the canonical chain are kept as sidechains. |

---

### 3. Reorg Algorithm Flow

#### A. In-Memory Reorg (High Frequency)
Triggered by `on_forkchoice_updated` when the new head is not a direct child of the current head.

1.  **Detection**: `on_new_head` walks backwards from the new head and the current head until a common ancestor is found.
2.  **Classification**: Returns `NewCanonicalChain::Reorg { new, old }`.
3.  **Application**: `on_canonical_chain_update` is called.
    *   **Update Head**: `tree_state.set_canonical_head(new_tip)`.
    *   **Swap**: `canonical_in_memory_state.update_chain(...)` replaces the chain view.
    *   **Preserve Old**: `reinsert_reorged_blocks(old)` ensures the discarded blocks remain in `blocks_by_hash` as sidechains.
    *   **Emit**: Emits `CanonicalChainCommitted`.

#### B. On-Disk Reorg (Background Correction)
Triggered during the `advance_persistence` check in the main engine loop.

1.  **Check**: `find_disk_reorg` compares `persistence_state.last_persisted_block` against `state.tree_state.current_canonical_head`.
2.  **Walk Back**: It walks the in-memory canonical chain backwards.
    *   If `last_persisted_block` is found in the canonical ancestry: **No Reorg**.
    *   If not found: **Reorg Detected**.
3.  **Find Ancestor**: It walks both chains (persisted vs canonical) backwards until `common_ancestor` is found.
4.  **Trigger Removal**: It calls `self.remove_blocks(common_ancestor.number)`.
5.  **Execution**:
    *   Sends `PersistenceAction::RemoveBlocksAbove` to the `PersistenceService`.
    *   Service calls `provider_rw.remove_block_and_execution_above`.
    *   Blocks/Receipts/State above the common ancestor are deleted from the DB.
6.  **Cleanup**: Once the persistence service confirms deletion, `on_new_persisted_block` ensures in-memory state is consistent.

#### C. Backfill Unwind (Bad Block)
Triggered if the pipeline returns `ControlFlow::Unwind`.

1.  **Trigger**: `on_backfill_sync_finished` receives the unwind signal.
2.  **Mark Invalid**: The bad block is added to `invalid_headers` cache.
3.  **Reset**: `tree_state.reset(valid_ancestor)` is called.
    *   This completely wipes the `blocks_by_hash` map in `TreeState`, effectively dropping all sidechains and pending blocks.
    *   The engine restarts synchronization from the valid ancestor.

---

### 4. Key Abstractions & Types

*   **`TreeState`** (`crates/engine/tree/src/tree/state.rs`):
    *   The "God View" of all executed blocks. It doesn't care about canonical vs sidechain, it stores everything validated. Reorgs here are just pointer updates.
*   **`CanonicalInMemoryState`** (`reth-chain-state` - referenced in `tree/mod.rs`):
    *   Represents the specific chain the node currently follows. It must always align with the tip of `TreeState`.
*   **`PersistenceState`** (`crates/engine/tree/src/tree/persistence_state.rs`):
    *   Tracks `last_persisted_block`. This pointer lags behind the in-memory head. The gap between this and the in-memory head creates the possibility for "Disk Reorgs".
*   **`NewCanonicalChain`** (Enum):
    *   Variants: `Commit` (Append) or `Reorg` (Switch). Used to signal changes to the rest of the system.

---

### 5. Where to Look Next

1.  **`crates/engine/tree/src/tree/mod.rs`**: Read `find_disk_reorg` (approx line 1500). It contains the exact logic for determining if the database has drifted from the memory state.
2.  **`crates/engine/tree/src/persistence.rs`**: Read `on_remove_blocks_above`. This is the "destructive" part of the rollback where data is actually purged.
3.  **`crates/engine/tree/src/tree/state.rs`**: Read `prune_finalized_sidechains`. This shows how old forks (history that wasn't chosen) are eventually garbage collected from memory.

---

### 6. Suggested Follow-up Questions

*   "How does `MemoryOverlayStateProvider` behave during a reorg? Does it have to discard cached state lookups?"
*   "What happens if the `PersistenceService` fails (crashes) during a `RemoveBlocksAbove` operation? Is the database left in an inconsistent state?"
*   "How does the `InvalidHeaderCache` prevent the engine from re-downloading and re-executing a block that caused a reorg/unwind previously?"

## Verification
- `on_new_head` walks back the new and old chains, returning `NewCanonicalChain::Commit` or `Reorg`.```708:802:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
fn on_new_head(&self, new_head: B256) -> ProviderResult<Option<NewCanonicalChain<N>>> { ... }
```
- Canonical update sets the canonical head, reinserts reorged blocks, updates canonical in-memory state, and emits `CanonicalChainCommitted`.```2320:2353:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
fn on_canonical_chain_update(&mut self, chain_update: NewCanonicalChain<N>) { ... }
```
```2364:2375:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
fn reinsert_reorged_blocks(&mut self, new_chain: Vec<ExecutedBlock<N>>) { ... }
```
- Disk reorg detection walks canonical and persisted chains to a common ancestor and returns the block number to truncate above.```2271:2314:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
fn find_disk_reorg(&self) -> ProviderResult<Option<u64>> { ... }
```
- Persistence loop checks `find_disk_reorg` and triggers `remove_blocks`; removal sends `PersistenceAction::RemoveBlocksAbove`.```1280:1292:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
fn advance_persistence(&mut self) -> Result<(), AdvancePersistenceError> { ... }
```
```1246:1255:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
fn remove_blocks(&mut self, new_tip_num: u64) { ... }
```
- Persistence service deletes above a target by calling `remove_block_and_execution_above`, then commits.```123:137:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/persistence.rs
fn on_remove_blocks_above(&self, new_tip_num: u64) -> Result<Option<BlockNumHash>, PersistenceError> { ... }
```
- Storage provider removal unwinds trie/state, removes blocks above, and updates pipeline stages; static file pruning happens in `remove_blocks_above`.```3095:3106:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/provider.rs
fn remove_block_and_execution_above(&self, block: BlockNumber) -> ProviderResult<()> { ... }
```
```3200:3253:/Users/vicnaum/github/stateless-history-node/reth/crates/storage/provider/src/providers/database/provider.rs
fn remove_blocks_above(&self, block: BlockNumber) -> ProviderResult<()> { ... }
```
- Backfill unwind marks invalid headers and resets the tree to the unwind target.```1558:1592:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
if let ControlFlow::Unwind { bad_block, target } = &ctrl { ... }
...
if ctrl.is_unwind() { self.state.tree_state.reset(backfill_num_hash) }
```
- `TreeState::reset` replaces the state with an empty tree pointing at the provided head.```55:57:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/state.rs
pub(crate) fn reset(&mut self, current_canonical_head: BlockNumHash) {
    *self = Self::new(current_canonical_head, self.engine_kind);
}
```
- After persistence completes, the engine calls `on_new_persisted_block` to reconcile in-memory state.```1823:1836:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/mod.rs
fn on_new_persisted_block(&mut self) -> ProviderResult<()> { ... }
```
- Old forks below finalized are pruned by `prune_finalized_sidechains`.```197:247:/Users/vicnaum/github/stateless-history-node/reth/crates/engine/tree/src/tree/state.rs
pub(crate) fn prune_finalized_sidechains(&mut self, finalized_num_hash: BlockNumHash) { ... }
```

## Corrections / Caveats
- `TreeState` does not keep all executed blocks indefinitely; pruning and backfill cleanup remove canonical blocks and old sidechains.
- Reorg triggering is not explicitly "new head is not a direct child"; `on_new_head` computes commit vs reorg based on chain connectivity and head position.
- Details of what is removed (state vs static files) live in storage provider code; the engine pack only shows the call sites.

## Actionable Pointers
- In-memory reorg algorithm: `reth/crates/engine/tree/src/tree/mod.rs` (`on_new_head`, `on_canonical_chain_update`, `reinsert_reorged_blocks`).
- Disk reorg detection and removal: `reth/crates/engine/tree/src/tree/mod.rs` (`find_disk_reorg`, `advance_persistence`, `remove_blocks`).
- Persistence deletion path: `reth/crates/engine/tree/src/persistence.rs` and `reth/crates/storage/provider/src/providers/database/provider.rs`.
</file>

<file path="questions/Q021-rpc-server-wiring.md">
# Q021 - RPC Server Wiring

Status: verified

## Question
How is the RPC server built and modules wired together?

## Context Packs
- `spec/reth_repo_context/context-rpc.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-rpc.xml

Question (Q021):
How is the RPC server built and modules wired together?
I want the construction flow, module registration (eth, net, admin, debug, trace, engine), and how transport configs map to modules.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Describe how RpcModuleBuilder/TransportRpcModules are created and configured.
3. Explain where auth RPC is configured and how rate limiting/middleware attach.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the provided `context-rpc.xml` pack, here is the architectural map of how the Reth RPC server is built, configured, and wired.

### 1. Summary
*   **Builder Pattern Construction**: The `RpcModuleBuilder` acts as the central factory, taking core node components (Provider, Pool, Network, EVM) and producing a registry of instantiated RPC handlers.
*   **Transport-Specific Isolation**: Modules are not globally enabled; they are explicitly mapped to transports (HTTP, WS, IPC) via `TransportRpcModuleConfig`. This allows exposing different APIs on different interfaces.
*   **Registry as Hub**: The `RpcRegistryInner` is the internal state container that holds the actual instances of API handlers (e.g., `EthApi`, `DebugApi`) and lazily generates `jsonrpsee` method tables.
*   **Separation of Auth**: The Engine API (Auth RPC) is constructed separately via `AuthServerConfig` and `AuthRpcModule` to enforce JWT authentication and isolate consensus-critical methods.
*   **Layered Configuration**: Configuration flows from CLI arguments (`RpcServerArgs`) -> Builder Configs (`RpcModuleConfig`) -> Server Configs (`RpcServerConfig`).
*   **Middleware Stack**: The server supports a middleware stack for functionality like CORS, JWT validation, and Rate Limiting (`RpcRequestRateLimiter`), applied via Tower layers.

---

### 2. Entry Points

| File Path | Type | Purpose |
| :--- | :--- | :--- |
| `crates/rpc/rpc-builder/src/lib.rs` | `RpcModuleBuilder` | The primary entry point for assembling the RPC layer. It requires node components to initialize. |
| `crates/node/core/src/args/rpc_server.rs` | `RpcServerArgs` | Parses CLI flags (`--http`, `--http.api`) and converts them into server configurations. |
| `crates/rpc/rpc-builder/src/lib.rs` | `RpcRegistryInner` | The internal container that instantiates API implementations (`EthApi`, `NetApi`, etc.) and maps them to namespaces. |
| `crates/rpc/rpc-builder/src/lib.rs` | `RpcServerConfig` | Configures and starts the actual `jsonrpsee` servers (binds ports, applies middleware). |
| `crates/rpc/rpc-builder/src/auth.rs` | `AuthServerConfig` | Specifically handles the configuration and startup of the Authenticated Engine API (port 8551). |

---

### 3. Construction & Wiring Flow

#### Step 1: Configuration Parsing
The process begins with `RpcServerArgs`. It defines methods like `transport_rpc_module_config()` which determines which `RethRpcModule`s (e.g., `eth`, `admin`) are active for which transport.
*   **File**: `crates/node/core/src/args/rpc_server.rs`

#### Step 2: Builder Initialization
The node initializes `RpcModuleBuilder` with the core components.
*   **Function**: `RpcModuleBuilder::new`
*   **File**: `crates/rpc/rpc-builder/src/lib.rs`

#### Step 3: Registry Creation (`into_registry`)
The builder creates an `RpcRegistryInner`. This struct instantiates the heavy-lifting API handlers.
*   **Logic**: It creates the `EthApi` (using `EthApiBuilder`), `EthFilter`, and `EthPubSub`. It stores these in `EthHandlers`.
*   **File**: `crates/rpc/rpc-builder/src/lib.rs`

#### Step 4: Module Registration & Method Generation
When `create_transport_rpc_modules` is called, the registry iterates over the selected `RethRpcModule` enums.
*   **Function**: `reth_methods` in `crates/rpc/rpc-builder/src/lib.rs`.
*   **Wiring**:
    *   Matches `RethRpcModule::Eth` -> Calls `register_eth` -> Merges `EthApi`, `EthFilter`, `EthPubSub`, and `EthBundle` into one `jsonrpsee::Methods` map.
    *   Matches `RethRpcModule::Debug` -> Instantiates `DebugApi`.
    *   Matches `RethRpcModule::Admin` -> Instantiates `AdminApi`.

#### Step 5: Transport Grouping
The builder outputs a `TransportRpcModules` struct. This contains three optional `RpcModule<()>` fields: one for HTTP, one for WS, and one for IPC. Each contains the specific set of methods allowed on that transport.
*   **File**: `crates/rpc/rpc-builder/src/lib.rs`

#### Step 6: Server Startup
Finally, `RpcServerConfig::start` is called with the `TransportRpcModules`.
*   **Logic**:
    1.  Builds the `jsonrpsee` server.
    2.  Applies Middleware (CORS, JWT, RateLimits).
    3.  Starts the server handle with the configured modules.
*   **File**: `crates/rpc/rpc-builder/src/lib.rs`

---

### 4. Auth (Engine) RPC Configuration

The Auth RPC is handled distinctly to ensure security for Consensus Layer communication.

1.  **Configuration**: Handled by `AuthServerConfig`.
2.  **Module**: `AuthRpcModule` is created via `RpcRegistryInner::create_auth_module`.
3.  **Wiring**:
    *   It registers the **Engine API** implementation provided by the node.
    *   It merges a **subset** of the `EthApi` (specifically `syncing`, `chainId`, `blockNumber`, `call`, `sendRawTransaction`, etc.) required by the Engine spec.
    *   It specifically uses `EngineEthApi` wrapper to limit the scope of exposed `eth_` methods.
4.  **Security**: `AuthServerConfig::start` enforces `AuthLayer` with a `JwtSecret`.

*   **Cite**: `crates/rpc/rpc-builder/src/auth.rs` and `crates/rpc/rpc-api/src/engine.rs`

---

### 5. Key Abstractions & Types

| Type | File Path | Description |
| :--- | :--- | :--- |
| `RethRpcModule` | `crates/rpc/rpc-server-types/src/module.rs` | Enum of supported namespaces (`Eth`, `Debug`, `Trace`, etc.). |
| `RpcModuleSelection` | `crates/rpc/rpc-server-types/src/module.rs` | logic for selecting modules (e.g., "All", "Standard", or specific list). |
| `TransportRpcModuleConfig` | `crates/rpc/rpc-builder/src/lib.rs` | Maps `RpcModuleSelection` to specific transports (Http, Ws, Ipc). |
| `RpcRegistryInner` | `crates/rpc/rpc-builder/src/lib.rs` | Holds the live instances of `EthApi`, `Pool`, `Network` and generates `Methods` on demand. |
| `RpcRequestRateLimiter` | `crates/rpc/rpc-builder/src/rate_limiter.rs` | Tower layer for rate-limiting expensive calls (like `trace_` or `debug_`) using a Semaphore. |
| `EthHandlers` | `crates/rpc/rpc-builder/src/eth.rs` | A struct grouping `EthApi`, `EthFilter`, and `EthPubSub` so they can be registered together. |

---

### 6. Where to Look Next

1.  **`crates/rpc/rpc-builder/src/lib.rs` -> `reth_methods`**: This massive match statement is the definitive source of truth for which Rust struct implements which RPC namespace string (e.g., "debug" maps to `DebugApi`).
2.  **`crates/rpc/rpc-builder/src/config.rs`**: Look here to see how CLI flags translate into the server configuration.
3.  **`crates/rpc/rpc-builder/src/auth.rs`**: To understand exactly how the Engine API is secured and isolated from the standard user-facing RPC.

---

### 7. Suggested Follow-up Questions

*   "How does `EthApi` handle the difference between `eth_call` (state access) and `eth_sendRawTransaction` (pool access) internally?"
*   "How are blocking tasks (like `debug_traceTransaction`) managed to prevent stalling the async RPC server loop?" (Hint: Look at `SpawnBlocking` in `helpers/blocking_task.rs`).
*   "How does the `RpcModuleBuilder` handle IPC socket permissions and endpoints on different operating systems?"

## Verification
- `RpcModuleBuilder::build` and `build_with_auth_server` create a registry, build transport modules, and optionally build the auth module.```322:399:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
pub fn build_with_auth_server<EthApi>(...) -> (TransportRpcModules, AuthRpcModule, RpcRegistryInner<...>) { ... }
...
pub fn build<EthApi>(...) -> TransportRpcModules<()> { ... }
```
- `RpcRegistryInner` holds `EthHandlers`, a module cache, and lazily generates `Methods` via `reth_methods`.```521:553:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
pub fn new(...) -> Self { ... let eth = EthHandlers::bootstrap(...); ... modules: Default::default(), ... }
```
```917:1007:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
pub fn reth_methods(...) -> Vec<Methods> { ... match namespace { RethRpcModule::Eth => { ... } ... } }
```
- `reth_methods` merges `EthApi`, `EthFilter`, `EthPubSub`, and `EthBundle` for the `eth_` namespace; other namespaces create their own APIs on demand.```947:999:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
RethRpcModule::Eth => { ... module.merge(eth_filter...).merge(eth_pubsub...)... EthBundle::new(...) ... }
```
- Auth module wiring merges engine API and a restricted `EngineEthApi` subset.```857:871:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
pub fn create_auth_module(&self, engine_api: impl IntoEngineApiRpcModule) -> AuthRpcModule { ... EngineEthApi::new(...) ... module.merge(engine_eth.into_rpc()) ... }
```
```24:147:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/engine.rs
pub struct EngineEthApi<...> { ... }
// methods: syncing, chain_id, block_number, call, get_code, block_by_hash, block_by_number, block_receipts, send_raw_transaction, transaction_receipt, logs, get_proof
```
- `AuthServerConfig::start` enforces JWT via `AuthLayer` and starts the auth server (HTTP/WS + optional IPC).```70:116:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/auth.rs
let middleware = tower::ServiceBuilder::new().layer(AuthLayer::new(JwtAuthValidator::new(secret)));
...
let server = ServerBuilder::new()...set_http_middleware(middleware)...build(socket_addr)...
```
- `RpcServerConfig::start` applies CORS/JWT/compression layers and uses `rpc_middleware` for custom RPC middleware.```1298:1448:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
ServerBuilder::new()
    .set_http_middleware(... option_layer(Self::maybe_cors_layer(...)) ... option_layer(Self::maybe_jwt_layer(...)) ... option_layer(Self::maybe_compression_layer(...)))
    .set_rpc_middleware(RpcServiceBuilder::default().layer(...).layer(self.rpc_middleware.clone()))
```
- `TransportRpcModules` stores per-transport `RpcModule` instances.```1629:1639:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
pub struct TransportRpcModules<Context = ()> { config: TransportRpcModuleConfig, http: Option<RpcModule<Context>>, ws: Option<RpcModule<Context>>, ipc: Option<RpcModule<Context>> }
```
- `RpcServerArgs` produces `TransportRpcModuleConfig`, defaulting http/ws to `standard_modules` and ipc to `default_ipc_modules`.```140:162:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/config.rs
if self.http { config = config.with_http(self.http_api.clone().unwrap_or_else(|| RpcModuleSelection::standard_modules().into())); }
if self.ws { config = config.with_ws(self.ws_api.clone().unwrap_or_else(|| RpcModuleSelection::standard_modules().into())); }
if self.is_ipc_enabled() { config = config.with_ipc(RpcModuleSelection::default_ipc_modules()); }
```
- `RethRpcModule` includes more namespaces than the summary list (txpool, rpc, reth, ots, mev, flashbots, etc.).```295:327:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-server-types/src/module.rs
pub enum RethRpcModule { Admin, Debug, Eth, Net, Trace, Txpool, Web3, Rpc, Reth, Ots, Flashbots, Miner, Mev, Testing, Other(String) }
```

## Corrections / Caveats
- `RpcRequestRateLimiter` is provided as a middleware layer, but it is not wired into `RpcServerConfig::start` by default; it must be injected via `rpc_middleware` if desired.
- The `register_*` helpers exist, but the transport flow uses `reth_methods` to lazily instantiate methods for selected modules.
- Engine API wiring uses `EngineEthApi` from `crates/rpc/rpc/src/engine.rs`, not `crates/rpc/rpc-api/src/engine.rs`.

## Actionable Pointers
- Module wiring and transport mapping: `reth/crates/rpc/rpc-builder/src/lib.rs` (`build`, `create_transport_rpc_modules`, `reth_methods`).
- CLI to server config: `reth/crates/rpc/rpc-builder/src/config.rs` (`transport_rpc_module_config`, `rpc_server_config`, `auth_server_config`).
- Auth server and engine subset: `reth/crates/rpc/rpc-builder/src/auth.rs`, `reth/crates/rpc/rpc/src/engine.rs`.
</file>

<file path="questions/Q022-eth-getlogs.md">
# Q022 - eth_getLogs

Status: verified

## Question
How is eth_getLogs implemented?

## Context Packs
- `spec/reth_repo_context/context-rpc.xml`
- (optional) `spec/reth_repo_context/context-storage.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-rpc.xml

Question (Q022):
How is eth_getLogs implemented?
I want the query pipeline, filtering limits, and how storage access is performed (DB vs static files).

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Trace the main call path from RPC handler to provider/storage.
3. Describe limits and safeguards (max blocks, max logs, pagination).
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-rpc.xml` pack, here is the architecture of the `eth_getLogs` implementation in Reth.

### 1. Summary
*   **Two-Stage Filtering**: The implementation first filters blocks using **Bloom Filters** (in memory or via header access) to identify candidate blocks, then fetches **Receipts** only for those blocks to perform exact log matching.
*   **Hybrid Execution Mode**: To optimize performance, the filter logic switches between a **Cached Mode** (for recent blocks likely in memory/LRU cache) and a **Range Mode** (for historical blocks requiring disk I/O).
*   **Parallel Receipt Fetching**: When processing large historical ranges, the system spawns parallel blocking tasks to fetch receipts for multiple candidate blocks concurrently.
*   **Strict Resource Limits**: Queries are bounded by `max_blocks_per_filter` (range size) and `max_logs_per_response` (result size) to prevent DoS attacks.
*   **Storage Agnostic**: The RPC layer calls `provider.receipts_by_block()`. Whether this reads from MDBX or Static Files (flat files) is determined by the Provider implementation (not in this pack), making the RPC layer backend-agnostic.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/rpc/rpc/src/eth/filter.rs` | `EthFilter::logs` | The public RPC handler for `eth_getLogs`. Validates input and delegates to internal logic. |
| `crates/rpc/rpc/src/eth/filter.rs` | `get_logs_in_block_range` | The coordination function that checks block limits and spawns the blocking task for processing. |
| `crates/rpc/rpc/src/eth/filter.rs` | `get_logs_in_block_range_inner` | The core loop that iterates headers, checks blooms, and fetches receipts. |
| `crates/rpc/rpc-eth-types/src/logs_utils.rs` | `append_matching_block_logs` | The matcher logic that compares a specific log against the filter topics/addresses. |

---

### 3. Algorithm Flow

#### Step 1: Input Resolution & Validation
When `eth_getLogs` is called, `logs_for_filter` resolves the block range.
*   **Logic**: It converts `fromBlock`/`toBlock` tags (e.g., "latest", "safe") to block numbers.
*   **Limit Check**: It checks if `to_block - from_block` exceeds `max_blocks_per_filter`.
*   **File**: `crates/rpc/rpc/src/eth/filter.rs` -> `logs_for_filter`.

#### Step 2: Bloom Filter Scanning (`get_logs_in_block_range_inner`)
The request is spawned onto a `blocking_task` (to avoid stalling the async runtime).
1.  **Iterate Headers**: It iterates over headers in the range in chunks (`MAX_HEADERS_RANGE = 1000`).
2.  **Bloom Check**: For each header, it calls `filter.matches_bloom(header.logs_bloom())`.
3.  **Candidate Collection**: If the bloom matches, the header is added to `matching_headers`.
*   **Optimization**: Non-matching headers are skipped entirely, saving DB lookups for receipts.

#### Step 3: Receipt Fetching (Cached vs. Range Mode)
The system initializes a `RangeMode` to process the matching headers.
*   **Cached Mode**: If the range is small and close to the tip, it uses `CachedMode`. It queries `eth_cache.get_receipts_and_maybe_block`, hitting the LRU cache first.
*   **Range Mode**: For older/larger ranges, it uses `RangeBlockMode`.
    *   **Parallelism**: If the number of matching headers exceeds `PARALLEL_PROCESSING_THRESHOLD` (1000), it chunks them and spawns parallel `tokio::task::spawn_blocking` jobs to fetch receipts from the provider.
*   **File**: `crates/rpc/rpc/src/eth/filter.rs` -> `RangeBlockMode` struct.

#### Step 4: Log Matching
Once receipts are loaded, `append_matching_block_logs` is called for every receipt.
*   It iterates `receipt.logs()`.
*   It matches the log address and topics against the user's filter.
*   Matches are appended to `all_logs`.
*   **Safety**: If `all_logs.len()` exceeds `max_logs_per_response`, the query aborts early with an error.
*   **File**: `crates/rpc/rpc-eth-types/src/logs_utils.rs`.

---

### 4. Limits and Safeguards

Defined in `crates/rpc/rpc-eth-types/src/builder/config.rs` and enforced in `crates/rpc/rpc/src/eth/filter.rs`.

1.  **Block Range Limit**:
    *   **Config**: `max_blocks_per_filter` (default `100,000`).
    *   **Check**: In `get_logs_in_block_range`. Returns `EthFilterError::QueryExceedsMaxBlocks`.
2.  **Response Size Limit**:
    *   **Config**: `max_logs_per_response` (default `20,000` logs).
    *   **Check**: In `get_logs_in_block_range_inner`. Returns `EthFilterError::QueryExceedsMaxResults`.
3.  **Parallel Concurrency**:
    *   **Constant**: `DEFAULT_PARALLEL_CONCURRENCY` (4 tasks) used in `RangeBlockMode` to prevent one query from saturating all I/O threads.

---

### 5. Storage Access (DB vs. Static Files)

*   **Abstraction**: The RPC layer does **not** know about Static Files or MDBX directly. It interacts purely via the `Provider` trait.
*   **Call Site**: `self.filter_inner.provider().receipts_by_block(header.hash().into())?` inside `RangeBlockMode`.
*   **Implication**: The logic determining whether to read from the "Hot" database (MDBX) or "Cold" history (Static Files/NippyJar) resides in the `reth-provider` crate (not in this pack), specifically in `DatabaseProvider::receipts_by_block`.

---

### 6. Where to Look Next
1.  **`crates/rpc/rpc/src/eth/filter.rs`**: Read `get_logs_in_block_range_inner` to see the exact loop structure and how `RangeMode` handles the complexity of switching strategies.
2.  **`crates/rpc/rpc-eth-types/src/cache/mod.rs`**: To understand how `EthStateCache` serves recent receipts to `CachedMode` to avoid disk I/O.
3.  **`crates/rpc/rpc-eth-types/src/logs_utils.rs`**: For the low-level matching logic (`matching_block_logs_with_tx_hashes`).

### 7. Suggested Follow-up Questions
*   "How does the `EthStateCache` decide what to evict, and does `eth_getLogs` cause cache churn for other RPC methods?"
*   "Where is the implementation of `receipts_by_block` that routes between MDBX and Static Files?" (Requires access to `reth-provider` pack).
*   "How does `eth_getLogs` handle reorgs if a query is running while the chain updates?"

## Verification
- `eth_getLogs` handler forwards to `logs_for_filter`, which delegates to `EthFilterInner::logs_for_filter`.```409:412:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/filter.rs
async fn logs(&self, filter: Filter) -> RpcResult<Vec<Log>> {
    Ok(self.logs_for_filter(filter, self.inner.query_limits).await?)
}
```
```462:569:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/filter.rs
async fn logs_for_filter(self: Arc<Self>, filter: Filter, limits: QueryLimits) -> Result<Vec<Log>, EthFilterError> { ... }
```
- Range queries resolve block tags, reject ranges past head, and call `get_logs_in_block_range`.```540:568:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/filter.rs
let from = from_block.map(|num| self.provider().convert_block_number(num)).transpose()?.flatten();
let to = to_block.map(|num| self.provider().convert_block_number(num)).transpose()?.flatten();
...
if let Some(t) = to && t > info.best_number { return Err(EthFilterError::BlockRangeExceedsHead); }
...
let (from_block_number, to_block_number) = logs_utils::get_filter_block_range(from, to, start_block, info)?;
self.get_logs_in_block_range(filter, from_block_number, to_block_number, limits).await
```
- `get_logs_in_block_range` checks range limits and spawns a blocking task for the inner loop.```603:631:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/filter.rs
if let Some(max_blocks_per_filter) = limits.max_blocks_per_filter.filter(|limit| to_block - from_block > *limit) {
    return Err(EthFilterError::QueryExceedsMaxBlocks(max_blocks_per_filter))
}
...
self.task_spawner.spawn_blocking(Box::pin(async move {
    let res = this.get_logs_in_block_range_inner(&filter, from_block, to_block, limits).await;
    let _ = tx.send(res);
}));
```
- Header bloom filtering happens in `get_logs_in_block_range_inner` before receipt fetching.```655:682:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/filter.rs
let headers = self.provider().headers_range(from..=to)?;
...
if !filter.matches_bloom(header.logs_bloom()) { continue; }
matching_headers.push(SealedHeader::new(header, block_hash));
```
- Cached vs range mode is chosen by `RangeMode::new` and `should_use_cached_mode`.```1002:1022:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/filter.rs
if use_cached_mode && !sealed_headers.is_empty() {
    Self::Cached(CachedMode { filter_inner, headers_iter: sealed_headers.into_iter() })
} else {
    Self::Range(RangeBlockMode { ... })
}
```
```1031:1044:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/filter.rs
const fn should_use_cached_mode(...) -> bool { ... block_count <= adjusted_threshold && distance_from_tip <= adjusted_threshold }
```
- Range mode uses cached receipts when available and falls back to provider receipts.```1201:1217:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/filter.rs
let (maybe_block, maybe_receipts) = self.filter_inner.eth_cache().maybe_cached_block_and_receipts(header.hash()).await?;
...
match self.filter_inner.provider().receipts_by_block(header.hash().into())? { Some(receipts) => Arc::new(receipts), None => continue }
```
- Parallel receipt fetching uses `tokio::task::spawn_blocking` for large remaining header sets.```1179:1289:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/filter.rs
if remaining_headers >= PARALLEL_PROCESSING_THRESHOLD { self.spawn_parallel_tasks(range_headers); }
...
let chunk_task = tokio::task::spawn_blocking(move || { ... provider().receipts_by_block(...) ... });
```
- Matching logs are appended using `append_matching_block_logs`, which checks topics/addresses and fetches tx hashes when needed.```70:146:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-eth-types/src/logs_utils.rs
pub fn append_matching_block_logs<P>(...){
    for (receipt_idx, receipt) in receipts.iter().enumerate() {
        for log in receipt.logs() {
            if filter.matches(log) { ... provider.transaction_by_id(...) ... }
        }
    }
}
```
- Response size limit is enforced for multi-block ranges only.```712:731:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/filter.rs
let is_multi_block_range = from_block != to_block;
if let Some(max_logs_per_response) = limits.max_logs_per_response && is_multi_block_range && all_logs.len() > max_logs_per_response { ... }
```
- Default limits come from `EthConfig`, which uses `DEFAULT_MAX_BLOCKS_PER_FILTER` and `DEFAULT_MAX_LOGS_PER_RESPONSE`.```122:133:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-eth-types/src/builder/config.rs
max_blocks_per_filter: DEFAULT_MAX_BLOCKS_PER_FILTER,
max_logs_per_response: DEFAULT_MAX_LOGS_PER_RESPONSE,
```
```12:16:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-server-types/src/constants.rs
pub const DEFAULT_MAX_BLOCKS_PER_FILTER: u64 = 100_000;
pub const DEFAULT_MAX_LOGS_PER_RESPONSE: usize = 20_000;
```

## Corrections / Caveats
- `max_blocks_per_filter` is checked in `get_logs_in_block_range`, not in `logs_for_filter`.
- The bloom filter stage is skipped for `FilterBlockOption::AtBlockHash` and for the pending-block shortcut; those paths go straight to receipts.
- `max_logs_per_response` is only enforced for multi-block ranges, so single-block queries can exceed it without error.
- Range mode still consults the cache (`maybe_cached_block_and_receipts`) before falling back to provider reads.
- Parallel receipt fetching uses `tokio::task::spawn_blocking` inside `RangeBlockMode`, not the generic task spawner used by `get_logs_in_block_range`.

## Actionable Pointers
- Core handler and range flow: `reth/crates/rpc/rpc/src/eth/filter.rs` (`logs_for_filter`, `get_logs_in_block_range`, `get_logs_in_block_range_inner`).
- Cache integration: `reth/crates/rpc/rpc-eth-types/src/cache/mod.rs` (`get_receipts_and_maybe_block`, `maybe_cached_block_and_receipts`).
- Log matching and tx hash lookup: `reth/crates/rpc/rpc-eth-types/src/logs_utils.rs` (`append_matching_block_logs`).
</file>

<file path="questions/Q023-rpc-safety-knobs.md">
# Q023 - RPC Safety Knobs

Status: verified

## Question
What RPC safety knobs exist (bind/auth/rate limit)?

## Context Packs
- `spec/reth_repo_context/context-rpc.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-rpc.xml

Question (Q023):
What RPC safety knobs exist (bind/auth/rate limit)?
I want how CLI flags map into server config, what defaults exist, and how middleware is applied for http/ws/ipc and auth.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Include defaults for ports, JWT, CORS, compression, max sizes, and max connections.
3. Explain how auth RPC differs from regular RPC in configuration and middleware.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-rpc.xml` pack, here is the analysis of the RPC safety mechanisms in Reth.

### 1. Summary
*   **Layered Safety**: Reth applies safety controls at multiple levels: **Network** (Socket binding, CORS), **Protocol** (JWT Auth, Compression), **Server** (Request/Response sizes, Max connections), and **Application** (Semaphores for heavy EVM tasks).
*   **Secure Defaults**: Servers bind to `127.0.0.1` (localhost) by default to prevent accidental public exposure.
*   **Mandatory Auth for Engine**: The Engine API (`auth_server`) strictly requires a JWT secret, whereas the regular RPC server makes authentication optional.
*   **Resource Semaphores**: Instead of simple request-per-second rate limiting, Reth limits **concurrency** based on workload type. It uses specific semaphores for "Blocking IO" (e.g., `eth_call`) and "CPU Heavy" (e.g., `debug_traceTransaction`) tasks to prevent starvation.
*   **DoS Protection**: Strict limits on request body size (15MB default) and response body size (160MB default) prevent memory exhaustion attacks.

---

### 2. Entry Points

| File Path | Type | Purpose |
| :--- | :--- | :--- |
| `crates/node/core/src/args/rpc_server.rs` | `RpcServerArgs` | Defines all CLI flags (`--http`, `--ws`, `--rpc.gascap`) and their default values. |
| `crates/rpc/rpc-builder/src/config.rs` | `RethRpcServerConfig` | Maps CLI arguments into the internal `RpcServerConfig` builder. |
| `crates/rpc/rpc-builder/src/lib.rs` | `RpcServerConfig` | Builds the `jsonrpsee` server, applying middleware layers (CORS, Auth, Compression). |
| `crates/rpc/rpc-builder/src/auth.rs` | `AuthServerConfig` | specialized builder for the Engine API (port 8551) with mandatory authentication. |
| `crates/rpc/rpc-eth-api/src/helpers/blocking_task.rs` | `SpawnBlocking` | Trait defining how internal semaphores (`blocking_io_task_guard`, `tracing_task_guard`) are acquired. |

---

### 3. Safety Mechanisms & Configuration

#### A. Network Binding & CORS
*   **Defaults**: HTTP/WS bind to `127.0.0.1`.
*   **Flags**: `--http.addr`, `--ws.addr`.
*   **CORS**: Configured via `CorsLayer`.
    *   **Http**: `--http.corsdomain`.
    *   **WS**: `--ws.origins`.
    *   **Logic**: If `*` is provided, it allows any origin. Otherwise, it parses comma-separated values.
    *   **Conflict Check**: If HTTP and WS run on the same port, their CORS settings must match, or `WsHttpSamePortError` is returned (`crates/rpc/rpc-builder/src/config.rs`).

#### B. Authentication (JWT)
*   **Engine API (Auth Server)**:
    *   **Mandatory**: `AuthServerConfig::start` adds `AuthLayer` with `JwtAuthValidator`.
    *   **Secret**: Loaded via `--authrpc.jwtsecret`. If missing, one is auto-generated in the data dir.
    *   **File**: `crates/rpc/rpc-builder/src/auth.rs`.
*   **Regular RPC**:
    *   **Optional**: Configured via `--rpc.jwtsecret`.
    *   **Layer**: If present, `RpcServerConfig` adds the same `AuthLayer`.
    *   **File**: `crates/rpc/rpc-builder/src/lib.rs`.

#### C. DoS Protection (Server Limits)
These limits are applied at the `jsonrpsee` server level before requests reach Reth logic.
*   **Request Size**: `--rpc.max-request-size` (Default: 15 MB). Prevents large payloads from consuming bandwidth/memory.
*   **Response Size**: `--rpc.max-response-size` (Default: 160 MB). Important for `trace_` calls which can generate huge outputs.
*   **Connections**: `--rpc.max-connections` (Default: 500).
*   **Subscriptions**: `--rpc.max-subscriptions-per-connection` (Default: 1024).
*   **File**: `crates/node/core/src/args/rpc_server.rs`.

#### D. Application Rate Limiting (Concurrency Control)
Reth limits the concurrency of expensive operations rather than raw request counts.

1.  **Blocking IO (EVM Execution)**:
    *   **Used by**: `eth_call`, `eth_estimateGas`.
    *   **Flag**: `--rpc.max-blocking-io-requests` (Default: 256).
    *   **Mechanism**: `EthApi` holds a `Semaphore`. Requests must acquire a permit via `acquire_owned_blocking_io` before spawning the task.
    *   **File**: `crates/rpc/rpc/src/eth/core.rs` and `crates/rpc/rpc-eth-api/src/helpers/blocking_task.rs`.

2.  **Tracing (CPU Heavy)**:
    *   **Used by**: `debug_traceTransaction`, `trace_call`.
    *   **Flag**: `--rpc.max-tracing-requests`.
    *   **Default**: `std::thread::available_parallelism() - 2` (reserving cores for the rest of the node).
    *   **Mechanism**: A `BlockingTaskGuard` acts as a semaphore for the Rayon thread pool dedicated to tracing.
    *   **File**: `crates/rpc/rpc-server-types/src/constants.rs` (`default_max_tracing_requests`).

---

### 4. Key Defaults Table

| Configuration | CLI Flag | Default Value | Defined In |
| :--- | :--- | :--- | :--- |
| **HTTP Port** | `--http.port` | `8545` | `constants.rs` |
| **WS Port** | `--ws.port` | `8546` | `constants.rs` |
| **Auth Port** | `--authrpc.port` | `8551` | `constants.rs` |
| **Gas Cap** | `--rpc.gascap` | `50,000,000` | `constants.rs` |
| **Tx Fee Cap** | `--rpc.txfeecap` | `1.0` ETH | `rpc_server.rs` |
| **Max Request** | `--rpc.max-request-size` | 15 MB | `rpc_server.rs` |
| **Max Response** | `--rpc.max-response-size`| 160 MB | `rpc_server.rs` |
| **Max Logs** | `--rpc.max-logs-per-response`| 20,000 | `constants.rs` |
| **Max Trace Blocks**| `--rpc.max-trace-filter-blocks` | 100 | `constants.rs` |

---

### 5. Middleware Flow (HTTP/WS)

The `RpcServerConfig::start` method (`crates/rpc/rpc-builder/src/lib.rs`) constructs the stack:

1.  **Socket Binding**: `server.build(socket_addr)`.
2.  **HTTP Middleware** (`tower::ServiceBuilder`):
    *   `CorsLayer`: Validates Origin.
    *   `AuthLayer`: Validates JWT (if configured).
    *   `CompressionLayer`: Handles Gzip/Zstd (unless `--http.disable-compression` is set).
3.  **RPC Middleware** (`RpcServiceBuilder`):
    *   `RpcRequestMetrics`: Tracks calls/errors for Prometheus.
    *   *Custom Middleware*: (e.g., `RpcRequestRateLimiter` if injected manually, though standard CLI uses internal semaphores).
4.  **Handler**: The Request is dispatched to `EthApi` / `DebugApi`.
5.  **Internal Safety**: `EthApi` checks semaphores (`blocking_io_task_guard`) before executing.

---

### 6. Suggested Follow-up Questions
*   "How does the `JwtAuthValidator` validate tokens, and does it support key rotation?"
*   "How does the `EthStateCache` eviction policy interact with `rpc_evm_memory_limit` to prevent OOM?"
*   "What happens to a `debug_traceTransaction` request if the `BlockingTaskGuard` has zero permits available? Does it queue or fail immediately?"

## Verification
- Default binding is localhost for HTTP/WS/auth, with default ports 8545/8546/8551.```362:380:/Users/vicnaum/github/stateless-history-node/reth/crates/node/core/src/args/rpc_server.rs
http_addr: Ipv4Addr::LOCALHOST.into(),
http_port: constants::DEFAULT_HTTP_RPC_PORT,
ws_addr: Ipv4Addr::LOCALHOST.into(),
ws_port: constants::DEFAULT_WS_RPC_PORT,
auth_addr: Ipv4Addr::LOCALHOST.into(),
auth_port: constants::DEFAULT_AUTH_PORT,
```
- Default request/response size, max connections, and subscriptions are defined in `RpcServerArgs` defaults.```32:47:/Users/vicnaum/github/stateless-history-node/reth/crates/node/core/src/args/rpc_server.rs
pub(crate) const RPC_DEFAULT_MAX_SUBS_PER_CONN: u32 = 1024;
pub(crate) const RPC_DEFAULT_MAX_REQUEST_SIZE_MB: u32 = 15;
pub(crate) const RPC_DEFAULT_MAX_RESPONSE_SIZE_MB: u32 = 160;
pub(crate) const RPC_DEFAULT_MAX_CONNECTIONS: u32 = 500;
```
- Request/response sizes are converted from MB to bytes and applied to server builders.```128:173:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/config.rs
fn rpc_max_request_size_bytes(&self) -> u32 { self.rpc_max_request_size.get().saturating_mul(1024 * 1024) }
fn rpc_max_response_size_bytes(&self) -> u32 { self.rpc_max_response_size.get().saturating_mul(1024 * 1024) }
...
ServerConfigBuilder::new()
    .max_connections(self.rpc_max_connections.get())
    .max_request_body_size(self.rpc_max_request_size_bytes())
    .max_response_body_size(self.rpc_max_response_size_bytes())
    .max_subscriptions_per_connection(self.rpc_max_subscriptions_per_connection.get())
```
- CORS supports `*` or a comma list, and WS/HTTP same-port requires matching CORS.```22:49:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/cors.rs
match http_cors_domains.trim() { "*" => CorsLayer::new().allow_origin(Any) ... }
...
if iter.clone().any(|o| o == "*") { return Err(CorsDomainError::WildCardNotAllowed { ... }) }
```
```1332:1344:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
if self.http_addr == self.ws_addr && self.http_server_config.is_some() && self.ws_server_config.is_some() {
    if ws_cors.trim() != http_cors.trim() { return Err(WsHttpSamePortError::ConflictingCorsDomains { ... }.into()); }
}
```
- Regular RPC uses optional JWT auth; auth server always uses `AuthLayer`.```184:206:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/config.rs
let mut config = RpcServerConfig::default().with_jwt_secret(self.rpc_secret_key());
```
```1283:1440:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/lib.rs
option_layer(Self::maybe_jwt_layer(self.jwt_secret))
```
```70:96:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/auth.rs
let middleware = tower::ServiceBuilder::new().layer(AuthLayer::new(JwtAuthValidator::new(secret)));
```
- Auth server defaults differ (larger request/response sizes, max connections).```202:225:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/auth.rs
ServerConfig::builder()
    .max_response_body_size(750 * 1024 * 1024)
    .max_connections(500)
    .max_request_body_size(128 * 1024 * 1024)
```
- JWT secret for auth is read from file or created if missing.```239:245:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-builder/src/config.rs
match self.auth_jwtsecret.as_ref() { Some(fpath) => JwtSecret::from_file(fpath), None => get_or_create_jwt_secret_from_path(&default_jwt_path) }
```
- Blocking IO concurrency uses a semaphore and is acquired in `eth_call`.```303:307:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/eth/core.rs
/// Semaphore to limit concurrent blocking IO requests (`eth_call`, `eth_estimateGas`, etc.)
blocking_io_request_semaphore: Arc<Semaphore>,
```
```216:226:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-eth-api/src/helpers/call.rs
let _permit = self.acquire_owned_blocking_io().await;
```
- Tracing concurrency is governed by `BlockingTaskGuard` with default based on available CPUs.```35:43:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc-server-types/src/constants.rs
pub fn default_max_tracing_requests() -> usize { ... available_parallelism ... }
```
```52:67:/Users/vicnaum/github/stateless-history-node/reth/crates/rpc/rpc/src/trace.rs
async fn acquire_trace_permit(&self) -> Result<OwnedSemaphorePermit, AcquireError> {
    self.inner.blocking_task_guard.clone().acquire_owned().await
}
```

## Corrections / Caveats
- HTTP/WS servers are disabled by default (`http: false`, `ws: false`), even though default bind addresses are localhost.
- CORS conflict checks happen in `RpcServerConfig::start` (lib.rs), not in config.rs.
- Auth server uses different default size limits than regular RPC (128MB request, 750MB response).
- The `RpcRequestRateLimiter` layer exists but is not attached by default; concurrency limits are primarily via semaphores.

## Actionable Pointers
- CLI defaults and knobs: `reth/crates/node/core/src/args/rpc_server.rs`.
- Server middleware + CORS/auth/compression: `reth/crates/rpc/rpc-builder/src/lib.rs` and `reth/crates/rpc/rpc-builder/src/cors.rs`.
- Auth server defaults and JWT enforcement: `reth/crates/rpc/rpc-builder/src/auth.rs`.
- Concurrency guards: `reth/crates/rpc/rpc-eth-api/src/helpers/blocking_task.rs`, `reth/crates/rpc/rpc-eth-api/src/helpers/call.rs`, `reth/crates/rpc/rpc/src/trace.rs`.
</file>

<file path="questions/Q024-eth-request-handler.md">
# Q024 - EthRequestHandler (DevP2P ETH Requests)

Status: verified

## Question
How are devp2p ETH requests served (EthRequestHandler)?

## Context Packs
- `spec/reth_repo_context/context-network.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML pack:
- spec/reth_repo_context/context-network.xml

Question (Q024):
How are devp2p ETH requests served (EthRequestHandler)?
I want the request flow, how it hits providers/storage, and how responses are built.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Trace at least one request end-to-end (headers, bodies, receipts, etc.).
3. Describe any rate limits, timeouts, or peer validation that affect request serving.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the `context-network.xml` pack, here is the architectural breakdown of how Reth handles incoming DevP2P ETH requests via the `EthRequestHandler`.

### 1. Summary
*   **Dedicated Task**: Request serving is offloaded to a standalone background task (`EthRequestHandler`) to prevent database I/O from blocking the main `NetworkManager` or peer sessions.
*   **Channel-Based Routing**: Requests flow from `ActiveSession` (TCP) $\to$ `SessionManager` $\to$ `NetworkManager` $\to$ `EthRequestHandler` via unbounded and bounded channels.
*   **Oneshot Responses**: The `ActiveSession` creates a `oneshot` channel for every incoming request. The `EthRequestHandler` executes the database query and sends the result back through this channel, where `ActiveSession` picks it up and writes to the socket.
*   **Soft Limits**: Responses are capped by "Soft Limits" (e.g., 2MB or 1024 items) to prevent DoS attacks and bandwidth saturation, adhering to the DevP2P specs.
*   **Trait-Abstracted Storage**: The handler does not know about MDBX or RocksDB directly; it interacts with a generic `client` implementing traits like `BlockReader` and `HeaderProvider`.
*   **Protocol Agnostic Logic**: The handler serves data based on the request type (e.g., stripping receipt blooms for `eth/69` requests) while using shared underlying data fetching logic.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/net/network/src/eth_requests.rs` | `EthRequestHandler` | The worker struct that polls for requests and fetches data from the DB. |
| `crates/net/network/src/eth_requests.rs` | `IncomingEthRequest` | Enum wrapping the specific request (Headers/Bodies/Receipts) and the `oneshot::Sender` for the response. |
| `crates/net/network/src/manager.rs` | `NetworkManager::on_eth_request` | The routing function that moves requests from the swarm into the request handler channel. |
| `crates/net/network/src/session/active.rs` | `ActiveSession::on_incoming_message` | The initial point where bytes off the wire are decoded into a `Get*` request and the `oneshot` channel is created. |

---

### 3. Request Flow: "GetBlockHeaders" End-to-End

#### Step 1: Ingress (Session Layer)
An `ActiveSession` receives an encrypted packet, decodes it into `EthMessage::GetBlockHeaders`.
*   **File**: `crates/net/network/src/session/active.rs`
*   **Function**: `on_incoming_message`
*   **Logic**:
    1.  It matches `EthMessage::GetBlockHeaders(req)`.
    2.  It calls `oneshot::channel()` to create a `(tx, rx)` pair.
    3.  It stores the `rx` (receiver) in `self.received_requests_from_remote`.
    4.  It wraps the `tx` and the request into `PeerRequest::GetBlockHeaders`.
    5.  It emits this up to the Manager via `try_emit_request`.

#### Step 2: Routing (Manager Layer)
The `SessionManager` receives the event and bubbles it to the `NetworkManager`.
*   **File**: `crates/net/network/src/manager.rs`
*   **Function**: `on_peer_message` $\to$ `on_eth_request`
*   **Logic**:
    1.  `NetworkManager` receives `PeerMessage::EthRequest`.
    2.  It delegates to `delegate_eth_request`.
    3.  It converts the `PeerRequest` into an `IncomingEthRequest`.
    4.  It sends this object into `self.to_eth_request_handler` (a bounded mpsc channel).

#### Step 3: Processing (Handler Layer)
The `EthRequestHandler` loop picks up the message.
*   **File**: `crates/net/network/src/eth_requests.rs`
*   **Function**: `EthRequestHandler::poll`
*   **Logic**:
    1.  It drains `incoming_requests` stream using a budget.
    2.  Matches `IncomingEthRequest::GetBlockHeaders`.
    3.  Calls `self.on_headers_request(peer_id, request, response)`.

#### Step 4: Storage Access & Response Building
Inside `get_headers_response`, the handler queries the database.
*   **File**: `crates/net/network/src/eth_requests.rs`
*   **Function**: `get_headers_response`
*   **Logic**:
    1.  **Resolve Start**: Converts `BlockHashOrNumber` to a concrete block number using `self.client.block_hash` or `header_by_hash_or_number`.
    2.  **Loop**: Iterates up to `request.limit` (max 1024).
    3.  **Fetch**: Calls `self.client.header_by_hash_or_number`. (Note: The implementation of `BlockReader` is outside this pack).
    4.  **Enforce Limits**: Accumulates byte size. If `total_bytes > SOFT_RESPONSE_LIMIT` (2MB), breaks the loop early.
    5.  **Traversal**: Calculates the next block number based on `request.skip` and `request.direction` (Rising/Falling).

#### Step 5: Egress
The handler sends the result back.
*   **Logic**: `response.send(Ok(BlockHeaders(headers)))`.
*   **Back to Session**: The `ActiveSession` polling loop sees the `oneshot` `rx` is ready, wraps the `BlockHeaders` in an RLP message, and queues it for TCP transmission.

---

### 4. Key Abstractions & Types

*   **`EthRequestHandler<C>`**: Generic over `C`, the client. This decoupling allows the network crate to function without knowing if the DB is MDBX, in-memory, or a mock.
*   **`IncomingEthRequest`**:
    ```rust
    pub enum IncomingEthRequest<N: NetworkPrimitives> {
        GetBlockHeaders {
            peer_id: PeerId,
            request: GetBlockHeaders,
            response: oneshot::Sender<RequestResult<BlockHeaders<N::BlockHeader>>>,
        },
        // ... Bodies, Receipts, etc.
    }
    ```
*   **Limits Constants** (`crates/net/network/src/eth_requests.rs`):
    *   `MAX_HEADERS_SERVE`: 1024
    *   `MAX_BODIES_SERVE`: 1024
    *   `MAX_RECEIPTS_SERVE`: 1024
    *   `SOFT_RESPONSE_LIMIT`: 2 * 1024 * 1024 (2MB)

---

### 5. Where to Look Next

1.  **`crates/net/network/src/eth_requests.rs`**: Read `get_headers_response` and `on_bodies_request` to see the exact logic for traversal, skipping, and size limiting.
2.  **`crates/net/network/src/session/active.rs`**: Look at `on_incoming_message` to see how the `oneshot` channel is created and how `received_requests_from_remote` tracks pending DB queries.
3.  **`crates/net/network/src/manager.rs`**: Look at `delegate_eth_request` to see how the node handles the case where the `EthRequestHandler` channel is full (metrics increment: `total_dropped_eth_requests_at_full_capacity`).

---

### 6. Suggested Follow-up Questions
*   "Since `BlockReader` is a trait, where is the concrete implementation that reads from MDBX located?"
*   "How does `EthRequestHandler` handle `GetReceipts` differently for `eth/66` versus `eth/69` (bloom filter removal)?"
*   "What happens to the `ActiveSession` if the `EthRequestHandler` takes too long to respond (timeout logic in `active.rs`)?"

## Verification
- `EthRequestHandler` is a background service (Future) that polls incoming requests with a budget and dispatches to request-specific handlers.```316:356:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/eth_requests.rs
impl<C, N> Future for EthRequestHandler<C, N> { ... metered_poll_nested_stream_with_budget!(..., this.incoming_requests.poll_next_unpin(cx), |incoming| { match incoming { ... }}) }
```
- Requests are created in `ActiveSession::on_incoming_message` with a `oneshot` sender and queued `ReceivedRequest` that is later polled for the response.```168:185:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/session/active.rs
let (tx, response) = oneshot::channel();
let received = ReceivedRequest { request_id, rx: PeerResponse::$resp_item { response }, received: Instant::now() };
self.received_requests_from_remote.push(received);
self.try_emit_request(PeerMessage::EthRequest(PeerRequest::$req_item { request, response: tx }))
```
- Pending responses are polled and encoded back onto the wire.```618:628:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/session/active.rs
match req.rx.poll(cx) { Poll::Ready(resp) => { this.handle_outgoing_response(req.request_id, resp); } ... }
```
```374:378:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/session/active.rs
fn handle_outgoing_response(&mut self, id: u64, resp: PeerResponseResult<N>) { ... self.queued_outgoing.push_back(msg.into()); }
```
- Routing is ActiveSession -> SessionManager -> Swarm -> NetworkManager -> EthRequestHandler.```470:472:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/session/mod.rs
ActiveSessionMessage::ValidMessage { peer_id, message } => Poll::Ready(SessionEvent::ValidMessage { peer_id, message })
```
```151:152:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/swarm.rs
SessionEvent::ValidMessage { peer_id, message } => Some(SwarmEvent::ValidMessage { peer_id, message })
```
```610:635:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/manager.rs
PeerMessage::EthRequest(req) => { self.on_eth_request(peer_id, req); }
```
```498:505:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/manager.rs
PeerRequest::GetBlockHeaders { request, response } => { self.delegate_eth_request(IncomingEthRequest::GetBlockHeaders { peer_id, request, response }) }
```
- Channel capacity for EthRequestHandler is bounded (256) and drops on full with metrics increment.```19:71:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/builder.rs
pub(crate) const ETH_REQUEST_CHANNEL_CAPACITY: usize = 256;
let (tx, rx) = mpsc::channel(ETH_REQUEST_CHANNEL_CAPACITY);
```
```484:492:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/manager.rs
if let TrySendError::Full(_) = e { self.metrics.total_dropped_eth_requests_at_full_capacity.increment(1); }
```
- Soft limits and max counts enforced for headers/bodies/receipts; SOFT_RESPONSE_LIMIT is 2MB and MAX_* is 1024.```32:49:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/eth_requests.rs
pub const MAX_RECEIPTS_SERVE: usize = 1024;
pub const MAX_HEADERS_SERVE: usize = 1024;
pub const MAX_BODIES_SERVE: usize = 1024;
pub const SOFT_RESPONSE_LIMIT: usize = 2 * 1024 * 1024;
```
- Headers are served via `block_hash` + `header_by_hash_or_number`, respecting skip/direction and size limits.```88:116:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/eth_requests.rs
let GetBlockHeaders { start_block, limit, skip, direction } = request;
...
if let Some(header) = self.client.header_by_hash_or_number(block).unwrap_or_default() {
    total_bytes += header.length();
    headers.push(header);
    if headers.len() >= MAX_HEADERS_SERVE || total_bytes > SOFT_RESPONSE_LIMIT { break }
    match direction { HeadersDirection::Rising => ..., HeadersDirection::Falling => ... }
}
```
- Receipts: eth/66 uses `ReceiptWithBloom`, eth/69 skips bloom, eth/70 supports partial last block.```190:219:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/eth_requests.rs
let receipts = self.get_receipts_response(request, |receipts_by_block| { receipts_by_block.into_iter().map(ReceiptWithBloom::from).collect::<Vec<_>>() });
```
```205:218:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/eth_requests.rs
// skip bloom filter for eth69
```
```221:281:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/eth_requests.rs
GetReceipts70 { first_block_receipt_index, block_hashes } ... last_block_incomplete ... partial_block ...
```

## Corrections / Caveats
- The routing channels are bounded (e.g., ETH request handler capacity is 256); there is no unbounded channel in this path.
- `IncomingEthRequest::GetNodeData` currently only increments metrics; it does not build or send a response.
- The "soft limit" is enforced by encoded byte length, not by number of items alone; MAX_* limits are additional caps.
- Request throttling on the session side is via `MAX_QUEUED_OUTGOING_RESPONSES` (4), which pauses receiving when too many responses are pending.

## Actionable Pointers
- Request handling and limits: `reth/crates/net/network/src/eth_requests.rs`.
- Session request/response lifecycle: `reth/crates/net/network/src/session/active.rs`.
- Routing + channel backpressure: `reth/crates/net/network/src/manager.rs` and `reth/crates/net/network/src/builder.rs`.
</file>

<file path="questions/Q025-config-plumbing-network-sync.md">
# Q025 - Config Plumbing into Network + Sync

Status: verified

## Question
How does config plumbing flow into network and sync?
I want how CLI/config values reach NetworkConfigBuilder, peers, downloader settings, and pipeline/stage configs.

## Context Packs
- `spec/reth_repo_context/context-sync.xml`
- `spec/reth_repo_context/context-network.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML packs:
- spec/reth_repo_context/context-sync.xml
- spec/reth_repo_context/context-network.xml

Question (Q025):
How does config plumbing flow into network and sync?
I want how CLI/config values reach NetworkConfigBuilder, peers, downloader settings, and pipeline/stage configs.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Trace at least one concrete config value from args to runtime usage.
3. Call out defaults vs overrides.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Algorithm flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the provided XML packs (`context-sync.xml` and `context-network.xml`), here is the architectural map of how configuration flows into the network and sync components.

### 1. Summary
*   **Central Configuration Struct**: The `reth-config` crate defines a root `Config` struct that mirrors `reth.toml`, containing specialized sub-configs like `PeersConfig`, `SessionsConfig`, and `StageConfig`.
*   **Builder Pattern Bridge**: Configuration values are not passed raw to runtime components. Instead, they are typically consumed by "Builder" structs (e.g., `NetworkConfigBuilder`, `ReverseHeadersDownloaderBuilder`) which allow for further programmatic overrides before instantiation.
*   **Network Flow**: `PeersConfig` and `SessionsConfig` flow into `NetworkConfigBuilder`, which produces a `NetworkConfig`. This config is consumed by `NetworkManager` to spawn sub-components like `PeersManager` and `SessionManager`.
*   **Sync Flow**: `StageConfig` is split. Downloader configs (Headers/Bodies) are used to build downloader instances *before* the pipeline is assembled. Stage-specific settings (like Execution thresholds) are passed directly into stage constructors via `DefaultStages`.
*   **Defaults via Serde**: Default values are defined in the `impl Default` for the config structs and applied during deserialization or builder initialization.

---

### 2. Entry Points

| File Path | Type | Purpose |
| :--- | :--- | :--- |
| `crates/config/src/config.rs` | `Config` | The root configuration structure. Contains `stages`, `peers`, `sessions`, `prune`. |
| `crates/net/network/src/config.rs` | `NetworkConfigBuilder` | The primary builder for the P2P stack. Accepts `PeersConfig` and `SessionsConfig`. |
| `crates/net/downloaders/src/headers/reverse_headers.rs` | `ReverseHeadersDownloaderBuilder` | Consumes `HeadersConfig` to configure the header downloader. |
| `crates/net/downloaders/src/bodies/bodies.rs` | `BodiesDownloaderBuilder` | Consumes `BodiesConfig` to configure the body downloader. |
| `crates/stages/stages/src/sets.rs` | `DefaultStages` | Accepts `StageConfig` to configure pipeline stages (e.g., Execution, SenderRecovery). |

---

### 3. Configuration Flow

#### A. Network Configuration Flow
1.  **Definition**: `PeersConfig` and `SessionsConfig` are defined in `crates/net/network-types/src/peers/config.rs` and `crates/net/network-types/src/session/config.rs`.
2.  **Builder**: A `NetworkConfigBuilder` is created. The node logic (outside this pack) calls `builder.peer_config(config.peers)` and `builder.sessions_config(config.sessions)` (`crates/net/network/src/config.rs`).
3.  **Build**: `builder.build(client)` creates a `NetworkConfig`.
4.  **Instantiation**: `NetworkManager::new(config)` is called (`crates/net/network/src/manager.rs`).
5.  **Distribution**:
    *   `PeersConfig` is passed to `PeersManager::new` (`crates/net/network/src/peers.rs`).
    *   `SessionsConfig` is passed to `SessionManager::new` (`crates/net/network/src/session/mod.rs`).

#### B. Sync/Downloader Configuration Flow
1.  **Definition**: `HeadersConfig` and `BodiesConfig` are defined in `crates/config/src/config.rs`.
2.  **Builder**:
    *   `ReverseHeadersDownloaderBuilder::new(config.stages.headers)` copies values like `request_limit` and `stream_batch_size`.
    *   `BodiesDownloaderBuilder::new(config.stages.bodies)` does the same.
3.  **Instantiation**: `.build()` is called on these builders to create the actual downloader instances (e.g., `ReverseHeadersDownloader`).
4.  **Pipeline**: These instances are passed into `DefaultStages::new` (`crates/stages/stages/src/sets.rs`), which wires them into `HeaderStage` and `BodyStage`.

#### C. Execution Stage Configuration Flow
1.  **Definition**: `ExecutionConfig` in `crates/config/src/config.rs`.
2.  **Usage**: `DefaultStages::new` receives `StageConfig`.
3.  **Instantiation**: `ExecutionStage::from_config` (`crates/stages/stages/src/stages/execution.rs`) uses `config.execution` to create `ExecutionStageThresholds` (max blocks, max changes, etc.).

---

### 4. Trace: `downloader_request_limit`
This value controls how many headers are requested from a peer at once.

1.  **Definition**: `HeadersConfig` struct has `pub downloader_request_limit: u64` (Default: `1_000`).
    *   *File*: `crates/config/src/config.rs`
2.  **Transfer to Builder**: `ReverseHeadersDownloaderBuilder::new(config)` reads this field.
    ```rust
    // crates/net/downloaders/src/headers/reverse_headers.rs
    pub fn new(config: HeadersConfig) -> Self {
        Self::default()
            .request_limit(config.downloader_request_limit)
            // ...
    }
    ```
3.  **Builder Field**: Stored in `ReverseHeadersDownloaderBuilder` as `request_limit`.
4.  **Instantiation**: `build()` creates `ReverseHeadersDownloader` struct, moving `request_limit` into it.
5.  **Runtime Usage**: inside `next_request()`, the downloader uses `self.request_limit` to calculate `limit`.
    ```rust
    // crates/net/downloaders/src/headers/reverse_headers.rs
    let request = calc_next_request(local_head, self.next_request_block_number, self.request_limit);
    ```

---

### 5. Trace: `max_inbound` (Peers)
This value controls how many inbound TCP connections are accepted.

1.  **Definition**: `ConnectionsConfig` struct has `max_inbound` (Default: `30`).
    *   *File*: `crates/net/network-types/src/peers/config.rs`
2.  **Transfer**: `NetworkConfig` holds `peers_config`.
3.  **Instantiation**: `NetworkManager::new` calls `PeersManager::new(peers_config)`.
4.  **Storage**: `PeersManager` initializes `ConnectionInfo::new(config.connection_info)`.
5.  **Runtime Usage**: `PeersManager::on_incoming_pending_session` checks this limit.
    ```rust
    // crates/net/network/src/peers.rs
    if !self.connection_info.has_in_capacity() { ... }
    ```
    *   `has_in_capacity` checks `num_inbound < config.max_inbound`.

---

### 6. Suggested Follow-up Questions
*   "How are the `PruneConfig` values passed from `reth-config` into the `PruneStage` during pipeline construction?"
*   "Where is the `ChainSpec` configuration handled in this flow, specifically for `NetworkConfig`?"
*   "How does `TransactionsManagerConfig` flow from `NetworkConfigBuilder` into the `TransactionsManager`?"

## Verification
- `Config` is the root config and includes `stages`, `peers`, and `sessions`; `from_path` loads TOML or writes defaults when missing.```18:69:/Users/vicnaum/github/stateless-history-node/reth/crates/config/src/config.rs
pub struct Config {
    /// Configuration for each stage in the pipeline.
    pub stages: StageConfig,
    /// Configuration for pruning.
    pub prune: PruneConfig,
    /// Configuration for the discovery service.
    pub peers: PeersConfig,
    /// Configuration for peer sessions.
    pub sessions: SessionsConfig,
    /// Configuration for static files.
    pub static_files: StaticFilesConfig,
}

pub fn from_path(path: impl AsRef<Path>) -> eyre::Result<Self> {
    let path = path.as_ref();
    match std::fs::read_to_string(path) {
        Ok(cfg_string) => {
            toml::from_str(&cfg_string).map_err(|e| eyre::eyre!("Failed to parse TOML: {e}"))
        }
        Err(e) if e.kind() == std::io::ErrorKind::NotFound => {
            if let Some(parent) = path.parent() {
                std::fs::create_dir_all(parent)
                    .map_err(|e| eyre::eyre!("Failed to create directory: {e}"))?;
            }
            let cfg = Self::default();
            let s = toml::to_string_pretty(&cfg)
                .map_err(|e| eyre::eyre!("Failed to serialize to TOML: {e}"))?;
            std::fs::write(path, s)
                .map_err(|e| eyre::eyre!("Failed to write configuration file: {e}"))?;
            Ok(cfg)
        }
        Err(e) => Err(eyre::eyre!("Failed to load configuration: {e}")),
    }
}
```
- `HeadersConfig`/`BodiesConfig` define the downloader limits; defaults are set in `Default`.```175:245:/Users/vicnaum/github/stateless-history-node/reth/crates/config/src/config.rs
pub struct HeadersConfig {
    /// The maximum number of headers to request from a peer at a time.
    pub downloader_request_limit: u64,
    /// The maximum number of headers to download before committing progress to the database.
    pub commit_threshold: u64,
    // ...
}

impl Default for HeadersConfig {
    fn default() -> Self {
        Self {
            commit_threshold: 10_000,
            downloader_request_limit: 1_000,
            downloader_max_concurrent_requests: 100,
            downloader_min_concurrent_requests: 5,
            downloader_max_buffered_responses: 100,
        }
    }
}

pub struct BodiesConfig {
    /// The batch size of non-empty blocks per one request
    pub downloader_request_limit: u64,
    /// The maximum number of block bodies returned at once from the stream
    pub downloader_stream_batch_size: usize,
    // ...
}

impl Default for BodiesConfig {
    fn default() -> Self {
        Self {
            downloader_request_limit: 200,
            downloader_stream_batch_size: 1_000,
            downloader_max_buffered_blocks_size_bytes: 2 * 1024 * 1024 * 1024,
            downloader_min_concurrent_requests: 5,
            downloader_max_concurrent_requests: 100,
        }
    }
}
```
- Header/body downloader builders consume those configs; header requests use `request_limit` at runtime.```1119:1129:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
pub fn new(config: HeadersConfig) -> Self {
    Self::default()
        .request_limit(config.downloader_request_limit)
        .min_concurrent_requests(config.downloader_min_concurrent_requests)
        .max_concurrent_requests(config.downloader_max_concurrent_requests)
        .max_buffered_responses(config.downloader_max_buffered_responses)
        .stream_batch_size(config.commit_threshold as usize)
}
```
```169:180:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
fn next_request(&mut self) -> Option<HeadersRequest> {
    if let Some(local_head) = self.local_block_number() &&
        self.next_request_block_number > local_head
    {
        let request =
            calc_next_request(local_head, self.next_request_block_number, self.request_limit);
        self.next_request_block_number -= request.limit;
        return Some(request)
    }
    None
}
```
```521:533:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/bodies.rs
pub fn new(config: BodiesConfig) -> Self {
    Self::default()
        .with_stream_batch_size(config.downloader_stream_batch_size)
        .with_request_limit(config.downloader_request_limit)
        .with_max_buffered_blocks_size_bytes(config.downloader_max_buffered_blocks_size_bytes)
        .with_concurrent_requests_range(
            config.downloader_min_concurrent_requests..=
                config.downloader_max_concurrent_requests,
        )
}
```
- CLI args are applied in `NetworkArgs::network_config` (outside the packs): max inbound/outbound flows from args into `PeersConfig`, which is then fed into `NetworkConfigBuilder`; sessions buffer is adjusted based on peer count.```261:345:/Users/vicnaum/github/stateless-history-node/reth/crates/node/core/src/args/network.rs
pub fn resolved_max_inbound_peers(&self) -> Option<usize> {
    if let Some(max_peers) = self.max_peers {
        if max_peers == 0 {
            Some(0)
        } else {
            let outbound = (max_peers / 3).max(1);
            Some(max_peers.saturating_sub(outbound))
        }
    } else {
        self.max_inbound_peers
    }
}

let peers_config = config
    .peers_config_with_basic_nodes_from_file(
        self.persistent_peers_file(peers_file).as_deref(),
    )
    .with_max_inbound_opt(self.resolved_max_inbound_peers())
    .with_max_outbound_opt(self.resolved_max_outbound_peers())
    .with_ip_filter(ip_filter);

NetworkConfigBuilder::<N>::new(secret_key)
    .sessions_config(
        config.sessions.clone().with_upscaled_event_buffer(peers_config.max_peers()),
    )
    .peer_config(peers_config)
```
- `NetworkConfigBuilder` stores peer/session configs and defaults them if unset.```336:353:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/config.rs
pub fn peer_config(mut self, config: PeersConfig) -> Self {
    self.peers_config = Some(config);
    self
}

pub const fn sessions_config(mut self, config: SessionsConfig) -> Self {
    self.sessions_config = Some(config);
    self
}
```
```688:690:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/config.rs
peers_config: peers_config.unwrap_or_default(),
sessions_config: sessions_config.unwrap_or_default(),
```
- `NetworkManager::new` consumes `NetworkConfig` and wires `PeersManager` and `SessionManager`; `PeersManager` gates inbound sessions using `max_inbound`.```231:303:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/manager.rs
let NetworkConfig {
    peers_config,
    sessions_config,
    // ...
} = config;

let peers_manager = PeersManager::new(peers_config);

let sessions = SessionManager::new(
    secret_key,
    sessions_config,
    executor,
    status,
    hello_message,
    fork_filter,
    extra_protocols,
    handshake,
);
```
```245:264:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
pub(crate) fn on_incoming_pending_session(
    &mut self,
    addr: IpAddr,
) -> Result<(), InboundConnectionError> {
    // check if we even have slots for a new incoming connection
    if !self.connection_info.has_in_capacity() {
        if self.trusted_peer_ids.is_empty() {
            return Err(InboundConnectionError::ExceedsCapacity)
        }
        // ...
    }
    // ...
}
```
```1134:1142:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/peers.rs
const fn has_in_capacity(&self) -> bool {
    self.num_inbound < self.config.max_inbound
}
```
- Stage config is used to build downloaders and wire stages into the pipeline.```32:130:/Users/vicnaum/github/stateless-history-node/reth/crates/node/builder/src/setup.rs
pub fn build_networked_pipeline<N, Client, Evm>(
    config: &StageConfig,
    // ...
) -> eyre::Result<Pipeline<N>> {
    let header_downloader = ReverseHeadersDownloaderBuilder::new(config.headers)
        .build(client.clone(), consensus.clone())
        .into_task_with(task_executor);

    let body_downloader = BodiesDownloaderBuilder::new(config.bodies)
        .build(client, consensus.clone(), provider_factory.clone())
        .into_task_with(task_executor);

    let pipeline = builder
        .add_stages(
            DefaultStages::new(
                provider_factory.clone(),
                tip_rx,
                Arc::clone(&consensus),
                header_downloader,
                body_downloader,
                evm_config.clone(),
                stage_config.clone(),
                prune_config.segments,
                era_import_source,
            )
            .set(ExecutionStage::new(
                evm_config,
                consensus,
                stage_config.execution.into(),
                stage_config.execution_external_clean_threshold(),
                exex_manager_handle,
            )),
        )
        .build(provider_factory, static_file_producer);
```
```385:392:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/stages/src/sets.rs
StageSetBuilder::default()
    .add_stage(SenderRecoveryStage::new(self.stages_config.sender_recovery))
    .add_stage(ExecutionStage::from_config(
        self.evm_config,
        self.consensus,
        self.stages_config.execution,
        self.stages_config.execution_external_clean_threshold(),
    ))
```

## Corrections / Caveats
- CLI arg plumbing is outside the `context-sync.xml` and `context-network.xml` packs; it lives in `crates/node/core/src/args/network.rs` and feeds `NetworkConfigBuilder` via `NetworkArgs::network_config`.
- `ReverseHeadersDownloaderBuilder::new` derives `stream_batch_size` from `HeadersConfig.commit_threshold`; there is no `HeadersConfig.stream_batch_size` field.
- Execution thresholds are applied both through `ExecutionStage::from_config` (stage sets) and via `ExecutionStage::new` in `build_pipeline`, which converts `ExecutionConfig` via `into()`.

## Actionable Pointers
- CLI -> network config plumbing: `reth/crates/node/core/src/args/network.rs`.
- Network builder defaults and wiring: `reth/crates/net/network/src/config.rs` and `reth/crates/net/network/src/manager.rs`.
- Sync pipeline wiring: `reth/crates/node/builder/src/setup.rs` and `reth/crates/stages/stages/src/sets.rs`.
- Downloader config usage: `reth/crates/net/downloaders/src/headers/reverse_headers.rs` and `reth/crates/net/downloaders/src/bodies/bodies.rs`.
</file>

<file path="questions/Q026-metrics-tracing-network-sync.md">
# Q026 - Metrics + Tracing Hooks in Network + Sync

Status: verified

## Question
Where are metrics/tracing hooks in network + sync?
I want where metrics are defined, how they are registered/emitted, and which code paths (network + sync) emit them.

## Context Packs
- `spec/reth_repo_context/context-sync.xml`
- `spec/reth_repo_context/context-network.xml`

## Gemini Prompt

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML packs:
- spec/reth_repo_context/context-sync.xml
- spec/reth_repo_context/context-network.xml

Question (Q026):
Where are metrics/tracing hooks in network + sync?
I want where metrics are defined, how they are registered/emitted, and which code paths (network + sync) emit them.

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Cover network (peers, sessions, request handler) and sync (pipeline, stages, downloaders).
3. Identify any tracing spans/events and how they are hooked/used.
4. If a claim is outside the pack, say so explicitly.

Please structure the answer as:
1. Summary (5-8 bullets)
2. Entry points (files + types + purpose)
3. Metrics/tracing flow (step-by-step)
4. Key abstractions/types (with file paths)
5. "Where to look next"
6. Suggested follow-up questions
```

## Gemini Answer
Based on the provided XML packs, here is the analysis of the metrics and tracing hooks in Reth's network and sync components.

### 1. Summary
*   **Centralized Definitions**: Metrics are defined in dedicated `metrics.rs` modules within their respective crates (`reth-network`, `reth-downloaders`, `reth-stages-api`) using the `#[derive(Metrics)]` macro from `reth_metrics`.
*   **Scope-Based Registry**: Metrics are automatically registered under specific scopes (e.g., `network`, `downloaders.bodies`, `sync`) via the `#[metrics(scope = "...")]` attribute.
*   **Event-Driven Sync Metrics**: The Sync Pipeline uses a decoupled `MetricsListener` pattern. Stages and the Pipeline emit `MetricEvent`s over a channel, which the listener consumes to update Prometheus gauges.
*   **Direct Network Metrics**: Network components (Manager, Session, Request Handler) typically hold a metrics struct instance directly and update counters/gauges inline during event processing or polling loops.
*   **Tracing Conventions**: Tracing is pervasive, utilizing the `target` field (e.g., `target: "net::session"`, `target: "sync::pipeline"`) to allow granular filtering of logs.

---

### 2. Entry Points

| File Path | Type / Function | Purpose |
| :--- | :--- | :--- |
| `crates/net/network/src/metrics.rs` | `NetworkMetrics` | Defines global network stats (connected peers, bandwidth, active sessions). |
| `crates/net/downloaders/src/metrics.rs` | `HeaderDownloaderMetrics` | Defines stats for header downloads (throughput, concurrent requests). |
| `crates/stages/api/src/metrics/sync_metrics.rs` | `StageMetrics` | Defines stats per stage (checkpoint number, entities processed). |
| `crates/stages/api/src/metrics/listener.rs` | `MetricsListener` | A background future that consumes `MetricEvent`s and updates `SyncMetrics`. |
| `crates/net/network/src/manager.rs` | `NetworkManager` | Holds `NetworkMetrics` and updates them inside the main event loop. |

---

### 3. Metrics & Tracing Flow

#### A. Network Layer (Direct Updates)

In the networking stack, components own their metrics structs and update them immediately when events occur.

1.  **Definitions**:
    *   `NetworkMetrics`, `SessionManagerMetrics`, `DisconnectMetrics` are defined in `crates/net/network/src/metrics.rs`.
    *   `EthRequestHandlerMetrics` is defined in the same file.

2.  **Instantiation**:
    *   `NetworkManager` creates these structs in `new()` (`crates/net/network/src/manager.rs`).

3.  **Update Flow (Example: Session Established)**:
    *   **Source**: `Swarm` emits `SwarmEvent::SessionEstablished`.
    *   **Handler**: `NetworkManager::on_swarm_event` handles this event.
    *   **Metric Update**:
        ```rust
        // crates/net/network/src/manager.rs
        self.metrics.connected_peers.set(total_active as f64);
        ```
    *   **Tracing**:
        ```rust
        debug!(target: "net", ?remote_addr, ... "Session established");
        ```

4.  **Update Flow (Example: Request Handling)**:
    *   `EthRequestHandler` (`crates/net/network/src/eth_requests.rs`) holds `EthRequestHandlerMetrics`.
    *   When `on_headers_request` is called:
        ```rust
        // crates/net/network/src/eth_requests.rs
        self.metrics.eth_headers_requests_received_total.increment(1);
        ```

#### B. Sync Layer (Event-Driven Updates)

The Sync Pipeline uses a channel-based approach to decouple execution from metric recording.

1.  **Definitions**:
    *   `StageMetrics` is defined in `crates/stages/api/src/metrics/sync_metrics.rs`. It tracks `checkpoint`, `entities_processed`, `entities_total`, `total_elapsed`.

2.  **Instantiation**:
    *   `Pipeline` holds a `metrics_tx: Option<MetricEventsSender>` (`crates/stages/api/src/pipeline/mod.rs`).
    *   A `MetricsListener` is created to consume the receiver end (`crates/stages/api/src/metrics/listener.rs`).

3.  **Update Flow (Example: Stage Completion)**:
    *   **Source**: `Pipeline::execute_stage_to_completion`.
    *   **Action**: Upon stage completion, it sends an event:
        ```rust
        // crates/stages/api/src/pipeline/mod.rs
        metrics_tx.send(MetricEvent::StageCheckpoint { stage_id, checkpoint, ... });
        ```
    *   **Listener**: The `MetricsListener::poll` loop receives the event.
    *   **Metric Update**:
        ```rust
        // crates/stages/api/src/metrics/listener.rs
        let stage_metrics = self.sync_metrics.get_stage_metrics(stage_id);
        stage_metrics.checkpoint.set(checkpoint.block_number as f64);
        ```

#### C. Downloaders (Hybrid)

Downloaders hold their own metrics structs but are driven by the pipeline.

1.  **Definitions**:
    *   `BodyDownloaderMetrics` in `crates/net/downloaders/src/metrics.rs`.
2.  **Usage**:
    *   `BodiesDownloader` (`crates/net/downloaders/src/bodies/bodies.rs`) updates metrics during polling.
    *   Example: `self.metrics.total_flushed.increment(next_batch.len() as u64);`.

---

### 4. Key Abstractions & Types

| Type | File Path | Description |
| :--- | :--- | :--- |
| `NetworkMetrics` | `crates/net/network/src/metrics.rs` | High-level network stats (IO, connection counts). Scope: `network`. |
| `DisconnectMetrics` | `crates/net/network/src/metrics.rs` | Counters for each `DisconnectReason`. Scope: `network`. |
| `MetricEvent` | `crates/stages/api/src/metrics/listener.rs` | Enum used to pass metric updates from Pipeline to Listener (`SyncHeight`, `StageCheckpoint`). |
| `SyncMetrics` | `crates/stages/api/src/metrics/sync_metrics.rs` | Container for `HashMap<StageId, StageMetrics>`. Scope: `sync`. |
| `EthRequestHandlerMetrics` | `crates/net/network/src/metrics.rs` | Stats for `eth` protocol requests (GetHeaders, GetBodies). |

---

### 5. Where to Look Next

1.  **`crates/net/network/src/manager.rs`**: Look for `on_swarm_event` to see how nearly all high-level network metrics are updated in reaction to swarm events.
2.  **`crates/stages/api/src/metrics/listener.rs`**: To see the logic mapping high-level pipeline events to specific Prometheus gauge updates.
3.  **`crates/net/network/src/session/active.rs`**: To see deep tracing hooks (e.g., `trace!(target: "net::session", ...)` which are useful for debugging specific connection issues.
4.  **`crates/net/downloaders/src/metrics.rs`**: Defines the specific counters used for measuring download throughput and buffer sizes.

---

### 6. Suggested Follow-up Questions

*   "How are the `poll_durations` metrics in `NetworkManager` calculated, and what do they measure specifically?"
*   "Where are the `TxTypesCounter` metrics updated when a new transaction is received via broadcast?"
*   "How does `MetricsListener` handle dynamic stages, or is the list of stages fixed at startup for metric registration?"

## Verification
- Network metrics are defined with `#[derive(Metrics)]` and `#[metrics(scope = "network")]`, including `NetworkMetrics`, `SessionManagerMetrics`, and `EthRequestHandlerMetrics`.```12:78:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/metrics.rs
#[derive(Metrics)]
#[metrics(scope = "network")]
pub struct NetworkMetrics { ... }

#[derive(Metrics)]
#[metrics(scope = "network")]
pub struct SessionManagerMetrics { ... }
```
```315:334:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/metrics.rs
#[derive(Metrics)]
#[metrics(scope = "network")]
pub struct EthRequestHandlerMetrics {
    pub(crate) eth_headers_requests_received_total: Counter,
    pub(crate) eth_receipts_requests_received_total: Counter,
    pub(crate) eth_bodies_requests_received_total: Counter,
    pub(crate) eth_node_data_requests_received_total: Counter,
    pub(crate) acc_duration_poll_eth_req_handler: Gauge,
}
```
- Downloader metrics are scoped via attributes, and both headers and bodies have their own metric structs.```19:52:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/metrics.rs
#[derive(Clone, Metrics)]
#[metrics(scope = "downloaders.bodies")]
pub struct BodyDownloaderMetrics { ... }
```
```90:123:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/metrics.rs
#[derive(Clone, Metrics)]
#[metrics(scope = "downloaders.headers")]
pub struct HeaderDownloaderMetrics { ... }
```
- Sync metrics are defined in `StageMetrics` with `sync` scope and are keyed by stage ID.```11:31:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/metrics/sync_metrics.rs
pub(crate) fn get_stage_metrics(&mut self, stage_id: StageId) -> &mut StageMetrics { ... }

#[derive(Metrics)]
#[metrics(scope = "sync")]
pub(crate) struct StageMetrics {
    pub(crate) checkpoint: Gauge,
    pub(crate) entities_processed: Gauge,
    pub(crate) entities_total: Gauge,
    pub(crate) total_elapsed: Gauge,
}
```
- The sync metrics channel uses `MetricEvent` and is consumed by `MetricsListener`, which updates gauges and traces on `sync::metrics`.```13:73:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/metrics/listener.rs
pub type MetricEventsSender = UnboundedSender<MetricEvent>;

pub enum MetricEvent { ... }

fn handle_event(&mut self, event: MetricEvent) {
    trace!(target: "sync::metrics", ?event, "Metric event received");
    match event {
        MetricEvent::StageCheckpoint { stage_id, checkpoint, max_block_number, elapsed } => {
            let stage_metrics = self.sync_metrics.get_stage_metrics(stage_id);
            stage_metrics.total_elapsed.increment(elapsed.as_secs_f64());
            stage_metrics.checkpoint.set(checkpoint.block_number as f64);
            // ...
        }
        // ...
    }
}
```
- The pipeline wires metrics into execution: `PipelineBuilder::with_metrics_tx` stores a sender, and `Pipeline::register_metrics` and `execute_stage_to_completion` emit `MetricEvent::StageCheckpoint`.```58:90:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/builder.rs
pub fn with_metrics_tx(mut self, metrics_tx: MetricEventsSender) -> Self {
    self.metrics_tx = Some(metrics_tx);
    self
}
```
```132:146:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/mod.rs
pub fn register_metrics(&mut self) -> Result<(), PipelineError> {
    let Some(metrics_tx) = &mut self.metrics_tx else { return Ok(()) };
    // ...
    let _ = metrics_tx.send(MetricEvent::StageCheckpoint { ... });
    Ok(())
}
```
```490:505:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/mod.rs
if let Some(metrics_tx) = &mut self.metrics_tx {
    let _ = metrics_tx.send(MetricEvent::StageCheckpoint {
        stage_id,
        checkpoint,
        max_block_number: target,
        elapsed: stage_started_at.elapsed(),
    });
}
```
- The metrics listener task is spawned in node launch code (outside the packs).```694:709:/Users/vicnaum/github/stateless-history-node/reth/crates/node/builder/src/launch/common.rs
/// This spawns a metrics task that listens for metrics related events and updates metrics for
/// prometheus.
pub fn with_metrics_task(self) -> LaunchContextWith<...> {
    let (metrics_sender, metrics_receiver) = unbounded_channel();
    debug!(target: "reth::cli", "Spawning stages metrics listener task");
    let sync_metrics_listener = reth_stages::MetricsListener::new(metrics_receiver);
    self.task_executor().spawn_critical("stages metrics listener task", sync_metrics_listener);
    // ...
}
```
- Network metrics are updated directly during swarm event handling (e.g., incoming connections and session establishment).```755:812:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/manager.rs
SwarmEvent::IncomingTcpConnection { remote_addr, session_id } => {
    trace!(target: "net", ?remote_addr, "Incoming connection");
    self.metrics.total_incoming_connections.increment(1);
    self.metrics.incoming_connections.set(
        self.swarm.state().peers().num_inbound_connections() as f64
    );
}
SwarmEvent::SessionEstablished { .. } => {
    let total_active = self.num_active_peers.fetch_add(1, Ordering::Relaxed) + 1;
    self.metrics.connected_peers.set(total_active as f64);
    // ...
}
```
- `EthRequestHandler` increments request counters on each request type.```151:169:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/eth_requests.rs
fn on_headers_request(...) {
    self.metrics.eth_headers_requests_received_total.increment(1);
    // ...
}

fn on_bodies_request(...) {
    self.metrics.eth_bodies_requests_received_total.increment(1);
    // ...
}
```
- Downloaders update their metrics inline during polling (examples from bodies and headers).```255:263:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/bodies/bodies.rs
let next_batch = self.queued_bodies.drain(..self.stream_batch_size).collect::<Vec<_>>();
self.metrics.total_flushed.increment(next_batch.len() as u64);
self.metrics.queued_blocks.set(self.queued_bodies.len() as f64);
```
```897:900:/Users/vicnaum/github/stateless-history-node/reth/crates/net/downloaders/src/headers/reverse_headers.rs
trace!(target: "downloaders::headers", batch=%next_batch.len(), "Returning validated batch");
this.metrics.total_flushed.increment(next_batch.len() as u64);
```
- Tracing uses explicit targets for filtering (examples in session and pipeline).```262:267:/Users/vicnaum/github/stateless-history-node/reth/crates/net/network/src/session/mod.rs
trace!(
    target: "net::session",
    ?remote_addr,
    ?session_id,
    "new pending incoming session"
);
```
```174:176:/Users/vicnaum/github/stateless-history-node/reth/crates/stages/api/src/pipeline/mod.rs
trace!(target: "sync::pipeline", ?target, ?result, "Pipeline finished");
```

## Corrections / Caveats
- Sync metrics live in `crates/stages/api/src/metrics/sync_metrics.rs` and `metrics/listener.rs`, not a single `metrics.rs` file.
- The listener task wiring is outside the two packs (node launch code in `crates/node/builder/src/launch/common.rs`).

## Actionable Pointers
- Network metrics definitions and updates: `reth/crates/net/network/src/metrics.rs` and `reth/crates/net/network/src/manager.rs`.
- Request handler metrics: `reth/crates/net/network/src/eth_requests.rs`.
- Downloader metrics + usage: `reth/crates/net/downloaders/src/metrics.rs`, `reth/crates/net/downloaders/src/headers/reverse_headers.rs`, `reth/crates/net/downloaders/src/bodies/bodies.rs`.
- Sync metrics channel and listener: `reth/crates/stages/api/src/metrics/listener.rs` and `reth/crates/stages/api/src/pipeline/mod.rs`.
</file>

<file path="INDEX.md">
# Reth Knowledge Base

Purpose: consolidate verified, navigable answers about the `reth/` architecture and implementation.

## How to Use
- Each question has a dedicated page in `spec/reth_kb/questions/` with the Gemini prompt, answer, and verification notes.
- This index tracks status and provides short, actionable pointers.

## Status Legend
- `pending`: prompt drafted, Gemini not run yet
- `needs_verification`: Gemini answered, verification pending
- `verified`: key claims checked against `reth/`

## Question Index

| ID | Question | Context Packs | Status | TL;DR | Key Files |
| --- | --- | --- | --- | --- | --- |
| Q001 | [Major crates/modules and how they fit together](spec/reth_kb/questions/Q001-repo-architecture-map.md) | `reth/stateless-history-context.xml` | `verified` | P2P core via `NetworkManager`/`Swarm`; ECIES→P2PStream→EthStream layering; primitives + era crates provide shared types and history formats | `crates/net/network/src/manager.rs`, `crates/net/network/src/swarm.rs`, `crates/net/network/src/session/conn.rs`, `crates/primitives-traits/src/node.rs`, `crates/era/src/lib.rs` |
| Q002 | [Networking entry points (NetworkManager/Handle, PeerRequest)](spec/reth_kb/questions/Q002-network-entrypoints.md) | `context-network.xml` | `verified` | Config via `NetworkConfigBuilder`, runtime via `NetworkManager`/`Swarm`, and request plumbing through `FetchClient` | `crates/net/network/src/config.rs`, `crates/net/network/src/manager.rs`, `crates/net/network/src/swarm.rs`, `crates/net/network/src/fetch/client.rs` |
| Q003 | [Sync pipeline entry points (stages, pipeline, downloaders)](spec/reth_kb/questions/Q003-sync-entrypoints.md) | `context-sync.xml` | `verified` | Pipeline runs stages serially with checkpoints; downloaders have builders and task wrappers; default stage order is defined in stage sets | `crates/stages/api/src/pipeline/mod.rs`, `crates/stages/api/src/stage.rs`, `crates/stages/stages/src/sets.rs`, `crates/net/downloaders/src/headers/reverse_headers.rs`, `crates/net/downloaders/src/bodies/bodies.rs` |
| Q004 | [Storage entry points (db open/provider/static files)](spec/reth_kb/questions/Q004-storage-entrypoints.md) | `context-storage.xml` | `verified` | MDBX init/versioning + ProviderFactory are core entry points; static files use NippyJar with separate index/offset/config files | `crates/storage/db/src/mdbx.rs`, `crates/storage/db/src/version.rs`, `crates/storage/provider/src/providers/database/mod.rs`, `crates/storage/provider/src/providers/static_file/manager.rs`, `crates/storage/nippy-jar/src/lib.rs` |
| Q005 | [RPC entry points (module builder, server wiring)](spec/reth_kb/questions/Q005-rpc-entrypoints.md) | `context-rpc.xml` | `verified` | RpcModuleBuilder builds TransportRpcModules; RpcServerConfig wires middleware and starts HTTP/WS/IPC | `crates/rpc/rpc-builder/src/lib.rs`, `crates/rpc/rpc-builder/src/auth.rs`, `crates/node/core/src/args/rpc_server.rs`, `crates/rpc/rpc/src/eth/core.rs` |
| Q006 | [MDBX connection lifecycle (env, txns, versioning)](spec/reth_kb/questions/Q006-mdbx-lifecycle.md) | `context-storage.xml` | `verified` | create_db checks version then opens env; init_db creates tables + records client version; DatabaseEnv::open sets MDBX flags + slow reader warnings | `crates/storage/db/src/mdbx.rs`, `crates/storage/db/src/version.rs`, `crates/storage/db/src/implementation/mdbx/mod.rs`, `crates/storage/db-api/src/database.rs`, `crates/storage/db/src/implementation/mdbx/tx.rs` |
| Q007 | [DB abstraction layer (db-api traits + provider layer)](spec/reth_kb/questions/Q007-db-abstraction-layer.md) | `context-storage.xml` | `verified` | db-api defines Database/DbTx/Table; DatabaseEnv+Tx implement MDBX; ProviderFactory wraps db.tx into DatabaseProvider | `crates/storage/db-api/src/database.rs`, `crates/storage/db-api/src/transaction.rs`, `crates/storage/db/src/implementation/mdbx/mod.rs`, `crates/storage/db/src/implementation/mdbx/tx.rs`, `crates/storage/provider/src/providers/database/mod.rs`, `crates/storage/provider/src/providers/database/provider.rs` |
| Q008 | [Table definitions + codecs mapping to stored data](spec/reth_kb/questions/Q008-table-definitions-codecs.md) | `context-storage.xml` | `verified` | tables! defines typed tables; keys encode via Encode; values compress via Compact; MDBX tx encodes/decodes | `crates/storage/db-api/src/tables/mod.rs`, `crates/storage/db-api/src/table.rs`, `crates/storage/db-api/src/models/mod.rs`, `crates/storage/db/src/implementation/mdbx/tx.rs`, `crates/storage/db/src/implementation/mdbx/cursor.rs`, `crates/storage/codecs/README.md` |
| Q009 | [Static files + NippyJar architecture and data split](spec/reth_kb/questions/Q009-static-files-nippyjar.md) | `context-storage.xml` | `verified` | NippyJar uses mmap + offsets/conf sidecars; static file segments split into 500k‑block ranges; save_blocks parallelizes static files with MDBX | `crates/storage/nippy-jar/src/lib.rs`, `crates/storage/nippy-jar/src/writer.rs`, `crates/storage/provider/src/providers/static_file/manager.rs`, `crates/static-file/types/src/segment.rs`, `crates/storage/provider/src/providers/database/provider.rs`, `crates/storage/provider/src/either_writer.rs` |
| Q010 | [Storage migrations/versioning](spec/reth_kb/questions/Q010-storage-migrations-versioning.md) | `context-storage.xml` | `verified` | DB_VERSION gate + version file; VersionHistory logs client versions; StorageSettings persisted in Metadata | `crates/storage/db/src/version.rs`, `crates/storage/db/src/mdbx.rs`, `crates/storage/db/src/implementation/mdbx/mod.rs`, `crates/storage/db-api/src/tables/mod.rs`, `crates/storage/storage-api/src/metadata.rs` |
| Q011 | [Pipeline orchestration + checkpoints](spec/reth_kb/questions/Q011-pipeline-orchestration.md) | `context-sync.xml` | `verified` | run_loop executes stages serially; execute_ready + execute persist checkpoints; unwind runs reverse order | `crates/stages/api/src/pipeline/mod.rs`, `crates/stages/api/src/stage.rs`, `crates/stages/types/src/checkpoints.rs`, `crates/stages/api/src/pipeline/ctrl.rs`, `crates/stages/stages/src/sets.rs` |
| Q012 | [ReverseHeadersDownloader algorithm](spec/reth_kb/questions/Q012-reverse-headers-downloader.md) | `context-sync.xml` | `verified` | Reverse downloader streams validated header batches; buffering + retries; detached head triggers unwind | `crates/net/downloaders/src/headers/reverse_headers.rs`, `crates/stages/stages/src/stages/headers.rs`, `crates/net/network-p2p/src/headers/client.rs` |
| Q013 | [Bodies downloader batching + retries](spec/reth_kb/questions/Q013-bodies-downloader.md) | `context-sync.xml` | `verified` | BodiesRequestFuture retries until pending headers done; BinaryHeap buffering yields ordered batches | `crates/net/downloaders/src/bodies/bodies.rs`, `crates/net/downloaders/src/bodies/request.rs`, `crates/net/p2p/src/bodies/response.rs` |
| Q014 | [Missing ranges/holes representation](spec/reth_kb/questions/Q014-missing-ranges-holes.md) | `context-sync.xml` | `verified` | Gaps are implicit via checkpoint/target and HeaderSyncGap; bodies retries fill partials | `crates/net/p2p/src/headers/downloader.rs`, `crates/stages/api/src/stage.rs`, `crates/net/downloaders/src/bodies/request.rs`, `crates/stages/stages/src/stages/bodies.rs` |
| Q015 | [PeersManager + reputation scoring](spec/reth_kb/questions/Q015-peers-manager-reputation.md) | `context-network.xml` | `verified` | PeersManager applies weighted reputation, bans at threshold, and backs off peers | `crates/net/network/src/peers.rs`, `crates/net/network-types/src/peers/reputation.rs`, `crates/net/network-types/src/peers/mod.rs`, `crates/net/network/src/error.rs` |
| Q016 | [Connection limits + dial/backoff controls](spec/reth_kb/questions/Q016-connection-limits-dial-backoff.md) | `context-network.xml` | `verified` | ConnectionInfo gates pending/active slots; dial selection prioritizes trusted/static; backoff uses capped linear scaling | `crates/net/network/src/peers.rs`, `crates/net/network-types/src/peers/config.rs`, `crates/net/network/src/session/mod.rs`, `crates/net/network/src/swarm.rs` |
| Q017 | [Peer persistence (disk format + lifecycle)](spec/reth_kb/questions/Q017-peer-persistence.md) | `context-network.xml` | `verified` | Peers saved on shutdown to known-peers.json as enode strings; loaded via PeersConfig | `crates/net/network/src/manager.rs`, `crates/net/network-types/src/peers/config.rs`, `crates/net/peers/src/node_record.rs`, `crates/node/builder/src/builder/mod.rs` |
| Q018 | [Canonical chain representation + reorg detection](spec/reth_kb/questions/Q018-canonical-chain-reorgs.md) | `context-engine.xml` | `verified` | TreeState + NewCanonicalChain drive reorg detection; canonical head updated with events | `crates/engine/tree/src/tree/state.rs`, `crates/engine/tree/src/tree/mod.rs`, `crates/chain-state/src/in_memory.rs`, `crates/engine/primitives/src/event.rs` |
| Q019 | [Engine tree vs pipeline handoff + backfill](spec/reth_kb/questions/Q019-engine-tree-backfill-handoff.md) | `context-engine.xml` | `verified` | Backfill uses thresholded forkchoice logic; ChainOrchestrator drives pipeline and applies results | `crates/engine/tree/src/tree/mod.rs`, `crates/engine/tree/src/chain.rs`, `crates/engine/tree/src/backfill.rs`, `crates/engine/tree/src/engine.rs` |
| Q020 | [Rollback semantics on reorgs](spec/reth_kb/questions/Q020-rollback-semantics-reorgs.md) | `context-engine.xml` | `verified` | Tree reorgs update in-memory canonical state; disk reorgs detected via find_disk_reorg and truncated via persistence | `crates/engine/tree/src/tree/mod.rs`, `crates/engine/tree/src/tree/state.rs`, `crates/engine/tree/src/persistence.rs`, `crates/storage/provider/src/providers/database/provider.rs` |
| Q021 | [RPC server build + module wiring](spec/reth_kb/questions/Q021-rpc-server-wiring.md) | `context-rpc.xml` | `verified` | RpcModuleBuilder builds transport-specific modules; RpcServerConfig starts servers and auth is separate | `crates/rpc/rpc-builder/src/lib.rs`, `crates/rpc/rpc-builder/src/auth.rs`, `crates/rpc/rpc-builder/src/config.rs`, `crates/rpc/rpc/src/engine.rs` |
| Q022 | [`eth_getLogs` implementation details](spec/reth_kb/questions/Q022-eth-getlogs.md) | `context-rpc.xml` | `verified` | Header bloom filtering + cached/range receipt fetch; limits enforced on range and results | `crates/rpc/rpc/src/eth/filter.rs`, `crates/rpc/rpc-eth-types/src/logs_utils.rs`, `crates/rpc/rpc-eth-types/src/cache/mod.rs` |
| Q023 | [RPC safety knobs (bind/auth/rate limit)](spec/reth_kb/questions/Q023-rpc-safety-knobs.md) | `context-rpc.xml` | `verified` | Defaults bind to localhost; JWT is optional for RPC and required for auth; size and concurrency limits apply | `crates/node/core/src/args/rpc_server.rs`, `crates/rpc/rpc-builder/src/config.rs`, `crates/rpc/rpc-builder/src/lib.rs`, `crates/rpc/rpc-builder/src/auth.rs` |
| Q024 | [Serving devp2p requests (EthRequestHandler)](spec/reth_kb/questions/Q024-eth-request-handler.md) | `context-network.xml` | `verified` | ActiveSession uses oneshot to EthRequestHandler; responses bounded by 2MB and 1024 items | `crates/net/network/src/eth_requests.rs`, `crates/net/network/src/session/active.rs`, `crates/net/network/src/manager.rs`, `crates/net/network/src/builder.rs` |
| Q025 | [Config plumbing into network/sync](spec/reth_kb/questions/Q025-config-plumbing-network-sync.md) | `context-sync.xml`, `context-network.xml` | `verified` | CLI NetworkArgs merges args + Config into NetworkConfigBuilder; StageConfig builds downloaders and stage sets; defaults via Default/serde and builder `unwrap_or_default` | `crates/node/core/src/args/network.rs`, `crates/net/network/src/config.rs`, `crates/node/builder/src/setup.rs`, `crates/net/downloaders/src/headers/reverse_headers.rs`, `crates/stages/stages/src/sets.rs` |
| Q026 | [Metrics/tracing hooks in network + sync](spec/reth_kb/questions/Q026-metrics-tracing-network-sync.md) | `context-sync.xml`, `context-network.xml` | `verified` | Metrics are scoped via `#[metrics(scope = ...)]`; pipeline emits MetricEvent to MetricsListener; network updates metrics inline during swarm/session events | `crates/net/network/src/metrics.rs`, `crates/net/network/src/manager.rs`, `crates/net/network/src/eth_requests.rs`, `crates/net/downloaders/src/metrics.rs`, `crates/stages/api/src/metrics/listener.rs` |
</file>

<file path="PROMPT_TEMPLATE.md">
# Gemini Prompt Template

Use this template to generate consistent, well-cited answers from Gemini.

## Template

```text
You are analyzing the Rust codebase for Reth. I attached the following Repomix XML packs:
- {CONTEXT_PACKS}

Question:
{QUESTION}

Requirements:
1. Cite file paths and function/type names for every major claim.
2. Prefer short quoted snippets or signatures when possible.
3. Separate facts from hypotheses; call out uncertainty explicitly.
4. Use plain language explanations in addition to citations.

Please structure the answer as:
1. Summary (plain English, 5-8 bullets)
2. Architecture map (major modules/crates and how they connect)
3. Key data flows (bulleted steps)
4. Key abstractions/types (with where they live)
5. File/function references (bullet list, each with a one-line purpose)
6. Suggested follow-up questions
```

## Notes
- Keep the response grounded in the provided packs only.
- If the packs are insufficient, explicitly say what additional context is needed.
</file>

</files>
