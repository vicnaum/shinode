This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: docs/**, benchmark/**, examples/**, simulation-test/**, etc/**, .github/**, .changeset/**, **/CHANGELOG.md, **/*.svg, **/*.png, **/*.jpg, **/*.jpeg, **/*.gif, **/*.pdf, **/*.zip
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Long base64 data strings (e.g., data:image/png;base64,...) have been truncated to reduce token count
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
packages/
  client/
    src/
      index.test.ts
      index.ts
    .gitignore
    package.json
    tsconfig.json
    tsup.config.ts
  core/
    src/
      _test/
        contracts/
          src/
            ERC20.sol
            Factory.sol
            Pair.sol
            Revert.sol
        e2e/
          erc20/
            src/
              api/
                index.ts
              index.ts
            erc20.test.ts
            ponder.config.ts
            ponder.schema.ts
          factory/
            src/
              api/
                index.ts
              index.ts
            factory.test.ts
            ponder.config.ts
            ponder.schema.ts
        .gitignore
        constants.ts
        globalSetup.ts
        setup.ts
        simulate.ts
        utils.ts
      bin/
        commands/
          codegen.ts
          createViews.ts
          dev.ts
          list.ts
          prune.ts
          serve.ts
          start.ts
        utils/
          codegen.ts
          exit.ts
        isolatedController.ts
        isolatedWorker.ts
        ponder.ts
      build/
        config.test.ts
        config.ts
        factory.test.ts
        factory.ts
        index.ts
        plugin.ts
        pre.test.ts
        pre.ts
        schema.test.ts
        schema.ts
        stacktrace.ts
      client/
        index.test.ts
        index.ts
      config/
        address.test-d.ts
        address.ts
        eventFilter.ts
        index.test.ts
        index.ts
        utilityTypes.test-d.ts
        utilityTypes.ts
      database/
        actions.test.ts
        actions.ts
        index.test.ts
        index.ts
        queryBuilder.test.ts
        queryBuilder.ts
      drizzle/
        kit/
          index.ts
        bigint.ts
        bytes.ts
        hex.ts
        index.test.ts
        index.ts
        json.ts
        onchain.ts
        text.ts
      graphql/
        graphiql.html.ts
        index.test.ts
        index.ts
        json.ts
        middleware.test.ts
        middleware.ts
      indexing/
        addStackTrace.ts
        client.test.ts
        client.ts
        index.test.ts
        index.ts
        profile.test.ts
        profile.ts
      indexing-store/
        cache.bench.ts
        cache.test.ts
        cache.ts
        index.bench.ts
        index.test.ts
        index.ts
        profile.test.ts
        profile.ts
        utils.bench.ts
        utils.test.ts
        utils.ts
      internal/
        common.ts
        errors.ts
        logger.ts
        metrics.ts
        options.ts
        shutdown.ts
        telemetry.test.ts
        telemetry.ts
        types.ts
      rpc/
        actions.ts
        http.test.ts
        http.ts
        index.test.ts
        index.ts
      runtime/
        events.test.ts
        events.ts
        filter.test.ts
        filter.ts
        fragments.test.ts
        fragments.ts
        historical.test.ts
        historical.ts
        index.test.ts
        index.ts
        init.ts
        isolated.ts
        multichain.ts
        omnichain.ts
        realtime.ts
      server/
        error.ts
        index.test.ts
        index.ts
      sync-historical/
        index.test.ts
        index.ts
      sync-realtime/
        bloom.test.ts
        bloom.ts
        index.test.ts
        index.ts
      sync-store/
        encode.ts
        index.test.ts
        index.ts
        migrations.ts
        schema.ts
      types/
        db.test-d.ts
        db.ts
        eth.ts
        utils.ts
        virtual.test-d.ts
        virtual.ts
      ui/
        app.ts
        index.ts
        patch.ts
      utils/
        abi.ts
        bigint.ts
        chains.test.ts
        chains.ts
        checkpoint.bench.ts
        checkpoint.test.ts
        checkpoint.ts
        chunk.test.ts
        chunk.ts
        copy.bench.ts
        copy.test.ts
        copy.ts
        date.ts
        debug.ts
        decodeAbiParameters.bench.ts
        decodeAbiParameters.test.ts
        decodeAbiParameters.ts
        decodeEventLog.test.ts
        decodeEventLog.ts
        dedupe.ts
        duplicates.ts
        estimate.ts
        finality.ts
        format.ts
        generators.test.ts
        generators.ts
        hash.ts
        interval.test.ts
        interval.ts
        lowercase.ts
        mutex.ts
        never.ts
        offset.test.ts
        offset.ts
        order.ts
        partition.test.ts
        partition.ts
        pg.test.ts
        pg.ts
        pglite.ts
        port.test.ts
        port.ts
        print.ts
        promiseAllSettledWithThrow.test.ts
        promiseAllSettledWithThrow.ts
        promiseWithResolvers.test-d.ts
        promiseWithResolvers.test.ts
        promiseWithResolvers.ts
        queue.test-d.ts
        queue.test.ts
        queue.ts
        range.ts
        result.ts
        sql-parse.test.ts
        sql-parse.ts
        timer.bench.ts
        timer.ts
        truncate.test.ts
        truncate.ts
        wait.ts
        zipper.test.ts
        zipper.ts
      index.test.ts
      index.ts
      types.d.ts
    .env.example
    .gitignore
    build.ts
    bunfig.toml
    drizzle.config.ts
    package.json
    README.md
    tsconfig.build.json
    tsconfig.json
    vite.config.ts
    wagmi.config.ts
  create-ponder/
    src/
      _test/
        cli.test.ts
        globalSetup.ts
      helpers/
        getPackageManager.ts
        mergeAbis.ts
        notifyUpdate.ts
        validate.ts
        wait.ts
      index.ts
    templates/
      empty/
        abis/
          ExampleContractAbi.ts
        src/
          api/
            index.ts
          index.ts
        _dot_env.local
        _dot_eslintrc.json
        _dot_gitignore
        package.json
        ponder-env.d.ts
        ponder.config.ts
        ponder.schema.ts
        tsconfig.json
    .env.example
    .gitignore
    bunfig.toml
    package.json
    README.md
    tsconfig.json
    tsup.config.ts
    vite.config.ts
  eslint-config-ponder/
    index.js
    package.json
    README.md
  react/
    src/
      checkpoint.ts
      context.ts
      hook.test-d.ts
      hook.ts
      index.ts
      utils.ts
    .gitignore
    build.ts
    package.json
    tsconfig.build.json
    tsconfig.json
  utils/
    src/
      _test/
        1rpc.test.ts
        alchemy.test.ts
        altitude.test.ts
        ankr-tac.test.ts
        ankr.test.ts
        arbitrum.test.ts
        aurora.test.ts
        avalanche.test.ts
        base.test.ts
        blast.test.ts
        blockpi.test.ts
        celo.test.ts
        chainstack.test.ts
        cloudflare.test.ts
        coinbase.test.ts
        globalSetup.ts
        harmony.test.ts
        hyperliquid.test.ts
        infura.test.ts
        llamarpc.test.ts
        merkle.test.ts
        moonriver.test.ts
        optimism.test.ts
        publicnode.test.ts
        quicknode.test.ts
        somnia.test.ts
        swell.test.ts
        thirdweb.test.ts
        tron.test.ts
        utils.ts
        zkevm.test.ts
        zksync.test.ts
      utils/
        promiseWithResolvers.test-d.ts
        promiseWithResolvers.test.ts
        promiseWithResolvers.ts
        queue.test-d.ts
        queue.test.ts
        queue.ts
      getLogsRetryHelper.ts
      index.ts
      loadBalance.test.ts
      loadBalance.ts
      mergeAbis.test.ts
      mergeAbis.ts
      rateLimit.test.ts
      rateLimit.ts
      replaceBigInts.test.ts
      replaceBigInts.ts
    .env.example
    .gitignore
    bunfig.toml
    package.json
    README.md
    tsconfig.json
    tsup.config.ts
    vite.config.ts
.gitignore
.npmrc
biome.json
funding.json
LICENSE
package.json
pnpm-workspace.yaml
README.md
tsconfig.base.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="packages/client/src/index.test.ts">
import { relations } from "drizzle-orm";
import { getTableConfig, pgEnum, pgTable } from "drizzle-orm/pg-core";
import { drizzle } from "drizzle-orm/pg-proxy";
import { expect, test } from "vitest";
import { setDatabaseSchema } from "./index.js";
test("setDatabaseSchema table", () => {
  const schema = {
    account: pgTable("account", (t) => ({
      address: t.text("address").primaryKey(),
      balance: t.text("balance"),
    })),
  };
  setDatabaseSchema(schema, "hi-kevin");
  const db = drizzle(() => Promise.resolve({ rows: [] }));
  expect(getTableConfig(schema.account).schema).toBe("hi-kevin");
  expect(db.select().from(schema.account).toSQL()).toMatchInlineSnapshot(`
    {
      "params": [],
      "sql": "select "address", "balance" from "hi-kevin"."account"",
    }
  `);
});
test("setDatabaseSchema enum", () => {
  const e = pgEnum("e", ["a", "b", "c"]);
  const schema = {
    e,
    account: pgTable("account", (t) => ({
      address: t.text("address").primaryKey(),
      balance: t.text("balance"),
      e: e(),
    })),
  };
  setDatabaseSchema(schema, "hi-kevin");
  const db = drizzle(() => Promise.resolve({ rows: [] }));
  expect(e.schema).toBe("hi-kevin");
  expect(db.select().from(schema.account).toSQL()).toMatchInlineSnapshot(`
    {
      "params": [],
      "sql": "select "address", "balance", "e" from "hi-kevin"."account"",
    }
  `);
});
test("setDatabaseSchema relations", () => {
  const person = pgTable("person", (t) => ({
    id: t.text("id").primaryKey(),
    name: t.text("name"),
    friendId: t.text("friend_id"),
  }));
  const friendRelation = relations(person, ({ one }) => ({
    friend: one(person, {
      fields: [person.friendId],
      references: [person.id],
    }),
  }));
  const schema = {
    person,
    friendRelation,
  };
  setDatabaseSchema(schema, "hi-kevin");
  const db = drizzle(() => Promise.resolve({ rows: [] }), { schema });
  expect(getTableConfig(friendRelation.table).schema).toBe("hi-kevin");
  expect(
    db.query.person.findMany({ with: { friend: true } }).toSQL(),
  ).toMatchInlineSnapshot(`
    {
      "params": [
        1,
      ],
      "sql": "select "person"."id", "person"."name", "person"."friend_id", "person_friend"."data" as "friend" from "hi-kevin"."person" "person" left join lateral (select json_build_array("person_friend"."id", "person_friend"."name", "person_friend"."friend_id") as "data" from (select * from "hi-kevin"."person" "person_friend" where "person_friend"."id" = "person"."friend_id" limit $1) "person_friend") "person_friend" on true",
      "typings": [
        "none",
      ],
    }
  `);
});
</file>

<file path="packages/client/src/index.ts">
import {
  type AnyColumn,
  Column,
  type QueryWithTypings,
  SQL,
  type SQLWrapper,
  type SelectedFieldsOrdered,
  Table,
  is,
  isTable,
  mapRelationalRow,
} from "drizzle-orm";
import { type PgDialect, isPgEnum } from "drizzle-orm/pg-core";
import { PgCountBuilder } from "drizzle-orm/pg-core/query-builders/count";
import { PgRelationalQuery } from "drizzle-orm/pg-core/query-builders/query";
import { PgRaw } from "drizzle-orm/pg-core/query-builders/raw";
import { type PgRemoteDatabase, drizzle } from "drizzle-orm/pg-proxy";
import { TypedQueryBuilder } from "drizzle-orm/query-builders/query-builder";
import { EventSource } from "eventsource";
import superjson from "superjson";
type Schema = { [name: string]: unknown };
type Prettify<T> = {
  [K in keyof T]: T[K];
} & {};
export type Status = {
  [chainName: string]: {
    id: number;
    block: { number: number; timestamp: number };
  };
};
type ClientDb<schema extends Schema = Schema> = Prettify<
  Omit<
    PgRemoteDatabase<schema>,
    | "insert"
    | "update"
    | "delete"
    | "transaction"
    | "refreshMaterializedView"
    | "_"
  >
>;
export type Client<schema extends Schema = Schema> = {
  /** Query the database. */
  db: ClientDb<schema>;
  /**
   * Subscribe to live updates.
   *
   * @param queryFn - The query to subscribe to.
   * @param onData - The callback to call with each new query result
   * @param onError - The callback to call when an error occurs.
   *
   * @example
   * ```ts
   * import { createClient } from "@ponder/client";
   * import * as schema from "../ponder.schema";
   *
   * const client = createClient("https://.../sql", { schema });
   *
   * client.live(
   *   (db) => db.select().from(schema.account),
   *   (result) => console.log(result),
   *   (error) => console.error(error),
   * );
   * ```
   */
  live: <result>(
    queryFn: (db: ClientDb<schema>) => Promise<result>,
    onData: (result: result) => void,
    onError?: (error: Error) => void,
  ) => {
    unsubscribe: () => void;
  };
  /** Get the status of all chains. */
  getStatus: () => Promise<Status>;
};
const getUrl = (
  baseUrl: string,
  method: "live" | "db",
  query?: QueryWithTypings,
) => {
  const url = new URL(baseUrl);
  url.pathname = `${url.pathname}/${method}`;
  if (query) {
    url.searchParams.set("sql", superjson.stringify(query));
  }
  return url;
};
const noopDatabase = drizzle(() => Promise.resolve({ rows: [] }), {
  casing: "snake_case",
});
// @ts-ignore
const dialect: PgDialect = noopDatabase.dialect;
export const compileQuery = (query: SQLWrapper) => {
  return dialect.sqlToQuery(query.getSQL());
};
/**
 * Create a client for querying Ponder apps.
 *
 * @param baseUrl - The URL of the Ponder app.
 * @param schema - The schema of the Ponder app.
 *
 * @example
 * ```ts
 * import { createClient } from "@ponder/client";
 * import * as schema from "../ponder.schema";
 *
 * const client = createClient("https://.../sql", { schema });
 * ```
 */
export const createClient = <schema extends Schema>(
  baseUrl: string,
  params: { schema?: schema } = {},
): Client<schema> => {
  const client: Client<schema> = {
    db: drizzle(
      async (sql, params, method, typings) => {
        const builtQuery = { sql, params, typings };
        const response = await fetch(getUrl(baseUrl, "db", builtQuery), {
          method: "GET",
        });
        if (response.ok === false) {
          const error = new Error(await response.text());
          error.stack = undefined;
          throw error;
        }
        const result = await response.json();
        if (method === "all") {
          return {
            ...result,
            rows: result.rows.map((row: object) => Object.values(row)),
          };
        }
        return result;
      },
      { schema: params.schema, casing: "snake_case" },
    ),
    live: (queryFn, onData, onError) => {
      const noopDatabase = drizzle(() => Promise.resolve({ rows: [] }), {
        schema: params.schema,
        casing: "snake_case",
      });
      const queryPromise = queryFn(noopDatabase);
      if ("getSQL" in queryPromise === false) {
        throw new Error(
          '"queryFn" must return SQL. You may have to remove `.execute()` from your query.',
        );
      }
      const queryBuilder = queryPromise as unknown as SQLWrapper;
      const query = compileQuery(queryBuilder);
      const sse = new EventSource(getUrl(baseUrl, "live", query));
      const onDataListener = async (event: MessageEvent) => {
        try {
          // @ts-ignore
          const result = JSON.parse(event.data);
          const drizzleShim = drizzle(
            (_, __, method) => {
              if (method === "all") {
                return Promise.resolve({
                  ...result,
                  rows: result.rows.map((row: object) => Object.values(row)),
                });
              }
              return Promise.resolve(result);
            },
            { schema: params.schema },
          );
          let data: unknown;
          if (queryBuilder instanceof TypedQueryBuilder) {
            const fields = queryBuilder._.selectedFields as Record<
              string,
              unknown
            >;
            const orderedFields = orderSelectedFields(fields);
            data = await drizzleShim._.session
              .prepareQuery(
                query,
                // @ts-ignore
                orderedFields,
                undefined,
                false,
              )
              .execute();
          } else if (queryBuilder instanceof PgRelationalQuery) {
            // @ts-ignore
            const selection = queryBuilder._toSQL().query.selection;
            data = await drizzleShim._.session
              .prepareQuery(
                query,
                undefined,
                undefined,
                true,
                (rawRows, mapColumnValue) => {
                  const rows = rawRows.map((row) =>
                    mapRelationalRow(
                      // @ts-ignore
                      queryBuilder.schema,
                      // @ts-ignore
                      queryBuilder.tableConfig,
                      // @ts-ignore
                      row,
                      selection,
                      mapColumnValue,
                    ),
                  );
                  // @ts-ignore
                  if (queryBuilder.mode === "first") {
                    return rows[0];
                  }
                  return rows;
                },
              )
              .execute();
          } else if (queryBuilder instanceof PgRaw) {
            data = await drizzleShim._.session
              .prepareQuery(query, undefined, undefined, false)
              .execute();
          } else if (queryBuilder instanceof PgCountBuilder) {
            data = await drizzleShim._.session.count(queryBuilder.getSQL());
          } else {
            throw new Error("Unsupported query builder");
          }
          // @ts-ignore
          onData(data);
        } catch (error) {
          onError?.(error as Error);
        }
      };
      const onErrorListener = (_event: MessageEvent) => {
        onError?.(new Error("server disconnected"));
      };
      sse.addEventListener("message", onDataListener);
      sse.addEventListener("error", onErrorListener);
      return {
        unsubscribe: () => {
          sse.removeEventListener("message", onDataListener);
          sse.removeEventListener("error", onErrorListener);
          sse.close();
        },
      };
    },
    getStatus: async () => {
      const response = await fetch(`${new URL(baseUrl).origin}/status`);
      return response.json();
    },
  };
  return client;
};
export {
  sql,
  eq,
  gt,
  gte,
  lt,
  lte,
  ne,
  isNull,
  isNotNull,
  inArray,
  notInArray,
  exists,
  notExists,
  between,
  notBetween,
  like,
  notLike,
  ilike,
  notIlike,
  not,
  asc,
  desc,
  and,
  or,
  count,
  countDistinct,
  avg,
  avgDistinct,
  sum,
  sumDistinct,
  max,
  min,
  relations,
  SQL,
} from "drizzle-orm";
export {
  alias,
  union,
  unionAll,
  intersect,
  intersectAll,
  except,
  exceptAll,
} from "drizzle-orm/pg-core";
export const setDatabaseSchema = <T extends { [name: string]: unknown }>(
  schema: T,
  schemaName: string,
) => {
  for (const table of Object.values(schema)) {
    if (isTable(table)) {
      // @ts-ignore
      table[Table.Symbol.Schema] = schemaName;
    } else if (isPgEnum(table)) {
      // @ts-ignore
      table.schema = schemaName;
    }
  }
};
function orderSelectedFields<TColumn extends AnyColumn>(
  fields: Record<string, unknown>,
  pathPrefix?: string[],
): SelectedFieldsOrdered<TColumn> {
  return Object.entries(fields).reduce<SelectedFieldsOrdered<AnyColumn>>(
    (result, [name, field]) => {
      if (typeof name !== "string") {
        return result;
      }
      const newPath = pathPrefix ? [...pathPrefix, name] : [name];
      if (is(field, Column) || is(field, SQL) || is(field, SQL.Aliased)) {
        result.push({ path: newPath, field });
      } else if (is(field, Table)) {
        result.push(
          // @ts-ignore
          ...orderSelectedFields(field[Table.Symbol.Columns], newPath),
        );
      } else {
        result.push(
          ...orderSelectedFields(field as Record<string, unknown>, newPath),
        );
      }
      return result;
    },
    [],
  ) as SelectedFieldsOrdered<TColumn>;
}
</file>

<file path="packages/client/.gitignore">
dist/
</file>

<file path="packages/client/package.json">
{
  "name": "@ponder/client",
  "version": "0.16.1",
  "description": "",
  "license": "MIT",
  "type": "module",
  "sideEffects": false,
  "repository": {
    "type": "git",
    "url": "https://github.com/ponder-sh/ponder",
    "directory": "packages/client"
  },
  "files": [
    "dist",
    "src/**/*.ts",
    "!src/**/*.test.ts",
    "!src/**/*.test-d.ts",
    "!src/_test/**/*"
  ],
  "module": "./dist/index.js",
  "types": "./dist/index.d.ts",
  "typings": "./dist/index.d.ts",
  "exports": {
    ".": {
      "types": "./dist/index.d.ts",
      "import": "./dist/index.js"
    }
  },
  "scripts": {
    "build": "tsup",
    "test": "vitest",
    "test:typecheck": "vitest --typecheck.only",
    "typecheck": "tsc --noEmit"
  },
  "peerDependencies": {
    "typescript": ">=5.0.4"
  },
  "peerDependenciesMeta": {
    "typescript": {
      "optional": true
    }
  },
  "dependencies": {
    "drizzle-orm": "0.41.0",
    "eventsource": "^3.0.5",
    "superjson": "^2.2.2"
  },
  "devDependencies": {
    "tsup": "^8.0.1",
    "vitest": "1.6.1"
  }
}
</file>

<file path="packages/client/tsconfig.json">
{
  // Adapted from viem (https://github.com/wagmi-dev/viem/blob/ed779e9d5667704fd7....base.json).
  "include": ["src"],
  "compilerOptions": {
    // Type checking
    "strict": true,
    "useDefineForClassFields": true,
    "noFallthroughCasesInSwitch": true,
    "noImplicitReturns": true,
    "useUnknownInCatchVariables": true,
    "noImplicitOverride": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noUncheckedIndexedAccess": true,

    // JavaScript support
    "allowJs": false,
    "checkJs": false,
    "jsx": "react",

    // Interop constraints
    "verbatimModuleSyntax": true,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "resolveJsonModule": true,

    // Language and environment
    "moduleResolution": "NodeNext",
    "module": "NodeNext",
    "target": "ESNext",
    "lib": [
      "ES2022", // By using ES2022 we get access to the `.cause` property on `Error` instances.
      "DOM" // We are adding `DOM` here to get the `fetch`, etc. types. This should be removed once these types are available via DefinitelyTyped.
    ],

    // Skip type checking for node modules
    "skipLibCheck": true
  }
}
</file>

<file path="packages/client/tsup.config.ts">
import { defineConfig } from "tsup";
export default defineConfig({
  name: "@ponder/client",
  entry: ["src/index.ts"],
  outDir: "dist",
  format: ["esm"],
  sourcemap: true,
  dts: true,
  clean: true,
  splitting: true,
});
</file>

<file path="packages/core/src/_test/contracts/src/ERC20.sol">
// SPDX-License-Identifier: AGPL-3.0-only
pragma solidity >=0.8.0;
/// @notice Modern and gas efficient ERC20 + EIP-2612 implementation.
/// @author Solmate (https://github.com/transmissions11/solmate/blob/main/src/tokens/ERC20.sol)
/// @author Modified from Uniswap (https://github.com/Uniswap/uniswap-v2-core/blob/master/contracts/UniswapV2ERC20.sol)
/// @dev Do not manually set balances without updating totalSupply, as the sum of all user balances must not exceed it.
contract ERC20 {
    /*//////////////////////////////////////////////////////////////
                                 EVENTS
    //////////////////////////////////////////////////////////////*/
    event Transfer(address indexed from, address indexed to, uint256 amount);
    event Transfer(address indexed to, address indexed from);
    event Approval(address indexed owner, address indexed spender, uint256 amount);
    /*//////////////////////////////////////////////////////////////
                            METADATA STORAGE
    //////////////////////////////////////////////////////////////*/
    string public name;
    string public symbol;
    uint8 public immutable decimals;
    /*//////////////////////////////////////////////////////////////
                              ERC20 STORAGE
    //////////////////////////////////////////////////////////////*/
    uint256 public totalSupply;
    mapping(address => uint256) public balanceOf;
    mapping(address => mapping(address => uint256)) public allowance;
    /*//////////////////////////////////////////////////////////////
                               CONSTRUCTOR
    //////////////////////////////////////////////////////////////*/
    constructor(string memory _name, string memory _symbol, uint8 _decimals) {
        name = _name;
        symbol = _symbol;
        decimals = _decimals;
    }
    /*//////////////////////////////////////////////////////////////
                               ERC20 LOGIC
    //////////////////////////////////////////////////////////////*/
    function approve(address spender, uint256 amount) public returns (bool) {
        allowance[msg.sender][spender] = amount;
        emit Approval(msg.sender, spender, amount);
        return true;
    }
    function transfer(address to, uint256 amount) public returns (bool) {
        balanceOf[msg.sender] -= amount;
        // Cannot overflow because the sum of all user
        // balances can't exceed the max uint256 value.
        unchecked {
            balanceOf[to] += amount;
        }
        emit Transfer(msg.sender, to, amount);
        return true;
    }
    function transferFrom(address from, address to, uint256 amount) public returns (bool) {
        uint256 allowed = allowance[from][msg.sender]; // Saves gas for limited approvals.
        if (allowed != type(uint256).max) allowance[from][msg.sender] = allowed - amount;
        balanceOf[from] -= amount;
        // Cannot overflow because the sum of all user
        // balances can't exceed the max uint256 value.
        unchecked {
            balanceOf[to] += amount;
        }
        emit Transfer(from, to, amount);
        return true;
    }
    function mint(address to, uint256 amount) external {
        totalSupply += amount;
        // Cannot overflow because the sum of all user
        // balances can't exceed the max uint256 value.
        unchecked {
            balanceOf[to] += amount;
        }
        emit Transfer(address(0), to, amount);
    }
    function burn(address from, uint256 amount) external {
        balanceOf[from] -= amount;
        // Cannot underflow because a user's balance
        // will never be larger than the total supply.
        unchecked {
            totalSupply -= amount;
        }
        emit Transfer(from, address(0), amount);
    }
}
</file>

<file path="packages/core/src/_test/contracts/src/Factory.sol">
// SPDX-License-Identifier: UNLICENSED
pragma solidity >=0.8.0;
import { Pair } from "./Pair.sol";
contract Factory {
    address[] public allPairs;
    event PairCreated(address indexed pair, uint256 pairIndex);
    function allPairsLength() external view returns (uint256) {
        return allPairs.length;
    }
    function createPair() external returns (address pair) {
        pair = address(new Pair());
        allPairs.push(pair);
        emit PairCreated(pair, allPairs.length);
    }
}
</file>

<file path="packages/core/src/_test/contracts/src/Pair.sol">
// SPDX-License-Identifier: UNLICENSED
pragma solidity >=0.8.0;
contract Pair {
    address public factory;
    event Swap(
        address indexed sender,
        uint256 amount0Out,
        uint256 amount1Out,
        address indexed to
    );
    constructor() {
        factory = msg.sender;
    }
    function swap(uint256 amount0Out, uint256 amount1Out, address to) external {
        emit Swap(msg.sender, amount0Out, amount1Out, to);
    }
}
</file>

<file path="packages/core/src/_test/contracts/src/Revert.sol">
// SPDX-License-Identifier: UNLICENSED
pragma solidity >=0.8.0;
contract Revert {
    error e();
    function revert(bool revert) external {
        if (revert) revert e();
    }
}
</file>

<file path="packages/core/src/_test/e2e/erc20/src/api/index.ts">
import { db } from "ponder:api";
import schema from "ponder:schema";
import { client } from "@/index.js";
import { Hono } from "hono";
const app = new Hono();
app.use("/sql/*", client({ db, schema }));
export default app;
</file>

<file path="packages/core/src/_test/e2e/erc20/src/index.ts">
declare const ponder: import("@/index.js").Virtual.Registry<
  typeof import("../ponder.config.js").default,
  typeof import("../ponder.schema.js")
>;
declare const schema: typeof import("../ponder.schema.js");
// @ts-ignore
// biome-ignore lint/suspicious/noRedeclare: <explanation>
import { ponder } from "ponder:registry";
// @ts-ignore
// biome-ignore lint/suspicious/noRedeclare: <explanation>
import schema from "ponder:schema";
ponder.on(
  "Erc20:Transfer(address indexed from, address indexed to, uint256 amount)",
  async ({ event, context }) => {
    await context.db
      .insert(schema.account)
      .values({
        address: event.args.from,
        balance: -event.args.amount,
      })
      .onConflictDoUpdate((row) => ({
        balance: row.balance - event.args.amount,
      }));
    await context.db
      .insert(schema.account)
      .values({
        address: event.args.to,
        balance: event.args.amount,
      })
      .onConflictDoUpdate((row) => ({
        balance: row.balance + event.args.amount,
      }));
  },
);
</file>

<file path="packages/core/src/_test/e2e/erc20/erc20.test.ts">
import path from "node:path";
import { ALICE } from "@/_test/constants.js";
import {
  setupAnvil,
  setupCommon,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { deployErc20, mintErc20 } from "@/_test/simulate.js";
import { getFreePort, waitForIndexedBlock } from "@/_test/utils.js";
import { start } from "@/bin/commands/start.js";
import { createClient } from "@ponder/client";
import { rimrafSync } from "rimraf";
import { parseEther, zeroAddress } from "viem";
import { beforeEach, expect, test } from "vitest";
import * as schema from "./ponder.schema.js";
const rootDir = path.join(".", "src", "_test", "e2e", "erc20");
beforeEach(() => {
  rimrafSync(path.join(rootDir, ".ponder"));
  rimrafSync(path.join(rootDir, "generated"));
});
beforeEach(setupCommon);
beforeEach(setupAnvil);
beforeEach(setupIsolatedDatabase);
const cliOptions = {
  schema: "public",
  root: rootDir,
  config: "ponder.config.ts",
  logLevel: "error",
  logFormat: "pretty",
};
test("erc20", async () => {
  const port = await getFreePort();
  const client = createClient(`http://localhost:${port}/sql`, { schema });
  const shutdown = await start({
    cliOptions: {
      ...cliOptions,
      command: "start",
      port,
      version: "0.0.0",
    },
  });
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await waitForIndexedBlock({
    port,
    chainName: "mainnet",
    block: { number: 2 },
  });
  const result = await client.db.select().from(schema.account);
  expect(result[0]).toMatchObject({
    address: zeroAddress,
    balance: -1n * 10n ** 18n,
  });
  expect(result[1]).toMatchObject({
    address: ALICE.toLowerCase(),
    balance: 10n ** 18n,
  });
  await shutdown!();
}, 15_000);
</file>

<file path="packages/core/src/_test/e2e/erc20/ponder.config.ts">
import { createConfig } from "../../../config/index.js";
import { erc20ABI } from "../../generated.js";
const poolId = Number(process.env.VITEST_POOL_ID ?? 1);
function getDatabase() {
  if (process.env.DATABASE_URL) {
    const databaseUrl = new URL(process.env.DATABASE_URL);
    databaseUrl.pathname = `/vitest_${poolId}`;
    const connectionString = databaseUrl.toString();
    return { kind: "postgres", connectionString } as const;
  } else {
    return { kind: "pglite" } as const;
  }
}
export default createConfig({
  database: getDatabase(),
  chains: {
    mainnet: {
      id: 1,
      rpc: `http://127.0.0.1:8545/${poolId}`,
    },
  },
  contracts: {
    Erc20: {
      chain: "mainnet",
      abi: erc20ABI,
      address: "0x5fbdb2315678afecb367f032d93f642f64180aa3",
    },
  },
});
</file>

<file path="packages/core/src/_test/e2e/erc20/ponder.schema.ts">
import { onchainTable } from "../../../drizzle/onchain.js";
export const account = onchainTable("account", (t) => ({
  address: t.hex().primaryKey(),
  balance: t.bigint().notNull(),
}));
</file>

<file path="packages/core/src/_test/e2e/factory/src/api/index.ts">
import { db } from "ponder:api";
import schema from "ponder:schema";
import { client } from "@/index.js";
import { Hono } from "hono";
const app = new Hono();
app.use("/sql/*", client({ db, schema }));
export default app;
</file>

<file path="packages/core/src/_test/e2e/factory/src/index.ts">
declare const ponder: import("@/index.js").Virtual.Registry<
  typeof import("../ponder.config.js").default,
  typeof import("../ponder.schema.js")
>;
declare const schema: typeof import("../ponder.schema.js");
// @ts-ignore
// biome-ignore lint/suspicious/noRedeclare: <explanation>
import { ponder } from "ponder:registry";
// @ts-ignore
// biome-ignore lint/suspicious/noRedeclare: <explanation>
import schema from "ponder:schema";
ponder.on("Pair:Swap", async ({ event, context }) => {
  await context.db.insert(schema.swapEvent).values({
    id: event.id,
    pair: event.log.address,
    from: event.args.sender,
    to: event.args.to,
  });
});
</file>

<file path="packages/core/src/_test/e2e/factory/factory.test.ts">
import path from "node:path";
import { ALICE } from "@/_test/constants.js";
import {
  setupAnvil,
  setupCommon,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { deployFactory } from "@/_test/simulate.js";
import { createPair } from "@/_test/simulate.js";
import { swapPair } from "@/_test/simulate.js";
import { getFreePort, waitForIndexedBlock } from "@/_test/utils.js";
import { start } from "@/bin/commands/start.js";
import { createClient } from "@ponder/client";
import { rimrafSync } from "rimraf";
import { beforeEach, expect, test } from "vitest";
import * as schema from "./ponder.schema.js";
const rootDir = path.join(".", "src", "_test", "e2e", "factory");
beforeEach(() => {
  rimrafSync(path.join(rootDir, ".ponder"));
  rimrafSync(path.join(rootDir, "generated"));
});
beforeEach(setupCommon);
beforeEach(setupAnvil);
beforeEach(setupIsolatedDatabase);
const cliOptions = {
  schema: "public",
  root: "./src/_test/e2e/factory",
  config: "ponder.config.ts",
  logLevel: "error",
  logFormat: "pretty",
};
test("factory", async () => {
  const port = await getFreePort();
  const client = createClient(`http://localhost:${port}/sql`, { schema });
  const shutdown = await start({
    cliOptions: {
      ...cliOptions,
      command: "start",
      port,
      version: "0.0.0",
    },
  });
  const { address } = await deployFactory({ sender: ALICE });
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  await swapPair({
    pair,
    amount0Out: 1n,
    amount1Out: 1n,
    to: ALICE,
    sender: ALICE,
  });
  await waitForIndexedBlock({
    port,
    chainName: "mainnet",
    block: { number: 3 },
  });
  let result = await client.db.select().from(schema.swapEvent);
  expect(result).toHaveLength(1);
  expect(result[0]).toMatchObject({
    id: expect.any(String),
    from: ALICE.toLowerCase(),
    to: ALICE.toLowerCase(),
    pair,
  });
  await swapPair({
    pair,
    amount0Out: 1n,
    amount1Out: 1n,
    to: ALICE,
    sender: ALICE,
  });
  await waitForIndexedBlock({
    port,
    chainName: "mainnet",
    block: { number: 4 },
  });
  result = await client.db.select().from(schema.swapEvent);
  expect(result).toHaveLength(2);
  await shutdown!();
}, 15_000);
</file>

<file path="packages/core/src/_test/e2e/factory/ponder.config.ts">
import { factory } from "@/config/address.js";
import { getAbiItem } from "viem";
import { createConfig } from "../../../config/index.js";
import { factoryABI, pairABI } from "../../generated.js";
const poolId = Number(process.env.VITEST_POOL_ID ?? 1);
function getDatabase() {
  if (process.env.DATABASE_URL) {
    const databaseUrl = new URL(process.env.DATABASE_URL);
    databaseUrl.pathname = `/vitest_${poolId}`;
    const connectionString = databaseUrl.toString();
    return { kind: "postgres", connectionString } as const;
  } else {
    return { kind: "pglite" } as const;
  }
}
export default createConfig({
  database: getDatabase(),
  chains: {
    mainnet: {
      id: 1,
      rpc: `http://127.0.0.1:8545/${poolId}`,
    },
  },
  contracts: {
    Pair: {
      chain: "mainnet",
      abi: pairABI,
      address: factory({
        address: "0x5fbdb2315678afecb367f032d93f642f64180aa3",
        event: getAbiItem({ abi: factoryABI, name: "PairCreated" }),
        parameter: "pair",
      }),
    },
  },
});
</file>

<file path="packages/core/src/_test/e2e/factory/ponder.schema.ts">
import { onchainTable } from "../../../drizzle/onchain.js";
export const swapEvent = onchainTable("swap_event", (t) => ({
  id: t.text().primaryKey(),
  pair: t.hex().notNull(),
  from: t.hex().notNull(),
  to: t.hex().notNull(),
}));
</file>

<file path="packages/core/src/_test/.gitignore">
contracts/cache/
contracts/out/
generated.ts
ponder-env.d.ts
</file>

<file path="packages/core/src/_test/constants.ts">
import type {
  BlockFilter,
  LogFilter,
  TraceFilter,
  TransactionFilter,
  TransferFilter,
} from "@/internal/types.js";
import { zeroHash } from "viem";
// Test accounts
export const ACCOUNTS = [
  "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
  "0x70997970C51812dc3A010C7d01b50e0d17dc79C8",
] as const;
// Named accounts
export const [ALICE, BOB] = ACCOUNTS;
export const EMPTY_LOG_FILTER: LogFilter = {
  type: "log",
  chainId: 1,
  sourceId: "test",
  address: undefined,
  topic0: zeroHash,
  topic1: null,
  topic2: null,
  topic3: null,
  fromBlock: undefined,
  toBlock: undefined,
  hasTransactionReceipt: false,
  include: [],
};
export const EMPTY_BLOCK_FILTER: BlockFilter = {
  type: "block",
  chainId: 1,
  sourceId: "test",
  interval: 1,
  offset: 0,
  fromBlock: undefined,
  toBlock: undefined,
  hasTransactionReceipt: false,
  include: [],
};
export const EMPTY_TRANSACTION_FILTER: TransactionFilter = {
  type: "transaction",
  chainId: 1,
  sourceId: "test",
  fromAddress: undefined,
  toAddress: undefined,
  includeReverted: false,
  fromBlock: undefined,
  toBlock: undefined,
  hasTransactionReceipt: true,
  include: [],
};
export const EMPTY_TRACE_FILTER: TraceFilter = {
  type: "trace",
  chainId: 1,
  sourceId: "test",
  callType: "CALL",
  functionSelector: "0x00000000",
  fromAddress: undefined,
  toAddress: undefined,
  includeReverted: false,
  fromBlock: undefined,
  toBlock: undefined,
  hasTransactionReceipt: false,
  include: [],
};
export const EMPTY_TRANSFER_FILTER: TransferFilter = {
  type: "transfer",
  chainId: 1,
  sourceId: "test",
  fromAddress: undefined,
  toAddress: undefined,
  includeReverted: false,
  fromBlock: undefined,
  toBlock: undefined,
  hasTransactionReceipt: false,
  include: [],
};
</file>

<file path="packages/core/src/_test/globalSetup.ts">
import { existsSync } from "node:fs";
import { dirname, join } from "node:path";
import { fileURLToPath } from "node:url";
import { startProxy } from "@viem/anvil";
import dotenv from "dotenv";
import { execa } from "execa";
import { Pool } from "pg";
const __dirname = dirname(fileURLToPath(import.meta.url));
async function globalSetup() {
  dotenv.config({ path: ".env.local" });
  const generatedFilePath = join(__dirname, "generated.ts");
  if (!existsSync(generatedFilePath)) {
    await execa("pnpm", ["wagmi", "generate"]);
  }
  await startProxy({
    options: {
      chainId: 1,
      noMining: true,
    },
  });
  let cleanupDatabase: () => Promise<void>;
  if (process.env.DATABASE_URL) {
    cleanupDatabase = async () => {
      const pool = new Pool({ connectionString: process.env.DATABASE_URL });
      const databaseRows = await pool.query(`
        SELECT datname FROM pg_database WHERE datname LIKE 'vitest_%';
      `);
      const databases = databaseRows.rows.map((r) => r.datname) as string[];
      await Promise.all(
        databases.map((databaseName) =>
          pool.query(`DROP DATABASE "${databaseName}"`),
        ),
      );
      await pool.end();
    };
  }
  return async () => {
    await cleanupDatabase?.();
  };
}
function resetPonderGlobals() {
  // @ts-ignore
  globalThis.PONDER_DATABASE = undefined;
  // @ts-ignore
  globalThis.PONDER_NAMESPACE_BUILD = undefined;
  // @ts-ignore
  globalThis.PONDER_INDEXING_BUILD = undefined;
}
if ("bun" in process.versions) {
  // must be run outside of hook because missing the generated files causes test to fail with 'Cannot find module' error
  await globalSetup();
  const { beforeEach, afterEach } = require("bun:test");
  beforeEach(resetPonderGlobals);
  afterEach(resetPonderGlobals);
}
export default globalSetup;
</file>

<file path="packages/core/src/_test/setup.ts">
import { buildSchema } from "@/build/schema.js";
import { type Database, createDatabase } from "@/database/index.js";
import type { Common } from "@/internal/common.js";
import { createLogger } from "@/internal/logger.js";
import { MetricsService } from "@/internal/metrics.js";
import { buildOptions } from "@/internal/options.js";
import { createShutdown } from "@/internal/shutdown.js";
import { createTelemetry } from "@/internal/telemetry.js";
import type {
  DatabaseConfig,
  EventCallback,
  IndexingBuild,
  NamespaceBuild,
  SchemaBuild,
} from "@/internal/types.js";
import { getFilterFactories, isAddressFactory } from "@/runtime/filter.js";
import { getFactoryFragments, getFragments } from "@/runtime/fragments.js";
import type { CachedIntervals, ChildAddresses } from "@/runtime/index.js";
import { type SyncStore, createSyncStore } from "@/sync-store/index.js";
import { createPglite } from "@/utils/pglite.js";
import type { PGlite } from "@electric-sql/pglite";
import pg from "pg";
import { afterAll } from "vitest";
import { IS_BUN_TEST, TEST_POOL_ID, testClient } from "./utils.js";
export const context = {} as {
  common: Common;
  databaseConfig: DatabaseConfig;
};
export function setupCommon() {
  const cliOptions = {
    command: "start",
    config: "",
    root: "",
    logLevel: "silent",
    logFormat: "pretty",
    version: "0.0.0",
  } as const;
  const options = { ...buildOptions({ cliOptions }), telemetryDisabled: true };
  const logger = createLogger({ level: cliOptions.logLevel });
  const metrics = new MetricsService();
  const shutdown = createShutdown();
  const telemetry = createTelemetry({ options, logger, shutdown });
  context.common = {
    options,
    logger,
    metrics,
    telemetry,
    shutdown,
    apiShutdown: shutdown,
    buildShutdown: shutdown,
  };
}
export function setupCleanup() {
  if (IS_BUN_TEST) {
    require("bun:test").afterEach(async () => {
      await context.common.shutdown.kill();
    });
    return;
  }
  return context.common.shutdown.kill;
}
const pgliteInstances = new Map<number, PGlite>();
afterAll(async () => {
  await Promise.all(
    Array.from(pgliteInstances.values()).map(async (instance) => {
      await instance.close();
    }),
  );
  pgliteInstances.clear();
});
/**
 * Sets up an isolated database on the test context.
 *
 * ```ts
 * // Add this to any test suite that uses the database.
 * beforeEach(setupIsolatedDatabase)
 * ```
 */
export async function setupIsolatedDatabase() {
  const connectionString = process.env.DATABASE_URL;
  if (connectionString) {
    const databaseName = `vitest_${TEST_POOL_ID}`;
    const client = new pg.Client({ connectionString });
    await client.connect();
    await client.query(
      `
      SELECT pg_terminate_backend(pg_stat_activity.pid)
      FROM pg_stat_activity
      WHERE pg_stat_activity.datname = $1 AND pid <> pg_backend_pid()
      `,
      [databaseName],
    );
    await client.query(`DROP DATABASE IF EXISTS "${databaseName}"`);
    await client.query(`DROP ROLE IF EXISTS "ponder_${databaseName}_public"`);
    await client.query(`CREATE DATABASE "${databaseName}"`);
    await client.end();
    const databaseUrl = new URL(connectionString);
    databaseUrl.pathname = `/${databaseName}`;
    const poolConfig = { max: 30, connectionString: databaseUrl.toString() };
    context.databaseConfig = { kind: "postgres", poolConfig };
  } else {
    let instance = pgliteInstances.get(TEST_POOL_ID);
    if (instance === undefined) {
      instance = createPglite({ dataDir: "memory://" });
      pgliteInstances.set(TEST_POOL_ID, instance);
    }
    // Because PGlite takes ~500ms to open a new connection, and it's not possible to drop the
    // current database from within a connection, we run this query to mimic the effect of
    // "DROP DATABASE" without closing the connection. This speeds up the tests quite a lot.
    await instance.exec(`
      DO $$
      DECLARE
        obj TEXT;
        schema TEXT;
      BEGIN
        -- Loop over all user-defined schemas
        FOR schema IN SELECT nspname FROM pg_namespace WHERE nspname NOT LIKE 'pg_%' AND nspname != 'information_schema'
        LOOP
          -- Drop all tables
          FOR obj IN SELECT table_name FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_schema = schema
          LOOP
            EXECUTE 'DROP TABLE IF EXISTS ' || quote_ident(schema) || '.' || quote_ident(obj) || ' CASCADE;';
          END LOOP;
          -- Drop all sequences
          FOR obj IN SELECT sequence_name FROM information_schema.sequences WHERE sequence_schema = schema
          LOOP
            EXECUTE 'DROP SEQUENCE IF EXISTS ' || quote_ident(schema) || '.' || quote_ident(obj) || ' CASCADE;';
          END LOOP;
          -- Drop all views
          FOR obj IN SELECT table_name FROM information_schema.views WHERE table_schema = schema
          LOOP
            EXECUTE 'DROP VIEW IF EXISTS ' || quote_ident(schema) || '.' || quote_ident(obj) || ' CASCADE;';
          END LOOP;
          -- Drop all functions
          FOR obj IN SELECT routine_name FROM information_schema.routines WHERE routine_type = 'FUNCTION' AND routine_schema = schema
          LOOP
            EXECUTE 'DROP FUNCTION IF EXISTS ' || quote_ident(schema) || '.' || quote_ident(obj) || ' CASCADE;';
          END LOOP;
          -- Drop all enum types first (this will cascade and drop their associated array types)
          FOR obj IN
            SELECT typname
            FROM pg_type
            JOIN pg_namespace ns ON pg_type.typnamespace = ns.oid
            WHERE ns.nspname = schema
              AND typtype = 'e'  -- 'e' stands for enum type
          LOOP
            EXECUTE 'DROP TYPE IF EXISTS ' || quote_ident(schema) || '.' || quote_ident(obj) || ' CASCADE;';
          END LOOP;
          -- Drop all remaining custom types (non-enum)
          FOR obj IN
            SELECT typname
            FROM pg_type
            JOIN pg_namespace ns ON pg_type.typnamespace = ns.oid
            WHERE ns.nspname = schema
              AND typtype <> 'e'
          LOOP
            EXECUTE 'DROP TYPE IF EXISTS ' || quote_ident(schema) || '.' || quote_ident(obj) || ' CASCADE;';
          END LOOP;
          -- Drop all extensions
          FOR obj IN SELECT extname FROM pg_extension WHERE extnamespace = (SELECT oid FROM pg_namespace WHERE nspname = schema)
          LOOP
            EXECUTE 'DROP EXTENSION IF EXISTS ' || quote_ident(obj) || ' CASCADE;';
          END LOOP;
        END LOOP;
      END $$;
    `);
    context.databaseConfig = { kind: "pglite_test", instance };
  }
}
export async function setupDatabaseServices(
  overrides: Partial<{
    namespaceBuild: NamespaceBuild;
    schemaBuild: Partial<SchemaBuild>;
    indexingBuild: Partial<IndexingBuild>;
  }> = {},
): Promise<{
  database: Database;
  syncStore: SyncStore;
}> {
  const { statements } = buildSchema({
    schema: overrides.schemaBuild?.schema ?? {},
    preBuild: { ordering: "multichain" },
  });
  const database = createDatabase({
    common: context.common,
    namespace: overrides.namespaceBuild ?? {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: overrides.schemaBuild?.schema ?? {},
      statements,
    },
  });
  await database.migrate({
    buildId: overrides.indexingBuild?.buildId ?? "abc",
    chains: overrides.indexingBuild?.chains ?? [],
    finalizedBlocks: overrides.indexingBuild?.finalizedBlocks ?? [],
  });
  await database.migrateSync().catch((err) => {
    console.log(err);
    throw err;
  });
  const syncStore = createSyncStore({
    common: context.common,
    qb: database.syncQB,
  });
  return {
    database,
    syncStore,
  };
}
/**
 * Sets up an isolated Ethereum client.
 *
 * @example
 * ```ts
 * // Add this to any test suite that uses the Ethereum client.
 * beforeEach(setupAnvil)
 * ```
 */
export async function setupAnvil() {
  const emptySnapshotId = await testClient.snapshot();
  const cleanup = async () => {
    await testClient.revert({ id: emptySnapshotId });
  };
  if (IS_BUN_TEST) return require("bun:test").afterEach(cleanup);
  return cleanup;
}
export const setupChildAddresses = (
  eventCallbacks: EventCallback[],
): ChildAddresses => {
  const childAddresses = new Map();
  for (const eventCallback of eventCallbacks) {
    switch (eventCallback.filter.type) {
      case "log":
        if (isAddressFactory(eventCallback.filter.address)) {
          childAddresses.set(eventCallback.filter.address.id, new Map());
        }
        break;
      case "transaction":
      case "transfer":
      case "trace":
        if (isAddressFactory(eventCallback.filter.fromAddress)) {
          childAddresses.set(eventCallback.filter.fromAddress.id, new Map());
        }
        if (isAddressFactory(eventCallback.filter.toAddress)) {
          childAddresses.set(eventCallback.filter.toAddress.id, new Map());
        }
    }
  }
  return childAddresses as ChildAddresses;
};
export const setupCachedIntervals = (
  eventCallbacks: EventCallback[],
): CachedIntervals => {
  const cachedIntervals: CachedIntervals = new Map();
  for (const eventCallback of eventCallbacks) {
    cachedIntervals.set(eventCallback.filter, []);
    for (const { fragment } of getFragments(eventCallback.filter)) {
      cachedIntervals
        .get(eventCallback.filter)!
        .push({ fragment, intervals: [] });
    }
    for (const factory of getFilterFactories(eventCallback.filter)) {
      cachedIntervals.set(factory, []);
      for (const fragment of getFactoryFragments(factory)) {
        cachedIntervals.get(factory)!.push({ fragment, intervals: [] });
      }
    }
  }
  return cachedIntervals;
};
</file>

<file path="packages/core/src/_test/simulate.ts">
import type {
  SyncBlock,
  SyncLog,
  SyncTrace,
  SyncTransaction,
  SyncTransactionReceipt,
} from "@/internal/types.js";
import { toLowerCase } from "@/utils/lowercase.js";
import {
  http,
  type Address,
  type Hex,
  createWalletClient,
  encodeFunctionData,
  encodeFunctionResult,
  multicall3Abi,
  numberToHex,
} from "viem";
import Erc20Bytecode from "./contracts/out/ERC20.sol/ERC20.json";
import FactoryBytecode from "./contracts/out/Factory.sol/Factory.json";
import RevertBytecode from "./contracts/out/Revert.sol/Revert.json";
import { erc20ABI, factoryABI, pairABI, revertABI } from "./generated.js";
import { anvil, publicClient, testClient } from "./utils.js";
/** Deploy Erc20 contract and mine block. */
export const deployErc20 = async (params: { sender: Address }): Promise<{
  address: Address;
}> => {
  const walletClient = createWalletClient({
    chain: anvil,
    transport: http(),
    account: params.sender,
  });
  const hash = await walletClient.deployContract({
    abi: erc20ABI,
    bytecode: Erc20Bytecode.bytecode.object as Hex,
    args: ["name", "symbol", 18],
  });
  await testClient.mine({ blocks: 1 });
  const { contractAddress } = await publicClient.getTransactionReceipt({
    hash,
  });
  return { address: contractAddress! };
};
/** Deploy Factory contract and mine block. */
export const deployFactory = async (params: { sender: Address }): Promise<{
  address: Address;
}> => {
  const walletClient = createWalletClient({
    chain: anvil,
    transport: http(),
    account: params.sender,
  });
  const hash = await walletClient.deployContract({
    abi: factoryABI,
    bytecode: FactoryBytecode.bytecode.object as Hex,
  });
  await testClient.mine({ blocks: 1 });
  const { contractAddress } = await publicClient.getTransactionReceipt({
    hash,
  });
  return { address: contractAddress! };
};
/** Deploy Revert contract and mine block. */
export const deployRevert = async (params: { sender: Address }): Promise<{
  address: Address;
}> => {
  const walletClient = createWalletClient({
    chain: anvil,
    transport: http(),
    account: params.sender,
  });
  const hash = await walletClient.deployContract({
    abi: revertABI,
    bytecode: RevertBytecode.bytecode.object as Hex,
  });
  await testClient.mine({ blocks: 1 });
  const { contractAddress } = await publicClient.getTransactionReceipt({
    hash,
  });
  return { address: contractAddress! };
};
export const deployMulticall = async (params: { sender: Address }): Promise<{
  address: Address;
}> => {
  const walletClient = createWalletClient({
    chain: anvil,
    transport: http(),
    account: params.sender,
  });
  const hash = await walletClient.deployContract({
    abi: multicall3Abi,
    bytecode:
      "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",
  });
  await testClient.mine({ blocks: 1 });
  const { contractAddress } = await publicClient.getTransactionReceipt({
    hash,
  });
  return { address: contractAddress! };
};
/** Create pair and mine block. */
export const createPair = async (params: {
  factory: Address;
  sender: Address;
}): Promise<{
  address: Address;
  block: SyncBlock;
  transaction: SyncTransaction;
  transactionReceipt: SyncTransactionReceipt;
  log: SyncLog;
}> => {
  const walletClient = createWalletClient({
    chain: anvil,
    transport: http(),
    account: params.sender,
  });
  const { result, request } = await publicClient.simulateContract({
    abi: factoryABI,
    functionName: "createPair",
    address: params.factory,
  });
  const hash = await walletClient.writeContract(request);
  await testClient.mine({ blocks: 1 });
  const block = await publicClient.request({
    method: "eth_getBlockByNumber",
    params: ["latest", true],
  });
  const receipt = await publicClient.request({
    method: "eth_getTransactionReceipt",
    params: [hash],
  });
  return {
    address: toLowerCase(result),
    block: block! as SyncBlock,
    transaction: block!.transactions[0]! as SyncTransaction,
    transactionReceipt: receipt!,
    log: receipt!.logs[0]!,
  };
};
/** Mint Erc20 tokens and mine block. */
export const mintErc20 = async (params: {
  erc20: Address;
  to: Address;
  amount: bigint;
  sender: Address;
}): Promise<{
  block: SyncBlock;
  log: SyncLog;
  transaction: SyncTransaction;
  transactionReceipt: SyncTransactionReceipt;
}> => {
  const walletClient = createWalletClient({
    chain: anvil,
    transport: http(),
    account: params.sender,
  });
  const hash = await walletClient.writeContract({
    abi: erc20ABI,
    functionName: "mint",
    address: params.erc20,
    args: [params.to, params.amount],
  });
  await testClient.mine({ blocks: 1 });
  const block = await publicClient.request({
    method: "eth_getBlockByNumber",
    params: ["latest", true],
  });
  const receipt = await publicClient.request({
    method: "eth_getTransactionReceipt",
    params: [hash],
  });
  return {
    block: block! as SyncBlock,
    transaction: block!.transactions[0]! as SyncTransaction,
    transactionReceipt: receipt!,
    log: receipt!.logs[0]!,
  };
};
/** Transfer Erc20 tokens and mine block. */
export const transferErc20 = async (params: {
  erc20: Address;
  to: Address;
  amount: bigint;
  sender: Address;
}): Promise<{
  block: SyncBlock;
  transaction: SyncTransaction;
  transactionReceipt: SyncTransactionReceipt;
  trace: SyncTrace;
  log: SyncLog;
}> => {
  const walletClient = createWalletClient({
    chain: anvil,
    transport: http(),
    account: params.sender,
  });
  const hash = await walletClient.writeContract({
    abi: erc20ABI,
    functionName: "transfer",
    address: params.erc20,
    args: [params.to, params.amount],
  });
  await testClient.mine({ blocks: 1 });
  const block = await publicClient.request({
    method: "eth_getBlockByNumber",
    params: ["latest", true],
  });
  const receipt = await publicClient.request({
    method: "eth_getTransactionReceipt",
    params: [hash],
  });
  const trace = {
    trace: {
      type: "CALL",
      from: params.sender,
      to: params.erc20,
      gas: "0x0",
      gasUsed: "0x0",
      input: encodeFunctionData({
        abi: erc20ABI,
        functionName: "transfer",
        args: [params.to, params.amount],
      }),
      output: encodeFunctionResult({
        abi: erc20ABI,
        functionName: "transfer",
        result: true,
      }),
      value: "0x0",
      index: 0,
      subcalls: 0,
    },
    transactionHash: hash,
  } satisfies SyncTrace;
  return {
    block: block! as SyncBlock,
    transaction: block!.transactions[0]! as SyncTransaction,
    transactionReceipt: receipt! as SyncTransactionReceipt,
    trace,
    log: receipt!.logs[0]!,
  };
};
/** Swap tokens in pair and mine block. */
export const swapPair = async (params: {
  pair: Address;
  amount0Out: bigint;
  amount1Out: bigint;
  to: Address;
  sender: Address;
}): Promise<{
  block: SyncBlock;
  transaction: SyncTransaction;
  transactionReceipt: SyncTransactionReceipt;
  trace: SyncTrace;
  log: SyncLog;
}> => {
  const walletClient = createWalletClient({
    chain: anvil,
    transport: http(),
    account: params.sender,
  });
  const hash = await walletClient.writeContract({
    abi: pairABI,
    functionName: "swap",
    address: params.pair,
    args: [params.amount0Out, params.amount1Out, params.to],
  });
  await testClient.mine({ blocks: 1 });
  const block = await publicClient.request({
    method: "eth_getBlockByNumber",
    params: ["latest", true],
  });
  const receipt = await publicClient.request({
    method: "eth_getTransactionReceipt",
    params: [hash],
  });
  const trace = {
    trace: {
      type: "CALL",
      from: params.sender,
      to: params.pair,
      gas: "0x0",
      gasUsed: "0x0",
      input: encodeFunctionData({
        abi: pairABI,
        functionName: "swap",
        args: [params.amount0Out, params.amount1Out, params.to],
      }),
      output: undefined,
      value: "0x0",
      index: 0,
      subcalls: 0,
    },
    transactionHash: hash,
  } satisfies SyncTrace;
  return {
    block: block! as SyncBlock,
    transaction: block!.transactions[0]! as SyncTransaction,
    transactionReceipt: receipt! as SyncTransactionReceipt,
    trace,
    log: receipt!.logs[0]!,
  };
};
/** Transfer native tokens and mine block. */
export const transferEth = async (params: {
  to: Address;
  amount: bigint;
  sender: Address;
}): Promise<{
  block: SyncBlock;
  transaction: SyncTransaction;
  transactionReceipt: SyncTransactionReceipt;
  trace: SyncTrace;
}> => {
  const walletClient = createWalletClient({
    chain: anvil,
    transport: http(),
    account: params.sender,
  });
  const hash = await walletClient.sendTransaction({
    to: params.to,
    value: params.amount,
  });
  await testClient.mine({ blocks: 1 });
  const block = await publicClient.request({
    method: "eth_getBlockByNumber",
    params: ["latest", true],
  });
  const receipt = await publicClient.request({
    method: "eth_getTransactionReceipt",
    params: [hash],
  });
  const trace = {
    trace: {
      type: "CALL",
      from: params.sender,
      to: params.to,
      gas: "0x0",
      gasUsed: "0x0",
      input: "0x",
      output: undefined,
      value: numberToHex(params.amount),
      index: 0,
      subcalls: 0,
    },
    transactionHash: hash,
  } satisfies SyncTrace;
  return {
    block: block! as SyncBlock,
    transaction: block!.transactions[0]! as SyncTransaction,
    transactionReceipt: receipt! as SyncTransactionReceipt,
    trace,
  };
};
export const simulateBlock = async (): Promise<{ block: SyncBlock }> => {
  await testClient.mine({ blocks: 1 });
  const block = await publicClient.request({
    method: "eth_getBlockByNumber",
    params: ["latest", true],
  });
  return { block: block! as SyncBlock };
};
</file>

<file path="packages/core/src/_test/utils.ts">
import { type AddressInfo, createServer } from "node:net";
import { buildLogFactory } from "@/build/factory.js";
import { factory } from "@/config/address.js";
import type { Common } from "@/internal/common.js";
import type {
  Chain,
  Contract,
  Event,
  EventCallback,
  Factory,
  FilterAddress,
  IndexingFunctions,
  SetupCallback,
  Status,
  TransactionFilter,
} from "@/internal/types.js";
import {
  buildEvents,
  decodeEvents,
  syncBlockToInternal,
  syncLogToInternal,
  syncTraceToInternal,
  syncTransactionReceiptToInternal,
  syncTransactionToInternal,
} from "@/runtime/events.js";
import {
  defaultBlockFilterInclude,
  defaultLogFilterInclude,
  defaultTraceFilterInclude,
  defaultTransactionFilterInclude,
  defaultTransactionReceiptInclude,
  defaultTransferFilterInclude,
} from "@/runtime/filter.js";
import { toLowerCase } from "@/utils/lowercase.js";
import {
  http,
  type Address,
  type Chain as ViemChain,
  createPublicClient,
  createTestClient,
  getAbiItem,
  toEventSelector,
  toFunctionSelector,
} from "viem";
import { mainnet } from "viem/chains";
import { vi } from "vitest";
import { erc20ABI, factoryABI, pairABI } from "./generated.js";
import type {
  mintErc20,
  simulateBlock,
  swapPair,
  transferErc20,
  transferEth,
} from "./simulate.js";
// Anvil test setup adapted from @viem/anvil `example-vitest` repository.
// https://github.com/wagmi-dev/anvil.js/tree/main/examples/example-vitest
// ID of the current test worker. Used by the `@viem/anvil` proxy server.
export const TEST_POOL_ID = Number(process.env.VITEST_POOL_ID ?? 1);
export const IS_BUN_TEST = "bun" in process.versions;
export const anvil = {
  ...mainnet, // We are using a mainnet fork for testing.
  id: 1, // We configured our anvil instance to use `1` as the chain id (see `globalSetup.ts`);
  rpcUrls: {
    default: {
      http: [`http://127.0.0.1:8545/${TEST_POOL_ID}`],
      webSocket: [`ws://127.0.0.1:8545/${TEST_POOL_ID}`],
    },
    public: {
      http: [`http://127.0.0.1:8545/${TEST_POOL_ID}`],
      webSocket: [`ws://127.0.0.1:8545/${TEST_POOL_ID}`],
    },
  },
} as const satisfies ViemChain;
export const testClient = createTestClient({
  chain: anvil,
  mode: "anvil",
  transport: http(),
});
export const publicClient = createPublicClient({
  chain: anvil,
  transport: http(),
});
export async function withStubbedEnv(
  env: Record<string, string | undefined>,
  testCase: () => void | Promise<void>,
) {
  const originalValues = {} as Record<string, string | undefined>;
  for (const [k, v] of Object.entries(env)) {
    originalValues[k] = process.env[k];
    if (v === undefined) delete process.env[k];
    else process.env[k] = v;
  }
  try {
    await testCase();
  } finally {
    for (const [k, v] of Object.entries(originalValues)) {
      if (v === undefined) delete process.env[k];
      else process.env[k] = v;
    }
  }
}
export function stubGlobal<Key extends keyof typeof globalThis>(
  key: Key,
  value: (typeof globalThis)[Key],
): () => void {
  const g = globalThis as any;
  const hadOwnProperty = Object.prototype.hasOwnProperty.call(g, key);
  const original = g[key];
  g[key] = value;
  return () => {
    if (hadOwnProperty) {
      g[key] = original;
    } else {
      // If it didn't exist before, remove it entirely
      delete g[key];
    }
  };
}
export const getErc20IndexingBuild = <
  includeCallTraces extends boolean = false,
>(params: {
  address: Address;
  includeCallTraces?: includeCallTraces;
  includeTransactionReceipts?: boolean;
}): includeCallTraces extends true
  ? {
      eventCallbacks: [EventCallback, EventCallback];
      setupCallbacks: [SetupCallback];
      indexingFunctions: IndexingFunctions;
      contracts: { [name: string]: Contract };
    }
  : {
      eventCallbacks: [EventCallback];
      setupCallbacks: [SetupCallback];
      indexingFunctions: IndexingFunctions;
      contracts: { [name: string]: Contract };
    } => {
  const eventCallbacks = params.includeCallTraces
    ? ([
        {
          filter: {
            type: "trace",
            chainId: 1,
            sourceId: "Erc20",
            fromAddress: undefined,
            toAddress: toLowerCase(params.address),
            callType: "CALL",
            functionSelector: toFunctionSelector(
              getAbiItem({ abi: erc20ABI, name: "transfer" }),
            ),
            includeReverted: false,
            fromBlock: undefined,
            toBlock: undefined,
            hasTransactionReceipt: params.includeTransactionReceipts ?? false,
            include: defaultTraceFilterInclude.concat(
              params.includeTransactionReceipts
                ? defaultTransactionReceiptInclude.map(
                    (value) => `transactionReceipt.${value}` as const,
                  )
                : [],
            ),
          },
          name: "Erc20.transfer()",
          fn: vi.fn(),
          chain: getChain(),
          type: "contract",
          abiItem: getAbiItem({ abi: erc20ABI, name: "transfer" }),
          metadata: {
            safeName: "transfer()",
            abi: erc20ABI,
          },
        },
        {
          filter: {
            type: "log",
            chainId: 1,
            sourceId: "Erc20",
            address: toLowerCase(params.address),
            topic0: toEventSelector(
              getAbiItem({ abi: erc20ABI, name: "Transfer" }),
            ),
            topic1: null,
            topic2: null,
            topic3: null,
            fromBlock: undefined,
            toBlock: undefined,
            hasTransactionReceipt: params.includeTransactionReceipts ?? false,
            include: defaultLogFilterInclude.concat(
              params.includeTransactionReceipts
                ? defaultTransactionReceiptInclude.map(
                    (value) => `transactionReceipt.${value}` as const,
                  )
                : [],
            ),
          },
          name: "Erc20:Transfer(address indexed from, address indexed to, uint256 amount)",
          fn: vi.fn(),
          chain: getChain(),
          type: "contract",
          abiItem: getAbiItem({ abi: erc20ABI, name: "Transfer" }),
          metadata: {
            safeName:
              "Transfer(address indexed from, address indexed to, uint256 amount)",
            abi: erc20ABI,
          },
        },
      ] satisfies [EventCallback, EventCallback])
    : ([
        {
          filter: {
            type: "log",
            chainId: 1,
            sourceId: "Erc20",
            address: toLowerCase(params.address),
            topic0: toEventSelector(
              getAbiItem({ abi: erc20ABI, name: "Transfer" }),
            ),
            topic1: null,
            topic2: null,
            topic3: null,
            fromBlock: undefined,
            toBlock: undefined,
            hasTransactionReceipt: params.includeTransactionReceipts ?? false,
            include: defaultLogFilterInclude.concat(
              params.includeTransactionReceipts
                ? defaultTransactionReceiptInclude.map(
                    (value) => `transactionReceipt.${value}` as const,
                  )
                : [],
            ),
          },
          name: "Erc20:Transfer(address indexed from, address indexed to, uint256 amount)",
          fn: vi.fn(),
          chain: getChain(),
          type: "contract",
          abiItem: getAbiItem({ abi: erc20ABI, name: "Transfer" }),
          metadata: {
            safeName:
              "Transfer(address indexed from, address indexed to, uint256 amount)",
            abi: erc20ABI,
          },
        },
      ] satisfies [EventCallback]);
  const setupCallbacks = [
    {
      name: "Erc20:setup",
      fn: vi.fn(),
      chain: getChain(),
      block: undefined,
    },
  ] satisfies [SetupCallback];
  const indexingFunctions = params.includeCallTraces
    ? [
        {
          name: "Erc20.transfer()",
          fn: vi.fn(),
        },
        {
          name: "Erc20:Transfer(address indexed from, address indexed to, uint256 amount)",
          fn: vi.fn(),
        },
        {
          name: "Erc20:setup",
          fn: vi.fn(),
        },
      ]
    : [
        {
          name: "Erc20:Transfer(address indexed from, address indexed to, uint256 amount)",
          fn: vi.fn(),
        },
        {
          name: "Erc20:setup",
          fn: vi.fn(),
        },
      ];
  const contracts = {
    Erc20: {
      abi: erc20ABI,
      address: params.address,
      startBlock: undefined,
      endBlock: undefined,
    },
  };
  // @ts-ignore
  return { eventCallbacks, setupCallbacks, indexingFunctions, contracts };
};
export const getPairWithFactoryIndexingBuild = <
  includeCallTraces extends boolean = false,
>(params: {
  address: Address;
  includeCallTraces?: includeCallTraces;
  includeTransactionReceipts?: boolean;
}): includeCallTraces extends true
  ? {
      eventCallbacks: [EventCallback, EventCallback];
      setupCallbacks: [SetupCallback];
      indexingFunctions: IndexingFunctions;
      contracts: { [name: string]: Contract };
    }
  : {
      eventCallbacks: [EventCallback];
      setupCallbacks: [SetupCallback];
      indexingFunctions: IndexingFunctions;
      contracts: { [name: string]: Contract };
    } => {
  const pairAddress = buildLogFactory({
    chainId: 1,
    sourceId: "Pair",
    fromBlock: undefined,
    toBlock: undefined,
    ...factory({
      address: params.address,
      event: getAbiItem({ abi: factoryABI, name: "PairCreated" }),
      parameter: "pair",
    }),
  }) satisfies FilterAddress<Factory>;
  const eventCallbacks = params.includeCallTraces
    ? ([
        {
          filter: {
            type: "trace",
            chainId: 1,
            sourceId: "Pair",
            fromAddress: undefined,
            toAddress: pairAddress,
            callType: "CALL",
            functionSelector: toFunctionSelector(
              getAbiItem({ abi: pairABI, name: "swap" }),
            ),
            includeReverted: false,
            fromBlock: undefined,
            toBlock: undefined,
            hasTransactionReceipt: params.includeTransactionReceipts ?? false,
            include: defaultTraceFilterInclude.concat(
              params.includeTransactionReceipts
                ? defaultTransactionReceiptInclude.map(
                    (value) => `transactionReceipt.${value}` as const,
                  )
                : [],
            ),
          },
          name: "Pair.swap()",
          fn: vi.fn(),
          chain: getChain(),
          type: "contract",
          abiItem: getAbiItem({ abi: pairABI, name: "swap" }),
          metadata: {
            safeName: "swap()",
            abi: pairABI,
          },
        },
        {
          filter: {
            type: "log",
            chainId: 1,
            sourceId: "Pair",
            address: pairAddress,
            topic0: toEventSelector(getAbiItem({ abi: pairABI, name: "Swap" })),
            topic1: null,
            topic2: null,
            topic3: null,
            fromBlock: undefined,
            toBlock: undefined,
            hasTransactionReceipt: params.includeTransactionReceipts ?? false,
            include: defaultLogFilterInclude.concat(
              params.includeTransactionReceipts
                ? defaultTransactionReceiptInclude.map(
                    (value) => `transactionReceipt.${value}` as const,
                  )
                : [],
            ),
          },
          name: "Pair:Swap",
          fn: vi.fn(),
          chain: getChain(),
          type: "contract",
          abiItem: getAbiItem({ abi: pairABI, name: "Swap" }),
          metadata: {
            safeName: "Swap",
            abi: pairABI,
          },
        },
      ] satisfies [EventCallback, EventCallback])
    : ([
        {
          filter: {
            type: "log",
            chainId: 1,
            sourceId: "Pair",
            address: pairAddress,
            topic0: toEventSelector(getAbiItem({ abi: pairABI, name: "Swap" })),
            topic1: null,
            topic2: null,
            topic3: null,
            fromBlock: undefined,
            toBlock: undefined,
            hasTransactionReceipt: params.includeTransactionReceipts ?? false,
            include: defaultLogFilterInclude.concat(
              params.includeTransactionReceipts
                ? defaultTransactionReceiptInclude.map(
                    (value) => `transactionReceipt.${value}` as const,
                  )
                : [],
            ),
          },
          name: "Pair:Swap",
          fn: vi.fn(),
          chain: getChain(),
          type: "contract",
          abiItem: getAbiItem({ abi: pairABI, name: "Swap" }),
          metadata: {
            safeName: "Swap",
            abi: pairABI,
          },
        },
      ] satisfies [EventCallback]);
  const setupCallbacks = [
    {
      name: "Pair:setup",
      fn: vi.fn(),
      chain: getChain(),
      block: undefined,
    },
  ] satisfies [SetupCallback];
  const indexingFunctions = params.includeCallTraces
    ? ([
        { name: "Pair.swap()", fn: vi.fn() },
        { name: "Pair:Swap", fn: vi.fn() },
        { name: "Pair:setup", fn: vi.fn() },
      ] satisfies [
        IndexingFunctions[number],
        IndexingFunctions[number],
        IndexingFunctions[number],
      ])
    : ([
        { name: "Pair:Swap", fn: vi.fn() },
        { name: "Pair:setup", fn: vi.fn() },
      ] satisfies [IndexingFunctions[number], IndexingFunctions[number]]);
  const contracts = {
    Pair: {
      abi: pairABI,
      address: params.address,
      startBlock: undefined,
      endBlock: undefined,
    },
  };
  // @ts-ignore
  return { eventCallbacks, setupCallbacks, indexingFunctions, contracts };
};
export const getBlocksIndexingBuild = (params: {
  interval: number;
}): {
  indexingFunctions: [IndexingFunctions[number]];
  eventCallbacks: [EventCallback];
} => {
  const eventCallbacks = [
    {
      filter: {
        type: "block",
        chainId: 1,
        sourceId: "Blocks",
        interval: params.interval,
        offset: 0,
        fromBlock: undefined,
        toBlock: undefined,
        hasTransactionReceipt: false,
        include: defaultBlockFilterInclude,
      },
      name: "Blocks:block",
      fn: vi.fn(),
      chain: getChain(),
      type: "block",
    },
  ] satisfies [EventCallback];
  const indexingFunctions = [{ name: "Blocks:block", fn: vi.fn() }] satisfies [
    IndexingFunctions[number],
  ];
  return { eventCallbacks, indexingFunctions };
};
export const getAccountsIndexingBuild = (params: {
  address: Address;
}): {
  eventCallbacks: [EventCallback, EventCallback, EventCallback, EventCallback];
  indexingFunctions: IndexingFunctions;
} => {
  const eventCallbacks = [
    {
      filter: {
        type: "transaction",
        chainId: 1,
        sourceId: "Accounts",
        fromAddress: undefined,
        toAddress: toLowerCase(params.address),
        includeReverted: false,
        fromBlock: undefined,
        toBlock: undefined,
        hasTransactionReceipt: true,
        include: defaultTransactionFilterInclude,
      } satisfies TransactionFilter,
      name: "Accounts:transaction:to",
      fn: vi.fn(),
      chain: getChain(),
      type: "account",
      direction: "to",
    },
    {
      filter: {
        type: "transaction",
        chainId: 1,
        sourceId: "Accounts",
        fromAddress: toLowerCase(params.address),
        toAddress: undefined,
        includeReverted: false,
        fromBlock: undefined,
        toBlock: undefined,
        hasTransactionReceipt: true,
        include: defaultTransactionFilterInclude,
      } satisfies TransactionFilter,
      name: "Accounts:transaction:from",
      fn: vi.fn(),
      chain: getChain(),
      type: "account",
      direction: "from",
    },
    {
      filter: {
        type: "transfer",
        chainId: 1,
        sourceId: "Accounts",
        fromAddress: undefined,
        toAddress: toLowerCase(params.address),
        includeReverted: false,
        fromBlock: undefined,
        toBlock: undefined,
        hasTransactionReceipt: false,
        include: defaultTransferFilterInclude,
      },
      name: "Accounts:transfer:to",
      fn: vi.fn(),
      chain: getChain(),
      type: "account",
      direction: "to",
    },
    {
      filter: {
        type: "transfer",
        chainId: 1,
        sourceId: "Accounts",
        fromAddress: toLowerCase(params.address),
        toAddress: undefined,
        includeReverted: false,
        fromBlock: undefined,
        toBlock: undefined,
        hasTransactionReceipt: false,
        include: defaultTransferFilterInclude,
      },
      name: "Accounts:transfer:from",
      fn: vi.fn(),
      chain: getChain(),
      type: "account",
      direction: "from",
    },
  ] satisfies [EventCallback, EventCallback, EventCallback, EventCallback];
  const indexingFunctions = [
    { name: "Accounts:transaction:from", fn: vi.fn() },
    { name: "Accounts:transaction:to", fn: vi.fn() },
    { name: "Accounts:transfer:from", fn: vi.fn() },
    { name: "Accounts:transfer:to", fn: vi.fn() },
  ] satisfies [
    IndexingFunctions[number],
    IndexingFunctions[number],
    IndexingFunctions[number],
    IndexingFunctions[number],
  ];
  return { eventCallbacks, indexingFunctions };
};
export const getSimulatedEvent = ({
  eventCallback,
  blockData,
}: {
  eventCallback: EventCallback;
  blockData:
    | Awaited<ReturnType<typeof simulateBlock>>
    | Awaited<ReturnType<typeof mintErc20>>
    | Awaited<ReturnType<typeof transferErc20>>
    | Awaited<ReturnType<typeof transferEth>>
    | Awaited<ReturnType<typeof swapPair>>;
}): Event => {
  const rawEvents = buildEvents({
    eventCallbacks: [eventCallback],
    blocks: [syncBlockToInternal({ block: blockData.block })],
    // @ts-ignore
    logs: blockData.log ? [syncLogToInternal({ log: blockData.log })] : [],
    // @ts-ignore
    transactions: blockData.transaction
      ? // @ts-ignore
        [syncTransactionToInternal({ transaction: blockData.transaction })]
      : [],
    // @ts-ignore
    transactionReceipts: blockData.transactionReceipt
      ? [
          syncTransactionReceiptToInternal({
            // @ts-ignore
            transactionReceipt: blockData.transactionReceipt,
          }),
        ]
      : [],
    // @ts-ignore
    traces: blockData.trace
      ? // @ts-ignore
        [syncTraceToInternal({ trace: blockData.trace })]
      : [],
    childAddresses: new Map(),
    chainId: 1,
  });
  const events = decodeEvents(
    {} as Common,
    getChain(),
    [eventCallback],
    rawEvents,
  );
  if (events.length !== 1) {
    throw new Error("getSimulatedEvent() failed to construct the event");
  }
  return events[0]!;
};
export const getChain = (params?: {
  finalityBlockCount?: number;
}) => {
  return {
    name: "mainnet",
    id: 1,
    rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}`,
    ws: undefined,
    pollingInterval: 1_000,
    finalityBlockCount: params?.finalityBlockCount ?? 1,
    disableCache: false,
    ethGetLogsBlockRange: undefined,
    viemChain: anvil,
  } satisfies Chain;
};
export function getFreePort(): Promise<number> {
  return new Promise((res) => {
    const srv = createServer();
    srv.listen(0, () => {
      const port = (srv.address() as AddressInfo).port;
      srv.close(() => res(port));
    });
  });
}
export async function waitForIndexedBlock({
  port,
  chainName,
  block,
}: {
  port: number;
  chainName: string;
  block: { number: number };
}) {
  return new Promise((resolve, reject) => {
    const timeout = setTimeout(() => {
      clearInterval(interval);
      reject(new Error("Timed out while waiting for the indexed block."));
    }, 5_000);
    const interval = setInterval(async () => {
      const response = await fetch(`http://localhost:${port}/status`);
      if (response.status === 200) {
        const status = (await response.json()) as Status;
        const sb = status[chainName]?.block;
        if (sb !== undefined && sb.number >= block.number) {
          clearTimeout(timeout);
          clearInterval(interval);
          resolve(undefined);
        }
      }
    }, 20);
  });
}
export function getRejectionValue(func: () => Promise<any>): Promise<any> {
  return func()
    .then(() => {
      throw Error("expected promise to reject");
    })
    .catch((rejection) => {
      return rejection;
    });
}
</file>

<file path="packages/core/src/bin/commands/codegen.ts">
import { runCodegen } from "@/bin/utils/codegen.js";
import { createLogger } from "@/internal/logger.js";
import { MetricsService } from "@/internal/metrics.js";
import { buildOptions } from "@/internal/options.js";
import { createShutdown } from "@/internal/shutdown.js";
import { createTelemetry } from "@/internal/telemetry.js";
import type { CliOptions } from "../ponder.js";
import { createExit } from "../utils/exit.js";
export async function codegen({ cliOptions }: { cliOptions: CliOptions }) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: options.logLevel,
    mode: options.logFormat,
  });
  const [major, minor, _patch] = process.versions.node
    .split(".")
    .map(Number) as [number, number, number];
  if (major < 18 || (major === 18 && minor < 14)) {
    logger.error({
      msg: "Invalid Node.js version",
      version: process.versions.node,
      expected: "18.14",
    });
    process.exit(1);
  }
  const metrics = new MetricsService();
  const shutdown = createShutdown();
  const telemetry = createTelemetry({ options, logger, shutdown });
  const common = {
    options,
    logger,
    metrics,
    telemetry,
    shutdown,
    buildShutdown: shutdown,
    apiShutdown: shutdown,
  };
  const exit = createExit({ common, options });
  telemetry.record({
    name: "lifecycle:session_start",
    properties: { cli_command: "codegen" },
  });
  runCodegen({ common });
  logger.info({ msg: `Wrote file "ponder-env.d.ts"` });
  await exit({ code: 0 });
}
</file>

<file path="packages/core/src/bin/commands/createViews.ts">
import { createBuild } from "@/build/index.js";
import {
  PONDER_CHECKPOINT_TABLE_NAME,
  PONDER_META_TABLE_NAME,
  type PonderApp0,
  type PonderApp1,
  type PonderApp2,
  type PonderApp3,
  type PonderApp4,
  type PonderApp5,
  type PonderApp6,
  SCHEMATA,
  createDatabase,
  getPonderMetaTable,
} from "@/database/index.js";
import {
  getLiveQueryChannelName,
  getLiveQueryNotifyProcedureName,
  getLiveQueryTempTableName,
  getViewsLiveQueryNotifyTriggerName,
} from "@/drizzle/onchain.js";
import { sql } from "@/index.js";
import { createLogger } from "@/internal/logger.js";
import { MetricsService } from "@/internal/metrics.js";
import { buildOptions } from "@/internal/options.js";
import { createShutdown } from "@/internal/shutdown.js";
import { createTelemetry } from "@/internal/telemetry.js";
import { startClock } from "@/utils/timer.js";
import { eq } from "drizzle-orm";
import type { CliOptions } from "../ponder.js";
import { createExit } from "../utils/exit.js";
const emptySchemaBuild = {
  schema: {},
  statements: {
    tables: { sql: [], json: [] },
    views: { sql: [], json: [] },
    enums: { sql: [], json: [] },
    indexes: { sql: [], json: [] },
    sequences: { sql: [], json: [] },
  },
};
export async function createViews({
  cliOptions,
}: {
  cliOptions: CliOptions & {
    schema?: string | undefined;
    viewsSchema?: string | undefined;
  };
}) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: "warn",
    mode: options.logFormat,
  });
  const metrics = new MetricsService();
  const shutdown = createShutdown();
  const telemetry = createTelemetry({ options, logger, shutdown });
  const common = {
    options,
    logger,
    metrics,
    telemetry,
    shutdown,
    buildShutdown: shutdown,
    apiShutdown: shutdown,
  };
  const build = await createBuild({ common, cliOptions });
  const exit = createExit({ common, options });
  if (cliOptions.schema === undefined) {
    logger.error({
      msg: "Required CLI option '--schema' not provided.",
    });
    await exit({ code: 1 });
    return;
  }
  if (cliOptions.viewsSchema === undefined) {
    logger.error({
      msg: "Required CLI option '--views-schema' not provided.",
    });
    await exit({ code: 1 });
    return;
  }
  const configResult = await build.executeConfig();
  if (configResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "config",
      error: configResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const buildResult = build.preCompile(configResult.result);
  if (buildResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "pre-compile",
      error: buildResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const databaseDiagnostic = await build.databaseDiagnostic({
    preBuild: buildResult.result,
  });
  if (databaseDiagnostic.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "diagnostic",
      error: databaseDiagnostic.error,
    });
    await exit({ code: 75 });
    return;
  }
  const database = createDatabase({
    common,
    // Note: `namespace` is not used in this command
    namespace: {
      schema: cliOptions.schema!,
      viewsSchema: undefined,
    },
    preBuild: buildResult.result,
    schemaBuild: emptySchemaBuild,
  });
  const endClock = startClock();
  const schemaExists = await database.adminQB
    .wrap((db) =>
      db
        .select()
        .from(SCHEMATA)
        .where(eq(SCHEMATA.schemaName, cliOptions.schema!)),
    )
    .then((res) => res.length > 0);
  if (schemaExists === false) {
    common.logger.error({
      msg: "Schema does not exist.",
      schema: cliOptions.schema!,
    });
    await exit({ code: 1 });
    return;
  }
  const PONDER_META = getPonderMetaTable(cliOptions.schema!);
  const meta = (await database.adminQB.wrap((db) =>
    db
      .select({ app: PONDER_META.value })
      .from(PONDER_META)
      .where(eq(PONDER_META.key, "app")),
  )) as
    | [
        {
          app:
            | Partial<PonderApp0>
            | PonderApp1
            | PonderApp2
            | PonderApp3
            | PonderApp4
            | PonderApp5
            | PonderApp6;
        },
      ]
    | [];
  if (meta.length === 0) {
    logger.warn({
      msg: "Found 0 Ponder apps",
      schema: cliOptions.schema!,
    });
    await exit({ code: 0 });
    return;
  }
  if ("table_names" in meta[0]!.app === false) {
    logger.warn({
      msg: "Ponder app version not compatible with this command",
      schema: cliOptions.schema!,
    });
    await exit({ code: 0 });
    return;
  }
  await database.adminQB.wrap((db) =>
    db.execute(`CREATE SCHEMA IF NOT EXISTS "${cliOptions.viewsSchema}"`),
  );
  // Note: Drop views before creating new ones because Postgres does not support
  // altering the schema of a view with CREATE OR REPLACE VIEW.
  for (const table of meta[0]!.app.table_names!) {
    await database.adminQB.wrap((db) =>
      db.execute(`DROP VIEW IF EXISTS "${cliOptions.viewsSchema}"."${table}"`),
    );
    await database.adminQB.wrap((db) =>
      db.execute(
        `CREATE VIEW "${cliOptions.viewsSchema}"."${table}" AS SELECT * FROM "${cliOptions.schema!}"."${table}"`,
      ),
    );
  }
  if ("view_names" in meta[0]!.app) {
    for (const view of meta[0]!.app.view_names!) {
      await database.adminQB.wrap((db) =>
        db.execute(`DROP VIEW IF EXISTS "${cliOptions.viewsSchema}"."${view}"`),
      );
      await database.adminQB.wrap((db) =>
        db.execute(
          `CREATE VIEW "${cliOptions.viewsSchema}"."${view}" AS SELECT * FROM "${cliOptions.schema!}"."${view}"`,
        ),
      );
    }
  }
  logger.warn({
    msg: "Created database views",
    schema: cliOptions.viewsSchema,
    count:
      meta[0]!.app.table_names!.length +
      ((meta[0]!.app as { view_names?: string[] }).view_names?.length ?? 0),
    duration: endClock(),
  });
  await database.adminQB.wrap((db) =>
    db.execute(
      sql.raw(
        `DROP VIEW IF EXISTS "${cliOptions.viewsSchema}"."${PONDER_META_TABLE_NAME}"`,
      ),
    ),
  );
  await database.adminQB.wrap((db) =>
    db.execute(
      sql.raw(
        `DROP VIEW IF EXISTS "${cliOptions.viewsSchema}"."${PONDER_CHECKPOINT_TABLE_NAME}"`,
      ),
    ),
  );
  await database.adminQB.wrap((db) =>
    db.execute(
      sql.raw(
        `CREATE VIEW "${cliOptions.viewsSchema}"."${PONDER_META_TABLE_NAME}" AS SELECT * FROM "${cliOptions.schema!}"."${PONDER_META_TABLE_NAME}"`,
      ),
    ),
  );
  await database.adminQB.wrap((db) =>
    db.execute(
      sql.raw(
        `CREATE VIEW "${cliOptions.viewsSchema}"."${PONDER_CHECKPOINT_TABLE_NAME}" AS SELECT * FROM "${cliOptions.schema!}"."${PONDER_CHECKPOINT_TABLE_NAME}"`,
      ),
    ),
  );
  const notifyProcedure = getLiveQueryNotifyProcedureName();
  const channel = getLiveQueryChannelName(cliOptions.viewsSchema);
  await database.adminQB.wrap((db) =>
    db.execute(`
CREATE OR REPLACE FUNCTION "${cliOptions.viewsSchema}".${notifyProcedure}
RETURNS TRIGGER LANGUAGE plpgsql
AS $$
  DECLARE
    table_names json;
    table_exists boolean := false;
  BEGIN
    SELECT EXISTS (
      SELECT 1
      FROM information_schema.tables
      WHERE table_name = '${getLiveQueryTempTableName()}'
      AND table_type = 'LOCAL TEMPORARY'
    ) INTO table_exists;
    IF table_exists THEN
      SELECT json_agg(table_name) INTO table_names
      FROM ${getLiveQueryTempTableName()};
      table_names := COALESCE(table_names, '[]'::json);
      PERFORM pg_notify('${channel}', table_names::text);
    END IF;
    RETURN NULL;
  END;
$$;`),
  );
  const trigger = getViewsLiveQueryNotifyTriggerName(cliOptions.viewsSchema);
  await database.adminQB.wrap((db) =>
    db.execute(
      `
CREATE OR REPLACE TRIGGER "${trigger}"
AFTER INSERT OR UPDATE OR DELETE
ON "${cliOptions.schema!}"."${PONDER_CHECKPOINT_TABLE_NAME}"
FOR EACH STATEMENT
EXECUTE PROCEDURE "${cliOptions.viewsSchema}".${notifyProcedure};`,
    ),
  );
  await exit({ code: 0 });
}
</file>

<file path="packages/core/src/bin/commands/dev.ts">
import fs from "node:fs";
import path from "node:path";
import { createBuild } from "@/build/index.js";
import { type Database, createDatabase } from "@/database/index.js";
import type { Common } from "@/internal/common.js";
import { NonRetryableUserError, ShutdownError } from "@/internal/errors.js";
import { createLogger } from "@/internal/logger.js";
import { MetricsService } from "@/internal/metrics.js";
import { buildOptions } from "@/internal/options.js";
import { createShutdown } from "@/internal/shutdown.js";
import { buildPayload, createTelemetry } from "@/internal/telemetry.js";
import type {
  CrashRecoveryCheckpoint,
  IndexingBuild,
  PreBuild,
} from "@/internal/types.js";
import { runMultichain } from "@/runtime/multichain.js";
import { runOmnichain } from "@/runtime/omnichain.js";
import { createServer } from "@/server/index.js";
import { createUi } from "@/ui/index.js";
import { createQueue } from "@/utils/queue.js";
import type { Result } from "@/utils/result.js";
import { isolatedController } from "../isolatedController.js";
import type { CliOptions } from "../ponder.js";
import { runCodegen } from "../utils/codegen.js";
import { createExit } from "../utils/exit.js";
export async function dev({ cliOptions }: { cliOptions: CliOptions }) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: options.logLevel,
    mode: options.logFormat,
  });
  const [major, minor, _patch] = process.versions.node
    .split(".")
    .map(Number) as [number, number, number];
  if (major < 18 || (major === 18 && minor < 14)) {
    logger.error({
      msg: "Invalid Node.js version",
      version: process.versions.node,
      expected: "18.14",
    });
    process.exit(1);
  }
  if (!fs.existsSync(path.join(options.rootDir, ".env.local"))) {
    logger.warn({
      msg: "Local environment file (.env.local) not found",
    });
  }
  const metrics = new MetricsService();
  const common = {
    options,
    logger,
    metrics,
    shutdown: createShutdown(),
    apiShutdown: createShutdown(),
    buildShutdown: createShutdown(),
  } as Common;
  const telemetry = createTelemetry(common);
  common.telemetry = telemetry;
  if (options.version) {
    metrics.ponder_version_info.set(
      {
        version: options.version.version,
        major: options.version.major,
        minor: options.version.minor,
        patch: options.version.patch,
      },
      1,
    );
  }
  runCodegen({ common });
  const build = await createBuild({ common, cliOptions });
  if (cliOptions.disableUi !== true) {
    createUi({ common });
  }
  const exit = createExit({ common, options });
  let isInitialBuild = true;
  const buildQueue = createQueue({
    initialStart: true,
    concurrency: 1,
    worker: async (result: Result<never> & { kind: "indexing" | "api" }) => {
      if (result.kind === "indexing") {
        await Promise.all([common.shutdown.kill(), common.apiShutdown.kill()]);
        common.shutdown = createShutdown();
        common.apiShutdown = createShutdown();
      } else {
        await common.apiShutdown.kill();
        common.apiShutdown = createShutdown();
      }
      if (result.status === "error") {
        if (isInitialBuild === false) {
          common.logger.error({
            error: result.error,
          });
        }
        // This handles indexing function build failures on hot reload.
        metrics.hasError = true;
        return;
      }
      if (result.kind === "indexing") {
        metrics.resetIndexingMetrics();
        const configResult = await build.executeConfig();
        if (configResult.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "config",
            error: configResult.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: configResult.error,
          });
          return;
        }
        const schemaResult = await build.executeSchema();
        if (schemaResult.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "schema",
            error: schemaResult.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: schemaResult.error,
          });
          return;
        }
        const preCompileResult = build.preCompile(configResult.result);
        if (preCompileResult.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "pre-compile",
            error: preCompileResult.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: preCompileResult.error,
          });
          return;
        }
        const databaseDiagnostic = await build.databaseDiagnostic({
          preBuild: preCompileResult.result,
        });
        if (databaseDiagnostic.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "diagnostic",
            error: databaseDiagnostic.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: databaseDiagnostic.error,
          });
          return;
        }
        const compileSchemaResult = build.compileSchema({
          ...schemaResult.result,
          preBuild: preCompileResult.result,
        });
        if (compileSchemaResult.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "schema",
            error: compileSchemaResult.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: compileSchemaResult.error,
          });
          return;
        }
        const configBuildResult = build.compileConfig({
          configResult: configResult.result,
        });
        if (configBuildResult.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "config",
            error: configBuildResult.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: configBuildResult.error,
          });
          return;
        }
        preBuild = preCompileResult.result;
        configBuild = configBuildResult.result;
        const rpcDiagnosticResult = await build.rpcDiagnostic({
          configBuild: configBuildResult.result,
        });
        if (rpcDiagnosticResult.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "diagnostic",
            error: rpcDiagnosticResult.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: rpcDiagnosticResult.error,
          });
          return;
        }
        const indexingResult = await build.executeIndexingFunctions();
        if (indexingResult.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "indexing",
            error: indexingResult.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: indexingResult.error,
          });
          return;
        }
        const indexingBuildResult = await build.compileIndexing({
          configResult: configResult.result,
          schemaResult: schemaResult.result,
          indexingResult: indexingResult.result,
          configBuild: configBuildResult.result,
        });
        if (indexingBuildResult.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "indexing",
            error: indexingBuildResult.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: indexingBuildResult.error,
          });
          return;
        }
        database = createDatabase({
          common,
          namespace: { schema, viewsSchema: undefined },
          preBuild: preCompileResult.result,
          schemaBuild: compileSchemaResult.result,
        });
        crashRecoveryCheckpoint = await database.migrate({
          buildId: indexingBuildResult.result.buildId,
          chains: indexingBuildResult.result.chains,
          finalizedBlocks: indexingBuildResult.result.finalizedBlocks,
        });
        await database.migrateSync();
        const apiResult = await build.executeApi({
          preBuild: preCompileResult.result,
          configBuild: configBuildResult.result,
          database,
        });
        if (apiResult.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "api",
            error: apiResult.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: apiResult.error,
          });
          return;
        }
        const apiBuildResult = await build.compileApi({
          apiResult: apiResult.result,
        });
        if (apiBuildResult.status === "error") {
          common.logger.error({
            msg: "Build failed",
            stage: "api",
            error: apiBuildResult.error,
          });
          buildQueue.add({
            status: "error",
            kind: "indexing",
            error: apiBuildResult.error,
          });
          return;
        }
        if (isInitialBuild) {
          isInitialBuild = false;
          telemetry.record({
            name: "lifecycle:session_start",
            properties: {
              cli_command: "dev",
              ...buildPayload({
                preBuild: preCompileResult.result,
                schemaBuild: compileSchemaResult.result,
                indexingBuild: indexingBuildResult.result,
              }),
            },
          });
        }
        metrics.resetApiMetrics();
        metrics.ponder_settings_info.set(
          {
            ordering: preCompileResult.result.ordering,
            database: preCompileResult.result.databaseConfig.kind,
            command: cliOptions.command,
          },
          1,
        );
        createServer({ common, database, apiBuild: apiBuildResult.result });
        metrics.initializeIndexingMetrics({
          indexingBuild: indexingBuildResult.result,
          schemaBuild: compileSchemaResult.result,
        });
        switch (preCompileResult.result.ordering) {
          case "omnichain":
            runOmnichain({
              common,
              database,
              preBuild: preCompileResult.result,
              namespaceBuild: { schema, viewsSchema: undefined },
              schemaBuild: compileSchemaResult.result,
              indexingBuild: indexingBuildResult.result,
              crashRecoveryCheckpoint,
            });
            break;
          case "multichain":
            runMultichain({
              common,
              database,
              preBuild: preCompileResult.result,
              namespaceBuild: { schema, viewsSchema: undefined },
              schemaBuild: compileSchemaResult.result,
              indexingBuild: indexingBuildResult.result,
              crashRecoveryCheckpoint,
            });
            break;
          case "experimental_isolated": {
            isolatedController({
              common,
              database,
              preBuild: preCompileResult.result,
              namespaceBuild: { schema, viewsSchema: undefined },
              schemaBuild: compileSchemaResult.result,
              indexingBuild: indexingBuildResult.result,
              crashRecoveryCheckpoint,
            });
            break;
          }
        }
      } else {
        metrics.resetApiMetrics();
        const apiResult = await build.executeApi({
          preBuild: preBuild!,
          configBuild: configBuild!,
          database: database!,
        });
        if (apiResult.status === "error") {
          buildQueue.add({
            status: "error",
            kind: "api",
            error: apiResult.error,
          });
          return;
        }
        const buildResult = await build.compileApi({
          apiResult: apiResult.result,
        });
        if (buildResult.status === "error") {
          buildQueue.add({
            status: "error",
            kind: "api",
            error: buildResult.error,
          });
          return;
        }
        const apiBuild = buildResult.result;
        createServer({ common, database: database!, apiBuild });
      }
    },
  });
  let preBuild: PreBuild | undefined;
  let configBuild: Pick<IndexingBuild, "chains" | "rpcs"> | undefined;
  let database: Database | undefined;
  let crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  const schema = cliOptions.schema ?? process.env.DATABASE_SCHEMA ?? "public";
  globalThis.PONDER_NAMESPACE_BUILD = { schema, viewsSchema: undefined };
  process.on("uncaughtException", (error: Error) => {
    if (error instanceof ShutdownError) return;
    if (error instanceof NonRetryableUserError) {
      common.logger.error({
        msg: "uncaughtException",
        error,
      });
      buildQueue.clear();
      buildQueue.add({ status: "error", kind: "indexing", error });
    } else {
      common.logger.error({
        msg: "uncaughtException",
        error,
      });
      exit({ code: 75 });
    }
  });
  process.on("unhandledRejection", (error: Error) => {
    if (error instanceof ShutdownError) return;
    if (error instanceof NonRetryableUserError) {
      common.logger.error({
        msg: "unhandledRejection",
        error,
      });
      buildQueue.clear();
      buildQueue.add({ status: "error", kind: "indexing", error });
    } else {
      common.logger.error({
        msg: "unhandledRejection",
        error,
      });
      exit({ code: 75 });
    }
  });
  build.startDev({
    onReload: (kind) => {
      buildQueue.clear();
      buildQueue.add({ status: "success", kind });
    },
  });
  buildQueue.add({ status: "success", kind: "indexing" });
  return () =>
    Promise.all([
      common.shutdown.kill(),
      common.apiShutdown.kill(),
      common.buildShutdown.kill(),
    ]);
}
</file>

<file path="packages/core/src/bin/commands/list.ts">
import { createBuild } from "@/build/index.js";
import {
  PONDER_META_TABLE_NAME,
  type PonderApp0,
  type PonderApp1,
  type PonderApp2,
  type PonderApp3,
  type PonderApp4,
  type PonderApp5,
  type PonderApp6,
  VIEWS,
  createDatabase,
  getPonderMetaTable,
} from "@/database/index.js";
import { TABLES } from "@/database/index.js";
import { createLogger } from "@/internal/logger.js";
import { MetricsService } from "@/internal/metrics.js";
import { buildOptions } from "@/internal/options.js";
import { createShutdown } from "@/internal/shutdown.js";
import { createTelemetry } from "@/internal/telemetry.js";
import { buildTable } from "@/ui/app.js";
import { formatEta } from "@/utils/format.js";
import { eq, sql } from "drizzle-orm";
import { unionAll } from "drizzle-orm/pg-core";
import type { CliOptions } from "../ponder.js";
import { createExit } from "../utils/exit.js";
const emptySchemaBuild = {
  schema: {},
  statements: {
    tables: { sql: [], json: [] },
    views: { sql: [], json: [] },
    enums: { sql: [], json: [] },
    indexes: { sql: [], json: [] },
    sequences: { sql: [], json: [] },
  },
};
export async function list({ cliOptions }: { cliOptions: CliOptions }) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: "warn",
    mode: options.logFormat,
  });
  const metrics = new MetricsService();
  const shutdown = createShutdown();
  const telemetry = createTelemetry({ options, logger, shutdown });
  const common = {
    options,
    logger,
    metrics,
    telemetry,
    shutdown,
    buildShutdown: shutdown,
    apiShutdown: shutdown,
  };
  const build = await createBuild({ common, cliOptions });
  const exit = createExit({ common, options });
  const configResult = await build.executeConfig();
  if (configResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "config",
      error: configResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const buildResult = build.preCompile(configResult.result);
  if (buildResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "pre-compile",
      error: buildResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const databaseDiagnostic = await build.databaseDiagnostic({
    preBuild: buildResult.result,
  });
  if (databaseDiagnostic.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "diagnostic",
      error: databaseDiagnostic.error,
    });
    await exit({ code: 75 });
    return;
  }
  const database = createDatabase({
    common,
    // Note: `namespace` is not used in this command
    namespace: { schema: "public", viewsSchema: undefined },
    preBuild: buildResult.result,
    schemaBuild: emptySchemaBuild,
  });
  const ponderSchemas = await database.adminQB.wrap((db) =>
    db
      .select({ schema: TABLES.table_schema })
      .from(TABLES)
      .where(eq(TABLES.table_name, PONDER_META_TABLE_NAME)),
  );
  const ponderViewSchemas = await database.adminQB.wrap((db) =>
    db
      .select({ schema: VIEWS.table_schema })
      .from(VIEWS)
      .where(eq(VIEWS.table_name, PONDER_META_TABLE_NAME)),
  );
  const queries = ponderSchemas.map((row) =>
    database.adminQB.raw
      .select({
        value: getPonderMetaTable(row.schema).value,
        schema: sql<string>`${row.schema}`.as("schema"),
      })
      .from(getPonderMetaTable(row.schema))
      .where(eq(getPonderMetaTable(row.schema).key, "app")),
  );
  if (queries.length === 0) {
    logger.warn({
      msg: "Found 0 'ponder start' apps",
    });
    await exit({ code: 0 });
    return;
  }
  let result: {
    value:
      | Partial<PonderApp0>
      | PonderApp1
      | PonderApp2
      | PonderApp3
      | PonderApp4
      | PonderApp5
      | PonderApp6;
    schema: string;
  }[];
  if (queries.length === 1) {
    result = await database.adminQB.wrap(() => queries[0]!);
  } else {
    // @ts-ignore
    result = await database.adminQB.wrap(() => unionAll(...queries));
  }
  const columns = [
    { title: "Schema", key: "table_schema", align: "left" },
    { title: "Active", key: "active", align: "right" },
    { title: "Last active", key: "last_active", align: "right" },
    { title: "Relation count", key: "relation_count", align: "right" },
    { title: "Is view", key: "is_view", align: "right" },
  ];
  // "start" apps with metadata version >=2
  const filteredResults = result.filter(
    (
      row,
    ): row is {
      value: PonderApp2 | PonderApp3 | PonderApp4 | PonderApp5 | PonderApp6;
      schema: string;
    } => "is_dev" in row.value && row.value.is_dev === 0,
  );
  const rows = filteredResults.map((row) => ({
    table_schema: row.schema,
    active:
      row.value.is_locked === 1 &&
      row.value.heartbeat_at + common.options.databaseHeartbeatTimeout >
        Date.now()
        ? "yes"
        : "no",
    last_active:
      row.value.is_locked === 1
        ? "---"
        : `${formatEta(Date.now() - row.value.heartbeat_at)} ago`,
    relation_count:
      (row.value.table_names?.length ?? 0) +
      ((row.value as { view_names?: string[] }).view_names?.length ?? 0),
    is_view: ponderViewSchemas.some((schema) => schema.schema === row.schema)
      ? "yes"
      : "no",
  }));
  if (rows.length === 0) {
    logger.warn({ msg: "Found 0 'ponder start' apps" });
    await exit({ code: 0 });
    return;
  }
  const lines = buildTable(rows, columns);
  const text = [...lines, ""].join("\n");
  console.log(text);
  await exit({ code: 0 });
}
</file>

<file path="packages/core/src/bin/commands/prune.ts">
import { createBuild } from "@/build/index.js";
import {
  PONDER_CHECKPOINT_TABLE_NAME,
  PONDER_META_TABLE_NAME,
  PONDER_STATUS_TABLE_NAME,
  type PonderApp0,
  type PonderApp1,
  type PonderApp2,
  type PonderApp3,
  type PonderApp4,
  type PonderApp5,
  type PonderApp6,
  VIEWS,
  createDatabase,
  getPonderMetaTable,
} from "@/database/index.js";
import { TABLES } from "@/database/index.js";
import {
  getLiveQueryNotifyProcedureName,
  getLiveQueryProcedureName,
  getReorgProcedureName,
  getReorgTableName,
} from "@/drizzle/onchain.js";
import { createLogger } from "@/internal/logger.js";
import { MetricsService } from "@/internal/metrics.js";
import { buildOptions } from "@/internal/options.js";
import { createShutdown } from "@/internal/shutdown.js";
import { createTelemetry } from "@/internal/telemetry.js";
import { startClock } from "@/utils/timer.js";
import { count, eq, inArray, sql } from "drizzle-orm";
import { unionAll } from "drizzle-orm/pg-core";
import type { CliOptions } from "../ponder.js";
import { createExit } from "../utils/exit.js";
const emptySchemaBuild = {
  schema: {},
  statements: {
    tables: { sql: [], json: [] },
    views: { sql: [], json: [] },
    enums: { sql: [], json: [] },
    indexes: { sql: [], json: [] },
    sequences: { sql: [], json: [] },
  },
};
export async function prune({ cliOptions }: { cliOptions: CliOptions }) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: "warn",
    mode: options.logFormat,
  });
  const metrics = new MetricsService();
  const shutdown = createShutdown();
  const telemetry = createTelemetry({ options, logger, shutdown });
  const common = {
    options,
    logger,
    metrics,
    telemetry,
    shutdown,
    buildShutdown: shutdown,
    apiShutdown: shutdown,
  };
  const build = await createBuild({ common, cliOptions });
  const exit = createExit({ common, options });
  const configResult = await build.executeConfig();
  if (configResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "config",
      error: configResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const buildResult = build.preCompile(configResult.result);
  if (buildResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "pre-compile",
      error: buildResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const databaseDiagnostic = await build.databaseDiagnostic({
    preBuild: buildResult.result,
  });
  if (databaseDiagnostic.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "diagnostic",
      error: databaseDiagnostic.error,
    });
    await exit({ code: 75 });
    return;
  }
  const database = createDatabase({
    common,
    // Note: `namespace` is not used in this command
    namespace: { schema: "public", viewsSchema: undefined },
    preBuild: buildResult.result,
    schemaBuild: emptySchemaBuild,
  });
  const ponderSchemas = await database.adminQB.wrap((db) =>
    db
      .select({ schema: TABLES.table_schema, tableCount: count() })
      .from(TABLES)
      .where(
        inArray(
          TABLES.table_schema,
          database.adminQB.raw
            .select({ schema: TABLES.table_schema })
            .from(TABLES)
            .where(eq(TABLES.table_name, PONDER_META_TABLE_NAME)),
        ),
      )
      .groupBy(TABLES.table_schema),
  );
  const ponderViewSchemas = await database.adminQB.wrap((db) =>
    db
      .select({ schema: VIEWS.table_schema })
      .from(VIEWS)
      .where(eq(VIEWS.table_name, PONDER_META_TABLE_NAME)),
  );
  const queries = ponderSchemas.map((row) =>
    database.adminQB.raw
      .select({
        value: getPonderMetaTable(row.schema).value,
        schema: sql<string>`${row.schema}`.as("schema"),
      })
      .from(getPonderMetaTable(row.schema))
      .where(eq(getPonderMetaTable(row.schema).key, "app")),
  );
  if (queries.length === 0) {
    logger.warn({
      msg: "Found 0 inactive Ponder apps",
    });
    await exit({ code: 0 });
    return;
  }
  let result: {
    value:
      | Partial<PonderApp0>
      | PonderApp1
      | PonderApp2
      | PonderApp3
      | PonderApp4
      | PonderApp5
      | PonderApp6;
    schema: string;
  }[];
  if (queries.length === 1) {
    result = await database.adminQB.wrap(() => queries[0]!);
  } else {
    // @ts-ignore
    result = await database.adminQB.wrap(() => unionAll(...queries));
  }
  const tablesToDrop: string[] = [];
  const viewsToDrop: string[] = [];
  const schemasToDrop: string[] = [];
  const functionsToDrop: string[] = [];
  // "start" apps with metadata version >=2
  const filteredResults = result.filter(
    (
      row,
    ): row is {
      value: PonderApp2 | PonderApp3 | PonderApp4 | PonderApp5 | PonderApp6;
      schema: string;
    } => "is_dev" in row.value && row.value.is_dev === 0,
  );
  for (const { value, schema } of filteredResults) {
    if (value.is_dev === 1) continue;
    if (
      value.is_locked === 1 &&
      value.heartbeat_at + common.options.databaseHeartbeatTimeout > Date.now()
    ) {
      continue;
    }
    if (ponderViewSchemas.some((vs) => vs.schema === schema)) {
      for (const table of value.table_names) {
        viewsToDrop.push(`"${schema}"."${table}"`);
      }
      if ("view_names" in value) {
        for (const view of value.view_names) {
          viewsToDrop.push(`"${schema}"."${view}"`);
        }
      }
      viewsToDrop.push(`"${schema}"."${PONDER_META_TABLE_NAME}"`);
      if (value.version === "2") {
        viewsToDrop.push(`"${schema}"."${PONDER_CHECKPOINT_TABLE_NAME}"`);
      } else {
        viewsToDrop.push(`"${schema}"."${PONDER_STATUS_TABLE_NAME}"`);
      }
      const tableCount = ponderSchemas.find(
        (s) => s.schema === schema,
      )!.tableCount;
      if (schema !== "public" && tableCount <= 2 + value.table_names.length) {
        schemasToDrop.push(`"${schema}"`);
      }
    } else {
      for (const table of value.table_names) {
        tablesToDrop.push(`"${schema}"."${table}"`);
        tablesToDrop.push(`"${schema}"."${getReorgTableName(table)}"`);
        functionsToDrop.push(`"${schema}"."${getReorgProcedureName(table)}"`);
      }
      functionsToDrop.push(`"${schema}".${getLiveQueryProcedureName()}`);
      functionsToDrop.push(`"${schema}".${getLiveQueryNotifyProcedureName()}`);
      if ("view_names" in value) {
        for (const view of value.view_names) {
          viewsToDrop.push(`"${schema}"."${view}"`);
        }
      }
      tablesToDrop.push(`"${schema}"."${PONDER_META_TABLE_NAME}"`);
      if (value.version === "2") {
        tablesToDrop.push(`"${schema}"."${PONDER_CHECKPOINT_TABLE_NAME}"`);
      } else {
        tablesToDrop.push(`"${schema}"."${PONDER_STATUS_TABLE_NAME}"`);
      }
      const tableCount = ponderSchemas.find(
        (s) => s.schema === schema,
      )!.tableCount;
      if (
        schema !== "public" &&
        tableCount <= 2 + value.table_names.length * 2
      ) {
        schemasToDrop.push(`"${schema}"`);
      }
    }
  }
  if (tablesToDrop.length === 0 && viewsToDrop.length === 0) {
    logger.warn({
      msg: "Found 0 inactive Ponder apps",
    });
    await exit({ code: 0 });
    return;
  }
  let endClock = startClock();
  if (tablesToDrop.length > 0) {
    await database.adminQB.wrap((db) =>
      db.execute(`DROP TABLE IF EXISTS ${tablesToDrop.join(", ")} CASCADE`),
    );
    logger.warn({
      msg: "Dropped database tables",
      count: tablesToDrop.length,
      duration: endClock(),
    });
  }
  endClock = startClock();
  if (viewsToDrop.length > 0) {
    await database.adminQB.wrap((db) =>
      db.execute(`DROP VIEW IF EXISTS ${viewsToDrop.join(", ")} CASCADE`),
    );
    logger.warn({
      msg: "Dropped database views",
      count: viewsToDrop.length,
      duration: endClock(),
    });
  }
  endClock = startClock();
  if (functionsToDrop.length > 0) {
    await database.adminQB.wrap((db) =>
      db.execute(
        `DROP FUNCTION IF EXISTS ${functionsToDrop.join(", ")} CASCADE`,
      ),
    );
    logger.warn({
      msg: "Dropped database functions",
      count: functionsToDrop.length,
      duration: endClock(),
    });
  }
  endClock = startClock();
  if (schemasToDrop.length > 0) {
    await database.adminQB.wrap((db) =>
      db.execute(`DROP SCHEMA IF EXISTS ${schemasToDrop.join(", ")} CASCADE`),
    );
    logger.warn({
      msg: "Dropped database schemas",
      count: schemasToDrop.length,
      duration: endClock(),
    });
  }
  await exit({ code: 0 });
}
</file>

<file path="packages/core/src/bin/commands/serve.ts">
import { createBuild } from "@/build/index.js";
import { SCHEMATA, createDatabase } from "@/database/index.js";
import { createLogger } from "@/internal/logger.js";
import { MetricsService } from "@/internal/metrics.js";
import { buildOptions } from "@/internal/options.js";
import { createShutdown } from "@/internal/shutdown.js";
import { buildPayload, createTelemetry } from "@/internal/telemetry.js";
import { createServer } from "@/server/index.js";
import { eq } from "drizzle-orm";
import type { CliOptions } from "../ponder.js";
import { createExit } from "../utils/exit.js";
export async function serve({ cliOptions }: { cliOptions: CliOptions }) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: options.logLevel,
    mode: options.logFormat,
  });
  const [major, minor, _patch] = process.versions.node
    .split(".")
    .map(Number) as [number, number, number];
  if (major < 18 || (major === 18 && minor < 14)) {
    logger.error({
      msg: "Invalid Node.js version",
      version: process.versions.node,
      expected: "18.14",
    });
    process.exit(1);
  }
  const metrics = new MetricsService();
  const shutdown = createShutdown();
  const telemetry = createTelemetry({ options, logger, shutdown });
  const common = {
    options,
    logger,
    metrics,
    telemetry,
    shutdown,
    buildShutdown: shutdown,
    apiShutdown: shutdown,
  };
  if (options.version) {
    metrics.ponder_version_info.set(
      {
        version: options.version.version,
        major: options.version.major,
        minor: options.version.minor,
        patch: options.version.patch,
      },
      1,
    );
  }
  const build = await createBuild({ common, cliOptions });
  const exit = createExit({ common, options });
  const namespaceResult = build.namespaceCompile();
  if (namespaceResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "namespace",
      error: namespaceResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const configResult = await build.executeConfig();
  if (configResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "config",
      error: configResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const schemaResult = await build.executeSchema();
  if (schemaResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "schema",
      error: schemaResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const preCompileResult = build.preCompile(configResult.result);
  if (preCompileResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "pre-compile",
      error: preCompileResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  if (preCompileResult.result.databaseConfig.kind === "pglite") {
    common.logger.error({
      msg: "The 'ponder serve' command does not support PGlite",
    });
    await exit({ code: 1 });
    return;
  }
  const databaseDiagnostic = await build.databaseDiagnostic({
    preBuild: preCompileResult.result,
  });
  if (databaseDiagnostic.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "diagnostic",
      error: databaseDiagnostic.error,
    });
    await exit({ code: 75 });
    return;
  }
  const compileSchemaResult = build.compileSchema({
    ...schemaResult.result,
    preBuild: preCompileResult.result,
  });
  if (compileSchemaResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "schema",
      error: compileSchemaResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const configBuildResult = build.compileConfig({
    configResult: configResult.result,
  });
  if (configBuildResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "indexing",
      error: configBuildResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  // Note: RPC diagnostic is skipped
  const database = createDatabase({
    common,
    namespace: namespaceResult.result,
    preBuild: preCompileResult.result,
    schemaBuild: compileSchemaResult.result,
  });
  const schemaExists = await database.adminQB
    .wrap((db) =>
      db
        .select()
        .from(SCHEMATA)
        .where(eq(SCHEMATA.schemaName, namespaceResult.result.schema)),
    )
    .then((res) => res.length > 0);
  if (schemaExists === false) {
    common.logger.error({
      msg: "Schema does not exist.",
      schema: namespaceResult.result.schema,
    });
    await exit({ code: 1 });
    return;
  }
  const apiResult = await build.executeApi({
    preBuild: preCompileResult.result,
    configBuild: configBuildResult.result,
    database,
  });
  if (apiResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "api",
      error: apiResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const apiBuildResult = await build.compileApi({
    apiResult: apiResult.result,
  });
  if (apiBuildResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "api",
      error: apiBuildResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  telemetry.record({
    name: "lifecycle:session_start",
    properties: {
      cli_command: "serve",
      ...buildPayload({
        preBuild: preCompileResult.result,
        schemaBuild: compileSchemaResult.result,
      }),
    },
  });
  metrics.ponder_settings_info.set(
    {
      database: preCompileResult.result.databaseConfig.kind,
      command: cliOptions.command,
    },
    1,
  );
  createServer({ common, database, apiBuild: apiBuildResult.result });
  return shutdown.kill;
}
</file>

<file path="packages/core/src/bin/commands/start.ts">
import { runCodegen } from "@/bin/utils/codegen.js";
import { createBuild } from "@/build/index.js";
import { type Database, createDatabase } from "@/database/index.js";
import type { Common } from "@/internal/common.js";
import { createLogger } from "@/internal/logger.js";
import { MetricsService } from "@/internal/metrics.js";
import { buildOptions } from "@/internal/options.js";
import { createShutdown } from "@/internal/shutdown.js";
import { buildPayload, createTelemetry } from "@/internal/telemetry.js";
import type {
  ApiBuild,
  CrashRecoveryCheckpoint,
  IndexingBuild,
  NamespaceBuild,
  PreBuild,
  SchemaBuild,
} from "@/internal/types.js";
import { runMultichain } from "@/runtime/multichain.js";
import { runOmnichain } from "@/runtime/omnichain.js";
import { createServer } from "@/server/index.js";
import { isolatedController } from "../isolatedController.js";
import type { CliOptions } from "../ponder.js";
import { createExit } from "../utils/exit.js";
export type PonderApp = {
  common: Common;
  preBuild: PreBuild;
  namespaceBuild: NamespaceBuild;
  schemaBuild: SchemaBuild;
  indexingBuild: IndexingBuild;
  apiBuild: ApiBuild;
  crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  database: Database;
};
export async function start({
  cliOptions,
  onBuild,
}: {
  cliOptions: CliOptions;
  onBuild?: (app: PonderApp) => Promise<PonderApp>;
}) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: options.logLevel,
    mode: options.logFormat,
  });
  const [major, minor, _patch] = process.versions.node
    .split(".")
    .map(Number) as [number, number, number];
  if (major < 18 || (major === 18 && minor < 14)) {
    logger.error({
      msg: "Invalid Node.js version",
      version: process.versions.node,
      expected: "18.14",
    });
    process.exit(1);
  }
  const metrics = new MetricsService();
  const shutdown = createShutdown();
  const telemetry = createTelemetry({ options, logger, shutdown });
  const common = {
    options,
    logger,
    metrics,
    telemetry,
    shutdown,
    buildShutdown: shutdown,
    apiShutdown: shutdown,
  };
  const exit = createExit({ common, options });
  if (options.version) {
    metrics.ponder_version_info.set(
      {
        version: options.version.version,
        major: options.version.major,
        minor: options.version.minor,
        patch: options.version.patch,
      },
      1,
    );
  }
  runCodegen({ common });
  const build = await createBuild({ common, cliOptions });
  // biome-ignore lint/style/useConst: <explanation>
  let database: Database | undefined;
  const namespaceResult = build.namespaceCompile();
  if (namespaceResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "namespace",
      error: namespaceResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const configResult = await build.executeConfig();
  if (configResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "config",
      error: configResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const schemaResult = await build.executeSchema();
  if (schemaResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "schema",
      error: schemaResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const preCompileResult = build.preCompile(configResult.result);
  if (preCompileResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "pre-compile",
      error: preCompileResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const databaseDiagnostic = await build.databaseDiagnostic({
    preBuild: preCompileResult.result,
  });
  if (databaseDiagnostic.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "diagnostic",
      error: databaseDiagnostic.error,
    });
    await exit({ code: 75 });
    return;
  }
  const compileSchemaResult = build.compileSchema({
    ...schemaResult.result,
    preBuild: preCompileResult.result,
  });
  if (compileSchemaResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "schema",
      error: compileSchemaResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const configBuildResult = build.compileConfig({
    configResult: configResult.result,
  });
  if (configBuildResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "config",
      error: configBuildResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const rpcDiagnosticResult = await build.rpcDiagnostic({
    configBuild: configBuildResult.result,
  });
  if (rpcDiagnosticResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "diagnostic",
      error: rpcDiagnosticResult.error,
    });
    await exit({ code: 75 });
    return;
  }
  const indexingResult = await build.executeIndexingFunctions();
  if (indexingResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "indexing",
      error: indexingResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const indexingBuildResult = await build.compileIndexing({
    configResult: configResult.result,
    schemaResult: schemaResult.result,
    indexingResult: indexingResult.result,
    configBuild: configBuildResult.result,
  });
  if (indexingBuildResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "indexing",
      error: indexingBuildResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  database = createDatabase({
    common,
    namespace: namespaceResult.result,
    preBuild: preCompileResult.result,
    schemaBuild: compileSchemaResult.result,
  });
  const crashRecoveryCheckpoint = await database.migrate({
    buildId: indexingBuildResult.result.buildId,
    chains: indexingBuildResult.result.chains,
    finalizedBlocks: indexingBuildResult.result.finalizedBlocks,
  });
  await database.migrateSync();
  const apiResult = await build.executeApi({
    preBuild: preCompileResult.result,
    configBuild: configBuildResult.result,
    database,
  });
  if (apiResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "api",
      error: apiResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  const apiBuildResult = await build.compileApi({
    apiResult: apiResult.result,
  });
  if (apiBuildResult.status === "error") {
    common.logger.error({
      msg: "Build failed",
      stage: "api",
      error: apiBuildResult.error,
    });
    await exit({ code: 1 });
    return;
  }
  telemetry.record({
    name: "lifecycle:session_start",
    properties: {
      cli_command: "start",
      ...buildPayload({
        preBuild: preCompileResult.result,
        schemaBuild: compileSchemaResult.result,
        indexingBuild: indexingBuildResult.result,
      }),
    },
  });
  metrics.ponder_settings_info.set(
    {
      ordering: preCompileResult.result.ordering,
      database: preCompileResult.result.databaseConfig.kind,
      command: cliOptions.command,
    },
    1,
  );
  let app: PonderApp = {
    common,
    preBuild: preCompileResult.result,
    namespaceBuild: namespaceResult.result,
    schemaBuild: compileSchemaResult.result,
    indexingBuild: indexingBuildResult.result,
    apiBuild: apiBuildResult.result,
    crashRecoveryCheckpoint,
    database,
  };
  if (onBuild) {
    app = await onBuild(app);
  }
  metrics.initializeIndexingMetrics(app);
  switch (preCompileResult.result.ordering) {
    case "omnichain":
      runOmnichain(app);
      break;
    case "multichain":
      runMultichain(app);
      break;
    case "experimental_isolated": {
      isolatedController(app);
      break;
    }
  }
  createServer(app);
  return shutdown.kill;
}
</file>

<file path="packages/core/src/bin/utils/codegen.ts">
import { writeFileSync } from "node:fs";
import path from "node:path";
import type { Common } from "@/internal/common.js";
export const ponderEnv = `/// <reference types="ponder/virtual" />
declare module "ponder:internal" {
  const config: typeof import("./ponder.config.ts");
  const schema: typeof import("./ponder.schema.ts");
}
declare module "ponder:schema" {
  export * from "./ponder.schema.ts";
}
// This file enables type checking and editor autocomplete for this Ponder project.
// After upgrading, you may find that changes have been made to this file.
// If this happens, please commit the changes. Do not manually edit this file.
// See https://ponder.sh/docs/requirements#typescript for more information.
`;
export function runCodegen({ common }: { common: Common }) {
  writeFileSync(
    path.join(common.options.rootDir, "ponder-env.d.ts"),
    ponderEnv,
    "utf8",
  );
  common.logger.debug({
    msg: `Wrote file "ponder-env.d.ts"`,
  });
}
</file>

<file path="packages/core/src/bin/utils/exit.ts">
import os from "node:os";
import readline from "node:readline";
import type { Common } from "@/internal/common.js";
import { NonRetryableUserError, ShutdownError } from "@/internal/errors.js";
import type { Options } from "@/internal/options.js";
const SHUTDOWN_GRACE_PERIOD_MS = 5_000;
/** Sets up shutdown handlers for the process. Accepts additional cleanup logic to run. */
export const createExit = ({
  common,
  options,
}: {
  common: Pick<
    Common,
    "logger" | "telemetry" | "shutdown" | "buildShutdown" | "apiShutdown"
  >;
  options: Options;
}) => {
  let isShuttingDown = false;
  const exit = async ({ code }: { code: 0 | 1 | 75 }) => {
    if (isShuttingDown) return;
    isShuttingDown = true;
    const timeout = setTimeout(async () => {
      common.logger.error({
        msg: "Failed to shutdown within 5 seconds",
        code,
      });
      process.exit(code);
    }, SHUTDOWN_GRACE_PERIOD_MS);
    common.logger.warn({
      msg: "Started shutdown sequence",
    });
    common.telemetry.record({
      name: "lifecycle:session_end",
      properties: { duration_seconds: process.uptime() },
    });
    await Promise.all([
      common.shutdown.kill(),
      common.apiShutdown.kill(),
      common.buildShutdown.kill(),
    ]);
    clearTimeout(timeout);
    if (process.stdin.isTTY) {
      process.stdin.setRawMode(false);
      process.stdin.pause();
    }
    process.exit(code);
  };
  if (os.platform() === "win32") {
    const readlineInterface = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
    });
    readlineInterface.on("SIGINT", () => {
      if (isShuttingDown) return;
      common.logger.warn({ msg: "Received SIGINT" });
      exit({ code: 0 });
    });
  }
  process.on("SIGINT", () => {
    if (isShuttingDown) return;
    common.logger.warn({ msg: "Received SIGINT" });
    exit({ code: 0 });
  });
  process.on("SIGTERM", () => {
    if (isShuttingDown) return;
    common.logger.warn({ msg: "Received SIGTERM" });
    exit({ code: 0 });
  });
  process.on("SIGQUIT", () => {
    if (isShuttingDown) return;
    common.logger.warn({ msg: "Received SIGQUIT" });
    exit({ code: 0 });
  });
  if (options.command !== "dev") {
    process.on("uncaughtException", (error: Error) => {
      if (error instanceof ShutdownError) return;
      common.logger.error({
        msg: "uncaughtException",
        error,
      });
      if (error instanceof NonRetryableUserError) {
        exit({ code: 1 });
      } else {
        exit({ code: 75 });
      }
    });
    process.on("unhandledRejection", (error: Error) => {
      if (error instanceof ShutdownError) return;
      common.logger.error({
        msg: "unhandledRejection",
        error,
      });
      if (error instanceof NonRetryableUserError) {
        exit({ code: 1 });
      } else {
        exit({ code: 75 });
      }
    });
  }
  return exit;
};
</file>

<file path="packages/core/src/bin/isolatedController.ts">
import path from "node:path";
import url from "node:url";
import v8 from "node:v8";
import { Worker } from "node:worker_threads";
import { createIndexes, createViews } from "@/database/actions.js";
import { type Database, getPonderMetaTable } from "@/database/index.js";
import type { Common } from "@/internal/common.js";
import {
  NonRetryableUserError,
  nonRetryableUserErrorNames,
} from "@/internal/errors.js";
import { AggregateMetricsService, getAppProgress } from "@/internal/metrics.js";
import type {
  CrashRecoveryCheckpoint,
  IndexingBuild,
  NamespaceBuild,
  PreBuild,
  SchemaBuild,
} from "@/internal/types.js";
import { runIsolated } from "@/runtime/isolated.js";
import { chunk } from "@/utils/chunk.js";
import { formatEta, formatPercentage } from "@/utils/format.js";
import { startClock } from "@/utils/timer.js";
import { isTable, isView, sql } from "drizzle-orm";
import type { isolatedWorker } from "./isolatedWorker.js";
const __filename = url.fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
type WorkerState = "backfill" | "live" | "complete";
export async function isolatedController({
  common,
  preBuild,
  namespaceBuild,
  schemaBuild,
  indexingBuild,
  crashRecoveryCheckpoint,
  database,
}: {
  common: Common;
  preBuild: PreBuild;
  namespaceBuild: NamespaceBuild;
  schemaBuild: SchemaBuild;
  indexingBuild: IndexingBuild;
  crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  database: Database;
}) {
  const backfillEndClock = startClock();
  const perChainState = new Map<number, WorkerState>();
  const etaInterval = setInterval(async () => {
    const { eta, progress } = await getAppProgress(common.metrics);
    if (eta === undefined && progress === undefined) {
      return;
    }
    common.logger.info({
      msg: "Updated backfill indexing progress",
      progress: progress === undefined ? undefined : formatPercentage(progress),
      estimate: eta === undefined ? undefined : formatEta(eta * 1_000),
    });
  }, 5_000);
  common.shutdown.add(() => {
    clearInterval(etaInterval);
  });
  let isAllReady = false;
  let isAllComplete = false;
  const callback = async () => {
    if (
      isAllReady === false &&
      indexingBuild.chains.every(
        (chain) => perChainState.get(chain.id) !== "backfill",
      )
    ) {
      isAllReady = true;
      common.logger.info({
        msg: "Completed backfill indexing across all chains",
        duration: backfillEndClock(),
      });
      clearInterval(etaInterval);
      let endClock = startClock();
      await createIndexes(database.adminQB, {
        statements: schemaBuild.statements,
      });
      if (schemaBuild.statements.indexes.sql.length > 0) {
        common.logger.info({
          msg: "Created database indexes",
          count: schemaBuild.statements.indexes.sql.length,
          duration: endClock(),
        });
      }
      if (namespaceBuild.viewsSchema !== undefined) {
        endClock = startClock();
        const tables = Object.values(schemaBuild.schema).filter(isTable);
        const views = Object.values(schemaBuild.schema).filter(isView);
        await createViews(database.adminQB, { tables, views, namespaceBuild });
        common.logger.info({
          msg: "Created database views",
          schema: namespaceBuild.viewsSchema,
          count: tables.length,
          duration: endClock(),
        });
        endClock = startClock();
      }
      await database.adminQB.wrap({ label: "update_ready" }, (db) =>
        db
          .update(getPonderMetaTable(namespaceBuild.schema))
          .set({ value: sql`jsonb_set(value, '{is_ready}', to_jsonb(1))` }),
      );
      common.logger.info({
        msg: "Started returning 200 responses",
        endpoint: "/ready",
      });
    }
    if (
      isAllComplete === false &&
      indexingBuild.chains.every(
        (chain) => perChainState.get(chain.id) === "complete",
      )
    ) {
      isAllComplete = true;
    }
  };
  if (
    common.options.command === "dev" ||
    indexingBuild.chains.length === 1 ||
    database.driver.dialect === "pglite" ||
    common.options.maxThreads === 1
  ) {
    common.options.indexingCacheMaxBytes = Math.floor(
      common.options.indexingCacheMaxBytes / indexingBuild.chains.length,
    );
    common.options.rpcMaxConcurrency = Math.floor(
      common.options.rpcMaxConcurrency / indexingBuild.chains.length,
    );
    common.options.syncEventsQuerySize = Math.floor(
      common.options.syncEventsQuerySize / indexingBuild.chains.length,
    );
    await Promise.all(
      indexingBuild.chains.map(async (chain) => {
        const chainIndex = indexingBuild.chains.findIndex(
          (c) => c.id === chain.id,
        );
        const _indexingBuild = {
          ...indexingBuild,
          chains: [indexingBuild.chains[chainIndex]!],
          rpcs: [indexingBuild.rpcs[chainIndex]!],
          finalizedBlocks: [indexingBuild.finalizedBlocks[chainIndex]!],
          eventCallbacks: [indexingBuild.eventCallbacks[chainIndex]!],
          setupCallbacks: [indexingBuild.setupCallbacks[chainIndex]!],
          contracts: [indexingBuild.contracts[chainIndex]!],
        };
        perChainState.set(chain.id, "backfill");
        await runIsolated({
          common,
          preBuild,
          namespaceBuild,
          schemaBuild,
          indexingBuild: _indexingBuild,
          crashRecoveryCheckpoint,
          database,
          onReady: () => {
            perChainState.set(chain.id, "live");
            callback();
          },
        });
        perChainState.set(chain.id, "complete");
        callback();
      }),
    );
  } else {
    const workerPath = path.join(__dirname, "isolatedWorker.js");
    const perThreadChains = chunk(
      indexingBuild.chains,
      Math.ceil(indexingBuild.chains.length / common.options.maxThreads),
    );
    const perThreadWorkers: Worker[] = [];
    for (const chains of perThreadChains) {
      const chainIds = chains.map((chain) => chain.id);
      // Note: This is a hack to force color support in the worker threads
      if (process.stdout.isTTY) {
        process.env.FORCE_COLOR = "1";
      }
      const heapSizeLimit = v8.getHeapStatistics().heap_size_limit;
      const perThreadHeapSizeLimit = Math.floor(
        heapSizeLimit / (common.options.maxThreads + 1) / 1024 / 1024,
      );
      // Note: This sets `--max-old-space-size` for the worker thread.
      // `resourceLimits` does not work because it gets overridden by
      // CLI flags or environment variables. It does not change the heap
      // size limit for the current main thread
      v8.setFlagsFromString(`--max-old-space-size=${perThreadHeapSizeLimit}`);
      const worker = new Worker(workerPath, {
        workerData: {
          options: common.options,
          chainIds,
          namespaceBuild,
          crashRecoveryCheckpoint,
        } satisfies Parameters<typeof isolatedWorker>[0],
      });
      for (const chainId of chainIds) {
        perChainState.set(chainId, "backfill");
      }
      worker.on(
        "message",
        (
          message:
            | { type: "ready"; chainId: number }
            | { type: "done"; chainId: number }
            | { type: "error"; error: Error },
        ) => {
          switch (message.type) {
            case "ready": {
              perChainState.set(message.chainId, "live");
              callback();
              break;
            }
            case "done": {
              perChainState.set(message.chainId, "complete");
              callback();
              break;
            }
            case "error": {
              let error: Error;
              if (nonRetryableUserErrorNames.includes(message.error.name)) {
                error = new NonRetryableUserError(message.error.message);
              } else {
                error = new Error(message.error.message);
              }
              error.name = message.error.name;
              error.stack = message.error.stack;
              throw error;
            }
          }
        },
      );
      worker.on("error", (error: Error) => {
        if (nonRetryableUserErrorNames.includes(error.name)) {
          error = new NonRetryableUserError(error.message);
        } else {
          error = new Error(error.message);
        }
        throw error;
      });
      worker.on("exit", (code: number) => {
        const error = new Error(`Worker thread exited with code ${code}.`);
        error.stack = undefined;
        throw error;
      });
      perThreadWorkers.push(worker);
    }
    common.logger.debug({
      msg: "Created worker threads",
      count: perThreadWorkers.length,
      duration: backfillEndClock(),
    });
    common.metrics = new AggregateMetricsService(
      common.metrics,
      perThreadWorkers,
    );
    common.shutdown.add(() => {
      for (const worker of perThreadWorkers) {
        worker.postMessage({ type: "kill" });
      }
    });
  }
}
</file>

<file path="packages/core/src/bin/isolatedWorker.ts">
import { isMainThread, parentPort, workerData } from "node:worker_threads";
import { createBuild } from "@/build/index.js";
import { createDatabase } from "@/database/index.js";
import type { Common } from "@/internal/common.js";
import { createLogger } from "@/internal/logger.js";
import { IsolatedMetricsService } from "@/internal/metrics.js";
import { createShutdown } from "@/internal/shutdown.js";
import { createTelemetry } from "@/internal/telemetry.js";
import type {
  CrashRecoveryCheckpoint,
  NamespaceBuild,
} from "@/internal/types.js";
import { runIsolated } from "@/runtime/isolated.js";
if (isMainThread) {
  throw new Error("'isolatedWorker.ts' must be run in a worker thread");
}
try {
  await isolatedWorker(workerData);
  parentPort!.postMessage({ type: "done" });
} catch (err) {
  const error = err as Error;
  parentPort!.postMessage({
    type: "error",
    error: {
      name: error.name,
      message: error.message,
      stack: error.stack,
    },
  });
}
export async function isolatedWorker({
  options,
  namespaceBuild,
  crashRecoveryCheckpoint,
  chainIds,
}: {
  options: Common["options"];
  namespaceBuild: NamespaceBuild;
  crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  chainIds: number[];
}) {
  // Note: telemetry is disabled because the main thread will report telemetry
  options.telemetryDisabled = true;
  const logger = createLogger({
    level: options.logLevel,
    mode: options.logFormat,
  });
  const metrics = new IsolatedMetricsService();
  const shutdown = createShutdown();
  const telemetry = createTelemetry({ options, logger, shutdown });
  const common = {
    options,
    logger,
    metrics,
    telemetry,
    shutdown,
    buildShutdown: shutdown,
    apiShutdown: shutdown,
  };
  let isKilled = false;
  parentPort!.on("message", async (msg) => {
    if (msg.type === "kill") {
      if (isKilled) return;
      isKilled = true;
      await shutdown.kill();
    }
  });
  const build = await createBuild({
    common,
    cliOptions: workerData.cliOptions,
  });
  // Note: build is guaranteed to be successful because the main thread
  // has already run the build.
  // Note: `namespaceCompile`
  globalThis.PONDER_NAMESPACE_BUILD = namespaceBuild;
  const configResult = await build.executeConfig();
  if (configResult.status === "error") {
    throw configResult.error;
  }
  const schemaResult = await build.executeSchema();
  if (schemaResult.status === "error") {
    throw schemaResult.error;
  }
  const preBuildResult = build.preCompile(configResult.result);
  if (preBuildResult.status === "error") {
    throw preBuildResult.error;
  }
  const schemaBuildResult = build.compileSchema({
    ...schemaResult.result,
    preBuild: preBuildResult.result,
  });
  if (schemaBuildResult.status === "error") {
    throw schemaBuildResult.error;
  }
  const configBuildResult = build.compileConfig({
    configResult: configResult.result,
  });
  if (configBuildResult.status === "error") {
    throw configBuildResult.error;
  }
  const indexingResult = await build.executeIndexingFunctions();
  if (indexingResult.status === "error") {
    throw indexingResult.error;
  }
  const indexingBuildResult = await build.compileIndexing({
    configResult: configResult.result,
    schemaResult: schemaResult.result,
    indexingResult: indexingResult.result,
    configBuild: configBuildResult.result,
  });
  if (indexingBuildResult.status === "error") {
    throw indexingBuildResult.error;
  }
  options.indexingCacheMaxBytes = Math.floor(
    options.indexingCacheMaxBytes / indexingBuildResult.result.chains.length,
  );
  options.rpcMaxConcurrency = Math.floor(
    options.rpcMaxConcurrency / indexingBuildResult.result.chains.length,
  );
  options.syncEventsQuerySize = Math.floor(
    options.syncEventsQuerySize / indexingBuildResult.result.chains.length,
  );
  const database = createDatabase({
    common,
    namespace: namespaceBuild,
    preBuild: preBuildResult.result,
    schemaBuild: schemaBuildResult.result,
  });
  await Promise.all(
    chainIds.map(async (chainId) => {
      const chainIndex = indexingBuildResult.result.chains.findIndex(
        (c) => c.id === chainId,
      );
      const indexingBuild = {
        ...indexingBuildResult.result,
        chains: [indexingBuildResult.result.chains[chainIndex]!],
        rpcs: [indexingBuildResult.result.rpcs[chainIndex]!],
        finalizedBlocks: [
          indexingBuildResult.result.finalizedBlocks[chainIndex]!,
        ],
        eventCallbacks: [
          indexingBuildResult.result.eventCallbacks[chainIndex]!,
        ],
        setupCallbacks: [
          indexingBuildResult.result.setupCallbacks[chainIndex]!,
        ],
        contracts: [indexingBuildResult.result.contracts[chainIndex]!],
      };
      common.metrics.initializeIndexingMetrics({
        indexingBuild,
        schemaBuild: schemaBuildResult.result,
      });
      await runIsolated({
        common,
        preBuild: preBuildResult.result,
        namespaceBuild,
        schemaBuild: schemaBuildResult.result,
        indexingBuild,
        crashRecoveryCheckpoint,
        database,
        onReady: () => {
          parentPort!.postMessage({ type: "ready", chainId });
        },
      });
      parentPort!.postMessage({ type: "done", chainId });
    }),
  );
}
</file>

<file path="packages/core/src/bin/ponder.ts">
#!/usr/bin/env node
import { readFileSync } from "node:fs";
import { dirname, resolve } from "node:path";
import { fileURLToPath } from "node:url";
import type { Prettify } from "@/types/utils.js";
import { Command } from "@commander-js/extra-typings";
import dotenv from "dotenv";
import { codegen } from "./commands/codegen.js";
import { createViews } from "./commands/createViews.js";
import { dev } from "./commands/dev.js";
import { list } from "./commands/list.js";
import { prune } from "./commands/prune.js";
import { serve } from "./commands/serve.js";
import { start } from "./commands/start.js";
dotenv.config({ path: ".env.local" });
const __dirname = dirname(fileURLToPath(import.meta.url));
const packageJsonPath = resolve(__dirname, "../../../package.json");
const packageJson = JSON.parse(
  readFileSync(packageJsonPath, { encoding: "utf8" }),
);
const ponder = new Command("ponder")
  .usage("<command> [OPTIONS]")
  .helpOption("-h, --help", "Show this help message")
  .helpCommand(false)
  .option(
    "--root <PATH>",
    "Path to the project root directory (default: working directory)",
  )
  .option(
    "--config <PATH>",
    "Path to the project config file",
    "ponder.config.ts",
  )
  .option(
    "-v, --debug",
    "Enable debug logs, e.g. realtime blocks, internal events",
  )
  .option(
    "-vv, --trace",
    "Enable trace logs, e.g. db queries, indexing checkpoints",
  )
  .option(
    "--log-level <LEVEL>",
    'Minimum log level ("error", "warn", "info", "debug", or "trace", default: "info")',
  )
  .option(
    "--log-format <FORMAT>",
    'The log format ("pretty" or "json")',
    "pretty",
  )
  .version(packageJson.version, "-V, --version", "Show the version number")
  .configureHelp({ showGlobalOptions: true })
  .allowExcessArguments(false)
  .showHelpAfterError()
  .enablePositionalOptions(false);
type GlobalOptions = {
  command: "dev" | "start" | "serve" | "codegen";
  version: string;
} & ReturnType<typeof ponder.opts>;
const devCommand = new Command("dev")
  .description("Start the development server with hot reloading")
  .option("--schema <SCHEMA>", "Database schema (max: 45 characters)", String)
  .option("-p, --port <PORT>", "Port for the web server", Number, 42069)
  // NOTE: Do not set a default for hostname. We currently rely on the Node.js
  // default behavior when passing undefined to http.Server.listen(), which
  // detects the available interfaces (IPv4 and/or IPv6) and uses them.
  // Documentation: https://arc.net/l/quote/dnjmtumq
  .option(
    "-H, --hostname <HOSTNAME>",
    'Hostname for the web server (default: "0.0.0.0" or "::")',
  )
  .option("--disable-ui", "Disable the terminal UI")
  .showHelpAfterError()
  .action(async (_, command) => {
    const cliOptions = {
      ...command.optsWithGlobals(),
      command: command.name(),
      version: packageJson.version,
    } as GlobalOptions & ReturnType<typeof command.opts>;
    await dev({ cliOptions });
  });
const startCommand = new Command("start")
  .description("Start the production server")
  .option("--schema <SCHEMA>", "Database schema (max: 45 characters)", String)
  .option(
    "--views-schema <SCHEMA>",
    "Views database schema (max: 45 characters)",
    String,
  )
  .option("-p, --port <PORT>", "Port for the web server", Number, 42069)
  .option(
    "-H, --hostname <HOSTNAME>",
    'Hostname for the web server (default: "0.0.0.0" or "::")',
  )
  .showHelpAfterError()
  .action(async (_, command) => {
    const cliOptions = {
      ...command.optsWithGlobals(),
      command: command.name(),
      version: packageJson.version,
    } as GlobalOptions & ReturnType<typeof command.opts>;
    await start({ cliOptions });
  });
const serveCommand = new Command("serve")
  .description("Start the production HTTP server without the indexer")
  .option("--schema <SCHEMA>", "Database schema (max: 45 characters)", String)
  .option("-p, --port <PORT>", "Port for the web server", Number, 42069)
  .option(
    "-H, --hostname <HOSTNAME>",
    'Hostname for the web server (default: "0.0.0.0" or "::")',
  )
  .showHelpAfterError()
  .action(async (_, command) => {
    const cliOptions = {
      ...command.optsWithGlobals(),
      command: command.name(),
      version: packageJson.version,
    } as GlobalOptions & ReturnType<typeof command.opts>;
    await serve({ cliOptions });
  });
const createViewsCommand = new Command("create-views")
  .description("Create database views for the views pattern")
  .option("--schema <SCHEMA>", "Database schema (max: 45 characters)", String)
  .option(
    "--views-schema <SCHEMA>",
    "Views database schema (max: 45 characters)",
    String,
  )
  .showHelpAfterError()
  .action(async (_, command) => {
    const cliOptions = {
      ...command.optsWithGlobals(),
      command: command.name(),
      version: packageJson.version,
    } as GlobalOptions & ReturnType<typeof command.opts>;
    await createViews({ cliOptions });
  });
const dbCommand = new Command("db").description("Database management commands");
const listCommand = new Command("list")
  .description("List all Ponder deployments")
  .showHelpAfterError()
  .action(async (_, command) => {
    const cliOptions = {
      ...command.optsWithGlobals(),
      command: command.name(),
      version: packageJson.version,
    } as GlobalOptions & ReturnType<typeof command.opts>;
    await list({ cliOptions });
  });
const pruneCommand = new Command("prune")
  .description(
    "Drop all database tables, functions, and schemas created by Ponder deployments that are not active",
  )
  .showHelpAfterError()
  .action(async (_, command) => {
    const cliOptions = {
      ...command.optsWithGlobals(),
      command: command.name(),
      version: packageJson.version,
    } as GlobalOptions & ReturnType<typeof command.opts>;
    await prune({ cliOptions });
  });
const codegenCommand = new Command("codegen")
  .description("Generate the ponder-env.d.ts file, then exit")
  .showHelpAfterError()
  .action(async (_, command) => {
    const cliOptions = {
      ...command.optsWithGlobals(),
      command: command.name(),
      version: packageJson.version,
    } as GlobalOptions & ReturnType<typeof command.opts>;
    await codegen({ cliOptions });
  });
dbCommand.addCommand(listCommand);
dbCommand.addCommand(pruneCommand);
dbCommand.addCommand(createViewsCommand);
ponder.addCommand(devCommand);
ponder.addCommand(startCommand);
ponder.addCommand(serveCommand);
ponder.addCommand(dbCommand);
ponder.addCommand(codegenCommand);
export type CliOptions = Prettify<
  GlobalOptions &
    Partial<
      ReturnType<typeof devCommand.opts> &
        ReturnType<typeof startCommand.opts> &
        ReturnType<typeof serveCommand.opts> &
        ReturnType<typeof dbCommand.opts> &
        ReturnType<typeof codegenCommand.opts>
    >
>;
await ponder.parseAsync();
</file>

<file path="packages/core/src/build/config.test.ts">
import { context, setupAnvil, setupCommon } from "@/_test/setup.js";
import { TEST_POOL_ID } from "@/_test/utils.js";
import { factory } from "@/config/address.js";
import { createConfig } from "@/config/index.js";
import type { LogFactory, LogFilter, TraceFilter } from "@/internal/types.js";
import { hyperliquidEvm } from "@/utils/chains.js";
import {
  type Address,
  parseAbiItem,
  toEventSelector,
  toFunctionSelector,
  zeroAddress,
} from "viem";
import { beforeEach, expect, test } from "vitest";
import {
  buildConfig,
  buildIndexingFunctions,
  safeBuildConfig,
  safeBuildIndexingFunctions,
} from "./config.js";
beforeEach(setupCommon);
beforeEach(setupAnvil);
const event0 = parseAbiItem("event Event0(bytes32 indexed arg)");
const event1 = parseAbiItem("event Event1()");
const event1Overloaded = parseAbiItem("event Event1(bytes32 indexed)");
const eventFactory = parseAbiItem("event EventFactory(address indexed child)");
const func0 = parseAbiItem(
  "function func0(address) external returns (uint256)",
);
const address1 = "0x0000000000000000000000000000000000000001";
const address2 = "0x0000000000000000000000000000000000000001";
const address3 = "0x0000000000000000000000000000000000000003";
const bytes1 =
  "0x0000000000000000000000000000000000000000000000000000000000000001";
const bytes2 =
  "0x0000000000000000000000000000000000000000000000000000000000000002";
beforeEach(setupAnvil);
test("buildIndexingFunctions() builds topics for multiple events", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: "mainnet",
        abi: [event0, event1],
        address: address1,
        startBlock: 16370000,
        endBlock: 16370020,
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [
      { name: "a:Event0", fn: () => {} },
      { name: "a:Event1", fn: () => {} },
    ],
    configBuild,
  });
  expect((eventCallbacks[0]![0]!.filter as LogFilter).topic0).toEqual(
    toEventSelector(event0),
  );
  expect((eventCallbacks[0]![1]!.filter as LogFilter).topic0).toEqual(
    toEventSelector(event1),
  );
});
test("buildIndexingFunctions() handles overloaded event signatures and combines topics", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: "mainnet",
        abi: [event1, event1Overloaded],
        address: address1,
        startBlock: 16370000,
        endBlock: 16370020,
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [
      { name: "a:Event1()", fn: () => {} },
      { name: "a:Event1(bytes32 indexed)", fn: () => {} },
    ],
    configBuild,
  });
  expect((eventCallbacks[0]![0]!.filter as LogFilter).topic0).toEqual(
    toEventSelector(event1),
  );
  expect((eventCallbacks[0]![1]!.filter as LogFilter).topic0).toEqual(
    toEventSelector(event1Overloaded),
  );
});
test("buildIndexingFunctions() handles multiple addresses", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: {
          mainnet: {
            address: [address1, address3],
            startBlock: 16370000,
            endBlock: 16370020,
          },
        },
        abi: [event1, event1Overloaded],
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [
      { name: "a:Event1()", fn: () => {} },
      { name: "a:Event1(bytes32 indexed)", fn: () => {} },
    ],
    configBuild,
  });
  expect((eventCallbacks[0]![0]!.filter as LogFilter).topic0).toEqual(
    toEventSelector(event1),
  );
  expect((eventCallbacks[0]![1]!.filter as LogFilter).topic0).toEqual(
    toEventSelector(event1Overloaded),
  );
});
test("buildIndexingFunctions() creates a source for each chain for multi-chain contracts", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
      optimism: { id: 10, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: { mainnet: {}, optimism: {} },
        abi: [event0],
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(eventCallbacks.length).toBe(2);
});
test("buildIndexingFunctions() throw useful error for common 0.11 migration mistakes", async () => {
  const indexingFunctions = [{ name: "a:Event0", fn: () => {} }];
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
      optimism: { id: 10, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        // @ts-expect-error
        network: { mainnet: {}, optimism: {} },
        abi: [event0],
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    // @ts-expect-error
    config,
  });
  const result = await safeBuildIndexingFunctions({
    common: context.common,
    // @ts-expect-error
    config,
    indexingFunctions,
    configBuild,
  });
  expect(result.status).toBe("error");
  expect(result.error?.message).toBe(
    "Validation failed: Chain for 'a' is null or undefined. Expected one of ['mainnet', 'optimism']. Did you forget to change 'network' to 'chain' when migrating to 0.11?",
  );
});
test("buildIndexingFunctions() builds topics for event filter", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: { mainnet: {} },
        abi: [event0],
        filter: {
          event: "Event0",
          args: {
            arg: bytes1,
          },
        },
        address: address1,
        startBlock: 16370000,
        endBlock: 16370020,
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(eventCallbacks).toHaveLength(1);
  expect((eventCallbacks[0]![0]!.filter as LogFilter).topic0).toEqual(
    toEventSelector(event0),
  );
  expect((eventCallbacks[0]![0]!.filter as LogFilter).topic1).toEqual(bytes1);
});
test("buildIndexingFunctions() builds topics for multiple event filters", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: { mainnet: {} },
        abi: [event0, event1Overloaded],
        filter: [
          {
            event: "Event1",
            args: [[bytes1, bytes2]],
          },
          {
            event: "Event0",
            args: {
              arg: bytes1,
            },
          },
        ],
        address: address1,
        startBlock: 16370000,
        endBlock: 16370020,
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [
      { name: "a:Event0", fn: () => {} },
      { name: "a:Event1", fn: () => {} },
    ],
    configBuild,
  });
  expect(eventCallbacks[0]).toHaveLength(2);
  expect((eventCallbacks[0]![1]!.filter as LogFilter).topic0).toEqual(
    toEventSelector(event1Overloaded),
  );
  expect((eventCallbacks[0]![1]!.filter as LogFilter).topic1).toEqual([
    bytes1,
    bytes2,
  ]);
  expect((eventCallbacks[0]![0]!.filter as LogFilter).topic0).toEqual(
    toEventSelector(event0),
  );
  expect((eventCallbacks[0]![0]!.filter as LogFilter).topic1).toEqual(bytes1);
});
test("buildIndexingFunctions() overrides default values with chain-specific values", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        abi: [event0],
        address: address1,
        startBlock: 16370000,
        endBlock: 16370020,
        chain: {
          mainnet: {
            address: address2,
          },
        },
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect((eventCallbacks[0]![0]!.filter as LogFilter).address).toBe(address2);
});
test("buildIndexingFunctions() handles chain name shortcut", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: "mainnet",
        abi: [event0],
        address: address1,
        startBlock: 16370000,
        endBlock: 16370020,
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(eventCallbacks[0]![0]!.chain.name).toBe("mainnet");
});
test("buildIndexingFunctions() validates chain name", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        // @ts-expect-error
        chain: "mainnetz",
        abi: [event0],
        address: address1,
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const result = await safeBuildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(result.status).toBe("error");
  expect(result.error?.message).toBe(
    "Validation failed: Invalid chain for 'a'. Got 'mainnetz', expected one of ['mainnet'].",
  );
});
// Note: Not possible to find an rpc url that returns a finalized block and is public.
test.skip("buildConfig() warns for public RPC URL", () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: "https://cloudflare-eth.com" },
    },
    contracts: {
      a: {
        chain: "mainnet",
        abi: [event0],
        address: address1,
      },
    },
  });
  const result = safeBuildConfig({
    common: context.common,
    config,
  });
  expect(result.status).toBe("success");
  expect(result.logs!.filter((l) => l.level === "warn")).toEqual([
    {
      level: "warn",
      msg: "Chain 'mainnet' is using a public RPC URL (https://cloudflare-eth.com). Most apps require an RPC URL with a higher rate limit.",
    },
  ]);
});
test("buildConfig() handles chains not found in viem", () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1909023431, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: "mainnet",
        abi: [event0],
        address: address1,
      },
    },
  });
  const result = safeBuildConfig({
    common: context.common,
    config,
  });
  expect(result.status).toBe("success");
});
test("buildIndexingFunctions() validates event filter event name must be present in ABI", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: "mainnet",
        abi: [event0],
        // @ts-expect-error
        filter: {
          event: "Event2",
          args: {
            arg: "0x",
          },
        },
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const result = await safeBuildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(result.status).toBe("error");
  expect(result.error?.message).toBe(
    "Validation failed: Invalid filter for contract 'a'. Got event name 'Event2', expected one of ['Event0'].",
  );
});
test("buildIndexingFunctions() validates address empty string", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: "mainnet",
        abi: [event0],
        address: "" as Address,
      },
    },
  });
  const configBuild = buildConfig({
    common: context.common,
    config,
  });
  const result = await safeBuildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(result.status).toBe("error");
  expect(result.error?.message).toBe(
    "Validation failed: Invalid prefix for address ''. Got '', expected '0x'.",
  );
});
test("buildIndexingFunctions() validates address prefix", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: "mainnet",
        abi: [event0],
        address: "0b0000000000000000000000000000000000000001" as Address,
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const result = await safeBuildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(result.status).toBe("error");
  expect(result.error?.message).toBe(
    "Validation failed: Invalid prefix for address '0b0000000000000000000000000000000000000001'. Got '0b', expected '0x'.",
  );
});
test("buildIndexingFunctions() validates address length", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: "mainnet",
        abi: [event0],
        address: "0x000000000001",
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const result = await safeBuildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(result.status).toBe("error");
  expect(result.error?.message).toBe(
    "Validation failed: Invalid length for address '0x000000000001'. Got 14, expected 42 characters.",
  );
});
test("buildIndexingFunctions() coerces NaN startBlock to undefined", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: { mainnet: {} },
        abi: [event0, event1],
        startBlock: Number.NaN,
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(eventCallbacks[0]![0]?.filter.fromBlock).toBe(undefined);
});
test("buildIndexingFunctions() coerces `latest` to number", async () => {
  const config = createConfig({
    chains: {
      mainnet: {
        id: 1,
        rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}`,
      },
    },
    contracts: {
      a: {
        chain: { mainnet: {} },
        abi: [event0, event1],
        startBlock: "latest",
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(eventCallbacks[0]![0]?.filter.fromBlock).toBeTypeOf("number");
});
test("buildIndexingFunctions() includeTransactionReceipts", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
      optimism: { id: 10, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        includeTransactionReceipts: true,
        chain: {
          mainnet: {},
          optimism: { includeTransactionReceipts: false },
        },
        abi: [event0],
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(eventCallbacks[0]![0]!.filter.hasTransactionReceipt).toBe(true);
  expect(eventCallbacks[1]![0]!.filter.hasTransactionReceipt).toBe(false);
});
test("buildIndexingFunctions() includeCallTraces", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
      optimism: { id: 10, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        includeCallTraces: true,
        chain: {
          mainnet: {},
          optimism: { includeCallTraces: false },
        },
        address: zeroAddress,
        abi: [func0],
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a.func0()", fn: () => {} }],
    configBuild,
  });
  expect(eventCallbacks).toHaveLength(1);
  expect(
    (eventCallbacks[0]![0]!.filter as TraceFilter).fromAddress,
  ).toBeUndefined();
  expect((eventCallbacks[0]![0]!.filter as TraceFilter).toAddress).toEqual(
    zeroAddress,
  );
  expect(
    (eventCallbacks[0]![0]!.filter as TraceFilter).functionSelector,
  ).toEqual(toFunctionSelector(func0));
  expect(eventCallbacks[0]![0]!.filter.hasTransactionReceipt).toBe(false);
});
test("buildIndexingFunctions() includeCallTraces with factory", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
      optimism: { id: 10, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        includeCallTraces: true,
        chain: {
          mainnet: {},
          optimism: { includeCallTraces: false },
        },
        address: factory({
          address: address2,
          event: eventFactory,
          parameter: "child",
        }),
        abi: [func0],
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a.func0()", fn: () => {} }],
    configBuild,
  });
  expect(eventCallbacks).toHaveLength(1);
  expect(
    (eventCallbacks[0]![0]!.filter as TraceFilter).fromAddress,
  ).toBeUndefined();
  expect(
    ((eventCallbacks[0]![0]!.filter as TraceFilter).toAddress as LogFactory)
      .address,
  ).toEqual(address2);
  expect(
    (eventCallbacks[0]![0]!.filter as TraceFilter).functionSelector,
  ).toEqual(toFunctionSelector(func0));
  expect(eventCallbacks[0]![0]!.filter.hasTransactionReceipt).toBe(false);
});
test("buildIndexingFunctions() coerces NaN endBlock to undefined", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: { mainnet: {} },
        abi: [event0, event1],
        endBlock: Number.NaN,
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(eventCallbacks[0]![0]!.filter.toBlock).toBe(undefined);
});
test("buildIndexingFunctions() account source", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    accounts: {
      a: {
        chain: { mainnet: {} },
        address: address1,
        startBlock: 16370000,
        endBlock: 16370020,
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [
      { name: "a:transfer:from", fn: () => {} },
      { name: "a:transaction:to", fn: () => {} },
    ],
    configBuild,
  });
  expect(eventCallbacks[0]).toHaveLength(2);
  expect(eventCallbacks[0]![0]?.chain.name).toBe("mainnet");
  expect(eventCallbacks[0]![1]?.chain.name).toBe("mainnet");
  expect(eventCallbacks[0]![0]?.name).toBe("a:transaction:to");
  expect(eventCallbacks[0]![1]?.name).toBe("a:transfer:from");
  expect(eventCallbacks[0]![0]?.filter.type).toBe("transaction");
  expect(eventCallbacks[0]![1]?.filter.type).toBe("transfer");
  expect(eventCallbacks[0]![0]?.filter.fromBlock).toBe(16370000);
  expect(eventCallbacks[0]![1]?.filter.fromBlock).toBe(16370000);
  expect(eventCallbacks[0]![0]?.filter.toBlock).toBe(16370020);
  expect(eventCallbacks[0]![1]?.filter.toBlock).toBe(16370020);
});
test("buildIndexingFunctions() block source", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    blocks: {
      a: {
        chain: { mainnet: {} },
        startBlock: 16370000,
        endBlock: 16370020,
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:block", fn: () => {} }],
    configBuild,
  });
  expect(eventCallbacks).toHaveLength(1);
  expect(eventCallbacks[0]![0]?.chain.name).toBe("mainnet");
  expect(eventCallbacks[0]![0]?.name).toBe("a:block");
  expect(eventCallbacks[0]![0]?.filter.type).toBe("block");
  // @ts-ignore
  expect(eventCallbacks[0]![0]?.filter.interval).toBe(1);
  expect(eventCallbacks[0]![0]?.filter.fromBlock).toBe(16370000);
  expect(eventCallbacks[0]![0]?.filter.toBlock).toBe(16370020);
});
test("buildIndexingFunctions() coerces undefined factory interval to source interval", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: { mainnet: {} },
        address: factory({
          address: address2,
          event: eventFactory,
          parameter: "child",
        }),
        abi: [event0, event1],
        startBlock: 16370000,
        endBlock: 16370100,
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { eventCallbacks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(
    ((eventCallbacks[0]![0]!.filter as LogFilter).address as LogFactory)
      .fromBlock === 16370000,
  );
  expect(
    ((eventCallbacks[0]![0]!.filter as LogFilter).address as LogFactory)
      .toBlock === 16370100,
  );
});
test("buildIndexingFunctions() validates factory interval", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: { mainnet: {} },
        address: factory({
          address: address2,
          event: eventFactory,
          parameter: "child",
          startBlock: 16370050,
        }),
        abi: [event0, event1],
        startBlock: 16370000,
        endBlock: 16370100,
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const result = await safeBuildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(result.status).toBe("error");
  expect(result.error?.message).toBe(
    "Validation failed: Start block for 'a' is before start block of factory address (16370050 > 16370000).",
  );
});
test("buildIndexingFunctions() validates start and end block", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    contracts: {
      a: {
        chain: { mainnet: {} },
        abi: [event0, event1],
        // @ts-expect-error
        startBlock: "16370000",
        // @ts-expect-error
        endBlock: "16370100",
      },
    },
  });
  // @ts-expect-error
  const configBuild = buildConfig({ common: context.common, config });
  const result = await safeBuildIndexingFunctions({
    common: context.common,
    // @ts-expect-error
    config,
    indexingFunctions: [{ name: "a:Event0", fn: () => {} }],
    configBuild,
  });
  expect(result).toMatchInlineSnapshot(`
    {
      "error": [BuildError: Validation failed: Invalid start block for 'a'. Got 16370000 typeof string, expected an integer.],
      "status": "error",
    }
  `);
});
test("buildIndexingFunctions() returns chain, rpc, and finalized block", async () => {
  const config = createConfig({
    chains: {
      mainnet: { id: 1, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    blocks: {
      b: {
        chain: "mainnet",
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { rpcs, chains, finalizedBlocks } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "b:block", fn: () => {} }],
    configBuild,
  });
  expect(rpcs).toHaveLength(1);
  expect(chains).toHaveLength(1);
  expect(finalizedBlocks).toHaveLength(1);
  expect(chains[0]!.name).toBe("mainnet");
  expect(chains[0]!.id).toBe(1);
  expect(finalizedBlocks[0]!.number).toBe("0x0");
});
test("buildIndexingFunctions() hyperliquid evm", async () => {
  const config = createConfig({
    chains: {
      hyperliquid: { id: 999, rpc: `http://127.0.0.1:8545/${TEST_POOL_ID}` },
    },
    blocks: {
      b: {
        chain: "hyperliquid",
      },
    },
  });
  const configBuild = buildConfig({ common: context.common, config });
  const { chains } = await buildIndexingFunctions({
    common: context.common,
    config,
    indexingFunctions: [{ name: "b:block", fn: () => {} }],
    configBuild,
  });
  expect(chains).toHaveLength(1);
  expect(chains[0]!.name).toBe("hyperliquid");
  expect(chains[0]!.id).toBe(999);
  expect(chains[0]!.viemChain).toBe(hyperliquidEvm);
});
</file>

<file path="packages/core/src/build/config.ts">
import type { Factory } from "@/config/address.js";
import type { Config } from "@/config/index.js";
import type { Common } from "@/internal/common.js";
import { BuildError } from "@/internal/errors.js";
import type {
  BlockFilter,
  Chain,
  Contract,
  EventCallback,
  FilterAddress,
  IndexingBuild,
  IndexingFunctions,
  LightBlock,
  LogFilter,
  SetupCallback,
  SyncBlock,
  TraceFilter,
  TransactionFilter,
  TransferFilter,
} from "@/internal/types.js";
import { eth_getBlockByNumber } from "@/rpc/actions.js";
import { type Rpc, createRpc } from "@/rpc/index.js";
import {
  defaultBlockFilterInclude,
  defaultLogFilterInclude,
  defaultTraceFilterInclude,
  defaultTransactionFilterInclude,
  defaultTransactionReceiptInclude,
  defaultTransferFilterInclude,
} from "@/runtime/filter.js";
import { buildTopics, toSafeName } from "@/utils/abi.js";
import { hyperliquidEvm, chains as viemChains } from "@/utils/chains.js";
import { dedupe } from "@/utils/dedupe.js";
import { getFinalityBlockCount } from "@/utils/finality.js";
import { toLowerCase } from "@/utils/lowercase.js";
import {
  type Abi,
  type AbiEvent,
  type AbiFunction,
  type Address,
  BlockNotFoundError,
  type Hex,
  type LogTopic,
  hexToNumber,
  numberToHex,
  toEventSelector,
  toFunctionSelector,
} from "viem";
import { buildLogFactory } from "./factory.js";
const flattenSources = <
  T extends Config["contracts"] | Config["accounts"] | Config["blocks"],
>(
  config: T,
): (Omit<T[string], "chain"> & { name: string; chain: string })[] => {
  return Object.entries(config).flatMap(
    ([name, source]: [string, T[string]]) => {
      if (typeof source.chain === "object") {
        return Object.entries(source.chain).map(([chain, sourceOverride]) => {
          const { chain: _chain, ...base } = source;
          return {
            name,
            chain,
            ...base,
            ...sourceOverride,
          };
        });
      } else {
        // Handles string, null, or undefined
        return {
          name,
          ...source,
        };
      }
    },
  );
};
export async function buildIndexingFunctions({
  common,
  config,
  indexingFunctions,
  configBuild: { chains, rpcs },
}: {
  common: Common;
  config: Config;
  indexingFunctions: IndexingFunctions;
  configBuild: Pick<IndexingBuild, "chains" | "rpcs">;
}): Promise<{
  chains: Chain[];
  rpcs: Rpc[];
  finalizedBlocks: LightBlock[];
  eventCallbacks: EventCallback[][];
  setupCallbacks: SetupCallback[][];
  contracts: {
    [name: string]: {
      abi: Abi;
      address?: Address | readonly Address[];
      startBlock?: number;
      endBlock?: number;
    };
  }[];
  logs: ({ level: "warn" | "info" | "debug"; msg: string } & Record<
    string,
    unknown
  >)[];
}> {
  const context = { logger: common.logger.child({ action: "build" }) };
  const logs: ({ level: "warn" | "info" | "debug"; msg: string } & Record<
    string,
    unknown
  >)[] = [];
  const perChainLatestBlockNumber = new Map<string, Promise<number>>();
  const resolveBlockNumber = async (
    blockNumberOrTag: number | "latest" | undefined,
    chain: Chain,
  ) => {
    if (blockNumberOrTag === undefined) {
      return undefined;
    }
    if (Number.isNaN(blockNumberOrTag)) {
      return undefined;
    }
    if (blockNumberOrTag === "latest") {
      if (perChainLatestBlockNumber.has(chain.name)) {
        return perChainLatestBlockNumber.get(chain.name)!;
      } else {
        const rpc = rpcs[chains.findIndex((c) => c.name === chain.name)]!;
        const blockPromise = rpc
          .request(
            {
              method: "eth_getBlockByNumber",
              params: ["latest", false],
            },
            context,
          )
          .then((block) => {
            if (!block)
              throw new BlockNotFoundError({ blockNumber: "latest" as any });
            return hexToNumber((block as SyncBlock).number);
          })
          .catch((e) => {
            throw new Error(
              `Unable to fetch "latest" block for chain '${chain.name}':\n${e.message}`,
            );
          });
        perChainLatestBlockNumber.set(chain.name, blockPromise);
        return blockPromise;
      }
    }
    return blockNumberOrTag;
  };
  const finalizedBlocks = await Promise.all(
    chains.map((chain) => {
      const rpc = rpcs[chains.findIndex((c) => c.name === chain.name)]!;
      const blockPromise = eth_getBlockByNumber(rpc, ["latest", false], {
        ...context,
        retryNullBlockRequest: true,
      })
        .then((block) => hexToNumber((block as SyncBlock).number))
        .catch((e) => {
          throw new Error(
            `Unable to fetch "latest" block for chain '${chain.name}':\n${e.message}`,
          );
        });
      perChainLatestBlockNumber.set(chain.name, blockPromise);
      return blockPromise.then((latest) =>
        eth_getBlockByNumber(
          rpc,
          [numberToHex(Math.max(latest - chain.finalityBlockCount, 0)), false],
          { ...context, retryNullBlockRequest: true },
        ).then((block) => ({
          hash: block.hash,
          parentHash: block.parentHash,
          number: block.number,
          timestamp: block.timestamp,
        })),
      );
    }),
  );
  const sourceNames = new Set<string>();
  for (const source of [
    ...Object.keys(config.contracts ?? {}),
    ...Object.keys(config.accounts ?? {}),
    ...Object.keys(config.blocks ?? {}),
  ]) {
    if (sourceNames.has(source)) {
      throw new Error(
        `Validation failed: Duplicate name '${source}' not allowed. The name must be unique across blocks, contracts, and accounts.`,
      );
    }
    sourceNames.add(source);
  }
  // Validate and build indexing functions
  if (indexingFunctions.length === 0) {
    throw new Error(
      "Validation failed: Found 0 registered indexing functions.",
    );
  }
  const eventNames = new Set<string>();
  for (const { name: eventName } of indexingFunctions) {
    const eventNameComponents = eventName.includes(".")
      ? eventName.split(".")
      : eventName.split(":");
    const [sourceName] = eventNameComponents;
    if (!sourceName) {
      throw new Error(
        `Validation failed: Invalid event '${eventName}', expected format '{sourceName}:{eventName}' or '{sourceName}.{functionName}'.`,
      );
    }
    if (eventNameComponents.length === 3) {
      const [, sourceType, fromOrTo] = eventNameComponents;
      if (
        (sourceType !== "transaction" && sourceType !== "transfer") ||
        (fromOrTo !== "from" && fromOrTo !== "to")
      ) {
        throw new Error(
          `Validation failed: Invalid event '${eventName}', expected format '{sourceName}:transaction:from', '{sourceName}:transaction:to', '{sourceName}:transfer:from', or '{sourceName}:transfer:to'.`,
        );
      }
    } else if (eventNameComponents.length === 2) {
      const [, sourceEventName] = eventNameComponents;
      if (!sourceEventName) {
        throw new Error(
          `Validation failed: Invalid event '${eventName}', expected format '{sourceName}:{eventName}' or '{sourceName}.{functionName}'.`,
        );
      }
    } else {
      throw new Error(
        `Validation failed: Invalid event '${eventName}', expected format '{sourceName}:{eventName}' or '{sourceName}.{functionName}'.`,
      );
    }
    if (eventNames.has(eventName)) {
      throw new Error(
        `Validation failed: Multiple indexing functions registered for event '${eventName}'.`,
      );
    }
    eventNames.add(eventName);
    // Validate that the indexing function uses a sourceName that is present in the config.
    const matchedSourceName = Object.keys({
      ...(config.contracts ?? {}),
      ...(config.accounts ?? {}),
      ...(config.blocks ?? {}),
    }).find((_sourceName) => _sourceName === sourceName);
    if (!matchedSourceName) {
      throw new Error(
        `Validation failed: Invalid event '${eventName}' uses an unrecognized contract, account, or block interval name. Expected one of [${Array.from(
          sourceNames,
        )
          .map((n) => `'${n}'`)
          .join(", ")}].`,
      );
    }
  }
  // common validation for all sources
  for (const source of [
    ...flattenSources(config.contracts ?? {}),
    ...flattenSources(config.accounts ?? {}),
    ...flattenSources(config.blocks ?? {}),
  ]) {
    if (source.chain === null || source.chain === undefined) {
      throw new Error(
        `Validation failed: Chain for '${source.name}' is null or undefined. Expected one of [${chains
          .map((n) => `'${n.name}'`)
          .join(
            ", ",
          )}]. Did you forget to change 'network' to 'chain' when migrating to 0.11?`,
      );
    }
    const chain = chains.find((n) => n.name === source.chain);
    if (!chain) {
      throw new Error(
        `Validation failed: Invalid chain for '${
          source.name
        }'. Got '${source.chain}', expected one of [${chains
          .map((n) => `'${n.name}'`)
          .join(", ")}].`,
      );
    }
    const startBlock = await resolveBlockNumber(source.startBlock, chain);
    const endBlock = await resolveBlockNumber(source.endBlock, chain);
    if (
      startBlock !== undefined &&
      endBlock !== undefined &&
      endBlock < startBlock
    ) {
      throw new Error(
        `Validation failed: Start block for '${source.name}' is after end block (${startBlock} > ${endBlock}).`,
      );
    }
    if (startBlock !== undefined && Number.isInteger(startBlock) === false) {
      throw new Error(
        `Validation failed: Invalid start block for '${source.name}'. Got ${startBlock} typeof ${typeof startBlock}, expected an integer.`,
      );
    }
    if (endBlock !== undefined && Number.isInteger(endBlock) === false) {
      throw new Error(
        `Validation failed: Invalid end block for '${source.name}'. Got ${endBlock} typeof ${typeof endBlock}, expected an integer.`,
      );
    }
    if (
      "address" in source &&
      typeof source.address === "object" &&
      !Array.isArray(source.address)
    ) {
      const factoryStartBlock =
        (await resolveBlockNumber(source.address.startBlock, chain)) ??
        startBlock;
      const factoryEndBlock =
        (await resolveBlockNumber(source.address.endBlock, chain)) ?? endBlock;
      if (
        factoryStartBlock !== undefined &&
        (startBlock === undefined || factoryStartBlock > startBlock)
      ) {
        throw new Error(
          `Validation failed: Start block for '${source.name}' is before start block of factory address (${factoryStartBlock} > ${startBlock}).`,
        );
      }
      if (
        endBlock !== undefined &&
        (factoryEndBlock === undefined || factoryEndBlock > endBlock)
      ) {
        throw new Error(
          `Validation failed: End block for ${source.name}  is before end block of factory address (${factoryEndBlock} > ${endBlock}).`,
        );
      }
      if (
        factoryStartBlock !== undefined &&
        factoryEndBlock !== undefined &&
        factoryEndBlock < factoryStartBlock
      ) {
        throw new Error(
          `Validation failed: Start block for '${source.name}' factory address is after end block (${factoryStartBlock} > ${factoryEndBlock}).`,
        );
      }
    }
  }
  const perChainEventCallbacks: Map<number, EventCallback[]> = new Map();
  const perChainSetupCallbacks: Map<number, SetupCallback[]> = new Map();
  const perChainContracts: Map<
    number,
    {
      [name: string]: Contract;
    }
  > = new Map();
  for (const chain of chains) {
    perChainEventCallbacks.set(chain.id, []);
    perChainSetupCallbacks.set(chain.id, []);
    perChainContracts.set(chain.id, {});
  }
  for (const source of flattenSources(config.contracts ?? {})) {
    const chain = chains.find((n) => n.name === source.chain)!;
    const fromBlock = await resolveBlockNumber(source.startBlock, chain);
    const toBlock = await resolveBlockNumber(source.endBlock, chain);
    if (indexingFunctions.some((f) => f.name === `${source.name}:setup`)) {
      perChainSetupCallbacks.get(chain.id)!.push({
        name: `${source.name}:setup`,
        fn: indexingFunctions.find((f) => f.name === `${source.name}:setup`)!
          .fn,
        chain,
        block: fromBlock,
      });
    }
    let address: FilterAddress;
    const resolvedAddress = source?.address;
    if (
      typeof resolvedAddress === "object" &&
      Array.isArray(resolvedAddress) === false
    ) {
      const factoryAddress = resolvedAddress as Factory;
      const factoryFromBlock =
        (await resolveBlockNumber(factoryAddress.startBlock, chain)) ??
        fromBlock;
      const factoryToBlock =
        (await resolveBlockNumber(factoryAddress.endBlock, chain)) ?? toBlock;
      // Note that this can throw.
      const logFactory = buildLogFactory({
        chainId: chain.id,
        sourceId: source.name,
        ...factoryAddress,
        fromBlock: factoryFromBlock,
        toBlock: factoryToBlock,
      });
      perChainContracts.get(chain.id)![source.name] = {
        abi: source.abi,
        address: undefined,
        startBlock: fromBlock,
        endBlock: toBlock,
      };
      address = logFactory;
    } else {
      if (resolvedAddress !== undefined) {
        for (const address of Array.isArray(resolvedAddress)
          ? resolvedAddress
          : [resolvedAddress as Address]) {
          if (!address!.startsWith("0x"))
            throw new Error(
              `Validation failed: Invalid prefix for address '${address}'. Got '${address!.slice(
                0,
                2,
              )}', expected '0x'.`,
            );
          if (address!.length !== 42)
            throw new Error(
              `Validation failed: Invalid length for address '${address}'. Got ${address!.length}, expected 42 characters.`,
            );
        }
      }
      const validatedAddress = Array.isArray(resolvedAddress)
        ? dedupe(resolvedAddress).map((r) => toLowerCase(r))
        : resolvedAddress !== undefined
          ? toLowerCase(resolvedAddress as Address)
          : undefined;
      perChainContracts.get(chain.id)![source.name] = {
        abi: source.abi,
        address: validatedAddress,
        startBlock: fromBlock,
        endBlock: toBlock,
      };
      address = validatedAddress;
    }
    const filteredEventSelectors = new Map<
      Hex,
      ReturnType<typeof buildTopics>[number]
    >();
    if (source.filter) {
      const eventFilters = Array.isArray(source.filter)
        ? source.filter
        : [source.filter];
      for (const filter of eventFilters) {
        const abiEvent = source.abi.find(
          (item): item is AbiEvent =>
            item.type === "event" &&
            toSafeName({ abi: source.abi, item }) === filter.event,
        );
        if (!abiEvent) {
          throw new Error(
            `Validation failed: Invalid filter for contract '${
              source.name
            }'. Got event name '${filter.event}', expected one of [${source.abi
              .filter((item): item is AbiEvent => item.type === "event")
              .map((item) => `'${toSafeName({ abi: source.abi, item })}'`)
              .join(", ")}].`,
          );
        }
      }
      const topics = buildTopics(source.abi, eventFilters);
      for (const { topic0, topic1, topic2, topic3 } of topics) {
        const abiItem = source.abi.find(
          (item): item is AbiEvent =>
            item.type === "event" && toEventSelector(item) === topic0,
        )!;
        const indexingFunction = indexingFunctions.find(
          (f) => f.name === `${source.name}:${abiItem.name}`,
        );
        if (indexingFunction === undefined) {
          throw new Error(
            `Validation failed: Event selector '${toSafeName({ abi: source.abi, item: abiItem })}' is used in a filter but does not have a corresponding indexing function.`,
          );
        }
        filteredEventSelectors.set(topic0, { topic0, topic1, topic2, topic3 });
      }
    }
    const registeredLogEvents: string[] = [];
    const registeredCallTraceEvents: string[] = [];
    for (const { name: eventName } of indexingFunctions) {
      // log event
      if (eventName.includes(":")) {
        const [logContractName, logEventName] = eventName.split(":") as [
          string,
          string,
        ];
        if (logContractName === source.name && logEventName !== "setup") {
          registeredLogEvents.push(logEventName);
        }
      }
      // trace event
      if (eventName.includes(".")) {
        const [functionContractName, functionName] = eventName.split(".") as [
          string,
          string,
        ];
        if (source.includeCallTraces !== true) {
          continue;
        }
        if (functionContractName === source.name) {
          registeredCallTraceEvents.push(functionName);
        }
      }
    }
    for (const logEventName of registeredLogEvents) {
      const abiEvent = source.abi.find(
        (item): item is AbiEvent =>
          item.type === "event" &&
          toSafeName({ abi: source.abi, item }) === logEventName,
      );
      if (abiEvent === undefined) {
        throw new Error(
          `Validation failed: Event name for event '${logEventName}' not found in the contract ABI. Got '${logEventName}', expected one of [${source.abi
            .filter((item): item is AbiEvent => item.type === "event")
            .map((item) => `'${toSafeName({ abi: source.abi, item })}'`)
            .join(", ")}].`,
        );
      }
      const eventName = `${source.name}:${logEventName}`;
      const indexingFunction = indexingFunctions.find(
        (f) => f.name === eventName,
      )!;
      let topic1: LogTopic;
      let topic2: LogTopic;
      let topic3: LogTopic;
      const eventSelector = toEventSelector(abiEvent);
      if (filteredEventSelectors.has(eventSelector)) {
        topic1 = filteredEventSelectors.get(eventSelector)!.topic1;
        topic2 = filteredEventSelectors.get(eventSelector)!.topic2;
        topic3 = filteredEventSelectors.get(eventSelector)!.topic3;
      } else {
        topic1 = null;
        topic2 = null;
        topic3 = null;
      }
      const filter = {
        type: "log",
        chainId: chain.id,
        sourceId: source.name,
        address,
        topic0: eventSelector,
        topic1,
        topic2,
        topic3,
        fromBlock,
        toBlock,
        hasTransactionReceipt: source.includeTransactionReceipts ?? false,
        include: defaultLogFilterInclude.concat(
          source.includeTransactionReceipts
            ? defaultTransactionReceiptInclude.map(
                (value) => `transactionReceipt.${value}` as const,
              )
            : [],
        ),
      } satisfies LogFilter;
      const eventCallback = {
        filter,
        name: eventName,
        fn: indexingFunction.fn,
        chain,
        type: "contract",
        abiItem: abiEvent,
        metadata: {
          safeName: logEventName,
          abi: source.abi,
        },
      } satisfies EventCallback;
      perChainEventCallbacks.get(chain.id)!.push(eventCallback);
    }
    for (const functionEventName of registeredCallTraceEvents) {
      const abiFunction = source.abi.find(
        (item): item is AbiFunction =>
          item.type === "function" &&
          toSafeName({ abi: source.abi, item }) === functionEventName,
      );
      if (abiFunction === undefined) {
        throw new Error(
          `Validation failed: Function name for function '${functionEventName}' not found in the contract ABI. Got '${functionEventName}', expected one of [${source.abi
            .filter((item): item is AbiFunction => item.type === "function")
            .map((item) => `'${toSafeName({ abi: source.abi, item })}'`)
            .join(", ")}].`,
        );
      }
      const eventName = `${source.name}.${functionEventName}`;
      const indexingFunction = indexingFunctions.find(
        (f) => f.name === eventName,
      )!;
      const filter = {
        type: "trace",
        chainId: chain.id,
        sourceId: source.name,
        fromAddress: undefined,
        toAddress: address,
        callType: "CALL",
        functionSelector: toFunctionSelector(abiFunction),
        includeReverted: false,
        fromBlock,
        toBlock,
        hasTransactionReceipt: source.includeTransactionReceipts ?? false,
        include: defaultTraceFilterInclude.concat(
          source.includeTransactionReceipts
            ? defaultTransactionReceiptInclude.map(
                (value) => `transactionReceipt.${value}` as const,
              )
            : [],
        ),
      } satisfies TraceFilter;
      const eventCallback = {
        filter,
        name: eventName,
        fn: indexingFunction.fn,
        chain,
        type: "contract",
        abiItem: abiFunction,
        metadata: {
          safeName: functionEventName,
          abi: source.abi,
        },
      } satisfies EventCallback;
      perChainEventCallbacks.get(chain.id)!.push(eventCallback);
    }
    if (
      registeredLogEvents.length === 0 &&
      registeredCallTraceEvents.length === 0
    ) {
      logs.push({
        level: "warn",
        msg: "No registered indexing functions",
        name: source.name,
        type: "contract",
      });
    }
  }
  for (const source of flattenSources(config.accounts ?? {})) {
    const chain = chains.find((n) => n.name === source.chain)!;
    const fromBlock = await resolveBlockNumber(source.startBlock, chain);
    const toBlock = await resolveBlockNumber(source.endBlock, chain);
    const resolvedAddress = source?.address;
    if (resolvedAddress === undefined) {
      throw new Error(
        `Validation failed: Account '${source.name}' must specify an 'address'.`,
      );
    }
    let address: FilterAddress;
    if (
      typeof resolvedAddress === "object" &&
      !Array.isArray(resolvedAddress)
    ) {
      const factoryFromBlock =
        (await resolveBlockNumber(resolvedAddress.startBlock, chain)) ??
        fromBlock;
      const factoryToBlock =
        (await resolveBlockNumber(resolvedAddress.endBlock, chain)) ?? toBlock;
      // Note that this can throw.
      const logFactory = buildLogFactory({
        chainId: chain.id,
        sourceId: source.name,
        ...resolvedAddress,
        fromBlock: factoryFromBlock,
        toBlock: factoryToBlock,
      });
      address = logFactory;
    } else {
      for (const address of Array.isArray(resolvedAddress)
        ? resolvedAddress
        : [resolvedAddress]) {
        if (!address!.startsWith("0x"))
          throw new Error(
            `Validation failed: Invalid prefix for address '${address}'. Got '${address!.slice(
              0,
              2,
            )}', expected '0x'.`,
          );
        if (address!.length !== 42)
          throw new Error(
            `Validation failed: Invalid length for address '${address}'. Got ${address!.length}, expected 42 characters.`,
          );
      }
      const validatedAddress = Array.isArray(resolvedAddress)
        ? dedupe(resolvedAddress).map((r) => toLowerCase(r))
        : resolvedAddress !== undefined
          ? toLowerCase(resolvedAddress)
          : undefined;
      address = validatedAddress;
    }
    const filters = [
      {
        type: "transaction",
        chainId: chain.id,
        sourceId: source.name,
        fromAddress: undefined,
        toAddress: address,
        includeReverted: false,
        fromBlock,
        toBlock,
        hasTransactionReceipt: true,
        include: defaultTransactionFilterInclude,
      },
      {
        type: "transaction",
        chainId: chain.id,
        sourceId: source.name,
        fromAddress: address,
        toAddress: undefined,
        includeReverted: false,
        fromBlock,
        toBlock,
        hasTransactionReceipt: true,
        include: defaultTransactionFilterInclude,
      },
      {
        type: "transfer",
        chainId: chain.id,
        sourceId: source.name,
        fromAddress: undefined,
        toAddress: address,
        includeReverted: false,
        fromBlock,
        toBlock,
        hasTransactionReceipt: source.includeTransactionReceipts ?? false,
        include: defaultTransferFilterInclude.concat(
          source.includeTransactionReceipts
            ? defaultTransactionReceiptInclude.map(
                (value) => `transactionReceipt.${value}` as const,
              )
            : [],
        ),
      },
      {
        type: "transfer",
        chainId: chain.id,
        sourceId: source.name,
        fromAddress: address,
        toAddress: undefined,
        includeReverted: false,
        fromBlock,
        toBlock,
        hasTransactionReceipt: source.includeTransactionReceipts ?? false,
        include: defaultTransferFilterInclude.concat(
          source.includeTransactionReceipts
            ? defaultTransactionReceiptInclude.map(
                (value) => `transactionReceipt.${value}` as const,
              )
            : [],
        ),
      },
    ] satisfies [
      TransactionFilter,
      TransactionFilter,
      TransferFilter,
      TransferFilter,
    ];
    let hasRegisteredIndexingFunction = false;
    for (const filter of filters) {
      const eventName =
        filter.type === "transaction"
          ? filter.fromAddress === undefined
            ? `${source.name}:transaction:to`
            : `${source.name}:transaction:from`
          : filter.fromAddress === undefined
            ? `${source.name}:transfer:to`
            : `${source.name}:transfer:from`;
      const indexingFunction = indexingFunctions.find(
        (f) => f.name === eventName,
      );
      if (indexingFunction) {
        hasRegisteredIndexingFunction = true;
        const eventCallback = {
          filter,
          name: eventName,
          fn: indexingFunction.fn,
          chain,
          type: "account",
          direction: filter.fromAddress === undefined ? "to" : "from",
        } satisfies EventCallback;
        perChainEventCallbacks.get(chain.id)!.push(eventCallback);
      }
    }
    if (hasRegisteredIndexingFunction === false) {
      logs.push({
        level: "warn",
        msg: "No registered indexing functions",
        name: source.name,
        type: "account",
      });
    }
  }
  for (const source of flattenSources(config.blocks ?? {})) {
    const chain = chains.find((n) => n.name === source.chain)!;
    const intervalMaybeNan = source.interval ?? 1;
    const interval = Number.isNaN(intervalMaybeNan) ? 0 : intervalMaybeNan;
    if (!Number.isInteger(interval) || interval === 0) {
      throw new Error(
        `Validation failed: Invalid interval for block interval '${source.name}'. Got ${interval}, expected a non-zero integer.`,
      );
    }
    const fromBlock = await resolveBlockNumber(source.startBlock, chain);
    const toBlock = await resolveBlockNumber(source.endBlock, chain);
    const eventName = `${source.name}:block`;
    const indexingFunction = indexingFunctions.find(
      (f) => f.name === eventName,
    );
    if (indexingFunction) {
      const filter = {
        type: "block",
        chainId: chain.id,
        sourceId: source.name,
        interval: interval,
        offset: (fromBlock ?? 0) % interval,
        fromBlock,
        toBlock,
        hasTransactionReceipt: false,
        include: defaultBlockFilterInclude,
      } satisfies BlockFilter;
      const eventCallback = {
        filter,
        name: eventName,
        fn: indexingFunction.fn,
        chain,
        type: "block",
      } satisfies EventCallback;
      perChainEventCallbacks.get(chain.id)!.push(eventCallback);
    } else {
      logs.push({
        level: "warn",
        msg: "No registered indexing functions",
        name: source.name,
        type: "block",
      });
    }
  }
  // Filter out any chains that don't have any sources registered.
  const chainsWithSources: Chain[] = [];
  const rpcsWithSources: Rpc[] = [];
  const finalizedBlocksWithSources: LightBlock[] = [];
  const eventCallbacksWithSources: EventCallback[][] = [];
  const setupCallbacksWithSources: SetupCallback[][] = [];
  const contractsWithSources: { [name: string]: Contract }[] = [];
  for (let i = 0; i < chains.length; i++) {
    const chain = chains[i]!;
    const rpc = rpcs[i]!;
    const hasIndexingFunctions =
      perChainEventCallbacks.get(chain.id)!.length > 0 ||
      perChainSetupCallbacks.get(chain.id)!.length > 0;
    if (hasIndexingFunctions) {
      chainsWithSources.push(chain);
      rpcsWithSources.push(rpc);
      finalizedBlocksWithSources.push(finalizedBlocks[i]!);
      eventCallbacksWithSources.push(perChainEventCallbacks.get(chain.id)!);
      setupCallbacksWithSources.push(perChainSetupCallbacks.get(chain.id)!);
      contractsWithSources.push(perChainContracts.get(chain.id)!);
    } else {
      logs.push({
        level: "warn",
        msg: "No registered indexing functions",
        chain: chain.name,
        chain_id: chain.id,
      });
    }
  }
  if (chainsWithSources.length === 0) {
    throw new Error(
      "Validation failed: Found 0 chains with registered indexing functions.",
    );
  }
  return {
    chains: chainsWithSources,
    rpcs: rpcsWithSources,
    finalizedBlocks: finalizedBlocksWithSources,
    eventCallbacks: eventCallbacksWithSources,
    setupCallbacks: setupCallbacksWithSources,
    contracts: contractsWithSources,
    logs,
  };
}
export function buildConfig({
  common,
  config,
}: { common: Common; config: Config }): {
  chains: Chain[];
  rpcs: Rpc[];
  logs: ({ level: "warn" | "info" | "debug"; msg: string } & Record<
    string,
    unknown
  >)[];
} {
  const logs: ({ level: "warn" | "info" | "debug"; msg: string } & Record<
    string,
    unknown
  >)[] = [];
  const chains: Chain[] = Object.entries(config.chains).map(
    ([chainName, chain]) => {
      if (chain.id > Number.MAX_SAFE_INTEGER) {
        throw new Error(
          `Chain "${chainName}" with id ${chain.id} has a chain_id that is too large.`,
        );
      }
      let matchedChain = Object.values(viemChains).find((c) =>
        "id" in c ? c.id === chain.id : false,
      );
      if (chain.id === 999) {
        matchedChain = hyperliquidEvm;
      }
      if (chain.rpc === undefined || chain.rpc === "") {
        if (matchedChain === undefined) {
          throw new Error(
            `Chain "${chainName}" with id ${chain.id} has no RPC defined and no default RPC URL was found in 'viem/chains'.`,
          );
        }
        chain.rpc = matchedChain.rpcUrls.default.http as string[];
      }
      if (typeof chain.rpc === "string" || Array.isArray(chain.rpc)) {
        const rpcs = Array.isArray(chain.rpc) ? chain.rpc : [chain.rpc];
        if (rpcs.length === 0) {
          throw new Error(
            `Chain "${chainName}" with id ${chain.id} has no RPC URLs.`,
          );
        }
        if (matchedChain) {
          for (const rpc of rpcs) {
            for (const http of matchedChain.rpcUrls.default.http) {
              if (http === rpc) {
                logs.push({
                  level: "warn",
                  msg: "Detected public RPC URL. Most apps require an RPC URL with a higher rate limit.",
                  chain: chainName,
                  chain_id: chain.id,
                  url: http,
                });
                break;
              }
            }
            for (const ws of matchedChain.rpcUrls.default.webSocket ?? []) {
              if (ws === rpc) {
                logs.push({
                  level: "warn",
                  msg: "Detected public RPC URL. Most apps require an RPC URL with a higher rate limit.",
                  chain: chainName,
                  chain_id: chain.id,
                  url: ws,
                });
                break;
              }
            }
          }
        }
      }
      if (chain.pollingInterval !== undefined && chain.pollingInterval! < 100) {
        throw new Error(
          `Invalid 'pollingInterval' for chain '${chainName}. Expected 100 milliseconds or greater, got ${chain.pollingInterval} milliseconds.`,
        );
      }
      return {
        id: chain.id,
        name: chainName,
        rpc: chain.rpc,
        ws: chain.ws,
        pollingInterval: chain.pollingInterval ?? 1_000,
        finalityBlockCount: getFinalityBlockCount({ chain: matchedChain }),
        disableCache: chain.disableCache ?? false,
        ethGetLogsBlockRange: chain.ethGetLogsBlockRange,
        viemChain: matchedChain,
      } satisfies Chain;
    },
  );
  const chainIds = new Set<number>();
  for (const chain of chains) {
    if (chainIds.has(chain.id)) {
      throw new Error(
        `Invalid id for chain "${chain.name}". ${chain.id} is already in use.`,
      );
    }
    chainIds.add(chain.id);
  }
  const rpcs = chains.map((chain) =>
    createRpc({
      common,
      chain,
      concurrency: Math.floor(common.options.rpcMaxConcurrency / chains.length),
    }),
  );
  return { chains, rpcs, logs };
}
export async function safeBuildIndexingFunctions({
  common,
  config,
  indexingFunctions,
  configBuild,
}: {
  common: Common;
  config: Config;
  indexingFunctions: IndexingFunctions;
  configBuild: Pick<IndexingBuild, "chains" | "rpcs">;
}) {
  try {
    const result = await buildIndexingFunctions({
      common,
      config,
      indexingFunctions,
      configBuild,
    });
    return {
      status: "success",
      chains: result.chains,
      rpcs: result.rpcs,
      finalizedBlocks: result.finalizedBlocks,
      eventCallbacks: result.eventCallbacks,
      setupCallbacks: result.setupCallbacks,
      contracts: result.contracts,
      logs: result.logs,
    } as const;
  } catch (_error) {
    const buildError = new BuildError((_error as Error).message);
    buildError.stack = undefined;
    return { status: "error", error: buildError } as const;
  }
}
export function safeBuildConfig({
  common,
  config,
}: { common: Common; config: Config }) {
  try {
    const result = buildConfig({ common, config });
    return {
      status: "success",
      chains: result.chains,
      rpcs: result.rpcs,
      logs: result.logs,
    } as const;
  } catch (_error) {
    const buildError = new BuildError((_error as Error).message);
    buildError.stack = undefined;
    return { status: "error", error: buildError } as const;
  }
}
</file>

<file path="packages/core/src/build/factory.test.ts">
import { getEventSelector, parseAbiItem } from "viem";
import { expect, test } from "vitest";
import { buildLogFactory } from "./factory.js";
const llamaFactoryEventAbiItem = parseAbiItem(
  "event LlamaInstanceCreated(address indexed deployer, string indexed name, address llamaCore, address llamaExecutor, address llamaPolicy, uint256 chainId)",
);
test("buildLogFactory throws if provided parameter not found in inputs", () => {
  expect(() =>
    buildLogFactory({
      address: "0xa",
      event: llamaFactoryEventAbiItem,
      parameter: "fakeParameter",
      chainId: 1,
      sourceId: "Llama",
      fromBlock: undefined,
      toBlock: undefined,
    }),
  ).toThrowError(
    "Factory event parameter not found in factory event signature. Got 'fakeParameter', expected one of ['deployer', 'name', 'llamaCore', 'llamaExecutor', 'llamaPolicy', 'chainId'].",
  );
});
test("buildLogFactory handles LlamaInstanceCreated llamaCore", () => {
  const criteria = buildLogFactory({
    address: "0xa",
    event: llamaFactoryEventAbiItem,
    parameter: "llamaCore",
    chainId: 1,
    sourceId: "Llama",
    fromBlock: undefined,
    toBlock: undefined,
  });
  expect(criteria).toMatchObject({
    address: "0xa",
    eventSelector: getEventSelector(llamaFactoryEventAbiItem),
    childAddressLocation: "offset0",
  });
});
test("buildLogFactory handles LlamaInstanceCreated llamaPolicy", () => {
  const criteria = buildLogFactory({
    address: "0xa",
    event: llamaFactoryEventAbiItem,
    parameter: "llamaPolicy",
    chainId: 1,
    sourceId: "Llama",
    fromBlock: undefined,
    toBlock: undefined,
  });
  expect(criteria).toMatchObject({
    address: "0xa",
    eventSelector: getEventSelector(llamaFactoryEventAbiItem),
    childAddressLocation: "offset64",
  });
});
const morphoFactoryEvent = parseAbiItem([
  "struct MarketParams { address loanToken; address collateralToken; address oracle; address irm; uint256 lltv;}",
  "event CreateMarket(bytes32 indexed id, MarketParams marketParams)",
]);
test("buildLogFactory handles Morpho CreateMarket struct parameter", () => {
  const criteria = buildLogFactory({
    address: "0xa",
    event: morphoFactoryEvent,
    parameter: "marketParams.oracle",
    chainId: 1,
    sourceId: "Llama",
    fromBlock: undefined,
    toBlock: undefined,
  });
  expect(criteria).toMatchObject({
    address: "0xa",
    eventSelector:
      "0xac4b2400f169220b0c0afdde7a0b32e775ba727ea1cb30b35f935cdaab8683ac",
    childAddressLocation: "offset64",
  });
});
const zoraFactoryEvent = parseAbiItem([
  "struct PoolKey { address currency0; address currency1; uint24 fee;int24 tickSpacing; address hooks; }",
  "event CoinCreatedV4(address indexed caller,address indexed payoutRecipient,address indexed platformReferrer,address currency,string uri,string name,string symbol,address coin,PoolKey poolKey,bytes32 poolKeyHash,string version)",
]);
test("buildLogFactory handles Morpho CreateMarket struct parameter", () => {
  const criteria = buildLogFactory({
    address: "0xa",
    event: zoraFactoryEvent,
    parameter: "poolKey.hooks",
    chainId: 1,
    sourceId: "Llama",
    fromBlock: undefined,
    toBlock: undefined,
  });
  expect(criteria).toMatchObject({
    address: "0xa",
    eventSelector:
      "0x2de436107c2096e039c98bbcc3c5a2560583738ce15c234557eecb4d3221aa81",
    childAddressLocation: "offset288",
  });
});
</file>

<file path="packages/core/src/build/factory.ts">
import type { LogFactory } from "@/internal/types.js";
import { dedupe } from "@/utils/dedupe.js";
import { toLowerCase } from "@/utils/lowercase.js";
import {
  type TupleAbiParameter,
  getBytesConsumedByParam,
  getNestedParamOffset,
} from "@/utils/offset.js";
import type { AbiEvent } from "abitype";
import { type Address, toEventSelector } from "viem";
export function buildLogFactory({
  address: _address,
  event,
  parameter,
  chainId,
  sourceId,
  fromBlock,
  toBlock,
}: {
  address?: Address | readonly Address[];
  event: AbiEvent;
  parameter: string;
  chainId: number;
  sourceId: string;
  fromBlock: number | undefined;
  toBlock: number | undefined;
}): LogFactory {
  let address: Address | Address[] | undefined;
  if (_address === undefined) {
    // noop
  } else if (Array.isArray(_address)) {
    address = dedupe(_address)
      .map(toLowerCase)
      .sort((a, b) => (a < b ? -1 : 1));
  } else {
    address = toLowerCase(_address);
  }
  const eventSelector = toEventSelector(event);
  const params = parameter.split(".");
  if (params.length === 1) {
    // Check if the provided parameter is present in the list of indexed inputs.
    const indexedInputPosition = event.inputs
      .filter((x) => "indexed" in x && x.indexed)
      .findIndex((input) => {
        return input.name === params[0];
      });
    if (indexedInputPosition > -1) {
      return {
        id: `log_${Array.isArray(address) ? address.join("_") : address}_${chainId}_topic${(indexedInputPosition + 1) as 1 | 2 | 3}_${eventSelector}_${fromBlock ?? "undefined"}_${toBlock ?? "undefined"}`,
        type: "log",
        chainId,
        sourceId,
        address,
        eventSelector,
        // Add 1 because inputs will not contain an element for topic0 (the signature).
        childAddressLocation: `topic${(indexedInputPosition + 1) as 1 | 2 | 3}`,
        fromBlock,
        toBlock,
      };
    }
  }
  const nonIndexedInputs = event.inputs.filter(
    (x) => !("indexed" in x && x.indexed),
  );
  const nonIndexedInputPosition = nonIndexedInputs.findIndex(
    (input) => input.name === params[0],
  );
  if (nonIndexedInputPosition === -1) {
    throw new Error(
      `Factory event parameter not found in factory event signature. Got '${parameter}', expected one of [${event.inputs
        .map((i) => `'${i.name}'`)
        .join(", ")}].`,
    );
  }
  const nonIndexedParameter = nonIndexedInputs[nonIndexedInputPosition]!;
  if (nonIndexedParameter.type !== "address" && params.length === 1) {
    throw new Error(
      `Factory event parameter type is not valid. Got '${nonIndexedParameter.type}', expected 'address'.`,
    );
  }
  if (params.length > 1 && nonIndexedParameter.type !== "tuple") {
    throw new Error(
      `Factory event parameter type is not valid. Got '${nonIndexedParameter.type}', expected 'tuple'.`,
    );
  }
  let offset = 0;
  for (let i = 0; i < nonIndexedInputPosition; i++) {
    offset += getBytesConsumedByParam(nonIndexedInputs[i]!);
  }
  if (params.length > 1) {
    const nestedOffset = getNestedParamOffset(
      nonIndexedInputs[nonIndexedInputPosition]! as TupleAbiParameter,
      params.slice(1),
    );
    offset += nestedOffset;
  }
  return {
    id: `log_${Array.isArray(address) ? address.join("_") : address}_${chainId}_offset${offset}_${eventSelector}_${fromBlock ?? "undefined"}_${toBlock ?? "undefined"}`,
    type: "log",
    chainId,
    sourceId,
    address,
    eventSelector,
    childAddressLocation: `offset${offset}`,
    fromBlock,
    toBlock,
  };
}
</file>

<file path="packages/core/src/build/index.ts">
import { createHash } from "node:crypto";
import fs from "node:fs";
import path from "node:path";
import type { CliOptions } from "@/bin/ponder.js";
import type { Config } from "@/config/index.js";
import type { Database } from "@/database/index.js";
import { createQB } from "@/database/queryBuilder.js";
import { MAX_DATABASE_OBJECT_NAME_LENGTH } from "@/drizzle/onchain.js";
import type { Common } from "@/internal/common.js";
import {
  BuildError,
  NonRetryableUserError,
  RetryableError,
} from "@/internal/errors.js";
import type {
  ApiBuild,
  IndexingBuild,
  IndexingFunctions,
  NamespaceBuild,
  PreBuild,
  Schema,
  SchemaBuild,
} from "@/internal/types.js";
import { createPool, getDatabaseName } from "@/utils/pg.js";
import { createPglite } from "@/utils/pglite.js";
import { getNextAvailablePort } from "@/utils/port.js";
import type { Result } from "@/utils/result.js";
import { startClock } from "@/utils/timer.js";
import { drizzle as drizzleNodePostgres } from "drizzle-orm/node-postgres";
import { drizzle as drizzlePglite } from "drizzle-orm/pglite";
import { glob } from "glob";
import { Hono } from "hono";
import superjson from "superjson";
import { hexToNumber } from "viem";
import { createServer } from "vite";
import { ViteNodeRunner } from "vite-node/client";
import { ViteNodeServer } from "vite-node/server";
import { installSourcemapsSupport } from "vite-node/source-map";
import { normalizeModuleId, toFilePath } from "vite-node/utils";
import viteTsconfigPathsPlugin from "vite-tsconfig-paths";
import { safeBuildConfig, safeBuildIndexingFunctions } from "./config.js";
import { vitePluginPonder } from "./plugin.js";
import { safeBuildPre } from "./pre.js";
import { safeBuildSchema } from "./schema.js";
import { parseViteNodeError } from "./stacktrace.js";
declare global {
  var PONDER_COMMON: Common;
  var PONDER_PRE_BUILD: PreBuild;
  var PONDER_NAMESPACE_BUILD: NamespaceBuild;
  var PONDER_INDEXING_BUILD: Pick<IndexingBuild, "chains" | "rpcs">;
  var PONDER_DATABASE: Database;
}
const BUILD_ID_VERSION = "2";
type ConfigResult = Result<{ config: Config; contentHash: string }>;
type SchemaResult = Result<{ schema: Schema; contentHash: string }>;
type IndexingResult = Result<{
  indexingFunctions: IndexingFunctions;
  contentHash: string;
}>;
type ApiResult = Result<{ app: Hono }>;
export type Build = {
  executeConfig: () => Promise<ConfigResult>;
  executeSchema: () => Promise<SchemaResult>;
  executeIndexingFunctions: () => Promise<IndexingResult>;
  executeApi: (params: {
    preBuild: PreBuild;
    configBuild: Pick<IndexingBuild, "chains" | "rpcs">;
    database: Database;
  }) => Promise<ApiResult>;
  namespaceCompile: () => Result<NamespaceBuild>;
  preCompile: (params: { config: Config }) => Result<PreBuild>;
  compileSchema: (params: {
    schema: Schema;
    preBuild: PreBuild;
  }) => Result<SchemaBuild>;
  compileConfig: (params: {
    configResult: Extract<ConfigResult, { status: "success" }>["result"];
  }) => Result<Pick<IndexingBuild, "chains" | "rpcs">>;
  compileIndexing: (params: {
    configResult: Extract<ConfigResult, { status: "success" }>["result"];
    schemaResult: Extract<SchemaResult, { status: "success" }>["result"];
    indexingResult: Extract<IndexingResult, { status: "success" }>["result"];
    configBuild: Pick<IndexingBuild, "chains" | "rpcs">;
  }) => Promise<Result<IndexingBuild>>;
  compileApi: (params: {
    apiResult: Extract<ApiResult, { status: "success" }>["result"];
  }) => Promise<Result<ApiBuild>>;
  startDev: (params: { onReload: (kind: "indexing" | "api") => void }) => void;
  rpcDiagnostic: (params: {
    configBuild: Pick<IndexingBuild, "chains" | "rpcs">;
  }) => Promise<Result<void>>;
  databaseDiagnostic: (params: { preBuild: PreBuild }) => Promise<Result<void>>;
};
export const createBuild = async ({
  common,
  cliOptions,
}: {
  common: Common;
  cliOptions: CliOptions;
}): Promise<Build> => {
  const escapeRegex = /[.*+?^${}()|[\]\\]/g;
  globalThis.PONDER_COMMON = common;
  const escapedIndexingDir = common.options.indexingDir
    // If on Windows, use a POSIX path for this regex.
    .replace(/\\/g, "/")
    // Escape special characters in the path.
    .replace(escapeRegex, "\\$&");
  const indexingRegex = new RegExp(`^${escapedIndexingDir}/.*\\.(ts|js)$`);
  const indexingPattern = path
    .join(common.options.indexingDir, "**/*.{js,mjs,ts,mts}")
    .replace(/\\/g, "/");
  const apiPattern = path
    .join(common.options.apiDir, "**/*.{js,mjs,ts,mts}")
    .replace(/\\/g, "/");
  const viteLogger = {
    warnedMessages: new Set<string>(),
    loggedErrors: new WeakSet<Error>(),
    hasWarned: false,
    clearScreen() {},
    hasErrorLogged: (error: Error) => viteLogger.loggedErrors.has(error),
    info: (msg: string) => {
      common.logger.trace({ msg, action: "build" });
    },
    warn: (msg: string) => {
      viteLogger.hasWarned = true;
      common.logger.trace({ msg, action: "build" });
    },
    warnOnce: (msg: string) => {
      if (viteLogger.warnedMessages.has(msg)) return;
      viteLogger.hasWarned = true;
      common.logger.trace({ msg, action: "build" });
      viteLogger.warnedMessages.add(msg);
    },
    error: (msg: string) => {
      viteLogger.hasWarned = true;
      common.logger.trace({ msg, action: "build" });
    },
  };
  const viteDevServer = await createServer({
    root: common.options.rootDir,
    cacheDir: path.join(common.options.ponderDir, "vite"),
    publicDir: false,
    customLogger: viteLogger,
    server: { hmr: false },
    plugins: [viteTsconfigPathsPlugin(), vitePluginPonder(common.options)],
  });
  common.buildShutdown.add(() => viteDevServer.close());
  // This is Vite boilerplate (initializes the Rollup container).
  await viteDevServer.pluginContainer.buildStart({});
  const viteNodeServer = new ViteNodeServer(viteDevServer);
  installSourcemapsSupport({
    getSourceMap: (source) => viteNodeServer.getSourceMap(source),
  });
  const viteNodeRunner = new ViteNodeRunner({
    root: viteDevServer.config.root,
    fetchModule: (id) => viteNodeServer.fetchModule(id, "ssr"),
    resolveId: (id, importer) => viteNodeServer.resolveId(id, importer, "ssr"),
    debug: (process.env.DEBUG ?? "").includes("vite-node"),
  });
  const executeFile = async ({
    file,
  }: { file: string }): Promise<
    { status: "success"; exports: any } | { status: "error"; error: Error }
  > => {
    try {
      const exports = await viteNodeRunner.executeFile(file);
      return { status: "success", exports } as const;
    } catch (error_) {
      const relativePath = path.relative(common.options.rootDir, file);
      const error = parseViteNodeError(relativePath, error_ as Error);
      return { status: "error", error } as const;
    }
  };
  const executeFileWithTimeout = async ({
    file,
  }: { file: string }): Promise<
    { status: "success"; exports: any } | { status: "error"; error: Error }
  > => {
    let timeoutId: ReturnType<typeof setTimeout>;
    const timeout = new Promise<NonRetryableUserError>((resolve) => {
      timeoutId = setTimeout(
        () =>
          resolve(
            new NonRetryableUserError(
              "File execution did not complete (waited 10s)",
            ),
          ),
        10_000,
      );
    });
    const res = await Promise.race([executeFile({ file }), timeout]);
    if (res instanceof NonRetryableUserError) {
      return { status: "error", error: res };
    }
    clearTimeout(timeoutId!);
    return res;
  };
  const build = {
    async executeConfig(): Promise<ConfigResult> {
      const executeResult = await executeFile({
        file: common.options.configFile,
      });
      if (executeResult.status === "error") {
        common.logger.error({
          msg: "Error while executing file",
          file: "ponder.config.ts",
          error: executeResult.error,
        });
        return executeResult;
      }
      const config = executeResult.exports.default as Config;
      const contentHash = createHash("sha256")
        .update(
          superjson.stringify({
            ordering: config.ordering,
            contracts: config.contracts,
            accounts: config.accounts,
            blocks: config.blocks,
          }),
        )
        .digest("hex");
      return {
        status: "success",
        result: { config, contentHash },
      } as const;
    },
    async executeSchema(): Promise<SchemaResult> {
      const executeResult = await executeFile({
        file: common.options.schemaFile,
      });
      if (executeResult.status === "error") {
        common.logger.error({
          msg: "Error while executing file",
          file: "ponder.schema.ts",
          error: executeResult.error,
        });
        return executeResult;
      }
      const schema = executeResult.exports;
      const contents = fs.readFileSync(common.options.schemaFile, "utf-8");
      return {
        status: "success",
        result: {
          schema,
          contentHash: createHash("sha256").update(contents).digest("hex"),
        },
      } as const;
    },
    async executeIndexingFunctions(): Promise<IndexingResult> {
      const files = glob.sync(indexingPattern, {
        ignore: apiPattern,
      });
      for (const file of files) {
        const executeResult = await executeFileWithTimeout({ file });
        if (executeResult.status === "error") {
          common.logger.error({
            msg: "Error while executing file",
            file: path.relative(common.options.rootDir, file),
            error: executeResult.error,
          });
          return executeResult;
        }
      }
      // Note that we are only hashing the file contents, not the exports. This is
      // different from the config/schema, where we include the serializable object itself.
      const hash = createHash("sha256");
      for (const file of files) {
        try {
          const contents = fs.readFileSync(file, "utf-8");
          hash.update(contents);
        } catch (e) {
          common.logger.warn({
            msg: "Unable to read file",
            file,
          });
          hash.update(file);
        }
      }
      const contentHash = hash.digest("hex");
      const exports = await viteNodeRunner.executeId("ponder:registry");
      return {
        status: "success",
        result: {
          indexingFunctions: exports.ponder.fns,
          contentHash,
        },
      };
    },
    async executeApi({ preBuild, configBuild, database }): Promise<ApiResult> {
      globalThis.PONDER_PRE_BUILD = preBuild;
      globalThis.PONDER_INDEXING_BUILD = configBuild;
      globalThis.PONDER_DATABASE = database;
      if (!fs.existsSync(common.options.apiFile)) {
        const error = new BuildError(
          `API endpoint file not found. Create a file at ${common.options.apiFile}. Read more: https://ponder.sh/docs/api-reference/ponder/api-endpoints`,
        );
        error.stack = undefined;
        return { status: "error", error };
      }
      const executeResult = await executeFile({
        file: common.options.apiFile,
      });
      if (executeResult.status === "error") {
        common.logger.error({
          msg: "Error while executing file",
          file: path.relative(common.options.rootDir, common.options.apiFile),
          error: executeResult.error,
        });
        return executeResult;
      }
      const app = executeResult.exports.default;
      if (!(app instanceof Hono || app?.constructor?.name === "Hono")) {
        const error = new BuildError(
          "API endpoint file does not export a Hono instance as the default export. Read more: https://ponder.sh/docs/api-reference/ponder/api-endpoints",
        );
        error.stack = undefined;
        return { status: "error", error };
      }
      return {
        status: "success",
        result: { app },
      };
    },
    namespaceCompile() {
      if (
        cliOptions.schema === undefined &&
        process.env.DATABASE_SCHEMA === undefined
      ) {
        const error = new BuildError(
          `Database schema required. Specify with "DATABASE_SCHEMA" env var or "--schema" CLI flag. Read more: https://ponder.sh/docs/database#database-schema`,
        );
        error.stack = undefined;
        return { status: "error", error } as const;
      }
      const schema = cliOptions.schema ?? process.env.DATABASE_SCHEMA!;
      const viewsSchema =
        cliOptions.viewsSchema ?? process.env.DATABASE_VIEWS_SCHEMA;
      if (viewsSchema === schema) {
        const error = new BuildError(
          "Views schema cannot be the same as the schema.",
        );
        error.stack = undefined;
        return { status: "error", error } as const;
      }
      if (schema === "ponder_sync") {
        const error = new BuildError(
          `Invalid schema name. "ponder_sync" is a reserved schema name.`,
        );
        error.stack = undefined;
        return { status: "error", error } as const;
      }
      if (viewsSchema === "ponder_sync") {
        const error = new BuildError(
          `Invalid views schema name. "ponder_sync" is a reserved schema name.`,
        );
        error.stack = undefined;
        return { status: "error", error } as const;
      }
      if (schema.length > MAX_DATABASE_OBJECT_NAME_LENGTH) {
        const error = new BuildError(
          `Schema name cannot be longer than ${MAX_DATABASE_OBJECT_NAME_LENGTH} characters.`,
        );
        error.stack = undefined;
        return { status: "error", error } as const;
      }
      if (viewsSchema && viewsSchema.length > MAX_DATABASE_OBJECT_NAME_LENGTH) {
        const error = new BuildError(
          `Views schema name cannot be longer than ${MAX_DATABASE_OBJECT_NAME_LENGTH} characters.`,
        );
        error.stack = undefined;
        return { status: "error", error } as const;
      }
      globalThis.PONDER_NAMESPACE_BUILD = { schema, viewsSchema };
      return {
        status: "success",
        result: { schema, viewsSchema },
      } as const;
    },
    preCompile({ config }): Result<PreBuild> {
      const preBuild = safeBuildPre({
        config,
        options: common.options,
        logger: common.logger,
      });
      if (preBuild.status === "error") {
        return preBuild;
      }
      return {
        status: "success",
        result: {
          databaseConfig: preBuild.databaseConfig,
          ordering: preBuild.ordering,
        },
      } as const;
    },
    compileSchema({ schema, preBuild }) {
      const buildSchemaResult = safeBuildSchema({ schema, preBuild });
      if (buildSchemaResult.status === "error") {
        return buildSchemaResult;
      }
      return {
        status: "success",
        result: {
          schema,
          statements: buildSchemaResult.statements,
        },
      } as const;
    },
    compileConfig({ configResult }) {
      // Validates and builds the config
      const buildConfigResult = safeBuildConfig({
        common,
        config: configResult.config,
      });
      if (buildConfigResult.status === "error") {
        return buildConfigResult;
      }
      for (const log of buildConfigResult.logs) {
        const { level, ...rest } = log;
        common.logger[level](rest);
      }
      return {
        status: "success",
        result: {
          chains: buildConfigResult.chains,
          rpcs: buildConfigResult.rpcs,
        },
      } as const;
    },
    async compileIndexing({
      configResult,
      schemaResult,
      indexingResult,
      configBuild,
    }) {
      // Validates and builds the config
      const buildIndexingFunctionsResult = await safeBuildIndexingFunctions({
        common,
        config: configResult.config,
        indexingFunctions: indexingResult.indexingFunctions,
        configBuild,
      });
      if (buildIndexingFunctionsResult.status === "error") {
        return buildIndexingFunctionsResult;
      }
      for (const log of buildIndexingFunctionsResult.logs) {
        const { level, ...rest } = log;
        common.logger[level](rest);
      }
      const buildId = createHash("sha256")
        .update(BUILD_ID_VERSION)
        .update(configResult.contentHash)
        .update(schemaResult.contentHash)
        .update(indexingResult.contentHash)
        .digest("hex")
        .slice(0, 10);
      return {
        status: "success",
        result: {
          buildId,
          chains: buildIndexingFunctionsResult.chains,
          rpcs: buildIndexingFunctionsResult.rpcs,
          finalizedBlocks: buildIndexingFunctionsResult.finalizedBlocks,
          eventCallbacks: buildIndexingFunctionsResult.eventCallbacks,
          setupCallbacks: buildIndexingFunctionsResult.setupCallbacks,
          contracts: buildIndexingFunctionsResult.contracts,
          indexingFunctions: indexingResult.indexingFunctions,
        },
      } as const;
    },
    async compileApi({ apiResult }) {
      for (const route of apiResult.app.routes) {
        if (typeof route.path === "string") {
          if (
            route.path === "/ready" ||
            route.path === "/status" ||
            route.path === "/metrics" ||
            route.path === "/health" ||
            route.path === "/client"
          ) {
            const error = new BuildError(
              `Validation failed: API route "${route.path}" is reserved for internal use.`,
            );
            error.stack = undefined;
            return { status: "error", error } as const;
          }
        }
      }
      const port = await getNextAvailablePort({ common });
      return {
        status: "success",
        result: {
          hostname: common.options.hostname,
          port,
          app: apiResult.app,
        },
      };
    },
    async startDev({ onReload }) {
      // Define the directories and files to ignore
      const ignoredDirs = [
        common.options.generatedDir,
        common.options.ponderDir,
      ];
      const ignoredFiles = [
        path.join(common.options.rootDir, "ponder-env.d.ts"),
        path.join(common.options.rootDir, ".env.local"),
      ];
      const isFileIgnored = (filePath: string) => {
        const isInIgnoredDir = ignoredDirs.some((dir) => {
          const rel = path.relative(dir, filePath);
          return !rel.startsWith("..") && !path.isAbsolute(rel);
        });
        const isIgnoredFile = ignoredFiles.includes(filePath);
        return isInIgnoredDir || isIgnoredFile;
      };
      const onFileChange = async (_file: string) => {
        if (isFileIgnored(_file)) return;
        // Note that `toFilePath` always returns a POSIX path, even if you pass a Windows path.
        const file = toFilePath(
          normalizeModuleId(_file),
          common.options.rootDir,
        ).path;
        // Invalidate all modules that depend on the updated files.
        // Note that `invalidateDepTree` accepts and returns POSIX paths, even on Windows.
        const invalidated = viteNodeRunner.moduleCache.invalidateDepTree([
          file,
        ]);
        // If no files were invalidated, no need to reload.
        if (invalidated.size === 0) return;
        // Note that the paths in `invalidated` are POSIX, so we need to
        // convert the paths in `options` to POSIX for this comparison.
        // The `srcDir` regex is already converted to POSIX.
        const hasConfigUpdate = invalidated.has(
          common.options.configFile.replace(/\\/g, "/"),
        );
        const hasSchemaUpdate = invalidated.has(
          common.options.schemaFile.replace(/\\/g, "/"),
        );
        const hasIndexingUpdate = Array.from(invalidated).some(
          (file) => indexingRegex.test(file) && file !== common.options.apiFile,
        );
        const hasApiUpdate = Array.from(invalidated).some(
          (file) => file === common.options.apiFile,
        );
        // This branch could trigger if you change a `note.txt` file within `src/`.
        // Note: We could probably do a better job filtering out files in `isFileIgnored`.
        if (
          !hasConfigUpdate &&
          !hasSchemaUpdate &&
          !hasIndexingUpdate &&
          !hasApiUpdate
        ) {
          return;
        }
        common.logger.info({
          msg: `Hot reload ${Array.from(invalidated)
            .map((f) => `"${path.relative(common.options.rootDir, f)}"`)
            .join(", ")}`,
        });
        // Fast path for when only the api has changed.
        if (
          hasApiUpdate === true &&
          hasConfigUpdate === false &&
          hasSchemaUpdate === false &&
          hasIndexingUpdate === false
        ) {
          viteNodeRunner.moduleCache.invalidateDepTree([
            common.options.apiFile,
          ]);
          onReload("api");
        } else {
          // Instead, just invalidate the files that have changed and ...
          // re-execute all files
          viteNodeRunner.moduleCache.invalidateDepTree([
            common.options.configFile,
          ]);
          viteNodeRunner.moduleCache.invalidateDepTree([
            common.options.schemaFile,
          ]);
          viteNodeRunner.moduleCache.invalidateDepTree(
            glob.sync(indexingPattern, {
              ignore: apiPattern,
            }),
          );
          viteNodeRunner.moduleCache.invalidateDepTree(glob.sync(apiPattern));
          viteNodeRunner.moduleCache.deleteByModuleId("ponder:registry");
          viteNodeRunner.moduleCache.deleteByModuleId("ponder:api");
          onReload("indexing");
        }
      };
      viteDevServer.watcher.on("change", onFileChange);
    },
    async rpcDiagnostic({ configBuild }) {
      const context = {
        logger: common.logger.child({ action: "rpc_diagnostic" }),
      };
      const endClock = startClock();
      const results = await Promise.all(
        configBuild.rpcs.map(async (rpc, index) => {
          const chain = configBuild.chains[index]!;
          try {
            const chainId = await rpc.request(
              { method: "eth_chainId" },
              context,
            );
            if (hexToNumber(chainId) !== chain.id) {
              common.logger.warn({
                msg: "Configured chain ID does not match JSON-RPC response",
                chain: chain.name,
                chain_id: chain.id,
                rpc_chain_id: hexToNumber(chainId),
              });
            }
          } catch (e) {
            const error = new RetryableError("Failed to connect to JSON-RPC");
            error.stack = undefined;
            return { status: "error", error } as const;
          }
          common.logger.info({
            msg: "Connected to JSON-RPC",
            chain: chain.name,
            chain_id: chain.id,
            hostnames: JSON.stringify(rpc.hostnames),
            duration: endClock(),
          });
          return { status: "success", result: undefined } as const;
        }),
      );
      for (const result of results) {
        if (result.status === "error") {
          return result;
        }
      }
      return { status: "success", result: undefined };
    },
    async databaseDiagnostic({ preBuild }) {
      const context = {
        logger: common.logger.child({ action: "database_diagnostic" }),
      };
      const endClock = startClock();
      const dialect = preBuild.databaseConfig.kind;
      if (dialect === "pglite") {
        const driver = createPglite(preBuild.databaseConfig.options);
        const qb = createQB(drizzlePglite(driver), { common });
        try {
          await qb.wrap((db) => db.execute("SELECT version()"), context);
        } catch (e) {
          const error = new RetryableError(
            `Failed to connect to PGlite database. Please check your database connection settings.\n\n${(e as any).message}`,
          );
          error.stack = undefined;
          return { status: "error", error };
        } finally {
          await driver.close();
        }
        const pgliteDir = preBuild.databaseConfig.options.dataDir;
        const pglitePath =
          pgliteDir === "memory://"
            ? "memory://"
            : path.relative(common.options.rootDir, pgliteDir);
        common.logger.info({
          msg: "Connected to database",
          type: dialect,
          database: pglitePath,
          duration: endClock(),
        });
      } else if (dialect === "postgres") {
        const pool = createPool(
          {
            ...preBuild.databaseConfig.poolConfig,
            application_name: "test",
            max: 1,
            statement_timeout: 10_000,
          },
          common.logger,
        );
        const qb = createQB(drizzleNodePostgres(pool), { common });
        try {
          await qb.wrap((db) => db.execute("SELECT version()"), context);
        } catch (e) {
          const error = new RetryableError(
            `Failed to connect to database. Please check your database connection settings.\n\n${(e as any).message}`,
          );
          error.stack = undefined;
          return { status: "error", error };
        } finally {
          await pool.end();
        }
        common.logger.info({
          msg: "Connected to database",
          type: dialect,
          database: getDatabaseName(preBuild.databaseConfig.poolConfig),
          duration: endClock(),
        });
      }
      return { status: "success", result: undefined };
    },
  } satisfies Build;
  return build;
};
</file>

<file path="packages/core/src/build/plugin.ts">
import type { Common } from "@/internal/common.js";
import type { Plugin } from "vite";
const virtualModule = () => `export const ponder = {
  fns: [],
  on(name, fn) {
    this.fns.push({ name, fn });
  },
};
`;
const schemaModule = (
  schemaPath: string,
) => `import * as schema from "${schemaPath}";
export * from "${schemaPath}";
export default schema;
`;
const apiModule = () => `import { createPublicClient, custom } from "viem";
if (globalThis.PONDER_INDEXING_BUILD === undefined || globalThis.PONDER_DATABASE === undefined) {
  throw new Error('Invalid dependency graph. Config, schema, and indexing function files cannot import objects from the API function file "src/api/index.ts".')
}
const publicClients = {};
for (let i = 0; i < globalThis.PONDER_INDEXING_BUILD.chains.length; i++) {
  const chain = globalThis.PONDER_INDEXING_BUILD.chains[i];
  const rpc = globalThis.PONDER_INDEXING_BUILD.rpcs[i];
  publicClients[chain.name] = createPublicClient({
    chain: chain.viemChain,
    transport: custom({
      request(body) {
        return rpc.request(body);
      }
    }),
  })
}
export const db = globalThis.PONDER_DATABASE.readonlyQB.raw;
export { publicClients };
`;
export const vitePluginPonder = (options: Common["options"]): Plugin => {
  // On Windows, options.schemaFile is a Windows-style path. We need to convert it to a
  // Unix-style path for codegen, because TS import paths are Unix-style even on Windows.
  const schemaPath = options.schemaFile.replace(/\\/g, "/");
  return {
    name: "ponder",
    load: (id) => {
      if (id === "ponder:registry") return virtualModule();
      if (id === "ponder:schema") return schemaModule(schemaPath);
      if (id === "ponder:api") return apiModule();
      return null;
    },
  };
};
</file>

<file path="packages/core/src/build/pre.test.ts">
import path from "node:path";
import { withStubbedEnv } from "@/_test/utils.js";
import { createLogger } from "@/internal/logger.js";
import type { Options } from "@/internal/options.js";
import { expect, test } from "vitest";
import { createConfig } from "../config/index.js";
import { buildPre } from "./pre.js";
const options = {
  ponderDir: ".ponder",
  rootDir: "rootDir",
} as const satisfies Pick<Options, "rootDir" | "ponderDir">;
const logger = createLogger({ level: "debug" });
test("buildPre() database uses pglite by default", () => {
  const config = createConfig({
    chains: { mainnet: { id: 1, rpc: "https://rpc.com" } },
    contracts: { a: { chain: "mainnet", abi: [] } },
  });
  withStubbedEnv(
    { DATABASE_URL: undefined, PRIVATE_DATABASE_URL: undefined },
    () => {
      const { databaseConfig } = buildPre({
        config,
        options,
        logger,
      });
      expect(databaseConfig).toMatchObject({
        kind: "pglite",
        options: {
          dataDir: expect.stringContaining(path.join(".ponder", "pglite")),
        },
      });
    },
  );
  withStubbedEnv({ DATABASE_URL: "", PRIVATE_DATABASE_URL: "" }, () => {
    const { databaseConfig: databaseConfig2 } = buildPre({
      config,
      options,
      logger,
    });
    expect(databaseConfig2).toMatchObject({
      kind: "pglite",
      options: {
        dataDir: expect.stringContaining(path.join(".ponder", "pglite")),
      },
    });
  });
});
test("buildPre() database respects custom pglite path", async () => {
  const config = createConfig({
    database: { kind: "pglite", directory: "custom-pglite/directory" },
    chains: { mainnet: { id: 1, rpc: "https://rpc.com" } },
    contracts: { a: { chain: "mainnet", abi: [] } },
  });
  const { databaseConfig } = buildPre({ config, options, logger });
  expect(databaseConfig).toMatchObject({
    kind: "pglite",
    options: {
      dataDir: expect.stringContaining(path.join("custom-pglite", "directory")),
    },
  });
});
test("buildPre() database uses pglite if specified even if DATABASE_URL env var present", async () => {
  const config = createConfig({
    database: { kind: "pglite" },
    chains: { mainnet: { id: 1, rpc: "https://rpc.com" } },
    contracts: { a: { chain: "mainnet", abi: [] } },
  });
  withStubbedEnv(
    { DATABASE_URL: "postgres://username@localhost:5432/database" },
    () => {
      const { databaseConfig } = buildPre({ config, options, logger });
      expect(databaseConfig).toMatchObject({
        kind: "pglite",
        options: {
          dataDir: expect.stringContaining(path.join(".ponder", "pglite")),
        },
      });
    },
  );
});
test("buildPre() database uses postgres if DATABASE_URL env var present", async () => {
  const config = createConfig({
    chains: { mainnet: { id: 1, rpc: "https://rpc.com" } },
    contracts: { a: { chain: "mainnet", abi: [] } },
  });
  withStubbedEnv(
    { DATABASE_URL: "postgres://username@localhost:5432/database" },
    () => {
      const { databaseConfig } = buildPre({ config, options, logger });
      expect(databaseConfig).toMatchObject({
        kind: "postgres",
        poolConfig: {
          connectionString: "postgres://username@localhost:5432/database",
        },
      });
    },
  );
});
test("buildPre() database uses postgres if DATABASE_PRIVATE_URL env var present", async () => {
  const config = createConfig({
    chains: { mainnet: { id: 1, rpc: "https://rpc.com" } },
    contracts: { a: { chain: "mainnet", abi: [] } },
  });
  withStubbedEnv(
    {
      DATABASE_URL: "postgres://username@localhost:5432/database",
      DATABASE_PRIVATE_URL:
        "postgres://username@localhost:5432/better_database",
    },
    () => {
      const { databaseConfig } = buildPre({ config, options, logger });
      expect(databaseConfig).toMatchObject({
        kind: "postgres",
        poolConfig: {
          connectionString:
            "postgres://username@localhost:5432/better_database",
        },
      });
    },
  );
});
test("buildPre() throws for postgres database with no connection string", async () => {
  const config = createConfig({
    database: { kind: "postgres" },
    chains: { mainnet: { id: 1, rpc: "https://rpc.com" } },
    contracts: { a: { chain: "mainnet", abi: [] } },
  });
  withStubbedEnv(
    { DATABASE_URL: undefined, PRIVATE_DATABASE_URL: undefined },
    () => {
      expect(() => buildPre({ config, options, logger })).toThrow(
        "Invalid database configuration: Either 'connectionString' or 'poolConfig' must be defined.",
      );
    },
  );
});
test("buildPre() database with postgres uses pool config", async () => {
  const config = createConfig({
    database: {
      kind: "postgres",
      connectionString: "postgres://username@localhost:5432/database",
      poolConfig: {
        max: 100,
        ssl: {
          ca: "ca",
          cert: "cert",
          key: "key",
        },
        idle_in_transaction_session_timeout: 10,
        keepAlive: true,
      },
    },
    chains: { mainnet: { id: 1, rpc: "https://rpc.com" } },
    contracts: { a: { chain: "mainnet", abi: [] } },
  });
  const { databaseConfig } = buildPre({ config, options, logger });
  expect(databaseConfig).toStrictEqual({
    kind: "postgres",
    poolConfig: {
      connectionString: "postgres://username@localhost:5432/database",
      max: 100,
      ssl: {
        ca: "ca",
        cert: "cert",
        key: "key",
      },
      idle_in_transaction_session_timeout: 10,
      keepAlive: true,
    },
  });
});
</file>

<file path="packages/core/src/build/pre.ts">
import path from "node:path";
import type { Config } from "@/config/index.js";
import { BuildError } from "@/internal/errors.js";
import type { Logger } from "@/internal/logger.js";
import type { Options } from "@/internal/options.js";
import type { DatabaseConfig } from "@/internal/types.js";
export function buildPre({
  config,
  options,
  logger,
}: {
  config: Config;
  options: Pick<Options, "rootDir" | "ponderDir">;
  logger: Logger;
}): {
  databaseConfig: DatabaseConfig;
  ordering: NonNullable<Config["ordering"]>;
} {
  // Build database.
  let databaseConfig: DatabaseConfig;
  // Determine PGlite directory, preferring config.database.directory if available
  const pgliteDir =
    config.database?.kind === "pglite" && config.database.directory
      ? config.database.directory === "memory://"
        ? "memory://"
        : path.resolve(config.database.directory)
      : path.join(options.ponderDir, "pglite");
  if (config.database?.kind) {
    if (config.database.kind === "postgres") {
      let connectionString: string | undefined = undefined;
      if (config.database.connectionString) {
        connectionString = config.database.connectionString;
      } else if (process.env.DATABASE_PRIVATE_URL) {
        connectionString = process.env.DATABASE_PRIVATE_URL;
      } else if (process.env.DATABASE_URL) {
        connectionString = process.env.DATABASE_URL;
      }
      if (connectionString === undefined) {
        if (config.database.poolConfig === undefined) {
          throw new Error(
            "Invalid database configuration: Either 'connectionString' or 'poolConfig' must be defined.",
          );
        }
        logger.warn({
          msg: "No database connection string set. Using 'poolConfig' for connection authentication.",
        });
      }
      const poolConfig = {
        // Note: Override `connectionString` with `poolConfig` if available.
        connectionString,
        ...(config.database.poolConfig ?? {}),
        max: config.database.poolConfig?.max ?? 30,
        ssl: config.database.poolConfig?.ssl ?? false,
      } satisfies (DatabaseConfig & { kind: "postgres" })["poolConfig"];
      databaseConfig = { kind: "postgres", poolConfig };
    } else {
      databaseConfig = { kind: "pglite", options: { dataDir: pgliteDir } };
    }
  } else {
    let connectionString: string | undefined = undefined;
    if (process.env.DATABASE_PRIVATE_URL) {
      connectionString = process.env.DATABASE_PRIVATE_URL;
    } else if (process.env.DATABASE_URL) {
      connectionString = process.env.DATABASE_URL;
    }
    // If either of the DATABASE_URL env vars are set, use Postgres.
    if (connectionString !== undefined) {
      const poolConfig = { connectionString, max: 30 };
      databaseConfig = { kind: "postgres", poolConfig };
    } else {
      // Fall back to PGlite.
      databaseConfig = { kind: "pglite", options: { dataDir: pgliteDir } };
    }
  }
  return {
    databaseConfig,
    ordering: config.ordering ?? "multichain",
  };
}
export function safeBuildPre({
  config,
  options,
  logger,
}: {
  config: Config;
  options: Pick<Options, "rootDir" | "ponderDir">;
  logger: Logger;
}) {
  try {
    const result = buildPre({ config, options, logger });
    return {
      status: "success",
      databaseConfig: result.databaseConfig,
      ordering: result.ordering,
    } as const;
  } catch (_error) {
    const buildError = new BuildError((_error as Error).message);
    buildError.stack = undefined;
    return { status: "error", error: buildError } as const;
  }
}
</file>

<file path="packages/core/src/build/schema.test.ts">
import {
  bigint,
  hex,
  onchainEnum,
  onchainTable,
  onchainView,
} from "@/index.js";
import { count, sql, sum } from "drizzle-orm";
import {
  check,
  index,
  pgSequence,
  primaryKey,
  serial,
} from "drizzle-orm/pg-core";
import { expect, test } from "vitest";
import { buildSchema } from "./schema.js";
test("buildSchema() success", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  buildSchema({ schema, preBuild: { ordering: "multichain" } });
});
test("buildSchema() error with multiple primary key", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().primaryKey(),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() error with no primary key", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex(),
      balance: p.bigint(),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() success with composite primary key", () => {
  const schema = {
    account: onchainTable(
      "account",
      (p) => ({
        address: p.hex().notNull(),
        balance: p.bigint().notNull(),
      }),
      (table) => ({
        pk: primaryKey({ columns: [table.address, table.balance] }),
      }),
    ),
  };
  buildSchema({ schema, preBuild: { ordering: "multichain" } });
});
test("buildSchema() error with sequences", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
    seq: pgSequence("seq"),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() error with generated", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.integer().primaryKey(),
      balance: p.bigint().notNull().generatedAlwaysAs(10n),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() error with generated identity", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      id: p
        .integer()
        .primaryKey()
        .generatedAlwaysAsIdentity({ startWith: 1000 }),
      balance: p.bigint().notNull(),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() error with serial", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: serial().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() success with default", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.integer().primaryKey(),
      balance: p.bigint().default(10n),
    })),
  };
  buildSchema({ schema, preBuild: { ordering: "multichain" } });
});
test("buildSchema() error with default sql", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.integer().primaryKey(),
      balance: p.bigint().default(sql`10`),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() error with $defaultFn sql", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.integer().primaryKey(),
      balance: p.bigint().$defaultFn(() => sql`10`),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() error with $onUpdateFn sql", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.integer().primaryKey(),
      balance: p.bigint().$onUpdateFn(() => sql`10`),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() error with foreign key", () => {
  // @ts-ignore
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.integer().primaryKey(),
      balance: p
        .bigint()
        .notNull()
        .references(() => schema.account.address),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() error with unique", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.integer().primaryKey(),
      balance: p.bigint().notNull().unique(),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() error with check", () => {
  const schema = {
    account: onchainTable(
      "account",
      (p) => ({
        address: p.hex().primaryKey(),
        balance: p.bigint().notNull(),
      }),
      () => ({
        check: check("test", sql``),
      }),
    ),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() success with enum", () => {
  const mood = onchainEnum("mood", ["good", "bad"]);
  const schema = {
    mood,
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      m: mood().notNull(),
    })),
  };
  buildSchema({ schema, preBuild: { ordering: "multichain" } });
});
test("buildSchema() duplicate table name", () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
    })),
    account2: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
    })),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema() duplicate index name", () => {
  const schema = {
    account: onchainTable(
      "account",
      (p) => ({
        address: p.hex().primaryKey(),
        balance: p.bigint().notNull(),
      }),
      (table) => ({
        balanceIdx: index("balance_idx").on(table.balance),
      }),
    ),
    account2: onchainTable(
      "account2",
      (p) => ({
        address: p.hex().primaryKey(),
        balance: p.bigint().notNull(),
      }),
      (table) => ({
        balanceIdx: index("balance_idx").on(table.balance),
      }),
    ),
  };
  expect(() =>
    buildSchema({ schema, preBuild: { ordering: "multichain" } }),
  ).toThrowError();
});
test("buildSchema exp", () => {
  const schema1 = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  expect(() =>
    buildSchema({
      schema: schema1,
      preBuild: { ordering: "experimental_isolated" },
    }),
  ).toThrowError();
  const schema2 = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      chainId: p.integer().notNull(),
      balance: p.bigint().notNull(),
    })),
  };
  expect(() =>
    buildSchema({
      schema: schema2,
      preBuild: { ordering: "experimental_isolated" },
    }),
  ).toThrowError();
  const schema3 = {
    account: onchainTable(
      "account",
      (p) => ({
        address: p.hex().notNull(),
        chainId: p.integer().notNull(),
        balance: p.bigint().notNull(),
      }),
      (table) => ({
        pk: primaryKey({ columns: [table.address, table.chainId] }),
      }),
    ),
  };
  buildSchema({
    schema: schema3,
    preBuild: { ordering: "experimental_isolated" },
  });
});
test("buildSchema view", () => {
  const account = onchainTable(
    "account",
    (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    }),
    (table) => ({
      balanceIdx: index("balance_idx").on(table.balance),
    }),
  );
  const schema = {
    account,
    accountView: onchainView("account_view").as((qb) =>
      qb.select().from(account),
    ),
  };
  buildSchema({ schema, preBuild: { ordering: "multichain" } });
});
test("buildSchema view raw sql", () => {
  const account = onchainTable(
    "account",
    (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    }),
    (table) => ({
      balanceIdx: index("balance_idx").on(table.balance),
    }),
  );
  const schema = {
    account,
    accountView: onchainView("account_view", {
      address: hex().primaryKey(),
      balance: bigint().notNull(),
    }).as(sql`SELECT * FROM account`),
  };
  buildSchema({ schema, preBuild: { ordering: "multichain" } });
});
test("buildSchema view with aggregate functions", () => {
  const trade = onchainTable("trade", (p) => ({
    id: p.text().primaryKey(),
    marketAddress: p.hex().notNull(),
    cost: p.bigint().notNull(),
  }));
  const schema = {
    trade,
    tradeVolume: onchainView("trade_volume").as((qb) =>
      qb
        .select({
          marketAddress: trade.marketAddress,
          volume: sum(trade.cost).as("volume"),
          count: count().as("count"),
        })
        .from(trade)
        .groupBy(trade.marketAddress),
    ),
  };
  buildSchema({ schema, preBuild: { ordering: "multichain" } });
});
test("buildSchema view with alias", () => {
  const transfer = onchainTable("transfer", (p) => ({
    id: p.text().primaryKey(),
    timestamp: p.bigint().notNull(),
    amount: p.bigint().notNull(),
  }));
  const schema = {
    transfer,
    hourlyBucket: onchainView("hourly_bucket").as((qb) =>
      qb
        .select({
          hour: sql`FLOOR(${transfer.timestamp} / 3600) * 3600`.as("hour"),
          totalVolume: sum(transfer.amount).as("total_volume"),
          transferCount: count().as("transfer_count"),
        })
        .from(transfer)
        .groupBy(sql`FLOOR(${transfer.timestamp} / 3600)`),
    ),
  };
  buildSchema({ schema, preBuild: { ordering: "multichain" } });
});
</file>

<file path="packages/core/src/build/schema.ts">
import {
  PONDER_CHECKPOINT_TABLE_NAME,
  PONDER_META_TABLE_NAME,
} from "@/database/index.js";
import { getPrimaryKeyColumns } from "@/drizzle/index.js";
import { getSql } from "@/drizzle/kit/index.js";
import { MAX_DATABASE_OBJECT_NAME_LENGTH } from "@/drizzle/onchain.js";
import { BuildError } from "@/internal/errors.js";
import type { PreBuild, Schema } from "@/internal/types.js";
import {
  SQL,
  getTableColumns,
  getTableName,
  getViewName,
  is,
} from "drizzle-orm";
import {
  PgBigSerial53,
  PgBigSerial64,
  PgColumn,
  PgSequence,
  PgSerial,
  PgSmallSerial,
  PgTable,
  PgView,
  getTableConfig,
  getViewConfig,
} from "drizzle-orm/pg-core";
/**
 * @dev The maximum notify message size is 8KB (8000 / 63 > 100).
 */
const TABLE_LIMIT = 100;
export const buildSchema = ({
  schema,
  preBuild,
}: { schema: Schema; preBuild: Pick<PreBuild, "ordering"> }) => {
  const statements = getSql(schema);
  const tableNames = new Set<string>();
  const viewNames = new Set<string>();
  const indexNames = new Set<string>();
  for (const [name, s] of Object.entries(schema)) {
    if (is(s, PgTable)) {
      let hasPrimaryKey = false;
      let hasChainIdColumn = false;
      if (
        name === PONDER_META_TABLE_NAME ||
        name === PONDER_CHECKPOINT_TABLE_NAME
      ) {
        throw new Error(
          `Schema validation failed: '${name}' is a reserved table name.`,
        );
      }
      if (name.length > MAX_DATABASE_OBJECT_NAME_LENGTH) {
        throw new Error(
          `Schema validation failed: '${name}' table name cannot be longer than ${MAX_DATABASE_OBJECT_NAME_LENGTH} characters.`,
        );
      }
      const columnNames = new Set<string>();
      for (const [columnName, column] of Object.entries(getTableColumns(s))) {
        if (column.primary) {
          if (hasPrimaryKey) {
            throw new Error(
              `Schema validation failed: '${name}' has multiple primary keys.`,
            );
          } else {
            hasPrimaryKey = true;
          }
        }
        if (
          column instanceof PgSerial ||
          column instanceof PgSmallSerial ||
          column instanceof PgBigSerial53 ||
          column instanceof PgBigSerial64
        ) {
          throw new Error(
            `Schema validation failed: '${name}.${columnName}' has a serial column and serial columns are unsupported.`,
          );
        }
        if (column.isUnique) {
          throw new Error(
            `Schema validation failed: '${name}.${columnName}' has a unique constraint and unique constraints are unsupported.`,
          );
        }
        if (column.generated !== undefined) {
          throw new Error(
            `Schema validation failed: '${name}.${columnName}' is a generated column and generated columns are unsupported.`,
          );
        }
        if (column.generatedIdentity !== undefined) {
          throw new Error(
            `Schema validation failed: '${name}.${columnName}' is a generated column and generated columns are unsupported.`,
          );
        }
        if (column.hasDefault) {
          if (column.default && column.default instanceof SQL) {
            throw new Error(
              `Schema validation failed: '${name}.${columnName}' is a default column and default columns with raw sql are unsupported.`,
            );
          }
          if (column.defaultFn && column.defaultFn() instanceof SQL) {
            throw new Error(
              `Schema validation failed: '${name}.${columnName}' is a default column and default columns with raw sql are unsupported.`,
            );
          }
          if (column.onUpdateFn && column.onUpdateFn() instanceof SQL) {
            throw new Error(
              `Schema validation failed: '${name}.${columnName}' is a default column and default columns with raw sql are unsupported.`,
            );
          }
        }
        // TODO(kyle) It is an invariant that `getColumnCasing(column, "snake_case") === column.name`
        if (columnName === "chainId" && column.name === "chain_id") {
          hasChainIdColumn = true;
        }
        // Note: Ponder lets postgres handle the column name length limit and truncation.
        if (
          column.name === "operation_id" ||
          column.name === "operation" ||
          column.name === "checkpoint"
        ) {
          throw new Error(
            `Schema validation failed: '${name}.${columnName}' is a reserved column name.`,
          );
        }
        if (columnNames.has(column.name)) {
          throw new Error(
            `Schema validation failed: '${name}.${column.name}' column name is used multiple times.`,
          );
        } else {
          columnNames.add(column.name);
        }
      }
      if (preBuild.ordering === "experimental_isolated") {
        if (hasChainIdColumn === false) {
          throw new Error(
            `Schema validation failed: '${name}' does not have required 'chainId' column.`,
          );
        }
        if (
          getTableColumns(s).chainId!.dataType !== "number" &&
          getTableColumns(s).chainId!.dataType !== "bigint"
        ) {
          throw new Error(
            `Schema validation failed: '${name}'.chainId column must be an integer or numeric.`,
          );
        }
        if (
          getPrimaryKeyColumns(s).some(({ sql }) => sql === "chain_id") ===
          false
        ) {
          throw new Error(
            `Schema validation failed: '${name}.chain_id' column is required to be in the primary key when ordering is 'isolated'.`,
          );
        }
      }
      if (tableNames.has(getTableName(s))) {
        throw new Error(
          `Schema validation failed: table name '${getTableName(s)}' is used multiple times.`,
        );
      } else {
        tableNames.add(getTableName(s));
      }
      if (getTableConfig(s).primaryKeys.length > 1) {
        throw new Error(
          `Schema validation failed: '${name}' has multiple primary keys.`,
        );
      }
      if (getTableConfig(s).primaryKeys.length === 1 && hasPrimaryKey) {
        throw new Error(
          `Schema validation failed: '${name}' has multiple primary keys.`,
        );
      }
      if (
        getTableConfig(s).primaryKeys.length === 0 &&
        hasPrimaryKey === false
      ) {
        throw new Error(
          `Schema validation failed: '${name}' has no primary key. Declare one with ".primaryKey()".`,
        );
      }
      if (getTableConfig(s).foreignKeys.length > 0) {
        throw new Error(
          `Schema validation failed: '${name}' has a foreign key constraint and foreign key constraints are unsupported.`,
        );
      }
      if (getTableConfig(s).checks.length > 0) {
        throw new Error(
          `Schema validation failed: '${name}' has a check constraint and check constraints are unsupported.`,
        );
      }
      if (getTableConfig(s).uniqueConstraints.length > 0) {
        throw new Error(
          `Schema validation failed: '${name}' has a unique constraint and unique constraints are unsupported.`,
        );
      }
      for (const index of getTableConfig(s).indexes) {
        // Note: Ponder lets postgres handle the index name length limit and truncation.
        if (index.config.name && indexNames.has(index.config.name)) {
          throw new Error(
            `Schema validation failed: index name '${index.config.name}' is used multiple times.`,
          );
        } else if (index.config.name) {
          indexNames.add(index.config.name);
        }
      }
    }
    if (is(s, PgSequence)) {
      throw new Error(
        `Schema validation failed: '${name}' is a sequence and sequences are unsupported.`,
      );
    }
    if (is(s, PgView)) {
      // Note: Ponder lets postgres handle the view name length limit and truncation.
      if (viewNames.has(getViewName(s))) {
        throw new Error(
          `Schema validation failed: view name '${getViewName(s)}' is used multiple times.`,
        );
      } else {
        viewNames.add(getViewName(s));
      }
      const viewConfig = getViewConfig(s);
      if (viewConfig.selectedFields.length === 0) {
        throw new Error(
          `Schema validation failed: view '${getViewName(s)}' has no selected fields.`,
        );
      }
      if (viewConfig.isExisting) {
        throw new Error(
          `Schema validation failed: view '${getViewName(s)}' is an existing view and existing views are unsupported.`,
        );
      }
      if (viewConfig.query === undefined) {
        throw new Error(
          `Schema validation failed: view '${getViewName(s)}' has no underlying query.`,
        );
      }
      const columnNames = new Set<string>();
      if (viewConfig)
        for (const [columnName, column] of Object.entries(
          viewConfig.selectedFields,
        )) {
          if (
            is(column, PgColumn) === false &&
            is(column, SQL.Aliased) === false
          ) {
            throw new Error(
              `Schema validation failed: view '${getViewName(s)}.${columnName}' is a non-column selected field.`,
            );
          }
          if (is(column, PgColumn)) {
            if (
              column instanceof PgSerial ||
              column instanceof PgSmallSerial ||
              column instanceof PgBigSerial53 ||
              column instanceof PgBigSerial64
            ) {
              throw new Error(
                `Schema validation failed: '${name}.${columnName}' has a serial column and serial columns are unsupported.`,
              );
            }
            if (column.isUnique) {
              throw new Error(
                `Schema validation failed: '${name}.${columnName}' has a unique constraint and unique constraints are unsupported.`,
              );
            }
            if (column.generated !== undefined) {
              throw new Error(
                `Schema validation failed: '${name}.${columnName}' is a generated column and generated columns are unsupported.`,
              );
            }
            if (column.generatedIdentity !== undefined) {
              throw new Error(
                `Schema validation failed: '${name}.${columnName}' is a generated column and generated columns are unsupported.`,
              );
            }
            if (columnNames.has((column as PgColumn).name)) {
              throw new Error(
                `Schema validation failed: '${name}.${(column as PgColumn).name}' column name is used multiple times.`,
              );
            } else {
              columnNames.add((column as PgColumn).name);
            }
          }
        }
    }
  }
  if (tableNames.size > TABLE_LIMIT) {
    throw new Error(
      `Schema validation failed: the maximum number of tables is ${TABLE_LIMIT}.`,
    );
  }
  return { statements };
};
export const safeBuildSchema = ({
  schema,
  preBuild,
}: { schema: Schema; preBuild: Pick<PreBuild, "ordering"> }) => {
  try {
    const result = buildSchema({ schema, preBuild });
    return {
      status: "success",
      ...result,
    } as const;
  } catch (_error) {
    const buildError = new BuildError((_error as Error).message);
    buildError.stack = undefined;
    return { status: "error", error: buildError } as const;
  }
};
</file>

<file path="packages/core/src/build/stacktrace.ts">
import { readFileSync } from "node:fs";
import { codeFrameColumns } from "@babel/code-frame";
import { parse as parseStackTrace } from "stacktrace-parser";
class ESBuildTransformError extends Error {
  override name = "ESBuildTransformError";
}
class ESBuildBuildError extends Error {
  override name = "ESBuildBuildError";
}
class ESBuildContextError extends Error {
  override name = "ESBuildContextError";
}
type ViteNodeError =
  | ESBuildTransformError
  | ESBuildBuildError
  | ESBuildContextError
  | Error;
export function parseViteNodeError(file: string, error: Error): ViteNodeError {
  let resolvedError: ViteNodeError;
  if (/^(Transform failed|Build failed|Context failed)/.test(error.message)) {
    // Handle ESBuild errors based on this error message construction logic:
    // https://github.com/evanw/esbuild/blob/4e11b50fe....ts#L1659
    const errorKind = error.message.split(" with ")[0] as
      | "Transform failed"
      | "Build failed"
      | "Context failed";
    const innerError = error.message
      .split("\n")
      .slice(1)
      .map((message) => {
        let location: string | undefined = undefined;
        let detail: string | undefined = undefined;
        if (message.includes(": ERROR: ")) {
          // /path/to/file.ts:11:9: ERROR: Expected ")" but found ";"
          const s = message.split(": ERROR: ");
          location = s[0];
          detail = s[1];
        } else {
          // error: some error without a location
          detail = message.slice(7);
        }
        return { location, detail };
      })[0];
    // If we aren't able to extract an inner error, just return the original.
    if (!innerError) return error;
    resolvedError =
      errorKind === "Transform failed"
        ? new ESBuildTransformError(innerError.detail)
        : errorKind === "Build failed"
          ? new ESBuildBuildError(innerError.detail)
          : new ESBuildContextError(innerError.detail);
    if (innerError.location)
      resolvedError.stack = `    at ${innerError.location}`;
  }
  // If it's not an ESBuild error, it's a user-land vm.runModuleInContext execution error.
  // Attempt to build a user-land stack trace.
  else if (error.stack) {
    const stackFrames = parseStackTrace(error.stack);
    const userStackFrames = [];
    for (const rawStackFrame of stackFrames) {
      if (rawStackFrame.methodName.includes("ViteNodeRunner.runModule")) break;
      userStackFrames.push(rawStackFrame);
    }
    const userStack = userStackFrames
      .map(({ file, lineNumber, column, methodName }) => {
        const prefix = "    at";
        const path = `${file}${lineNumber !== null ? `:${lineNumber}` : ""}${
          column !== null ? `:${column}` : ""
        }`;
        if (methodName === null || methodName === "<unknown>") {
          return `${prefix} ${path}`;
        } else {
          return `${prefix} ${methodName} (${path})`;
        }
      })
      .join("\n");
    resolvedError = error;
    resolvedError.stack = userStack;
  }
  // Still a vm.runModuleInContext execution error, but no stack.
  else {
    resolvedError = error;
  }
  // Attempt to build a code frame for the top of the user stack. This works for
  // both ESBuild and vm.runModuleInContext errors.
  if (resolvedError.stack) {
    const userStackFrames = parseStackTrace(resolvedError.stack);
    let codeFrame: string | undefined = undefined;
    for (const { file, lineNumber, column } of userStackFrames) {
      if (file !== null && lineNumber !== null) {
        try {
          const sourceFileContents = readFileSync(file, { encoding: "utf-8" });
          codeFrame = codeFrameColumns(
            sourceFileContents,
            { start: { line: lineNumber, column: column ?? undefined } },
            { highlightCode: true },
          );
          break;
        } catch (err) {
          // No-op.
        }
      }
    }
    resolvedError.stack = `${resolvedError.name}: ${resolvedError.message}\n${resolvedError.stack}`;
    if (codeFrame) resolvedError.stack += `\n${codeFrame}`;
  }
  // Finally, add a useful relative file name and verb to the error message.
  const verb =
    resolvedError.name === "ESBuildTransformError"
      ? "transforming"
      : resolvedError.name === "ESBuildBuildError" ||
          resolvedError.name === "ESBuildContextError"
        ? "building"
        : "executing";
  // This can throw with "Cannot set property message of [object Object] which has only a getter"
  try {
    resolvedError.message = `Error while ${verb} ${file}: ${resolvedError.message}`;
  } catch (e) {}
  return resolvedError;
}
</file>

<file path="packages/core/src/client/index.test.ts">
import {
  context,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { getPonderMetaTable } from "@/database/index.js";
import { bigint, hex, onchainTable } from "@/drizzle/onchain.js";
import { type QueryWithTypings, sql } from "drizzle-orm";
import { pgSchema } from "drizzle-orm/pg-core";
import { Hono } from "hono";
import superjson from "superjson";
import { beforeEach, expect, test, vi } from "vitest";
import { client } from "./index.js";
beforeEach(setupCommon);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
const queryToParams = (query: QueryWithTypings) =>
  new URLSearchParams({ sql: superjson.stringify(query) });
test("client.db", async () => {
  globalThis.PONDER_COMMON = context.common;
  globalThis.PONDER_PRE_BUILD = {
    ordering: "multichain",
    databaseConfig: context.databaseConfig,
  };
  globalThis.PONDER_NAMESPACE_BUILD = {
    schema: "public",
    viewsSchema: undefined,
  };
  const account = onchainTable("account", (p) => ({
    address: p.hex().primaryKey(),
    balance: p.bigint(),
  }));
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    client({
      db: database.readonlyQB.raw,
      schema: { account },
    }),
  );
  let query = {
    sql: "SELECT * FROM account",
    params: [],
  };
  let response = await app.request(`/sql/db?${queryToParams(query)}`);
  expect(response.status).toBe(200);
  const result = await response.json();
  expect(result.rows).toStrictEqual([]);
  query = {
    sql: "SELECT 1;",
    params: [],
  };
  response = await app.request(`/sql/db?${queryToParams(query)}`);
  expect(response.status).toBe(200);
});
test("client.db error", async () => {
  globalThis.PONDER_COMMON = context.common;
  const { database } = await setupDatabaseServices();
  globalThis.PONDER_DATABASE = database;
  globalThis.PONDER_PRE_BUILD = {
    ordering: "multichain",
    databaseConfig: context.databaseConfig,
  };
  globalThis.PONDER_NAMESPACE_BUILD = {
    schema: "public",
    viewsSchema: undefined,
  };
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    client({
      db: database.readonlyQB.raw,
      schema: {},
    }),
  );
  const query = {
    sql: "SELECT * FROM account",
    params: [],
  };
  const response = await app.request(`/sql/db?${queryToParams(query)}`);
  expect(response.status).toBe(500);
  expect(await response.text()).toContain('relation "account" does not exist');
});
test("client.db search_path", async () => {
  globalThis.PONDER_COMMON = context.common;
  globalThis.PONDER_PRE_BUILD = {
    ordering: "multichain",
    databaseConfig: context.databaseConfig,
  };
  globalThis.PONDER_NAMESPACE_BUILD = {
    schema: "Ponder",
    viewsSchema: undefined,
  };
  const schemaAccount = pgSchema("Ponder").table("account", {
    address: hex().primaryKey(),
    balance: bigint(),
  });
  const { database } = await setupDatabaseServices({
    namespaceBuild: {
      schema: "Ponder",
      viewsSchema: undefined,
    },
    schemaBuild: { schema: { account: schemaAccount } },
  });
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    client({
      db: database.readonlyQB.raw,
      schema: { account: schemaAccount },
    }),
  );
  const query = {
    sql: "SELECT * FROM account",
    params: [],
  };
  const response = await app.request(`/sql/db?${queryToParams(query)}`);
  expect(response.status).toBe(200);
});
test("client.db readonly", async () => {
  globalThis.PONDER_COMMON = context.common;
  globalThis.PONDER_PRE_BUILD = {
    ordering: "multichain",
    databaseConfig: context.databaseConfig,
  };
  globalThis.PONDER_NAMESPACE_BUILD = {
    schema: "public",
    viewsSchema: undefined,
  };
  const account = onchainTable("account", (p) => ({
    address: p.hex().primaryKey(),
    balance: p.bigint(),
  }));
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    client({ db: database.readonlyQB.raw, schema: { account } }),
  );
  const query = {
    sql: "INSERT INTO account (address, balance) VALUES ('0x123', 100)",
    params: [],
  };
  const response = await app.request(`/sql/db?${queryToParams(query)}`);
  expect(response.status).toBe(500);
  expect(await response.text()).toContain("InsertStmt not supported");
});
test("client.db recursive", async () => {
  globalThis.PONDER_COMMON = context.common;
  globalThis.PONDER_PRE_BUILD = {
    ordering: "multichain",
    databaseConfig: context.databaseConfig,
  };
  globalThis.PONDER_NAMESPACE_BUILD = {
    schema: "public",
    viewsSchema: undefined,
  };
  const account = onchainTable("account", (p) => ({
    address: p.hex().primaryKey(),
    balance: p.bigint(),
  }));
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    client({ db: database.readonlyQB.raw, schema: { account } }),
  );
  const query = {
    sql: `
WITH RECURSIVE infinite_cte AS (
  SELECT 1 AS num
  UNION ALL
  SELECT num + 1
  FROM infinite_cte
)
SELECT *
FROM infinite_cte;`,
    params: [],
  };
  const response = await app.request(`/sql/db?${queryToParams(query)}`);
  expect(response.status).toBe(500);
  expect(await response.text()).toContain("Recursive CTEs not supported");
});
test("client.db load", async () => {
  globalThis.PONDER_COMMON = context.common;
  globalThis.PONDER_PRE_BUILD = {
    ordering: "multichain",
    databaseConfig: context.databaseConfig,
  };
  globalThis.PONDER_NAMESPACE_BUILD = {
    schema: "public",
    viewsSchema: undefined,
  };
  const account = onchainTable("account", (p) => ({
    address: p.hex().primaryKey(),
    balance: p.bigint(),
  }));
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    client({
      db: database.readonlyQB.raw,
      schema: { account },
    }),
  );
  const promises = new Array(250).map(async (_, i) => {
    const response = await app.request(
      `/sql/db?${queryToParams({
        sql: `SELECT ${i}`,
        params: [],
      })}`,
    );
    const result = await response.json();
    return result;
  });
  await Promise.all(promises);
});
test("client.db cache", async () => {
  // "spy" not possible with pglite
  if (context.databaseConfig.kind !== "postgres") return;
  globalThis.PONDER_COMMON = context.common;
  globalThis.PONDER_PRE_BUILD = {
    ordering: "multichain",
    databaseConfig: context.databaseConfig,
  };
  globalThis.PONDER_NAMESPACE_BUILD = {
    schema: "public",
    viewsSchema: undefined,
  };
  const account = onchainTable("account", (p) => ({
    address: p.hex().primaryKey(),
    balance: p.bigint(),
  }));
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  const PONDER_META = getPonderMetaTable();
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    client({
      db: database.readonlyQB.raw,
      schema: { account },
    }),
  );
  await database.adminQB.wrap({ label: "update_ready" }, (db) =>
    db
      .update(PONDER_META)
      .set({ value: sql`jsonb_set(value, '{is_ready}', to_jsonb(1))` }),
  );
  await new Promise((resolve) => setTimeout(resolve, 1200));
  const transactionSpy = vi.spyOn(database.readonlyQB.raw, "transaction");
  const query = {
    sql: "SELECT 1",
    params: [],
  };
  const promise1 = app.request(`/sql/db?${queryToParams(query)}`);
  const promise2 = app.request(`/sql/db?${queryToParams(query)}`);
  const promise3 = app.request(`/sql/db?${queryToParams(query)}`);
  const [response1, response2, response3] = await Promise.all([
    promise1,
    promise2,
    promise3,
  ]);
  expect(response1.status).toBe(200);
  expect(response2.status).toBe(200);
  expect(response3.status).toBe(200);
  expect(transactionSpy).toHaveBeenCalledTimes(1);
});
</file>

<file path="packages/core/src/client/index.ts">
import type { PonderApp6 } from "@/database/index.js";
import { getLiveQueryChannelName } from "@/drizzle/onchain.js";
import type { Schema } from "@/internal/types.js";
import type { ReadonlyDrizzle } from "@/types/db.js";
import {
  type PromiseWithResolvers,
  promiseWithResolvers,
} from "@/utils/promiseWithResolvers.js";
import {
  getSQLQueryRelations,
  validateAllowableSQLQuery,
} from "@/utils/sql-parse.js";
import {
  type QueryWithTypings,
  getTableName,
  getViewName,
  isTable,
  isView,
} from "drizzle-orm";
import {
  type PgDialect,
  type PgSession,
  type PgView,
  getViewConfig,
  pgSchema,
  pgTable,
} from "drizzle-orm/pg-core";
import { createMiddleware } from "hono/factory";
import { streamSSE } from "hono/streaming";
import type * as pg from "pg";
import superjson from "superjson";
type QueryString = string;
type QueryResult = unknown;
const MAX_LIVE_QUERIES = 1000;
/**
 * @dev This is copied to avoid bundling another dependency.
 */
const getPonderMetaTable = (schema?: string) => {
  if (schema === undefined || schema === "public") {
    return pgTable("_ponder_meta", (t) => ({
      key: t.text().primaryKey().$type<"app">(),
      value: t.jsonb().$type<PonderApp6>().notNull(),
    }));
  }
  return pgSchema(schema).table("_ponder_meta", (t) => ({
    key: t.text().primaryKey().$type<"app">(),
    value: t.jsonb().$type<PonderApp6>().notNull(),
  }));
};
/**
 * Middleware for `@ponder/client`.
 *
 * @param db - Drizzle database instance
 * @param schema - Ponder schema
 *
 * @example
 * ```ts
 * import { db } from "ponder:api";
 * import schema from "ponder:schema";
 * import { Hono } from "hono";
 * import { client } from "ponder";
 *
 * const app = new Hono();
 *
 * app.use("/sql/*", client({ db, schema }));
 *
 * export default app;
 * ```
 */
export const client = ({
  db,
  schema,
}: { db: ReadonlyDrizzle<Schema>; schema: Schema }) => {
  if (
    globalThis.PONDER_COMMON === undefined ||
    globalThis.PONDER_DATABASE === undefined ||
    globalThis.PONDER_NAMESPACE_BUILD === undefined ||
    globalThis.PONDER_PRE_BUILD === undefined
  ) {
    throw new Error(
      "client() middleware cannot be initialized outside of a Ponder project",
    );
  }
  const tables = Object.values(schema).filter(isTable);
  const views = Object.values(schema).filter(isView);
  const tableNames = new Set(tables.map(getTableName));
  const viewNames = new Set(views.map(getViewName));
  // Note: Add system tables to the live query registry.
  tableNames.add("_ponder_checkpoint");
  // @ts-ignore
  const session: PgSession = db._.session;
  // @ts-ignore
  const dialect: PgDialect = session.dialect;
  const driver = globalThis.PONDER_DATABASE.driver;
  const perTableResolver = new Map<string, PromiseWithResolvers<void>>();
  const perViewTables = new Map<string, Set<string>>();
  /** `true` if the app is indexing live blocks. */
  let liveQueryCount = 0;
  let isReady = false;
  (async () => {
    while (globalThis.PONDER_COMMON.apiShutdown.isKilled === false) {
      try {
        isReady = await globalThis.PONDER_DATABASE.readonlyQB.wrap(
          { label: "select_ready" },
          (db) =>
            db
              .select()
              .from(getPonderMetaTable())
              .then((result) => result[0]!.value.is_ready === 1),
        );
      } catch {}
      if (isReady) return;
      await new Promise((resolve) => setTimeout(resolve, 1000));
    }
  })();
  const cache = new Map<QueryString, WeakRef<Promise<unknown>>>();
  const perQueryReferences = new Map<QueryString, Set<string>>();
  const registry = new FinalizationRegistry<QueryString>((queryString) => {
    // Note: When a cache entry is garbage collected, delete the key from `perQueryReferences`.
    cache.delete(queryString);
    perQueryReferences.delete(queryString);
  });
  for (const table of tableNames) {
    perTableResolver.set(table, promiseWithResolvers<void>());
  }
  const parseViewPromise = (async () => {
    const unresolvedViewRelations = new Map<string, Set<string>>();
    for (const view of views) {
      const query = dialect.sqlToQuery(getViewConfig(view as PgView).query!);
      const relations = await getSQLQueryRelations(query.sql);
      unresolvedViewRelations.set(getViewName(view), relations);
    }
    /**
     * Recursively resolve nested views (views that reference other views).
     *
     * @dev This assumes views cannot be infinitely cursive - an invariant enforced by Postgres.
     */
    const resolveRelation = (relation: string): Set<string> => {
      if (perViewTables.has(relation)) {
        return perViewTables.get(relation)!;
      }
      if (tableNames.has(relation)) {
        return new Set([relation]);
      }
      if (viewNames.has(relation)) {
        const result = new Set<string>();
        for (const _relation of unresolvedViewRelations.get(relation)!) {
          for (const __relation of resolveRelation(_relation)) {
            result.add(__relation);
          }
        }
        return result;
      }
      return new Set();
    };
    for (const [viewName, relations] of unresolvedViewRelations) {
      const resolvedRelations = new Set<string>();
      for (const relation of relations) {
        for (const _relation of resolveRelation(relation)) {
          resolvedRelations.add(_relation);
        }
      }
      perViewTables.set(viewName, resolvedRelations);
    }
  })();
  if (driver.dialect === "pglite") {
    const channel = getLiveQueryChannelName(
      globalThis.PONDER_NAMESPACE_BUILD.schema,
    );
    driver.instance.query(`LISTEN "${channel}"`);
    driver.instance.onNotification((_, payload) => {
      const tables = JSON.parse(payload!) as string[];
      tables.push("_ponder_checkpoint");
      let invalidQueryCount = 0;
      for (const [queryString, referencedTables] of perQueryReferences) {
        let isQueryInvalid = false;
        for (const table of tables) {
          if (referencedTables.has(table)) {
            isQueryInvalid = true;
            break;
          }
        }
        if (isQueryInvalid) {
          invalidQueryCount++;
          const resultPromise = cache.get(queryString)?.deref();
          if (resultPromise) registry.unregister(resultPromise);
          cache.delete(queryString);
          perQueryReferences.delete(queryString);
        }
      }
      for (const table of tables) {
        perTableResolver.get(table)!.resolve();
        perTableResolver.set(table, promiseWithResolvers<void>());
      }
      if (invalidQueryCount > 0) {
        globalThis.PONDER_COMMON.logger.debug({
          msg: "Updated live queries",
          tables: JSON.stringify(Array.from(tables)),
          query_count: invalidQueryCount,
        });
      }
    });
  } else {
    (async () => {
      let client: pg.PoolClient | undefined;
      let hasRegisteredShutdown = false;
      while (globalThis.PONDER_COMMON.apiShutdown.isKilled === false) {
        // biome-ignore lint/suspicious/noAsyncPromiseExecutor: <explanation>
        await new Promise<void>(async (resolve) => {
          try {
            client = await driver.admin.connect();
            if (hasRegisteredShutdown === false) {
              globalThis.PONDER_COMMON.apiShutdown.add(() => {
                client?.release();
                client = undefined;
              });
              hasRegisteredShutdown = true;
            }
            globalThis.PONDER_COMMON.logger.info({
              msg: `Established listen connection for "@ponder/client" middleware`,
            });
            client.on("notification", (notification) => {
              let tables = JSON.parse(notification.payload!) as string[];
              // Convert partition names to table names
              if (
                globalThis.PONDER_PRE_BUILD.ordering === "experimental_isolated"
              ) {
                tables = tables.map((table) => {
                  const _table = table.split("_");
                  _table.pop();
                  return _table.join("_");
                });
              }
              tables.push("_ponder_checkpoint");
              let invalidQueryCount = 0;
              for (const [
                queryString,
                referencedTables,
              ] of perQueryReferences) {
                let isQueryInvalid = false;
                for (const table of tables) {
                  if (referencedTables.has(table)) {
                    isQueryInvalid = true;
                    break;
                  }
                }
                if (isQueryInvalid) {
                  invalidQueryCount++;
                  const resultPromise = cache.get(queryString)?.deref();
                  if (resultPromise) registry.unregister(resultPromise);
                  cache.delete(queryString);
                  perQueryReferences.delete(queryString);
                }
              }
              for (const table of tables) {
                perTableResolver.get(table)!.resolve();
                perTableResolver.set(table, promiseWithResolvers<void>());
              }
              if (invalidQueryCount > 0) {
                globalThis.PONDER_COMMON.logger.debug({
                  msg: "Updated live queries",
                  tables: JSON.stringify(tables),
                  query_count: invalidQueryCount,
                });
              }
            });
            client.on("error", async (error) => {
              globalThis.PONDER_COMMON.logger.warn({
                msg: `Failed listen connection for "@ponder/client" middleware`,
                retry_delay: 250,
                error,
              });
              client?.release();
              client = undefined;
              await new Promise((resolve) => setTimeout(resolve, 250));
              resolve();
            });
            const channel = getLiveQueryChannelName(
              globalThis.PONDER_NAMESPACE_BUILD.schema,
            );
            await client.query(`LISTEN "${channel}"`);
          } catch (error) {
            globalThis.PONDER_COMMON.logger.warn({
              msg: `Failed listen connection for "@ponder/client" middleware`,
              retry_delay: 250,
              error: error as Error,
            });
            client?.release();
            client = undefined;
            await new Promise((resolve) => setTimeout(resolve, 250));
            resolve();
          }
        });
      }
    })();
  }
  const getQueryResult = (query: QueryWithTypings): Promise<QueryResult> => {
    if (driver.dialect === "pglite") {
      return session.prepareQuery(query, undefined, undefined, false).execute();
    } else {
      return globalThis.PONDER_DATABASE.readonlyQB.raw.transaction(
        (tx) => {
          return tx._.session
            .prepareQuery(query, undefined, undefined, false)
            .execute();
        },
        { accessMode: "read only" },
      );
    }
  };
  return createMiddleware(async (c, next) => {
    const crypto = await import(/* webpackIgnore: true */ "node:crypto");
    await parseViewPromise;
    if (c.req.path === "/sql/db") {
      const queryString = c.req.query("sql");
      if (queryString === undefined) {
        return c.text('Missing "sql" query parameter', 400);
      }
      const query = superjson.parse(queryString) as QueryWithTypings;
      try {
        await validateAllowableSQLQuery(query.sql);
      } catch (error) {
        (error as Error).stack = undefined;
        return c.text((error as Error).message, 500);
      }
      const relations = await getSQLQueryRelations(query.sql);
      const referencedTables = new Set<string>();
      for (const relation of relations) {
        if (tableNames.has(relation)) {
          referencedTables.add(relation);
        } else if (viewNames.has(relation)) {
          for (const tableName of perViewTables.get(relation)!) {
            referencedTables.add(tableName);
          }
        }
      }
      let resultPromise: Promise<unknown>;
      if (isReady === false) {
        resultPromise = getQueryResult(query);
      } else if (cache.has(queryString)) {
        const resultRef = cache.get(queryString)!.deref();
        if (resultRef === undefined) {
          cache.delete(queryString);
          resultPromise = getQueryResult(query);
          cache.set(queryString, new WeakRef(resultPromise));
          perQueryReferences.set(queryString, referencedTables);
          registry.register(resultPromise, queryString);
        } else {
          resultPromise = resultRef;
        }
      } else {
        resultPromise = getQueryResult(query);
        cache.set(queryString, new WeakRef(resultPromise));
        perQueryReferences.set(queryString, referencedTables);
        registry.register(resultPromise, queryString);
      }
      try {
        return c.json((await resultPromise) as object);
      } catch (error) {
        (error as Error).stack = undefined;
        return c.text((error as Error).message, 500);
      }
    }
    if (c.req.path === "/sql/live") {
      if (isReady === false) {
        return c.text(
          "Live queries are not available until the backfill is complete",
          503,
        );
      }
      if (liveQueryCount >= MAX_LIVE_QUERIES) {
        return c.text("Maximum number of live queries reached", 503);
      }
      liveQueryCount++;
      c.header("Content-Type", "text/event-stream");
      c.header("Cache-Control", "no-cache");
      c.header("Connection", "keep-alive");
      const queryString = c.req.query("sql");
      if (queryString === undefined) {
        return c.text('Missing "sql" query parameter', 400);
      }
      const query = superjson.parse(queryString) as QueryWithTypings;
      try {
        await validateAllowableSQLQuery(query.sql);
      } catch (error) {
        (error as Error).stack = undefined;
        return c.text((error as Error).message, 500);
      }
      const relations = await getSQLQueryRelations(query.sql);
      const referencedTables = new Set<string>();
      for (const relation of relations) {
        if (tableNames.has(relation)) {
          referencedTables.add(relation);
        } else if (viewNames.has(relation)) {
          for (const tableName of perViewTables.get(relation)!) {
            referencedTables.add(tableName);
          }
        }
      }
      let result: QueryResult;
      if (cache.has(queryString)) {
        const resultRef = cache.get(queryString)!.deref();
        if (resultRef === undefined) {
          cache.delete(queryString);
          const resultPromise = getQueryResult(query);
          cache.set(queryString, new WeakRef(resultPromise));
          perQueryReferences.set(queryString, referencedTables);
          registry.register(resultPromise, queryString);
          result = await resultPromise;
        } else {
          result = await resultRef;
        }
      } else {
        const resultPromise = getQueryResult(query);
        cache.set(queryString, new WeakRef(resultPromise));
        perQueryReferences.set(queryString, referencedTables);
        registry.register(resultPromise, queryString);
        result = await resultPromise;
      }
      let resultHash = crypto
        .createHash("MD5")
        // @ts-ignore
        .update(JSON.stringify(result.rows))
        .digest("hex")
        .slice(0, 10);
      return streamSSE(c, async (stream) => {
        stream.onAbort(() => {
          liveQueryCount--;
        });
        while (stream.closed === false && stream.aborted === false) {
          await Promise.race(
            Array.from(referencedTables).map(
              (relation) => perTableResolver.get(relation)!.promise,
            ),
          );
          try {
            let resultPromise: Promise<unknown>;
            if (cache.has(queryString)) {
              const resultRef = cache.get(queryString)!.deref();
              if (resultRef === undefined) {
                cache.delete(queryString);
                resultPromise = getQueryResult(query);
                cache.set(queryString, new WeakRef(resultPromise));
                perQueryReferences.set(queryString, referencedTables);
                registry.register(resultPromise, queryString);
              } else {
                resultPromise = resultRef;
              }
            } else {
              resultPromise = getQueryResult(query);
              cache.set(queryString, new WeakRef(resultPromise));
              perQueryReferences.set(queryString, referencedTables);
              registry.register(resultPromise, queryString);
            }
            const result = await resultPromise;
            const _resultHash = crypto
              .createHash("MD5")
              // @ts-ignore
              .update(JSON.stringify(result.rows))
              .digest("hex")
              .slice(0, 10);
            if (_resultHash === resultHash) continue;
            resultHash = _resultHash;
            // @ts-ignore
            await stream.writeSSE({ data: JSON.stringify(result) });
          } catch {
            stream.abort();
          }
        }
      });
    }
    return next();
  });
};
</file>

<file path="packages/core/src/config/address.test-d.ts">
import { type AbiEvent, parseAbiItem } from "abitype";
import { test } from "vitest";
import { factory } from "./address.js";
const event0 = parseAbiItem(
  "event Event0(bytes32 indexed arg, bytes32 indexed arg1)",
);
const func = parseAbiItem("function func()");
test("factory with invalid event", () => {
  factory({
    // ^?
    address: "0x",
    // @ts-expect-error
    event: func,
    parameter: "arg",
  });
});
test("factory with weak event", () => {
  factory({
    // ^?
    address: "0x",
    event: {} as AbiEvent,
    parameter: "arg",
  });
});
test("factory", () => {
  factory({
    //  ^?
    address: "0x",
    event: event0,
    parameter: "arg",
  });
});
const event1 = parseAbiItem([
  "struct Foo {address arg0;address arg1;address arg2;address arg3;uint256 arg4;}",
  "event CreateMarket(Id indexed id, Foo args)",
]);
test("factory", () => {
  factory({
    address: "0xa",
    event: event1,
    parameter: "args.arg2",
  });
});
</file>

<file path="packages/core/src/config/address.ts">
import type { AbiEvent, AbiParameter } from "viem";
// Note: Currently limit the depth to 1 level.
type ParameterNames<T extends AbiParameter> = T extends {
  components: readonly AbiParameter[];
}
  ? T["components"][number] extends {
      components: readonly AbiParameter[];
    }
    ? never
    : `${T["name"]}.${T["components"][number]["name"]}`
  : T["name"];
export type Factory<event extends AbiEvent = AbiEvent> = {
  /** Address of the factory contract that creates this contract. */
  address?: `0x${string}` | readonly `0x${string}`[];
  /** ABI event that announces the creation of a new instance of this contract. */
  event: event;
  /** Name of the factory event parameter that contains the new child contract address. */
  parameter: Exclude<ParameterNames<event["inputs"][number]>, undefined>;
  /** From block */
  startBlock?: number | "latest";
  /** To block */
  endBlock?: number | "latest";
};
export const factory = <event extends AbiEvent>(factory: Factory<event>) =>
  factory;
export type AddressConfig = {
  address?: `0x${string}` | readonly `0x${string}`[] | Factory;
};
</file>

<file path="packages/core/src/config/eventFilter.ts">
import type { Abi, GetEventArgs } from "viem";
import type { ParseAbiEvent, SafeEventNames } from "./utilityTypes.js";
type FilterArgs<abi extends Abi, event extends string> = GetEventArgs<
  abi,
  string,
  {
    EnableUnion: true;
    IndexedOnly: true;
    Required: false;
  },
  ParseAbiEvent<abi, event>
>;
export type GetEventFilter<
  abi extends Abi,
  ///
  safeEventNames extends string = SafeEventNames<abi>,
> = {
  filter?:
    | (safeEventNames extends safeEventNames
        ? {
            event: safeEventNames;
            args: FilterArgs<abi, safeEventNames>;
          }
        : never)
    | (safeEventNames extends safeEventNames
        ? {
            event: safeEventNames;
            args: FilterArgs<abi, safeEventNames>;
          }
        : never)[];
};
</file>

<file path="packages/core/src/config/index.test.ts">
import { type Abi, parseAbiItem } from "viem";
import { expectTypeOf, test } from "vitest";
import { factory } from "./address.js";
import { createConfig } from "./index.js";
const event0 = parseAbiItem(
  "event Event0(bytes32 indexed arg, bytes32 indexed arg1)",
);
const event1 = parseAbiItem("event Event1()");
const func = parseAbiItem("function func()");
test("createConfig basic", () => {
  createConfig({
    chains: {
      mainnet: {
        id: 1,
        rpc: "https://rpc.com",
      },
      optimism: {
        id: 10,
        rpc: "https://rpc.com",
      },
    },
    contracts: {
      c1: {
        abi: [event1],
        chain: "mainnet",
        startBlock: 0,
      },
      c2: {
        abi: [event1],
        chain: "optimism",
        startBlock: 0,
      },
    },
  });
});
test("createConfig no extra properties", () => {
  createConfig({
    chains: {
      mainnet: {
        id: 1,
        rpc: "https://rpc.com",
        // @ts-expect-error
        a: 0,
      },
    },
    contracts: {
      c2: {
        abi: [event0],
        chain: "mainnet",
        // @ts-expect-error
        a: 0,
      },
    },
  });
});
test("createConfig address", () => {
  createConfig({
    chains: {
      mainnet: {
        id: 1,
        rpc: "https://rpc.com",
      },
      optimism: {
        id: 10,
        rpc: "https://rpc.com",
      },
    },
    contracts: {
      c2: {
        abi: [event1],
        chain: "mainnet",
        address: "0x1",
      },
    },
  });
});
test("createConfig factory", () => {
  createConfig({
    chains: {
      mainnet: {
        id: 1,
        rpc: "https://rpc.com",
      },
      optimism: {
        id: 10,
        rpc: "https://rpc.com",
      },
    },
    contracts: {
      c2: {
        abi: [event1],
        chain: "mainnet",
        address: factory({
          address: "0x",
          event: event0,
          parameter: "arg",
        }),
      },
    },
  });
});
test("createConfig with filter", () => {
  createConfig({
    chains: {
      mainnet: {
        id: 1,
        rpc: "https://rpc.com",
      },
      optimism: {
        id: 10,
        rpc: "https://rpc.com",
      },
    },
    contracts: {
      c2: {
        abi: [event0, event1],
        chain: "mainnet",
        filter: {
          event: "Event0",
          //^?
          args: {
            arg: ["0x"],
            //^?
          },
        },
      },
    },
  });
});
test("createConfig with multiple filters", () => {
  createConfig({
    chains: {
      mainnet: {
        id: 1,
        rpc: "https://rpc.com",
      },
      optimism: {
        id: 10,
        rpc: "https://rpc.com",
      },
    },
    contracts: {
      c2: {
        abi: [event0, event1],
        chain: "mainnet",
        filter: [
          {
            event: "Event0",
            //^?
            args: {
              arg: ["0x"],
              //^?
            },
          },
          {
            event: "Event1",
            args: [],
          },
        ],
      },
    },
  });
});
test("createConfig chain overrides", () => {
  createConfig({
    chains: {
      mainnet: {
        id: 1,
        rpc: "https://rpc.com",
      },
      optimism: {
        id: 10,
        rpc: "https://rpc.com",
      },
    },
    contracts: {
      c1: {
        abi: [event1],
        chain: "mainnet",
        startBlock: 0,
      },
      c2: {
        abi: [event0, event1],
        chain: {
          optimism: {
            address: "0x",
            filter: {
              event: "Event0",
              args: {
                arg: ["0x"],
              },
            },
          },
        },
        startBlock: 0,
      },
    },
  });
});
test("createConfig weak Abi", () => {
  const abi = [event0, func] as Abi;
  createConfig({
    chains: {
      mainnet: {
        id: 1,
        rpc: "https://rpc.com",
      },
      optimism: {
        id: 10,
        rpc: "https://rpc.com",
      },
    },
    contracts: {
      c2: {
        abi,
        chain: "mainnet",
        filter: {
          event: "event0",
          //^?
          args: {},
          //^?
        },
      },
    },
  });
});
test("createConfig strict return type", () => {
  const config = createConfig({
    //  ^?
    chains: {
      mainnet: {
        id: 1,
        rpc: "https://rpc.com",
      },
      optimism: {
        id: 10,
        rpc: "https://rpc.com",
      },
    },
    contracts: {
      c2: {
        abi: [event0, event1],
        chain: "mainnet",
        filter: {
          event: "Event0",
          args: {
            arg: ["0x"],
          },
        },
      },
    },
  });
  expectTypeOf<{ mainnet: { id: 1; rpc: string } }>(config.chains);
  expectTypeOf<{
    c2: {
      abi: readonly [typeof event0, typeof event1];
      chain: "mainnet";
      filter: {
        event: "Event0";
        args: {
          arg: ["0x"];
        };
      };
    };
  }>(config.contracts);
});
test("createConfig accounts", () => {
  createConfig({
    chains: {
      mainnet: {
        id: 1,
        rpc: "https://rpc.com",
      },
      optimism: {
        id: 10,
        rpc: "https://rpc.com",
      },
    },
    accounts: {
      me: {
        chain: "mainnet",
        address: ["0x"],
      },
    },
  });
});
</file>

<file path="packages/core/src/config/index.ts">
import type { ConnectionOptions } from "node:tls";
import type { Prettify } from "@/types/utils.js";
import type { Abi } from "abitype";
import type { PoolConfig } from "pg";
import type { Narrow, Transport } from "viem";
import type { AddressConfig } from "./address.js";
import type { GetEventFilter } from "./eventFilter.js";
export type Config = {
  database?: DatabaseConfig;
  ordering?: "omnichain" | "multichain" | "experimental_isolated";
  chains: { [chainName: string]: ChainConfig<unknown> };
  contracts: { [contractName: string]: GetContract };
  accounts: { [accountName: string]: AccountConfig<unknown> };
  blocks: {
    [sourceName: string]: GetBlockFilter<unknown>;
  };
};
export type CreateConfigReturnType<chains, contracts, accounts, blocks> = {
  database?: DatabaseConfig;
  ordering?: "omnichain" | "multichain" | "experimental_isolated";
  chains: chains;
  contracts: contracts;
  accounts: accounts;
  blocks: blocks;
};
export const createConfig = <
  const chains,
  const contracts = {},
  const accounts = {},
  const blocks = {},
>(config: {
  database?: DatabaseConfig;
  ordering?: "omnichain" | "multichain" | "experimental_isolated";
  // TODO: add jsdoc to these properties.
  chains: ChainsConfig<Narrow<chains>>;
  contracts?: ContractsConfig<chains, Narrow<contracts>>;
  accounts?: AccountsConfig<chains, Narrow<accounts>>;
  blocks?: BlockFiltersConfig<chains, blocks>;
}): CreateConfigReturnType<chains, contracts, accounts, blocks> =>
  config as Prettify<
    CreateConfigReturnType<chains, contracts, accounts, blocks>
  >;
// database
type DatabaseConfig =
  | {
      kind: "pglite";
      /** Directory path to use for PGlite database files. Default: `".ponder/pglite"`. */
      directory?: string;
    }
  | {
      kind: "postgres";
      /** Postgres database connection string. Default: `DATABASE_PRIVATE_URL` > `DATABASE_URL` environment variable. */
      connectionString?: string;
      /** Postgres pool configuration passed to `node-postgres`. */
      poolConfig?: {
        /** Maximum number of clients in the pool. Default: `30`. */
        max?: number;
        /** Enable SSL, or provide a custom SSL configuration. Default: `undefined`. */
        ssl?: boolean | Prettify<ConnectionOptions>;
      } & Omit<PoolConfig, "max" | "ssl" | "connectionString">;
    };
// base
type BlockConfig = {
  /** Block number at which to start indexing events (inclusive). If `undefined`, events will be processed from block 0. Default: `undefined`. */
  startBlock?: number | "latest";
  /** Block number at which to stop indexing events (inclusive). If `undefined`, events will be processed in real-time. Default: `undefined`. */
  endBlock?: number | "latest";
};
type TransactionReceiptConfig = {
  includeTransactionReceipts?: boolean;
};
type FunctionCallConfig = {
  /*
   * Enable call trace indexing for this contract.
   *
   * - Docs: https://ponder.sh/docs/guides/call-traces
   */
  includeCallTraces?: boolean;
};
// chain
type ChainConfig<chain> = {
  /** Chain ID of the chain. */
  id: chain extends { id: infer id extends number } ? id | number : number;
  /** RPC url. */
  rpc: string | string[] | Transport | undefined;
  ws?: string;
  /** Polling interval (in ms). Default: `1_000`. */
  pollingInterval?: number;
  /**
   * Maximum number of RPC requests per second.
   * @deprecated Handled automatically instead.
   */
  maxRequestsPerSecond?: number;
  /** Disable RPC request caching. Default: `false`. */
  disableCache?: boolean;
  /**
   * Maximum block range for eth_getLogs. If undefined, Ponder will
   * attempt to determine the block range automatically based on error messages.
   */
  ethGetLogsBlockRange?: number;
};
type ChainsConfig<chains> = {} extends chains
  ? {}
  : {
      [chainName in keyof chains]: ChainConfig<chains[chainName]>;
    };
// contracts
type AbiConfig<abi extends Abi | readonly unknown[]> = {
  /** Contract application byte interface. */
  abi: abi;
};
type GetContractChain<
  chains,
  abi extends Abi,
  ///
  allChainNames extends string = [keyof chains] extends [never]
    ? string
    : keyof chains & string,
> = {
  /**
   * Chain that this contract is deployed to. Must match a chain name in `chains`.
   * Any filter information overrides the values in the higher level "contracts" property.
   */
  chain:
    | allChainNames
    | {
        [name in allChainNames]?: Prettify<
          AddressConfig &
            GetEventFilter<abi> &
            TransactionReceiptConfig &
            FunctionCallConfig &
            BlockConfig
        >;
      };
};
type ContractConfig<chains, abi extends Abi> = Prettify<
  AbiConfig<abi> &
    GetContractChain<chains, abi> &
    AddressConfig &
    GetEventFilter<abi> &
    TransactionReceiptConfig &
    FunctionCallConfig &
    BlockConfig
>;
type GetContract<chains = unknown, contract = unknown> = contract extends {
  abi: infer abi extends Abi;
}
  ? // 1. Contract has a valid abi
    ContractConfig<chains, abi>
  : // 2. Contract has an invalid abi
    ContractConfig<chains, Abi>;
type ContractsConfig<chains, contracts> = {} extends contracts
  ? // contracts empty, return empty
    {}
  : {
      [name in keyof contracts]: GetContract<chains, contracts[name]>;
    };
// accounts
type GetAccountChain<
  chains,
  ///
  allChainNames extends string = [keyof chains] extends [never]
    ? string
    : keyof chains & string,
> = {
  /**
   * Chain that this account is deployed to. Must match a chain name in `chains`.
   * Any filter information overrides the values in the higher level "accounts" property.
   */
  chain:
    | allChainNames
    | {
        [name in allChainNames]?: Prettify<
          AddressConfig & TransactionReceiptConfig & BlockConfig
        >;
      };
};
type AccountConfig<chains> = Prettify<
  GetAccountChain<chains> &
    Required<AddressConfig> &
    TransactionReceiptConfig &
    BlockConfig
>;
type AccountsConfig<chains, accounts> = {} extends accounts
  ? {}
  : {
      [name in keyof accounts]: AccountConfig<chains>;
    };
// blocks
type BlockFilterConfig = {
  /** Block number at which to start indexing events (inclusive). If `undefined`, events will be processed from block 0. Default: `undefined`. */
  startBlock?: number | "latest";
  /** Block number at which to stop indexing events (inclusive). If `undefined`, events will be processed in real-time. Default: `undefined`. */
  endBlock?: number | "latest";
  interval?: number;
};
type GetBlockFilter<
  chains,
  ///
  allChainNames extends string = [keyof chains] extends [never]
    ? string
    : keyof chains & string,
> = BlockFilterConfig & {
  chain:
    | allChainNames
    | {
        [name in allChainNames]?: BlockFilterConfig;
      };
};
type BlockFiltersConfig<chains = unknown, blocks = unknown> = {} extends blocks
  ? {}
  : {
      [name in keyof blocks]: GetBlockFilter<chains>;
    };
</file>

<file path="packages/core/src/config/utilityTypes.test-d.ts">
import type { AbiEvent, AbiFunction, ParseAbiItem } from "abitype";
import type { Abi } from "viem";
import { assertType, test } from "vitest";
import type {
  ExtractAbiEvents,
  ExtractAbiFunctions,
  FormatAbiEvent,
  FormatAbiFunction,
  ParseAbiEvent,
  ParseAbiFunction,
  SafeEventNames,
  SafeFunctionNames,
} from "./utilityTypes.js";
type Event0 = ParseAbiItem<"event Event0(bytes32 indexed arg)">;
type Event1 = ParseAbiItem<"event Event1()">;
type Event1Overloaded = ParseAbiItem<"event Event1(bytes32 indexed)">;
type Func0 = ParseAbiItem<"function func0(address) external returns (uint256)">;
type Func1 = ParseAbiItem<"function func1()">;
type Func1Overloaded = ParseAbiItem<"function func1(bytes32)">;
test("ExtractAbiEvents", () => {
  type a = ExtractAbiEvents<readonly [Event0, Event1, Event1Overloaded, Func0]>;
  //   ^?
  assertType<Event0 | Event1 | Event1Overloaded>({} as unknown as a);
  assertType<a>({} as unknown as Event0 | Event1 | Event1Overloaded);
});
test("ExtractAbiEvents semi-weak abi", () => {
  type a = ExtractAbiEvents<(Event0 | Event1 | Event1Overloaded | Func0)[]>;
  //   ^?
  assertType<Event0 | Event1 | Event1Overloaded>({} as unknown as a);
  assertType<a>({} as unknown as Event0 | Event1 | Event1Overloaded);
});
test("ExtractAbiEvents no events", () => {
  type a = ExtractAbiEvents<readonly [Func0]>;
  //   ^?
  assertType<AbiEvent>({} as unknown as a);
  assertType<a>({} as unknown as AbiEvent);
});
test("ExtractAbiFunctions", () => {
  type a = ExtractAbiFunctions<
    // ^?
    readonly [Func0, Func1, Func1Overloaded, Event0]
  >;
  assertType<Func0 | Func1 | Func1Overloaded>({} as unknown as a);
  assertType<a>({} as unknown as Func0 | Func1 | Func1Overloaded);
});
test("ExtractAbiFunctions semi-weak abi", () => {
  type a = ExtractAbiFunctions<(Func0 | Func1 | Func1Overloaded | Event0)[]>;
  //   ^?
  assertType<Func0 | Func1 | Func1Overloaded>({} as unknown as a);
  assertType<a>({} as unknown as Func0 | Func1 | Func1Overloaded);
});
test("ExtractAbiFunctions no events", () => {
  type a = ExtractAbiFunctions<readonly [Event0]>;
  //   ^?
  assertType<AbiFunction>({} as unknown as a);
  assertType<a>({} as unknown as AbiFunction);
});
test("ParseAbiEvent no overloaded events ", () => {
  type a = ParseAbiEvent<
    // ^?
    readonly [Event0, Event1],
    "Event0"
  >;
  assertType<Event0>({} as unknown as a);
  assertType<a>({} as unknown as Event0);
});
test("ParseAbiEvent overloaded events", () => {
  type a = ParseAbiEvent<
    // ^?
    readonly [Event1, Event1Overloaded],
    "Event1(bytes32 indexed)"
  >;
  assertType<Event1Overloaded>({} as unknown as a);
  assertType<a>({} as unknown as Event1Overloaded);
});
test("ParseAbiEvent with semi-weak abi", () => {
  type a = ParseAbiEvent<
    // ^?
    (Event0 | Event1)[],
    "Event0"
  >;
  assertType<Event0>({} as unknown as a);
  assertType<a>({} as unknown as Event0);
});
test("ParseAbiEvent with weak abi", () => {
  type a = ParseAbiEvent<
    // ^?
    Abi,
    "Event0"
  >;
  assertType<AbiEvent>({} as unknown as a);
  assertType<a>({} as unknown as AbiEvent);
});
test("ParseAbiEvent no matching events", () => {
  type a = ParseAbiEvent<
    // ^?
    readonly [Event0, Event1],
    "Event2"
  >;
  assertType<AbiEvent>({} as unknown as a);
  assertType<a>({} as unknown as AbiEvent);
});
test("ParseAbiFunction no overloaded events ", () => {
  type a = ParseAbiFunction<
    // ^?
    readonly [Func0, Func1],
    "func0()"
  >;
  assertType<Func0>({} as unknown as a);
  assertType<a>({} as unknown as Func0);
});
test("ParseAbiFunction overloaded events", () => {
  type a = ParseAbiFunction<
    // ^?
    readonly [Func1, Func1Overloaded],
    "func1()"
  >;
  assertType<Func1>({} as unknown as a);
  assertType<a>({} as unknown as Func1);
});
test("ParseAbiFunction with semi-weak abi", () => {
  type a = ParseAbiFunction<
    // ^?
    (Func0 | Func1)[],
    "func0()"
  >;
  assertType<Func0>({} as unknown as a);
  assertType<a>({} as unknown as Func0);
});
test("ParseAbiFunction with weak abi", () => {
  type a = ParseAbiFunction<
    // ^?
    Abi,
    "func0()"
  >;
  assertType<AbiFunction>({} as unknown as a);
  assertType<a>({} as unknown as AbiFunction);
});
test("ParseAbiFunction no matching events", () => {
  type a = ParseAbiFunction<
    // ^?
    readonly [Func0, Func1],
    "func2()"
  >;
  assertType<AbiFunction>({} as unknown as a);
  assertType<a>({} as unknown as AbiFunction);
});
test("FormatAbiEvent no overloaded events", () => {
  type a = FormatAbiEvent<readonly [Event0, Event1], Event0>;
  //   ^?
  assertType<"Event0">({} as unknown as a);
  assertType<a>({} as unknown as "Event0");
});
test("FormatAbiEvent overloaded events", () => {
  type a = FormatAbiEvent<
    // ^?
    readonly [Event1, Event1Overloaded],
    Event1Overloaded
  >;
  assertType<"Event1(bytes32 indexed)">({} as unknown as a);
  assertType<a>({} as unknown as "Event1(bytes32 indexed)");
});
test("FormatAbiEvent with semi-weak abi", () => {
  type a = FormatAbiEvent<readonly (Event0 | Event1)[], Event0>;
  //   ^?
  assertType<"Event0">({} as unknown as a);
  assertType<a>({} as unknown as "Event0");
});
test("FormatAbiEvent with weak abi", () => {
  type a = FormatAbiEvent<Abi, Event0>;
  //   ^?
  assertType<"Event0">({} as unknown as a);
  assertType<a>({} as unknown as "Event0");
});
test("FormatAbiEvent with no matching events", () => {
  type a = FormatAbiEvent<
    // ^?
    readonly [Event1, Event1Overloaded],
    Event0
  >;
  assertType<never>({} as unknown as a);
  assertType<a>({} as unknown as never);
});
test("FormatAbiFunction no overloaded events", () => {
  type a = FormatAbiFunction<readonly [Func0, Func1], Func0>;
  //   ^?
  assertType<"func0()">({} as unknown as a);
  assertType<a>({} as unknown as "func0()");
});
test("FormatAbiFunction overloaded events", () => {
  type a = FormatAbiFunction<
    // ^?
    readonly [Func1, Func1Overloaded],
    Func1Overloaded
  >;
  assertType<"func1(bytes32)">({} as unknown as a);
  assertType<a>({} as unknown as "func1(bytes32)");
});
test("FormatAbiFunction with semi-weak abi", () => {
  type a = FormatAbiFunction<readonly (Func0 | Func1)[], Func0>;
  //   ^?
  assertType<"func0()">({} as unknown as a);
  assertType<a>({} as unknown as "func0()");
});
test("FormatAbiFunction with weak abi", () => {
  type a = FormatAbiFunction<Abi, Func0>;
  //   ^?
  assertType<"func0()">({} as unknown as a);
  assertType<a>({} as unknown as "func0()");
});
test("FormatAbiFunction with no matching events", () => {
  type a = FormatAbiFunction<
    // ^?
    readonly [Func1, Func1Overloaded],
    Func0
  >;
  assertType<never>({} as unknown as a);
  assertType<a>({} as unknown as never);
});
test("SafeEventNames no overloaded events", () => {
  type a = SafeEventNames<
    // ^?
    readonly [Event0, Event1, Func0]
  >;
  assertType<"Event0" | "Event1">({} as unknown as a);
  assertType<a>({} as unknown as "Event0" | "Event1");
});
test("SafeEventNames overloaded events", () => {
  type a = SafeEventNames<
    // ^?
    readonly [Event0, Event1, Event1Overloaded, Func0]
  >;
  assertType<"Event0" | "Event1()" | "Event1(bytes32 indexed)">(
    {} as unknown as a,
  );
  assertType<a>(
    {} as unknown as "Event0" | "Event1()" | "Event1(bytes32 indexed)",
  );
});
test("SafeEventNames semi-weak abi", () => {
  type a = SafeEventNames<
    // ^?
    (Event0 | Event1 | Func0)[]
  >;
  assertType<"Event0" | "Event1">({} as unknown as a);
  assertType<a>({} as unknown as "Event0" | "Event1");
});
test("SafeEventNames weak abi", () => {
  type a = SafeEventNames<Abi>;
  //   ^?
  assertType<string>({} as unknown as a);
  assertType<a>({} as unknown as string);
});
test("SafeFunctionNames no overloaded events", () => {
  type a = SafeFunctionNames<
    // ^?
    readonly [Func0, Func1, Event0]
  >;
  assertType<"func0()" | "func1()">({} as unknown as a);
  assertType<a>({} as unknown as "func0()" | "func1()");
});
test("SafeFunctionNames overloaded events", () => {
  type a = SafeFunctionNames<
    // ^?
    readonly [Func0, Func1, Func1Overloaded, Event0]
  >;
  assertType<"func0()" | "func1()" | "func1(bytes32)">({} as unknown as a);
  assertType<a>({} as unknown as "func0()" | "func1()" | "func1(bytes32)");
});
test("SafeFunctionNames semi-weak abi", () => {
  type a = SafeFunctionNames<
    // ^?
    (Func0 | Func1 | Event0)[]
  >;
  assertType<"func0()" | "func1()">({} as unknown as a);
  assertType<a>({} as unknown as "func0()" | "func1()");
});
test("SafeFunctionNames weak abi", () => {
  type a = SafeFunctionNames<Abi>;
  //   ^?
  assertType<`${string}()`>({} as unknown as a);
  assertType<a>({} as unknown as `${string}()`);
});
</file>

<file path="packages/core/src/config/utilityTypes.ts">
import type {
  Abi,
  AbiEvent,
  AbiFunction,
  AbiParametersToPrimitiveTypes,
  FormatAbiItem,
} from "abitype";
import type { GetEventArgs, ParseAbiItem } from "viem";
export type NonStrictPick<T, K> = {
  [P in Extract<keyof T, K>]: T[P];
};
export type ExtractAbiEvents<
  abi extends Abi,
  events = Extract<abi[number], { type: "event" }>,
> = [events] extends [never] ? AbiEvent : events;
export type ExtractAbiFunctions<
  abi extends Abi,
  functions = Extract<abi[number], { type: "function" }>,
> = [functions] extends [never] ? AbiFunction : functions;
/** Return the abi event given the abi and compact signature. */
export type ParseAbiEvent<
  abi extends Abi,
  signature extends string,
  ///
  abiEvents extends AbiEvent = ExtractAbiEvents<abi>,
  noOverloadEvent = Extract<abiEvents, { name: signature }>,
  overloadEvent = Extract<abiEvents, ParseAbiItem<`event ${signature}`>>,
> = [noOverloadEvent] extends [never]
  ? [overloadEvent] extends [never]
    ? AbiEvent
    : overloadEvent
  : noOverloadEvent;
/** Return the abi function given the abi and compact signature. */
export type ParseAbiFunction<
  abi extends Abi,
  signature extends string,
  ///
  abiFunctions extends AbiFunction = ExtractAbiFunctions<abi>,
  noOverloadFunction = Extract<
    abiFunctions,
    { name: signature extends `${infer _signature}()` ? _signature : never }
  >,
  overloadFunction = Extract<
    abiFunctions,
    ParseAbiItem<`function ${signature}`>
  >,
> = [overloadFunction] extends [never]
  ? [noOverloadFunction] extends [never]
    ? AbiFunction
    : noOverloadFunction
  : overloadFunction;
/** Return the compact signature given the abi and abi event. */
export type FormatAbiEvent<
  abi extends Abi,
  event extends AbiEvent,
  ///
  abiEvents extends AbiEvent = ExtractAbiEvents<abi>,
  matchingNameEvents extends AbiEvent = Extract<
    abiEvents,
    { name: event["name"] }
  >,
> = [matchingNameEvents] extends [never]
  ? Abi extends abi
    ? event["name"]
    : never
  : [Exclude<matchingNameEvents, event>] extends [never]
    ? event["name"]
    : FormatAbiItem<event> extends `event ${infer signature}`
      ? signature
      : never;
/** Return the compact signature given the abi and abi function. */
export type FormatAbiFunction<
  abi extends Abi,
  _function extends AbiFunction,
  ///
  abiFunctions extends AbiFunction = ExtractAbiFunctions<abi>,
  matchingNameFunctions extends AbiFunction = Extract<
    abiFunctions,
    { name: _function["name"] }
  >,
> = [matchingNameFunctions] extends [never]
  ? Abi extends abi
    ? `${_function["name"]}()`
    : never
  : [Exclude<matchingNameFunctions, _function>] extends [never]
    ? `${_function["name"]}()`
    : FormatAbiItem<_function> extends `function ${infer signature}`
      ? signature
      : never;
/**
 * Return an union of safe event names that handle event overriding.
 */
export type SafeEventNames<
  abi extends Abi,
  ///
  abiEvents extends AbiEvent = ExtractAbiEvents<abi>,
> = abiEvents extends abiEvents ? FormatAbiEvent<abi, abiEvents> : never;
/**
 * Return an union of safe function names that handle function overriding.
 */
export type SafeFunctionNames<
  abi extends Abi,
  ///
  abiFunctions extends AbiFunction = ExtractAbiFunctions<abi>,
> = abiFunctions extends abiFunctions
  ? FormatAbiFunction<abi, abiFunctions>
  : never;
export type FormatEventArgs<
  abi extends Abi,
  signature extends string,
> = GetEventArgs<
  Abi,
  string,
  {
    EnableUnion: false;
    IndexedOnly: false;
    Required: true;
  },
  ParseAbiEvent<abi, signature>
>;
export type FormatFunctionArgs<
  abi extends Abi,
  signature extends string,
  ///
  args = AbiParametersToPrimitiveTypes<
    ParseAbiFunction<abi, signature>["inputs"]
  >,
> = readonly [] extends args ? never : args;
export type FormatFunctionResult<
  abi extends Abi,
  signature extends string,
  ///
  result = AbiParametersToPrimitiveTypes<
    ParseAbiFunction<abi, signature>["outputs"]
  >,
> = readonly [] extends result
  ? never
  : result extends readonly [unknown]
    ? result[0]
    : result;
</file>

<file path="packages/core/src/database/actions.test.ts">
import {
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { buildSchema } from "@/build/schema.js";
import { getReorgTable } from "@/drizzle/kit/index.js";
import { onchainTable, primaryKey } from "@/drizzle/onchain.js";
import type { RetryableError } from "@/internal/errors.js";
import type { IndexingErrorHandler } from "@/internal/types.js";
import {
  type Checkpoint,
  MAX_CHECKPOINT_STRING,
  ZERO_CHECKPOINT,
  encodeCheckpoint,
} from "@/utils/checkpoint.js";
import { and, eq, sql } from "drizzle-orm";
import { index } from "drizzle-orm/pg-core";
import { zeroAddress } from "viem";
import { beforeEach, expect, test } from "vitest";
import {
  commitBlock,
  createIndexes,
  createTriggers,
  createViews,
  dropTriggers,
  finalizeMultichain,
  revertMultichain,
} from "./actions.js";
import { type Database, getPonderCheckpointTable } from "./index.js";
beforeEach(setupCommon);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
const account = onchainTable("account", (p) => ({
  address: p.hex().primaryKey(),
  balance: p.bigint(),
}));
function createCheckpoint(checkpoint: Partial<Checkpoint>): string {
  return encodeCheckpoint({ ...ZERO_CHECKPOINT, ...checkpoint });
}
const indexingErrorHandler: IndexingErrorHandler = {
  getRetryableError: () => {
    return indexingErrorHandler.error;
  },
  setRetryableError: (error: RetryableError) => {
    indexingErrorHandler.error = error;
  },
  clearRetryableError: () => {
    indexingErrorHandler.error = undefined;
  },
  error: undefined as RetryableError | undefined,
};
test("finalize()", async () => {
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  // setup tables, reorg tables, and metadata checkpoint
  await database.userQB.wrap((tx) =>
    tx.insert(getPonderCheckpointTable()).values({
      chainName: "mainnet",
      chainId: 1,
      safeCheckpoint: createCheckpoint({ chainId: 1n, blockNumber: 0n }),
      finalizedCheckpoint: createCheckpoint({ chainId: 1n, blockNumber: 0n }),
      latestCheckpoint: createCheckpoint({ chainId: 1n, blockNumber: 0n }),
    }),
  );
  await createTriggers(database.userQB, { tables: [account] });
  await database.userQB.raw
    .insert(account)
    .values({ address: zeroAddress, balance: 10n });
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 9n }),
    table: account,
    preBuild: { ordering: "multichain" },
  });
  await database.userQB.raw
    .update(account)
    .set({ balance: 88n })
    .where(eq(account.address, zeroAddress));
  await database.userQB.raw
    .insert(account)
    .values({ address: "0x0000000000000000000000000000000000000001" });
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 11n }),
    table: account,
    preBuild: { ordering: "multichain" },
  });
  await finalizeMultichain(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
    tables: [account],
    namespaceBuild: { schema: "public", viewsSchema: undefined },
  });
  // reorg tables
  const rows = await database.userQB.wrap((tx) =>
    tx.select().from(getReorgTable(account)),
  );
  expect(rows).toHaveLength(2);
});
test("createIndexes()", async () => {
  const account = onchainTable(
    "account",
    (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint(),
    }),
    (table) => ({
      balanceIdx: index("balance_index").on(table.balance),
    }),
  );
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  await createIndexes(database.userQB, {
    statements: buildSchema({
      schema: { account },
      preBuild: { ordering: "multichain" },
    }).statements,
  });
  const indexNames = await getUserIndexNames(database, "public", "account");
  expect(indexNames).toContain("balance_index");
});
test("createTriggers()", async () => {
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  await createTriggers(database.userQB, { tables: [account] });
  await database.userQB.raw
    .insert(account)
    .values({ address: zeroAddress, balance: 10n });
  const { rows } = await database.userQB.wrap((tx) =>
    tx.execute(sql`SELECT * FROM _reorg__account`),
  );
  expect(rows).toStrictEqual([
    {
      address: zeroAddress,
      balance: "10",
      operation: 0,
      operation_id: 1,
      checkpoint: MAX_CHECKPOINT_STRING,
    },
  ]);
});
test("createTriggers() duplicate", async () => {
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  await createTriggers(database.userQB, { tables: [account] });
  await createTriggers(database.userQB, { tables: [account] });
});
test("commitBlock()", async () => {
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  await createTriggers(database.userQB, { tables: [account] });
  await database.userQB.raw
    .insert(account)
    .values({ address: zeroAddress, balance: 10n });
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
    table: account,
    preBuild: { ordering: "multichain" },
  });
  const { rows } = await database.userQB.wrap((tx) =>
    tx.execute(sql`SELECT * FROM _reorg__account`),
  );
  expect(rows).toStrictEqual([
    {
      address: zeroAddress,
      balance: "10",
      operation: 0,
      operation_id: 1,
      checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
    },
  ]);
});
test("commitBlock() isolated", async () => {
  const account = onchainTable(
    "account",
    (p) => ({
      chainId: p.integer().notNull(),
      address: p.hex().notNull(),
      balance: p.bigint(),
    }),
    (table) => ({
      pk: primaryKey({ columns: [table.chainId, table.address] }),
    }),
  );
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  await createTriggers(database.userQB, { tables: [account] });
  await database.userQB.raw
    .insert(account)
    .values({ chainId: 1, address: zeroAddress, balance: 10n });
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 2n, blockNumber: 10n }),
    table: account,
    preBuild: { ordering: "experimental_isolated" },
  });
  const { rows: rows1 } = await database.userQB.wrap((tx) =>
    tx.execute(sql`SELECT * FROM _reorg__account`),
  );
  expect(rows1).toMatchInlineSnapshot(`
    [
      {
        "address": "0x0000000000000000000000000000000000000000",
        "balance": "10",
        "chain_id": 1,
        "checkpoint": "999999999999999999999999999999999999999999999999999999999999999999999999999",
        "operation": 0,
        "operation_id": 1,
      },
    ]
  `);
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
    table: account,
    preBuild: { ordering: "experimental_isolated" },
  });
  const { rows: rows2 } = await database.userQB.wrap((tx) =>
    tx.execute(sql`SELECT * FROM _reorg__account`),
  );
  expect(rows2).toMatchInlineSnapshot(`
    [
      {
        "address": "0x0000000000000000000000000000000000000000",
        "balance": "10",
        "chain_id": 1,
        "checkpoint": "000000000000000000000000010000000000000010000000000000000000000000000000000",
        "operation": 0,
        "operation_id": 1,
      },
    ]
  `);
});
test("revert()", async () => {
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { account } },
  });
  // setup tables, reorg tables, and metadata checkpoint
  await createTriggers(database.userQB, { tables: [account] });
  await database.userQB.raw
    .insert(account)
    .values({ address: zeroAddress, balance: 10n });
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 9n }),
    table: account,
    preBuild: { ordering: "multichain" },
  });
  await database.userQB.raw
    .update(account)
    .set({ balance: 88n })
    .where(eq(account.address, zeroAddress));
  await database.userQB.raw
    .insert(account)
    .values({ address: "0x0000000000000000000000000000000000000001" });
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
    table: account,
    preBuild: { ordering: "multichain" },
  });
  await database.userQB.raw
    .delete(account)
    .where(eq(account.address, zeroAddress));
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 11n }),
    table: account,
    preBuild: { ordering: "multichain" },
  });
  await database.userQB.transaction(async (tx) => {
    await revertMultichain(tx, {
      checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 9n }),
      tables: [account],
    });
  });
  const rows = await database.userQB.wrap((tx) => tx.select().from(account));
  expect(rows).toHaveLength(1);
  expect(rows[0]).toStrictEqual({ address: zeroAddress, balance: 10n });
});
test("revert() with composite primary key", async () => {
  const test = onchainTable(
    "Test",
    (p) => ({
      a: p.integer("A").notNull(),
      b: p.integer("B").notNull(),
      c: p.integer("C"),
    }),
    (table) => ({
      pk: primaryKey({ columns: [table.a, table.b] }),
    }),
  );
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: { test } },
  });
  // setup tables, reorg tables, and metadata checkpoint
  await createTriggers(database.userQB, { tables: [test] });
  await database.userQB.raw.insert(test).values({ a: 1, b: 1 });
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 11n }),
    table: test,
    preBuild: { ordering: "multichain" },
  });
  await database.userQB.raw
    .update(test)
    .set({ c: 1 })
    .where(and(eq(test.a, 1), eq(test.b, 1)));
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 12n }),
    table: test,
    preBuild: { ordering: "multichain" },
  });
  await database.userQB.transaction(async (tx) => {
    await revertMultichain(tx, {
      checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 11n }),
      tables: [test],
    });
  });
  const rows = await database.userQB.wrap((tx) => tx.select().from(test));
  expect(rows).toHaveLength(1);
  expect(rows[0]).toStrictEqual({ a: 1, b: 1, c: null });
});
test("empty schema", async () => {
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema: {} },
  });
  await createTriggers(database.userQB, { tables: [] });
  await dropTriggers(database.userQB, { tables: [] });
  await createViews(database.userQB, {
    tables: [],
    views: [],
    namespaceBuild: { schema: "public", viewsSchema: undefined },
  });
  await revertMultichain(database.userQB, {
    tables: [],
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
  });
  await finalizeMultichain(database.userQB, {
    tables: [],
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
    namespaceBuild: { schema: "public", viewsSchema: undefined },
  });
});
async function getUserIndexNames(
  database: Database,
  namespace: string,
  tableName: string,
) {
  const rows = await database.userQB.wrap((tx) =>
    tx
      .select({ name: sql<string>`indexname`.as("name") })
      .from(sql`pg_indexes`)
      .where(
        and(eq(sql`schemaname`, namespace), eq(sql`tablename`, tableName)),
      ),
  );
  return rows.map((r) => r.name);
}
</file>

<file path="packages/core/src/database/actions.ts">
import { getPrimaryKeyColumns } from "@/drizzle/index.js";
import { getColumnCasing, getReorgTable } from "@/drizzle/kit/index.js";
import {
  getLiveQueryChannelName,
  getLiveQueryNotifyProcedureName,
  getLiveQueryNotifyTriggerName,
  getLiveQueryProcedureName,
  getLiveQueryTempTableName,
  getLiveQueryTriggerName,
  getPartitionName,
  getReorgProcedureName,
  getReorgTableName,
  getReorgTriggerName,
  getViewsLiveQueryNotifyTriggerName,
} from "@/drizzle/onchain.js";
import type { Logger } from "@/internal/logger.js";
import type {
  NamespaceBuild,
  PreBuild,
  SchemaBuild,
} from "@/internal/types.js";
import { MAX_CHECKPOINT_STRING, decodeCheckpoint } from "@/utils/checkpoint.js";
import {
  type SQL,
  type Table,
  type View,
  and,
  eq,
  getTableColumns,
  getTableName,
  getViewName,
  lte,
  sql,
} from "drizzle-orm";
import { getTableConfig } from "drizzle-orm/pg-core";
import {
  PONDER_CHECKPOINT_TABLE_NAME,
  PONDER_META_TABLE_NAME,
  getPonderCheckpointTable,
} from "./index.js";
import type { QB } from "./queryBuilder.js";
export const createIndexes = async (
  qb: QB,
  { statements }: { statements: SchemaBuild["statements"] },
  context?: { logger?: Logger },
) => {
  for (const statement of statements.indexes.sql) {
    await qb.transaction(
      { label: "create_indexes" },
      async (tx) => {
        // 60 minutes
        await tx.wrap((tx) => tx.execute("SET statement_timeout = 3600000;"));
        await tx.wrap((tx) => tx.execute(statement));
      },
      undefined,
      context,
    );
  }
};
export const createTriggers = async (
  qb: QB,
  { tables, chainId }: { tables: Table[]; chainId?: number },
  context?: { logger?: Logger },
) => {
  await qb.transaction(
    async (tx) => {
      await Promise.all(
        tables.map(async (table) => {
          const schema = getTableConfig(table).schema ?? "public";
          const columns = getTableColumns(table);
          const columnNames = Object.values(columns).map(
            (column) => `"${getColumnCasing(column, "snake_case")}"`,
          );
          await tx.wrap({ label: "create_trigger" }, (tx) =>
            tx.execute(
              `
  CREATE OR REPLACE FUNCTION "${schema}".${getReorgProcedureName(table)}
  RETURNS TRIGGER AS $$
  BEGIN
  IF TG_OP = 'INSERT' THEN
  INSERT INTO "${schema}"."${getReorgTableName(table)}" (${columnNames.join(",")}, operation, checkpoint)
  VALUES (${columnNames.map((name) => `NEW.${name}`).join(",")}, 0, '${MAX_CHECKPOINT_STRING}');
  ELSIF TG_OP = 'UPDATE' THEN
  INSERT INTO "${schema}"."${getReorgTableName(table)}" (${columnNames.join(",")}, operation, checkpoint)
  VALUES (${columnNames.map((name) => `OLD.${name}`).join(",")}, 1, '${MAX_CHECKPOINT_STRING}');
  ELSIF TG_OP = 'DELETE' THEN
  INSERT INTO "${schema}"."${getReorgTableName(table)}" (${columnNames.join(",")}, operation, checkpoint)
  VALUES (${columnNames.map((name) => `OLD.${name}`).join(",")}, 2, '${MAX_CHECKPOINT_STRING}');
  END IF;
  RETURN NULL;
  END;
  $$ LANGUAGE plpgsql`,
            ),
          );
          await tx.wrap({ label: "create_trigger" }, (tx) =>
            tx.execute(
              `
  CREATE OR REPLACE TRIGGER "${getReorgTriggerName()}"
  AFTER INSERT OR UPDATE OR DELETE ON "${schema}"."${chainId === undefined ? getTableName(table) : getPartitionName(table, chainId)}"
  FOR EACH ROW EXECUTE PROCEDURE "${schema}".${getReorgProcedureName(table)};
  `,
            ),
          );
        }),
      );
    },
    undefined,
    context,
  );
};
export const dropTriggers = async (
  qb: QB,
  { tables, chainId }: { tables: Table[]; chainId?: number },
  context?: { logger?: Logger },
) => {
  await qb.transaction(
    async (tx) => {
      await Promise.all(
        tables.map(async (table) => {
          const schema = getTableConfig(table).schema ?? "public";
          await tx.wrap({ label: "drop_trigger" }, (tx) =>
            tx.execute(
              `DROP TRIGGER IF EXISTS "${getReorgTriggerName()}" ON "${schema}"."${chainId === undefined ? getTableName(table) : getPartitionName(table, chainId)}"`,
            ),
          );
        }),
      );
    },
    undefined,
    context,
  );
};
export const createLiveQueryTriggers = async (
  qb: QB,
  {
    namespaceBuild,
    tables,
    chainId,
  }: { namespaceBuild: NamespaceBuild; tables: Table[]; chainId?: number },
  context?: { logger?: Logger },
) => {
  await qb.transaction(
    async (tx) => {
      const notifyProcedure = getLiveQueryNotifyProcedureName();
      const notifyTrigger = getLiveQueryNotifyTriggerName();
      await tx.wrap((tx) =>
        tx.execute(
          `
CREATE OR REPLACE TRIGGER "${notifyTrigger}"
AFTER INSERT OR UPDATE OR DELETE ON "${namespaceBuild.schema}"."${PONDER_CHECKPOINT_TABLE_NAME}"
FOR EACH STATEMENT EXECUTE PROCEDURE "${namespaceBuild.schema}".${notifyProcedure};`,
        ),
      );
      const trigger = getLiveQueryTriggerName();
      const procedure = getLiveQueryProcedureName();
      for (const table of tables) {
        const schema = getTableConfig(table).schema ?? "public";
        // Note: Because the realtime indexing store writes to the parent table, we create the trigger on
        // the parent table instead of the partition table.
        await tx.wrap((tx) =>
          tx.execute(
            `
CREATE OR REPLACE TRIGGER "${trigger}"
AFTER INSERT OR UPDATE OR DELETE ON "${schema}"."${chainId === undefined ? getTableName(table) : getPartitionName(table, chainId)}"
FOR EACH ROW EXECUTE PROCEDURE "${schema}".${procedure};`,
          ),
        );
      }
    },
    undefined,
    context,
  );
};
export const dropLiveQueryTriggers = async (
  qb: QB,
  {
    namespaceBuild,
    tables,
    chainId,
  }: { namespaceBuild: NamespaceBuild; tables: Table[]; chainId?: number },
  context?: { logger?: Logger },
) => {
  await qb.transaction(
    async (tx) => {
      const notifyTrigger = getLiveQueryNotifyTriggerName();
      await tx.wrap((tx) =>
        tx.execute(
          `DROP TRIGGER IF EXISTS "${notifyTrigger}" ON "${namespaceBuild.schema}"."${PONDER_CHECKPOINT_TABLE_NAME}";`,
        ),
      );
      const trigger = getLiveQueryTriggerName();
      for (const table of tables) {
        const schema = getTableConfig(table).schema ?? "public";
        await tx.wrap((tx) =>
          tx.execute(
            `DROP TRIGGER IF EXISTS "${trigger}" ON "${schema}"."${chainId === undefined ? getTableName(table) : getPartitionName(table, chainId)}";`,
          ),
        );
      }
    },
    undefined,
    context,
  );
};
export const createLiveQueryProcedures = async (
  qb: QB,
  { namespaceBuild }: { namespaceBuild: NamespaceBuild },
  context?: { logger?: Logger },
) => {
  await qb.transaction(
    async (tx) => {
      const schema = namespaceBuild.schema;
      const procedure = getLiveQueryProcedureName();
      await tx.wrap(
        (tx) =>
          tx.execute(
            `
CREATE OR REPLACE FUNCTION "${schema}".${procedure}
RETURNS TRIGGER LANGUAGE plpgsql
AS $$
BEGIN
  INSERT INTO ${getLiveQueryTempTableName()} (table_name)
  VALUES (TG_TABLE_NAME)
  ON CONFLICT (table_name) DO NOTHING;
  RETURN NULL;
END;
$$;`,
          ),
        context,
      );
      const notifyProcedure = getLiveQueryNotifyProcedureName();
      const channel = getLiveQueryChannelName(namespaceBuild.schema);
      await tx.wrap(
        (tx) =>
          tx.execute(`
CREATE OR REPLACE FUNCTION "${schema}".${notifyProcedure}
RETURNS TRIGGER LANGUAGE plpgsql
AS $$
  DECLARE
    table_names json;
    table_exists boolean := false;
  BEGIN
    SELECT EXISTS (
      SELECT 1
      FROM information_schema.tables
      WHERE table_name = '${getLiveQueryTempTableName()}'
      AND table_type = 'LOCAL TEMPORARY'
    ) INTO table_exists;
    IF table_exists THEN
      SELECT json_agg(table_name) INTO table_names
      FROM ${getLiveQueryTempTableName()};
      table_names := COALESCE(table_names, '[]'::json);
      PERFORM pg_notify('${channel}', table_names::text);
    END IF;
    RETURN NULL;
  END;
$$;`),
        context,
      );
    },
    undefined,
    context,
  );
};
export const createViews = async (
  qb: QB,
  {
    tables,
    views,
    namespaceBuild,
  }: { tables: Table[]; views: View[]; namespaceBuild: NamespaceBuild },
  context?: { logger?: Logger },
) => {
  await qb.transaction(
    { label: "create_views" },
    async (tx) => {
      await tx.wrap((tx) =>
        tx.execute(
          `CREATE SCHEMA IF NOT EXISTS "${namespaceBuild.viewsSchema}"`,
        ),
      );
      // Note: Drop views before creating new ones because Postgres does not support
      // altering the schema of a view with CREATE OR REPLACE VIEW.
      for (const table of tables) {
        await tx.wrap((tx) =>
          tx.execute(
            `DROP VIEW IF EXISTS "${namespaceBuild.viewsSchema}"."${getTableName(table)}"`,
          ),
        );
        await tx.wrap((tx) =>
          tx.execute(
            `CREATE VIEW "${namespaceBuild.viewsSchema}"."${getTableName(table)}" AS SELECT * FROM "${namespaceBuild.schema}"."${getTableName(table)}"`,
          ),
        );
      }
      for (const view of views) {
        await tx.wrap((tx) =>
          tx.execute(
            `DROP VIEW IF EXISTS "${namespaceBuild.viewsSchema}"."${getViewName(view)}"`,
          ),
        );
        await tx.wrap((tx) =>
          tx.execute(
            `CREATE VIEW "${namespaceBuild.viewsSchema}"."${getViewName(view)}" AS SELECT * FROM "${namespaceBuild.schema}"."${getViewName(view)}"`,
          ),
        );
      }
      await tx.wrap((tx) =>
        tx.execute(
          `DROP VIEW IF EXISTS "${namespaceBuild.viewsSchema}"."${PONDER_META_TABLE_NAME}"`,
        ),
      );
      await tx.wrap((tx) =>
        tx.execute(
          `DROP VIEW IF EXISTS "${namespaceBuild.viewsSchema}"."${PONDER_CHECKPOINT_TABLE_NAME}"`,
        ),
      );
      await tx.wrap((tx) =>
        tx.execute(
          `CREATE VIEW "${namespaceBuild.viewsSchema}"."${PONDER_META_TABLE_NAME}" AS SELECT * FROM "${namespaceBuild.schema}"."${PONDER_META_TABLE_NAME}"`,
        ),
      );
      await tx.wrap((tx) =>
        tx.execute(
          `CREATE VIEW "${namespaceBuild.viewsSchema}"."${PONDER_CHECKPOINT_TABLE_NAME}" AS SELECT * FROM "${namespaceBuild.schema}"."${PONDER_CHECKPOINT_TABLE_NAME}"`,
        ),
      );
      const notifyProcedure = getLiveQueryNotifyProcedureName();
      const channel = getLiveQueryChannelName(namespaceBuild.viewsSchema!);
      await tx.wrap((tx) =>
        tx.execute(`
CREATE OR REPLACE FUNCTION "${namespaceBuild.viewsSchema}".${notifyProcedure}
RETURNS TRIGGER LANGUAGE plpgsql
AS $$
  DECLARE
    table_names json;
    table_exists boolean := false;
  BEGIN
    SELECT EXISTS (
      SELECT 1
      FROM information_schema.tables
      WHERE table_name = '${getLiveQueryTempTableName()}'
      AND table_type = 'LOCAL TEMPORARY'
    ) INTO table_exists;
    IF table_exists THEN
      SELECT json_agg(table_name) INTO table_names
      FROM ${getLiveQueryTempTableName()};
      table_names := COALESCE(table_names, '[]'::json);
      PERFORM pg_notify('${channel}', table_names::text);
    END IF;
    RETURN NULL;
  END;
$$;`),
      );
      const trigger = getViewsLiveQueryNotifyTriggerName(
        namespaceBuild.viewsSchema!,
      );
      await tx.wrap((tx) =>
        tx.execute(
          `
CREATE OR REPLACE TRIGGER "${trigger}"
AFTER INSERT OR UPDATE OR DELETE
ON "${namespaceBuild.schema!}"."${PONDER_CHECKPOINT_TABLE_NAME}"
FOR EACH STATEMENT
EXECUTE PROCEDURE "${namespaceBuild.viewsSchema}".${notifyProcedure};`,
        ),
      );
    },
    undefined,
    context,
  );
};
export const revertOmnichain = async (
  qb: QB,
  {
    checkpoint,
    tables,
  }: {
    checkpoint: string;
    tables: Table[];
  },
  context?: { logger?: Logger },
): Promise<number[]> => {
  if (tables.length === 0) return [];
  return qb.transaction(
    { label: "revert" },
    async (tx) => {
      const counts: number[] = [];
      for (const table of tables) {
        const primaryKeyColumns = getPrimaryKeyColumns(table);
        const schema = getTableConfig(table).schema ?? "public";
        const result = await tx.wrap((tx) =>
          tx.execute(`
WITH reverted1 AS (
  DELETE FROM "${schema}"."${getReorgTableName(table)}"
  WHERE checkpoint > '${checkpoint}' RETURNING * 
), reverted2 AS (
  SELECT ${primaryKeyColumns.map(({ sql }) => `"${sql}"`).join(", ")}, MIN(operation_id) AS operation_id FROM reverted1
  GROUP BY ${primaryKeyColumns.map(({ sql }) => `"${sql}"`).join(", ")}
), reverted3 AS (
  SELECT ${Object.values(getTableColumns(table))
    .map((column) => `reverted1."${getColumnCasing(column, "snake_case")}"`)
    .join(", ")}, reverted1.operation FROM reverted2
  INNER JOIN reverted1
  ON ${primaryKeyColumns.map(({ sql }) => `reverted2."${sql}" = reverted1."${sql}"`).join("AND ")}
  AND reverted2.operation_id = reverted1.operation_id
), ${getRevertSql({ table })};`),
        );
        // @ts-ignore
        counts.push(result.rows[0]!.count);
      }
      return counts;
    },
    undefined,
    context,
  );
};
export const revertMultichain = async (
  qb: QB,
  {
    checkpoint,
    tables,
  }: {
    checkpoint: string;
    tables: Table[];
  },
  context?: { logger?: Logger },
): Promise<number[]> => {
  if (tables.length === 0) return [];
  return qb.transaction(
    { label: "revert" },
    async (tx) => {
      const counts: number[] = [];
      const minOperationId = await tx
        .wrap((tx) =>
          tx.execute(`
SELECT MIN(operation_id) AS operation_id FROM (
${tables
  .map(
    (table) => `
SELECT MIN(operation_id) AS operation_id FROM "${getTableConfig(table).schema ?? "public"}"."${getReorgTableName(table)}"
WHERE SUBSTRING(checkpoint, 11, 16)::numeric = ${String(decodeCheckpoint(checkpoint).chainId)}
AND checkpoint > '${checkpoint}'`,
  )
  .join(" UNION ALL ")}) AS all_mins;`),
        )
        .then((result) => {
          // @ts-ignore
          return result.rows[0]?.operation_id as string | null;
        });
      for (const table of tables) {
        const primaryKeyColumns = getPrimaryKeyColumns(table);
        const schema = getTableConfig(table).schema ?? "public";
        const result = await tx.wrap((tx) =>
          tx.execute(`
WITH reverted1 AS (
  DELETE FROM "${schema}"."${getReorgTableName(table)}"
  WHERE ${minOperationId!} IS NOT NULL AND operation_id >= ${minOperationId!}
  RETURNING * 
), reverted2 AS (
  SELECT ${primaryKeyColumns.map(({ sql }) => `"${sql}"`).join(", ")}, MIN(operation_id) AS operation_id FROM reverted1
  GROUP BY ${primaryKeyColumns.map(({ sql }) => `"${sql}"`).join(", ")}
), reverted3 AS (
  SELECT ${Object.values(getTableColumns(table))
    .map((column) => `reverted1."${getColumnCasing(column, "snake_case")}"`)
    .join(", ")}, reverted1.operation FROM reverted2
  INNER JOIN reverted1
  ON ${primaryKeyColumns.map(({ sql }) => `reverted2."${sql}" = reverted1."${sql}"`).join("AND ")}
  AND reverted2.operation_id = reverted1.operation_id
), ${getRevertSql({ table })};`),
        );
        // @ts-ignore
        counts.push(result.rows[0]!.count);
      }
      return counts;
    },
    undefined,
    context,
  );
};
export const revertIsolated = async (
  qb: QB,
  {
    checkpoint,
    tables,
  }: {
    checkpoint: string;
    tables: Table[];
  },
  context?: { logger?: Logger },
) => {
  if (tables.length === 0) return [];
  return qb.transaction(
    { label: "revert" },
    async (tx) => {
      const counts: number[] = [];
      for (const table of tables) {
        const primaryKeyColumns = getPrimaryKeyColumns(table);
        const schema = getTableConfig(table).schema ?? "public";
        const result = await tx.wrap((tx) =>
          tx.execute(`
WITH reverted1 AS (
  DELETE FROM "${schema}"."${getReorgTableName(table)}"
  WHERE checkpoint > '${checkpoint}' AND SUBSTRING(checkpoint, 11, 16)::numeric = ${String(decodeCheckpoint(checkpoint).chainId)} RETURNING * 
), reverted2 AS (
  SELECT ${primaryKeyColumns.map(({ sql }) => `"${sql}"`).join(", ")}, MIN(operation_id) AS operation_id FROM reverted1
  GROUP BY ${primaryKeyColumns.map(({ sql }) => `"${sql}"`).join(", ")}
), reverted3 AS (
  SELECT ${Object.values(getTableColumns(table))
    .map((column) => `reverted1."${getColumnCasing(column, "snake_case")}"`)
    .join(", ")}, reverted1.operation FROM reverted2
  INNER JOIN reverted1
  ON ${primaryKeyColumns.map(({ sql }) => `reverted2."${sql}" = reverted1."${sql}"`).join("AND ")}
  AND reverted2.operation_id = reverted1.operation_id
), ${getRevertSql({ table })};`),
        );
        // @ts-ignore
        counts.push(result.rows[0]!.count);
      }
      return counts;
    },
    undefined,
    context,
  );
};
export const finalizeOmnichain = async (
  qb: QB,
  {
    checkpoint,
    tables,
    namespaceBuild,
  }: {
    checkpoint: string;
    tables: Table[];
    namespaceBuild: NamespaceBuild;
  },
  context?: { logger?: Logger },
) => {
  const PONDER_CHECKPOINT = getPonderCheckpointTable(namespaceBuild.schema);
  // TODO(kyle) is this breaking an invariant?
  if (tables.length === 0) {
    await qb.wrap(
      (db) =>
        db
          .update(PONDER_CHECKPOINT)
          .set({ finalizedCheckpoint: checkpoint, safeCheckpoint: checkpoint }),
      context,
    );
    return;
  }
  return qb.transaction(
    { label: "finalize" },
    async (tx) => {
      await tx.wrap((tx) =>
        tx.update(PONDER_CHECKPOINT).set({
          finalizedCheckpoint: checkpoint,
          safeCheckpoint: checkpoint,
        }),
      );
      for (const table of tables) {
        await tx.wrap((tx) =>
          tx
            .delete(getReorgTable(table))
            .where(lte(getReorgTable(table).checkpoint, checkpoint)),
        );
      }
    },
    undefined,
    context,
  );
};
export const finalizeMultichain = async (
  qb: QB,
  {
    checkpoint,
    tables,
    namespaceBuild,
  }: {
    checkpoint: string;
    tables: Table[];
    namespaceBuild: NamespaceBuild;
  },
  context?: { logger?: Logger },
) => {
  const PONDER_CHECKPOINT = getPonderCheckpointTable(namespaceBuild.schema);
  // TODO(kyle) is this breaking an invariant?
  if (tables.length === 0) {
    await qb.wrap(
      (db) =>
        db
          .update(PONDER_CHECKPOINT)
          .set({ finalizedCheckpoint: checkpoint, safeCheckpoint: checkpoint }),
      context,
    );
    return;
  }
  // NOTE: It is invariant that PONDER_CHECKPOINT has a value for each chain.
  return qb.transaction(
    { label: "finalize" },
    async (tx) => {
      await tx.wrap((tx) =>
        tx
          .update(PONDER_CHECKPOINT)
          .set({ finalizedCheckpoint: checkpoint })
          .where(
            eq(
              PONDER_CHECKPOINT.chainId,
              Number(decodeCheckpoint(checkpoint).chainId),
            ),
          ),
      );
      const minOperationId = await tx
        .wrap((tx) =>
          tx.execute(`
SELECT MIN(operation_id) AS operation_id FROM (
${tables
  .map(
    (table) => `
SELECT MIN(operation_id) AS operation_id FROM "${getTableConfig(table).schema ?? "public"}"."${getTableName(getReorgTable(table))}"
WHERE checkpoint > (
  SELECT finalized_checkpoint 
  FROM "${getTableConfig(PONDER_CHECKPOINT).schema ?? "public"}"."${getTableName(PONDER_CHECKPOINT)}" 
  WHERE chain_id = SUBSTRING(checkpoint, 11, 16)::numeric
)`,
  )
  .join(" UNION ALL ")}) AS all_mins;`),
        )
        .then((result) => {
          // @ts-ignore
          return result.rows[0]?.operation_id as string | null;
        });
      const result = await tx.wrap((tx) =>
        tx.execute(`
    WITH ${tables
      .map(
        (table, index) => `
    deleted_${index} AS (
      DELETE FROM "${getTableConfig(table).schema ?? "public"}"."${getTableName(getReorgTable(table))}"
      WHERE ${minOperationId} IS NULL OR operation_id < ${minOperationId}
      RETURNING *
    )`,
      )
      .join(",\n")},
    all_deleted AS (
      ${tables
        .map((_, index) => `SELECT checkpoint FROM deleted_${index}`)
        .join(" UNION ALL ")}
    )
    SELECT MAX(checkpoint) as safe_checkpoint, SUBSTRING(checkpoint, 11, 16)::numeric as chain_id
    FROM all_deleted
    GROUP BY SUBSTRING(checkpoint, 11, 16)::numeric;`),
      );
      for (const { chain_id, safe_checkpoint } of result.rows) {
        await tx.wrap((tx) =>
          tx
            .update(PONDER_CHECKPOINT)
            .set({ safeCheckpoint: safe_checkpoint as string })
            .where(eq(PONDER_CHECKPOINT.chainId, chain_id as number)),
        );
      }
    },
    undefined,
    context,
  );
};
export const finalizeIsolated = async (
  qb: QB,
  {
    checkpoint,
    tables,
    namespaceBuild,
  }: {
    checkpoint: string;
    tables: Table[];
    namespaceBuild: NamespaceBuild;
  },
  context?: { logger?: Logger },
) => {
  const PONDER_CHECKPOINT = getPonderCheckpointTable(namespaceBuild.schema);
  const chainId = Number(decodeCheckpoint(checkpoint).chainId);
  if (tables.length === 0) {
    await qb.wrap(
      (db) =>
        db
          .update(PONDER_CHECKPOINT)
          .set({ finalizedCheckpoint: checkpoint, safeCheckpoint: checkpoint })
          .where(eq(PONDER_CHECKPOINT.chainId, chainId)),
      context,
    );
    return;
  }
  return qb.transaction({ label: "finalize" }, async (tx) => {
    await tx.wrap((tx) =>
      tx
        .update(PONDER_CHECKPOINT)
        .set({ finalizedCheckpoint: checkpoint, safeCheckpoint: checkpoint })
        .where(eq(PONDER_CHECKPOINT.chainId, chainId)),
    );
    for (const table of tables) {
      await tx.wrap((tx) =>
        tx
          .delete(getReorgTable(table))
          .where(
            and(
              lte(getReorgTable(table).checkpoint, checkpoint),
              eq(sql`chain_id`, chainId),
            ),
          ),
      );
    }
  });
};
export const commitBlock = async (
  qb: QB,
  {
    checkpoint,
    table,
    preBuild,
  }: { checkpoint: string; table: Table; preBuild: Pick<PreBuild, "ordering"> },
  context?: { logger?: Logger },
) => {
  const reorgTable = getReorgTable(table);
  let whereClause: SQL;
  if (preBuild.ordering === "experimental_isolated") {
    // Note: Query must include `chain_id` because it's possible for multiple chains to be indexing in parallel.
    const chainId = Number(decodeCheckpoint(checkpoint).chainId);
    whereClause = and(
      eq(reorgTable.checkpoint, MAX_CHECKPOINT_STRING),
      eq(sql`chain_id`, chainId),
    )!;
  } else {
    whereClause = eq(reorgTable.checkpoint, MAX_CHECKPOINT_STRING);
  }
  await qb.wrap(
    { label: "commit_block" },
    (db) => db.update(reorgTable).set({ checkpoint }).where(whereClause),
    context,
  );
};
export const crashRecovery = async (
  qb: QB,
  { table }: { table: Table },
  context?: { logger?: Logger },
) => {
  const primaryKeyColumns = getPrimaryKeyColumns(table);
  const schema = getTableConfig(table).schema ?? "public";
  await qb.wrap(
    (db) =>
      db.execute(`
WITH reverted1 AS (
  DELETE FROM "${schema}"."${getTableName(getReorgTable(table))}"
  RETURNING *
), reverted2 AS (
  SELECT ${primaryKeyColumns.map(({ sql }) => `"${sql}"`).join(", ")}, MIN(operation_id) AS operation_id FROM reverted1
  GROUP BY ${primaryKeyColumns.map(({ sql }) => `"${sql}"`).join(", ")}
), reverted3 AS (
  SELECT ${Object.values(getTableColumns(table))
    .map((column) => `reverted1."${getColumnCasing(column, "snake_case")}"`)
    .join(", ")}, reverted1.operation FROM reverted2
  INNER JOIN reverted1
  ON ${primaryKeyColumns.map(({ sql }) => `reverted2."${sql}" = reverted1."${sql}"`).join("AND ")}
  AND reverted2.operation_id = reverted1.operation_id
), ${getRevertSql({ table })}`),
    context,
  );
};
const getRevertSql = ({ table }: { table: Table }) => {
  const primaryKeyColumns = getPrimaryKeyColumns(table);
  const schema = getTableConfig(table).schema ?? "public";
  return `
inserted AS (
  DELETE FROM "${schema}"."${getTableName(table)}" as t
  WHERE EXISTS (
    SELECT * FROM reverted3
    WHERE ${primaryKeyColumns.map(({ sql }) => `t."${sql}" = reverted3."${sql}"`).join("AND ")}
    AND OPERATION = 0
  )
  RETURNING *
), updated_or_deleted AS (
  INSERT INTO  "${schema}"."${getTableName(table)}"
  SELECT ${Object.values(getTableColumns(table))
    .map((column) => `"${getColumnCasing(column, "snake_case")}"`)
    .join(", ")} FROM reverted3
  WHERE operation = 1 OR operation = 2
  ON CONFLICT (${primaryKeyColumns.map(({ sql }) => `"${sql}"`).join(", ")})
  DO UPDATE SET
    ${Object.values(getTableColumns(table))
      .map(
        (column) =>
          `"${getColumnCasing(column, "snake_case")}" = EXCLUDED."${getColumnCasing(column, "snake_case")}"`,
      )
      .join(", ")}
  RETURNING *
) SELECT COUNT(*) FROM reverted1 as count;`;
};
</file>

<file path="packages/core/src/database/index.test.ts">
import { context, setupCommon, setupIsolatedDatabase } from "@/_test/setup.js";
import { getChain } from "@/_test/utils.js";
import { buildSchema } from "@/build/schema.js";
import {
  onchainEnum,
  onchainTable,
  onchainView,
  primaryKey,
} from "@/drizzle/onchain.js";
import type { RetryableError } from "@/internal/errors.js";
import { createShutdown } from "@/internal/shutdown.js";
import type { IndexingErrorHandler } from "@/internal/types.js";
import {
  type Checkpoint,
  ZERO_CHECKPOINT,
  encodeCheckpoint,
} from "@/utils/checkpoint.js";
import { wait } from "@/utils/wait.js";
import { and, eq, sql } from "drizzle-orm";
import { index } from "drizzle-orm/pg-core";
import { zeroAddress } from "viem";
import { beforeEach, expect, test, vi } from "vitest";
import {
  commitBlock,
  crashRecovery,
  createIndexes,
  createLiveQueryProcedures,
  createLiveQueryTriggers,
  createTriggers,
  createViews,
  dropLiveQueryTriggers,
  dropTriggers,
  finalizeMultichain,
  revertMultichain,
} from "./actions.js";
import {
  type Database,
  TABLES,
  VIEWS,
  createDatabase,
  getPonderCheckpointTable,
  getPonderMetaTable,
} from "./index.js";
beforeEach(setupCommon);
beforeEach(setupIsolatedDatabase);
const account = onchainTable("account", (p) => ({
  address: p.hex().primaryKey(),
  balance: p.bigint(),
}));
function createCheckpoint(checkpoint: Partial<Checkpoint>): string {
  return encodeCheckpoint({ ...ZERO_CHECKPOINT, ...checkpoint });
}
const indexingErrorHandler: IndexingErrorHandler = {
  getRetryableError: () => {
    return indexingErrorHandler.error;
  },
  setRetryableError: (error: RetryableError) => {
    indexingErrorHandler.error = error;
  },
  clearRetryableError: () => {
    indexingErrorHandler.error = undefined;
  },
  error: undefined as RetryableError | undefined,
};
test("migrate() succeeds with empty schema", async () => {
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      ordering: "multichain",
      databaseConfig: context.databaseConfig,
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  const tableNames = await getUserTableNames(database, "public");
  expect(tableNames).toContain("account");
  expect(tableNames).toContain("_reorg__account");
  expect(tableNames).toContain("_ponder_meta");
  const metadata = await database.userQB.wrap((db) =>
    db.select().from(sql`_ponder_meta`),
  );
  expect(metadata).toHaveLength(1);
  await context.common.shutdown.kill();
});
test("migrate() with empty schema creates tables, views, and enums", async () => {
  const mood = onchainEnum("mood", ["sad", "happy"]);
  const kyle = onchainTable("kyle", (p) => ({
    age: p.integer().primaryKey(),
    mood: mood().notNull(),
  }));
  const user = onchainTable(
    "table",
    (p) => ({
      name: p.text(),
      age: p.integer(),
      address: p.hex(),
    }),
    (table) => ({
      primaryKeys: primaryKey({ columns: [table.name, table.address] }),
    }),
  );
  const userView = onchainView("user_view").as((qb) => qb.select().from(user));
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account, kyle, mood, user, userView },
      statements: buildSchema({
        schema: { account, kyle, mood, user, userView },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  const tableNames = await getUserTableNames(database, "public");
  expect(tableNames).toContain("account");
  expect(tableNames).toContain("_reorg__account");
  expect(tableNames).toContain("kyle");
  expect(tableNames).toContain("_reorg__kyle");
  expect(tableNames).toContain("kyle");
  expect(tableNames).toContain("_reorg__kyle");
  expect(tableNames).toContain("_ponder_meta");
  expect(tableNames).toContain("_ponder_checkpoint");
  const viewNames = await getUserViewNames(database, "public");
  expect(viewNames).toContain("user_view");
  await context.common.shutdown.kill();
});
test("migrate() throws with schema used", async () => {
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  await context.common.shutdown.kill();
  context.common.shutdown = createShutdown();
  const databaseTwo = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  const error = await databaseTwo
    .migrate({
      buildId: "def",
      chains: [],
      finalizedBlocks: [],
    })
    .catch((err) => err);
  expect(error).toBeDefined();
  await context.common.shutdown.kill();
});
// PGlite not being able to concurrently connect to the same database from two different clients
// makes this test impossible.
test("migrate() throws with schema used after waiting for lock", async () => {
  if (context.databaseConfig.kind !== "postgres") return;
  context.common.options.databaseHeartbeatInterval = 250;
  context.common.options.databaseHeartbeatTimeout = 1000;
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  const databaseTwo = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  const error = await databaseTwo
    .migrate({
      buildId: "abc",
      chains: [],
      finalizedBlocks: [],
    })
    .catch((err) => err);
  expect(error).toBeDefined();
  await context.common.shutdown.kill();
});
test("migrate() succeeds with crash recovery", async () => {
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  await context.common.shutdown.kill();
  context.common.shutdown = createShutdown();
  const databaseTwo = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await databaseTwo.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  const metadata = await databaseTwo.userQB.wrap((db) =>
    db.select().from(getPonderMetaTable("public")),
  );
  expect(metadata).toHaveLength(1);
  const tableNames = await getUserTableNames(databaseTwo, "public");
  expect(tableNames).toContain("account");
  expect(tableNames).toContain("_reorg__account");
  expect(tableNames).toContain("_ponder_meta");
  await context.common.shutdown.kill();
});
test("migrate() succeeds with crash recovery after waiting for lock", async () => {
  context.common.options.databaseHeartbeatInterval = 750;
  context.common.options.databaseHeartbeatTimeout = 500;
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  const databaseTwo = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await databaseTwo.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  await context.common.shutdown.kill();
});
test("migrateSync()", async () => {
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrateSync();
  // Note: this is a hack to avoid trying to update the metadata table on shutdown
  context.common.options.command = "list";
  await context.common.shutdown.kill();
});
// Note: this test doesn't do anything because we don't have a migration using the
// new design yet.
test.skip("migrateSync() handles concurrent migrations", async () => {
  if (context.databaseConfig.kind !== "postgres") return;
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  // The second migration should error, then retry and succeed
  const spy = vi.spyOn(database.userQB, "transaction");
  await Promise.all([database.migrateSync(), database.migrateSync()]);
  // transaction gets called when performing a migration
  expect(spy).toHaveBeenCalledTimes(3);
  // Note: this is a hack to avoid trying to update the metadata table on shutdown
  context.common.options.command = "list";
  await context.common.shutdown.kill();
});
test("migrate() with crash recovery reverts rows", async () => {
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  // setup tables, reorg tables, and metadata checkpoint
  await database.userQB.wrap((db) =>
    db.update(getPonderMetaTable("public")).set({
      value: sql`jsonb_set(value, '{is_ready}', to_jsonb(1))`,
    }),
  );
  await database.userQB.raw
    .insert(account)
    .values({ address: zeroAddress, balance: 10n });
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 9n }),
    table: account,
    preBuild: { ordering: "multichain" },
  });
  await createTriggers(database.userQB, { tables: [account] });
  await database.userQB.raw
    .insert(account)
    .values({ address: "0x0000000000000000000000000000000000000001" });
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 11n }),
    table: account,
    preBuild: { ordering: "multichain" },
  });
  await database.userQB.wrap((db) =>
    db.insert(getPonderCheckpointTable()).values({
      chainId: 1,
      chainName: "mainnet",
      latestCheckpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
      finalizedCheckpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
      safeCheckpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
    }),
  );
  await context.common.shutdown.kill();
  context.common.shutdown = createShutdown();
  const databaseTwo = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  const checkpoint = await databaseTwo.migrate({
    buildId: "abc",
    chains: [getChain()],
    finalizedBlocks: [
      {
        timestamp: "0x1",
        number: "0xa",
        hash: "0x",
        parentHash: "0x",
      },
    ],
  });
  expect(checkpoint).toMatchInlineSnapshot(`
    [
      {
        "chainId": 1,
        "checkpoint": "000000000000000000000000010000000000000010000000000000000000000000000000000",
      },
    ]
  `);
  const rows = await databaseTwo.userQB.wrap((db) =>
    db.execute(sql`SELECT * from "account"`).then((result) => result.rows),
  );
  expect(rows).toHaveLength(1);
  expect(rows[0]!.address).toBe(zeroAddress);
  const metadata = await databaseTwo.userQB.wrap((db) =>
    db.select().from(getPonderMetaTable("public")),
  );
  expect(metadata).toHaveLength(1);
  await context.common.shutdown.kill();
});
test("migrate() with crash recovery drops indexes and triggers", async () => {
  const account = onchainTable(
    "account",
    (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint(),
    }),
    (table) => ({
      balanceIdx: index().on(table.balance),
    }),
  );
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  await createIndexes(database.userQB, {
    statements: buildSchema({
      schema: { account },
      preBuild: { ordering: "multichain" },
    }).statements,
  });
  await database.userQB.wrap((db) =>
    db.insert(getPonderCheckpointTable()).values({
      chainId: 1,
      chainName: "mainnet",
      latestCheckpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
      finalizedCheckpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
      safeCheckpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
    }),
  );
  await database.userQB.wrap((db) =>
    db.update(getPonderMetaTable("public")).set({
      value: sql`jsonb_set(value, '{is_ready}', to_jsonb(1))`,
    }),
  );
  await context.common.shutdown.kill();
  context.common.shutdown = createShutdown();
  const databaseTwo = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await databaseTwo.migrate({
    buildId: "abc",
    chains: [getChain()],
    finalizedBlocks: [
      {
        timestamp: "0x1",
        number: "0xa",
        hash: "0x",
        parentHash: "0x",
      },
    ],
  });
  const indexNames = await getUserIndexNames(databaseTwo, "public", "account");
  expect(indexNames).toHaveLength(1);
  await context.common.shutdown.kill();
});
test("heartbeat updates the heartbeat_at value", async () => {
  context.common.options.databaseHeartbeatInterval = 250;
  context.common.options.databaseHeartbeatTimeout = 625;
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: undefined,
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { account },
      statements: buildSchema({
        schema: { account },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  const row = await database.userQB.wrap((db) =>
    db
      .select()
      .from(getPonderMetaTable("public"))
      .then((result) => result[0]!.value),
  );
  await wait(500);
  const rowAfterHeartbeat = await database.userQB.wrap((db) =>
    db
      .select()
      .from(getPonderMetaTable("public"))
      .then((result) => result[0]!.value),
  );
  expect(BigInt(rowAfterHeartbeat!.heartbeat_at)).toBeGreaterThan(
    row!.heartbeat_at,
  );
  await context.common.shutdown.kill();
});
test("camelCase", async () => {
  const accountCC = onchainTable("accountCc", (p) => ({
    address: p.hex("addressCc").primaryKey(),
    balance: p.bigint(),
  }));
  const accountViewCC = onchainView("accountViewCc").as((qb) =>
    qb.select().from(accountCC),
  );
  const database = createDatabase({
    common: context.common,
    namespace: {
      schema: "public",
      viewsSchema: "viewCc",
    },
    preBuild: {
      databaseConfig: context.databaseConfig,
      ordering: "multichain",
    },
    schemaBuild: {
      schema: { accountCC, accountViewCC },
      statements: buildSchema({
        schema: { accountCC, accountViewCC },
        preBuild: { ordering: "multichain" },
      }).statements,
    },
  });
  await database.migrate({
    buildId: "abc",
    chains: [],
    finalizedBlocks: [],
  });
  const tableNames = await getUserTableNames(database, "public");
  expect(tableNames).toContain("accountCc");
  expect(tableNames).toContain("_reorg__accountCc");
  expect(tableNames).toContain("_ponder_meta");
  const metadata = await database.userQB.wrap((db) =>
    db.select().from(sql`_ponder_meta`),
  );
  expect(metadata).toHaveLength(1);
  await createTriggers(database.userQB, { tables: [accountCC] });
  await createIndexes(database.userQB, {
    statements: buildSchema({
      schema: { accountCC },
      preBuild: { ordering: "multichain" },
    }).statements,
  });
  await createViews(database.userQB, {
    tables: [accountCC],
    views: [accountViewCC],
    namespaceBuild: { schema: "public", viewsSchema: "viewCc" },
  });
  await commitBlock(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
    table: accountCC,
    preBuild: { ordering: "multichain" },
  });
  await dropTriggers(database.userQB, { tables: [accountCC] });
  await createLiveQueryProcedures(database.userQB, {
    namespaceBuild: { schema: "public", viewsSchema: "viewCc" },
  });
  await createLiveQueryTriggers(database.userQB, {
    tables: [accountCC],
    namespaceBuild: { schema: "public", viewsSchema: "viewCc" },
  });
  await dropLiveQueryTriggers(database.userQB, {
    tables: [accountCC],
    namespaceBuild: { schema: "public", viewsSchema: "viewCc" },
  });
  await revertMultichain(database.userQB, {
    tables: [accountCC],
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
  });
  await finalizeMultichain(database.userQB, {
    checkpoint: createCheckpoint({ chainId: 1n, blockNumber: 10n }),
    tables: [accountCC],
    namespaceBuild: { schema: "public", viewsSchema: "viewCc" },
  });
  await crashRecovery(database.userQB, {
    table: accountCC,
  });
  await context.common.shutdown.kill();
});
async function getUserTableNames(database: Database, namespace: string) {
  const rows = await database.userQB.wrap((db) =>
    db
      .select({ name: TABLES.table_name })
      .from(TABLES)
      .where(
        and(
          eq(TABLES.table_schema, namespace),
          eq(TABLES.table_type, "BASE TABLE"),
        ),
      ),
  );
  return rows.map(({ name }) => name);
}
async function getUserViewNames(database: Database, namespace: string) {
  const rows = await database.userQB.wrap((db) =>
    db
      .select({ name: VIEWS.table_name })
      .from(VIEWS)
      .where(and(eq(VIEWS.table_schema, namespace))),
  );
  return rows.map(({ name }) => name);
}
async function getUserIndexNames(
  database: Database,
  namespace: string,
  tableName: string,
) {
  const rows = await database.userQB.wrap((db) =>
    db
      .select({
        name: sql<string>`indexname`.as("name"),
      })
      .from(sql`pg_indexes`)
      .where(
        and(eq(sql`schemaname`, namespace), eq(sql`tablename`, tableName)),
      ),
  );
  return rows.map((r) => r.name);
}
</file>

<file path="packages/core/src/database/index.ts">
import {
  getLiveQueryNotifyProcedureName,
  getLiveQueryProcedureName,
  getPartitionName,
  getReorgSequenceName,
  getReorgTableName,
} from "@/drizzle/onchain.js";
import type { Common } from "@/internal/common.js";
import {
  MigrationError,
  NonRetryableUserError,
  ShutdownError,
} from "@/internal/errors.js";
import type {
  CrashRecoveryCheckpoint,
  IndexingBuild,
  NamespaceBuild,
  PreBuild,
  SchemaBuild,
} from "@/internal/types.js";
import { buildMigrationProvider } from "@/sync-store/migrations.js";
import * as PONDER_SYNC from "@/sync-store/schema.js";
import { decodeCheckpoint } from "@/utils/checkpoint.js";
import { formatEta } from "@/utils/format.js";
import { createPool, createReadonlyPool } from "@/utils/pg.js";
import { createPglite, createPgliteKyselyDialect } from "@/utils/pglite.js";
import { startClock } from "@/utils/timer.js";
import { wait } from "@/utils/wait.js";
import type { PGlite } from "@electric-sql/pglite";
import {
  eq,
  getTableName,
  getViewName,
  isTable,
  isView,
  sql,
} from "drizzle-orm";
import { drizzle as drizzleNodePg } from "drizzle-orm/node-postgres";
import { pgSchema, pgTable } from "drizzle-orm/pg-core";
import { drizzle as drizzlePglite } from "drizzle-orm/pglite";
import { Kysely, Migrator, PostgresDialect, WithSchemaPlugin } from "kysely";
import type { Pool } from "pg";
import prometheus from "prom-client";
import { hexToBigInt } from "viem";
import {
  crashRecovery,
  createLiveQueryProcedures,
  dropLiveQueryTriggers,
  dropTriggers,
} from "./actions.js";
import { type QB, createQB, parseDbError } from "./queryBuilder.js";
export type Database = {
  driver: PostgresDriver | PGliteDriver;
  syncQB: QB<typeof PONDER_SYNC>;
  adminQB: QB;
  userQB: QB;
  readonlyQB: QB;
  /** Migrate the `ponder_sync` schema. */
  migrateSync(): Promise<void>;
  /**
   * Migrate the user schema.
   *
   * @returns The crash recovery checkpoint for each chain if there is a cache hit, else undefined.
   */
  migrate({
    buildId,
    chains,
    finalizedBlocks,
  }: Pick<
    IndexingBuild,
    "buildId" | "chains" | "finalizedBlocks"
  >): Promise<CrashRecoveryCheckpoint>;
};
export const SCHEMATA = pgSchema("information_schema").table(
  "schemata",
  (t) => ({
    schemaName: t.text().primaryKey(),
  }),
);
export const TABLES = pgSchema("information_schema").table("tables", (t) => ({
  table_name: t.text().notNull(),
  table_schema: t.text().notNull(),
  table_type: t.text().notNull(),
}));
export const VIEWS = pgSchema("information_schema").table("views", (t) => ({
  table_name: t.text().notNull(),
  table_schema: t.text().notNull(),
}));
export const PONDER_META_TABLE_NAME = "_ponder_meta";
/**
 * @dev Used since version 0.11.
 */
export const PONDER_CHECKPOINT_TABLE_NAME = "_ponder_checkpoint";
/**
 * @dev Used from version 0.9 to 0.11.
 */
export const PONDER_STATUS_TABLE_NAME = "_ponder_status";
// Note: "version" was introduced in 0.9
export type PonderApp0 = {
  version: undefined;
  is_locked: 0 | 1;
  is_dev: 0 | 1;
  heartbeat_at: number;
  build_id: string;
  checkpoint: string;
  table_names: string[];
};
export type PonderApp1 = {
  version: "1";
  build_id: string;
  table_names: string[];
  is_locked: 0 | 1;
  is_ready: 0 | 1;
  heartbeat_at: number;
};
export type PonderApp2 = Omit<PonderApp1, "version"> & {
  version: "2";
  is_dev: 0 | 1;
};
export type PonderApp3 = Omit<PonderApp2, "version"> & {
  version: "3";
};
export type PonderApp4 = Omit<PonderApp3, "version"> & {
  version: "4";
};
export type PonderApp5 = Omit<PonderApp4, "version"> & {
  version: "5";
  view_names: string[];
};
export type PonderApp6 = Omit<PonderApp5, "version"> & {
  version: "6";
};
const VERSION = "6";
type PGliteDriver = {
  dialect: "pglite";
  instance: PGlite;
};
type PostgresDriver = {
  dialect: "postgres";
  admin: Pool;
  sync: Pool;
  user: Pool;
  readonly: Pool;
};
export const getPonderMetaTable = (schema?: string) => {
  if (schema === undefined || schema === "public") {
    return pgTable(PONDER_META_TABLE_NAME, (t) => ({
      key: t.text().primaryKey().$type<"app">(),
      value: t.jsonb().$type<PonderApp6>().notNull(),
    }));
  }
  return pgSchema(schema).table(PONDER_META_TABLE_NAME, (t) => ({
    key: t.text().primaryKey().$type<"app">(),
    value: t.jsonb().$type<PonderApp6>().notNull(),
  }));
};
/**
 * @dev It is an invariant that `latestCheckpoint` refers to the same chain as `chainId`.
 */
export const getPonderCheckpointTable = (schema?: string) => {
  if (schema === undefined || schema === "public") {
    return pgTable(PONDER_CHECKPOINT_TABLE_NAME, (t) => ({
      chainName: t.text().primaryKey(),
      chainId: t.bigint({ mode: "number" }).notNull(),
      safeCheckpoint: t.varchar({ length: 75 }).notNull(),
      latestCheckpoint: t.varchar({ length: 75 }).notNull(),
      finalizedCheckpoint: t.varchar({ length: 75 }).notNull(),
    }));
  }
  return pgSchema(schema).table(PONDER_CHECKPOINT_TABLE_NAME, (t) => ({
    chainName: t.text().primaryKey(),
    chainId: t.bigint({ mode: "number" }).notNull(),
    safeCheckpoint: t.varchar({ length: 75 }).notNull(),
    latestCheckpoint: t.varchar({ length: 75 }).notNull(),
    finalizedCheckpoint: t.varchar({ length: 75 }).notNull(),
  }));
};
export const createDatabase = ({
  common,
  namespace,
  preBuild,
  schemaBuild,
}: {
  common: Common;
  namespace: NamespaceBuild;
  preBuild: Pick<PreBuild, "databaseConfig" | "ordering">;
  schemaBuild: Omit<SchemaBuild, "graphqlSchema">;
}): Database => {
  let heartbeatInterval: NodeJS.Timeout | undefined;
  const PONDER_META = getPonderMetaTable(namespace.schema);
  const PONDER_CHECKPOINT = getPonderCheckpointTable(namespace.schema);
  ////////
  // Create schema, drivers, roles, and query builders
  ////////
  let driver: PGliteDriver | PostgresDriver;
  let syncQB: QB<typeof PONDER_SYNC>;
  let adminQB: QB;
  let userQB: QB;
  let readonlyQB: QB;
  const dialect = preBuild.databaseConfig.kind;
  if (dialect === "pglite" || dialect === "pglite_test") {
    driver = {
      dialect: "pglite",
      instance:
        dialect === "pglite"
          ? createPglite(preBuild.databaseConfig.options)
          : preBuild.databaseConfig.instance,
    };
    common.shutdown.add(async () => {
      clearInterval(heartbeatInterval);
      if (["start", "dev"].includes(common.options.command)) {
        await adminQB.wrap({ label: "unlock" }, (db) =>
          db
            .update(PONDER_META)
            .set({ value: sql`jsonb_set(value, '{is_locked}', to_jsonb(0))` }),
        );
      }
      if (dialect === "pglite") {
        await (driver as PGliteDriver).instance.close();
      }
    });
    syncQB = createQB(
      drizzlePglite((driver as PGliteDriver).instance, {
        casing: "snake_case",
        schema: PONDER_SYNC,
      }),
      { common, isAdmin: false },
    );
    adminQB = createQB(
      drizzlePglite((driver as PGliteDriver).instance, {
        casing: "snake_case",
        schema: schemaBuild.schema,
      }),
      { common, isAdmin: true },
    );
    userQB = createQB(
      drizzlePglite((driver as PGliteDriver).instance, {
        casing: "snake_case",
        schema: schemaBuild.schema,
      }),
      { common, isAdmin: false },
    );
    readonlyQB = createQB(
      drizzlePglite((driver as PGliteDriver).instance, {
        casing: "snake_case",
        schema: schemaBuild.schema,
      }),
      { common, isAdmin: false },
    );
  } else {
    const internalMax = 2;
    const equalMax = Math.floor(
      (preBuild.databaseConfig.poolConfig.max - internalMax) / 3,
    );
    const [readonlyMax, userMax, syncMax] =
      common.options.command === "serve"
        ? [preBuild.databaseConfig.poolConfig.max - internalMax, 0, 0]
        : [equalMax, equalMax, equalMax];
    driver = {
      dialect: "postgres",
      admin: createPool(
        {
          ...preBuild.databaseConfig.poolConfig,
          application_name: `${namespace.schema}_internal`,
          max: internalMax,
          statement_timeout: 10 * 60 * 1000, // 10 minutes to accommodate slow sync store migrations.
        },
        common.logger,
      ),
      user: createPool(
        {
          ...preBuild.databaseConfig.poolConfig,
          application_name: `${namespace.schema}_user`,
          max: userMax,
        },
        common.logger,
      ),
      readonly: createReadonlyPool(
        {
          ...preBuild.databaseConfig.poolConfig,
          application_name: `${namespace.schema}_readonly`,
          max: readonlyMax,
        },
        common.logger,
        namespace.schema,
      ),
      sync: createPool(
        {
          ...preBuild.databaseConfig.poolConfig,
          application_name: "ponder_sync",
          max: syncMax,
        },
        common.logger,
      ),
    };
    syncQB = createQB(
      drizzleNodePg(driver.sync, {
        casing: "snake_case",
        schema: PONDER_SYNC,
      }),
      { common, isAdmin: false },
    );
    adminQB = createQB(
      drizzleNodePg(driver.admin, {
        casing: "snake_case",
        schema: schemaBuild.schema,
      }),
      { common, isAdmin: true },
    );
    userQB = createQB(
      drizzleNodePg(driver.user, {
        casing: "snake_case",
        schema: schemaBuild.schema,
      }),
      { common, isAdmin: false },
    );
    readonlyQB = createQB(
      drizzleNodePg(driver.readonly, {
        casing: "snake_case",
        schema: schemaBuild.schema,
      }),
      { common, isAdmin: false },
    );
    common.shutdown.add(async () => {
      clearInterval(heartbeatInterval);
      if (["start", "dev"].includes(common.options.command)) {
        await adminQB.wrap({ label: "unlock" }, (db) =>
          db
            .update(PONDER_META)
            .set({ value: sql`jsonb_set(value, '{is_locked}', to_jsonb(0))` }),
        );
      }
      const d = driver as PostgresDriver;
      await Promise.all([
        d.admin.end(),
        d.user.end(),
        d.readonly.end(),
        d.sync.end(),
      ]);
    });
    // Register Postgres-only metrics
    const d = driver as PostgresDriver;
    common.metrics.registry.removeSingleMetric(
      "ponder_postgres_pool_connections",
    );
    common.metrics.ponder_postgres_pool_connections = new prometheus.Gauge({
      name: "ponder_postgres_pool_connections",
      help: "Number of connections in the pool",
      labelNames: ["pool", "kind"] as const,
      registers: [common.metrics.registry],
      collect() {
        this.set({ pool: "admin", kind: "idle" }, d.admin.idleCount);
        this.set({ pool: "admin", kind: "total" }, d.admin.totalCount);
        this.set({ pool: "sync", kind: "idle" }, d.sync.idleCount);
        this.set({ pool: "sync", kind: "total" }, d.sync.totalCount);
        this.set({ pool: "user", kind: "idle" }, d.user.idleCount);
        this.set({ pool: "user", kind: "total" }, d.user.totalCount);
        this.set({ pool: "readonly", kind: "idle" }, d.readonly.idleCount);
        this.set({ pool: "readonly", kind: "total" }, d.readonly.totalCount);
      },
    });
    common.metrics.registry.removeSingleMetric(
      "ponder_postgres_query_queue_size",
    );
    common.metrics.ponder_postgres_query_queue_size = new prometheus.Gauge({
      name: "ponder_postgres_query_queue_size",
      help: "Number of queries waiting for an available connection",
      labelNames: ["pool"] as const,
      registers: [common.metrics.registry],
      collect() {
        this.set({ pool: "admin" }, d.admin.waitingCount);
        this.set({ pool: "sync" }, d.sync.waitingCount);
        this.set({ pool: "user" }, d.user.waitingCount);
        this.set({ pool: "readonly" }, d.readonly.waitingCount);
      },
    });
  }
  const tables = Object.values(schemaBuild.schema).filter(isTable);
  const views = Object.values(schemaBuild.schema).filter(isView);
  return {
    driver,
    syncQB,
    adminQB,
    userQB,
    readonlyQB,
    async migrateSync() {
      const kysely = new Kysely({
        dialect:
          dialect === "postgres"
            ? new PostgresDialect({ pool: (driver as PostgresDriver).admin })
            : createPgliteKyselyDialect((driver as PGliteDriver).instance),
        log(event) {
          if (event.level === "query") {
            common.metrics.ponder_postgres_query_total.inc({ pool: "migrate" });
          }
        },
        plugins: [new WithSchemaPlugin("ponder_sync")],
      });
      const migrationProvider = buildMigrationProvider(common.logger);
      const migrator = new Migrator({
        db: kysely,
        provider: migrationProvider,
        migrationTableSchema: "ponder_sync",
      });
      // Note: inline operation of database wrapper because this is the only place where kysely is used
      for (let i = 0; i <= 9; i++) {
        const endClock = startClock();
        try {
          const { error } = await migrator.migrateToLatest();
          if (error) throw error;
          common.metrics.ponder_database_method_duration.observe(
            { method: "migrate_sync" },
            endClock(),
          );
          return;
        } catch (_error) {
          const error = parseDbError(_error);
          if (common.shutdown.isKilled) {
            throw new ShutdownError();
          }
          common.metrics.ponder_database_method_duration.observe(
            { method: "migrate_sync" },
            endClock(),
          );
          common.metrics.ponder_database_method_error_total.inc({
            method: "migrate_sync",
          });
          common.logger.warn({
            msg: "Failed database query",
            query: "migrate_sync",
            retry_count: i,
            error,
          });
          if (error instanceof NonRetryableUserError) {
            common.logger.warn({
              msg: "Failed database query",
              query: "migrate_sync",
              error,
            });
            throw error;
          }
          if (i === 9) {
            common.logger.warn({
              msg: "Failed database query",
              query: "migrate_sync",
              retry_count: i,
              error,
            });
            throw error;
          }
          const duration = 125 * 2 ** i;
          common.logger.debug({
            msg: "Failed database query",
            query: "migrate_sync",
            retry_count: i,
            retry_delay: duration,
            error,
          });
          await wait(duration);
        }
      }
    },
    async migrate({ buildId, chains, finalizedBlocks }) {
      const context = { logger: common.logger.child({ action: "migrate" }) };
      const createTables = async (tx: QB) => {
        for (let i = 0; i < schemaBuild.statements.tables.sql.length; i++) {
          try {
            const schemaName = schemaBuild.statements.tables.json[i]!.schema;
            const tableName = schemaBuild.statements.tables.json[i]!.tableName;
            if (
              preBuild.ordering === "experimental_isolated" &&
              tableName.startsWith("_reorg__") === false
            ) {
              const sql = schemaBuild.statements.tables.sql[i]!;
              await tx.wrap((tx) =>
                tx.execute(
                  `${sql.slice(0, sql.length - 2)} PARTITION BY LIST (chain_id);`,
                ),
              );
              for (const chain of chains) {
                await tx.wrap((tx) =>
                  tx.execute(
                    `CREATE TABLE "${schemaName}"."${getPartitionName(tableName, chain.id)}" PARTITION OF "${schemaName}"."${tableName}" FOR VALUES IN (${chain.id});`,
                  ),
                );
              }
            } else {
              await tx.wrap(
                (tx) => tx.execute(schemaBuild.statements.tables.sql[i]!),
                context,
              );
            }
          } catch (_error) {
            let error = _error as Error;
            if (!error.message.includes("already exists")) throw error;
            error = new MigrationError(
              `Unable to create table '${namespace.schema}'.'${schemaBuild.statements.tables.json[i]!.tableName}' because a table with that name already exists.`,
            );
            error.stack = undefined;
            throw error;
          }
        }
        for (const table of tables) {
          await tx.wrap(
            (tx) =>
              tx.execute(
                `CREATE INDEX IF NOT EXISTS "${getTableName(table)}_checkpoint_index" ON "${namespace.schema}"."${getReorgTableName(table)}" ("checkpoint")`,
              ),
            context,
          );
        }
      };
      const createViews = async (tx: QB) => {
        for (let i = 0; i < schemaBuild.statements.views.sql.length; i++) {
          await tx
            .wrap(
              (tx) => tx.execute(schemaBuild.statements.views.sql[i]!),
              context,
            )
            .catch((_error) => {
              const error = _error as Error;
              if (!error.message.includes("already exists")) throw error;
              const e = new MigrationError(
                `Unable to create view "${namespace.schema}"."${schemaBuild.statements.views.json[i]!.name}" because a view with that name already exists.`,
              );
              e.stack = undefined;
              throw e;
            });
        }
      };
      const createEnums = async (tx: QB) => {
        for (let i = 0; i < schemaBuild.statements.enums.sql.length; i++) {
          await tx
            .wrap(
              (tx) => tx.execute(schemaBuild.statements.enums.sql[i]!),
              context,
            )
            .catch((_error) => {
              const error = _error as Error;
              if (!error.message.includes("already exists")) throw error;
              const e = new MigrationError(
                `Unable to create enum "${namespace.schema}"."${schemaBuild.statements.enums.json[i]!.name}" because an enum with that name already exists.`,
              );
              e.stack = undefined;
              throw e;
            });
        }
      };
      const createAdminObjects = async (tx: QB) => {
        await tx.wrap(
          (tx) =>
            tx.execute(
              `
CREATE TABLE IF NOT EXISTS "${namespace.schema}"."${PONDER_META_TABLE_NAME}" (
  "key" TEXT PRIMARY KEY,
  "value" JSONB NOT NULL
)`,
            ),
          context,
        );
        await tx.wrap(
          (tx) =>
            tx.execute(
              `
CREATE TABLE IF NOT EXISTS "${namespace.schema}"."${PONDER_CHECKPOINT_TABLE_NAME}" (
  "chain_name" TEXT PRIMARY KEY,
  "chain_id" BIGINT NOT NULL,
  "safe_checkpoint" VARCHAR(75) NOT NULL,
  "latest_checkpoint" VARCHAR(75) NOT NULL,
  "finalized_checkpoint" VARCHAR(75) NOT NULL
)`,
            ),
          context,
        );
        await tx.wrap(
          (tx) =>
            tx.execute(
              `CREATE SEQUENCE IF NOT EXISTS "${namespace.schema}"."${getReorgSequenceName()}" AS integer INCREMENT BY 1`,
            ),
          context,
        );
      };
      const tryAcquireLockAndMigrate = () =>
        adminQB.transaction({ label: "migrate" }, async (tx) => {
          await tx.wrap(
            (tx) =>
              tx.execute(`CREATE SCHEMA IF NOT EXISTS "${namespace.schema}"`),
            context,
          );
          if (dialect === "pglite" || dialect === "pglite_test") {
            await tx.wrap(
              (tx) => tx.execute(`SET search_path TO "${namespace.schema}"`),
              context,
            );
          }
          await createAdminObjects(tx);
          let endClock = startClock();
          common.logger.debug({
            msg: "Created internal database objects",
            schema: namespace.schema,
            table_count: 2,
            trigger_count: 2,
            duration: endClock(),
          });
          // Note: All ponder versions are compatible with the next query (every version of the "_ponder_meta" table have the same columns)
          const previousApp = await tx.wrap(
            (tx) =>
              tx
                .select({ value: PONDER_META.value })
                .from(PONDER_META)
                .where(eq(PONDER_META.key, "app"))
                .then((result) => result[0]?.value),
            context,
          );
          const metadata = {
            version: VERSION,
            build_id: buildId,
            table_names: tables.map(getTableName),
            view_names: views.map(getViewName),
            is_dev: common.options.command === "dev" ? 1 : 0,
            is_locked: 1,
            is_ready: 0,
            heartbeat_at: Date.now(),
          } satisfies PonderApp6;
          if (previousApp === undefined) {
            endClock = startClock();
            await createEnums(tx);
            await createTables(tx);
            await createViews(tx);
            await createLiveQueryProcedures(
              tx,
              { namespaceBuild: namespace },
              context,
            );
            common.logger.info({
              msg: "Created database tables",
              count: tables.length,
              tables: JSON.stringify(tables.map(getTableName)),
              duration: endClock(),
            });
            if (views.length > 0) {
              common.logger.info({
                msg: "Created database views",
                count: views.length,
                views: JSON.stringify(views.map(getViewName)),
                duration: endClock(),
              });
            }
            await tx.wrap(
              (tx) =>
                tx.insert(PONDER_META).values({ key: "app", value: metadata }),
              context,
            );
            return {
              status: "success",
              crashRecoveryCheckpoint: undefined,
            } as const;
          }
          if (previousApp.is_dev === 1) {
            endClock = startClock();
            for (const table of previousApp.table_names ?? []) {
              await tx.wrap(
                (tx) =>
                  tx.execute(
                    `DROP TABLE IF EXISTS "${namespace.schema}"."${table}" CASCADE`,
                  ),
                context,
              );
              await tx.wrap(
                (tx) =>
                  tx.execute(
                    `DROP TABLE IF EXISTS "${namespace.schema}"."${getReorgTableName(table)}" CASCADE`,
                  ),
                context,
              );
            }
            for (const view of previousApp.view_names ?? []) {
              await tx.wrap(
                (tx) =>
                  tx.execute(
                    `DROP VIEW IF EXISTS "${namespace.schema}"."${view}" CASCADE`,
                  ),
                context,
              );
            }
            for (const enumName of schemaBuild.statements.enums.json) {
              await tx.wrap(
                (tx) =>
                  tx.execute(
                    `DROP TYPE IF EXISTS "${namespace.schema}"."${enumName.name}"`,
                  ),
                context,
              );
            }
            common.logger.warn({
              msg: "Dropped existing database tables",
              count: previousApp.table_names?.length,
              tables: JSON.stringify(previousApp.table_names),
              duration: endClock(),
            });
            if (previousApp.view_names?.length > 0) {
              common.logger.warn({
                msg: "Dropped existing database views",
                count: previousApp.view_names?.length,
                views: JSON.stringify(previousApp.view_names),
              });
            }
            endClock = startClock();
            await createEnums(tx);
            await createTables(tx);
            await createViews(tx);
            common.logger.info({
              msg: "Created database tables",
              count: tables.length,
              tables: JSON.stringify(tables.map(getTableName)),
              duration: endClock(),
            });
            if (views.length > 0) {
              common.logger.info({
                msg: "Created database views",
                count: views.length,
                views: JSON.stringify(views.map(getViewName)),
                duration: endClock(),
              });
            }
            endClock = startClock();
            await tx.wrap(
              (tx) =>
                tx.execute(
                  `DROP TABLE IF EXISTS "${namespace.schema}"."${PONDER_CHECKPOINT_TABLE_NAME}" CASCADE`,
                ),
              context,
            );
            await tx.wrap(
              (tx) =>
                tx.execute(
                  `DROP TABLE IF EXISTS "${namespace.schema}"."${PONDER_META_TABLE_NAME}" CASCADE`,
                ),
              context,
            );
            await tx.wrap((tx) =>
              tx.execute(
                `DROP FUNCTION IF EXISTS "${namespace.schema}".${getLiveQueryProcedureName()}`,
              ),
            );
            await tx.wrap((tx) =>
              tx.execute(
                `DROP FUNCTION IF EXISTS "${namespace.schema}".${getLiveQueryNotifyProcedureName()}`,
              ),
            );
            await createAdminObjects(tx);
            await createLiveQueryProcedures(
              tx,
              { namespaceBuild: namespace },
              context,
            );
            common.logger.debug({
              msg: "Reset internal database objects",
              schema: namespace.schema,
              duration: endClock(),
            });
            await tx.wrap(
              (tx) =>
                tx.insert(PONDER_META).values({ key: "app", value: metadata }),
              context,
            );
            return {
              status: "success",
              crashRecoveryCheckpoint: undefined,
            } as const;
          }
          // Note: ponder <=0.8 will evaluate this as true because the version is undefined
          if (previousApp.version !== VERSION) {
            const error = new MigrationError(
              `Schema "${namespace.schema}" was previously used by a Ponder app with a different minor version. Drop the schema first, or use a different schema. Read more: https://ponder.sh/docs/database#database-schema`,
            );
            error.stack = undefined;
            throw error;
          }
          if (
            process.env.PONDER_EXPERIMENTAL_DB !== "platform" &&
            (common.options.command === "dev" ||
              previousApp.build_id !== buildId)
          ) {
            const error = new MigrationError(
              `Schema "${namespace.schema}" was previously used by a different Ponder app. Drop the schema first, or use a different schema. Read more: https://ponder.sh/docs/database#database-schema`,
            );
            error.stack = undefined;
            throw error;
          }
          const expiry =
            previousApp.heartbeat_at + common.options.databaseHeartbeatTimeout;
          const isAppUnlocked =
            previousApp.is_locked === 0 || expiry <= Date.now();
          if (isAppUnlocked === false) {
            return { status: "locked", expiry } as const;
          }
          common.logger.info({
            msg: "Detected crash recovery",
            build_id: buildId,
            last_active: `${formatEta(Date.now() - previousApp.heartbeat_at)}s`,
            schema: namespace.schema,
          });
          const checkpoints = await tx.wrap(
            (tx) => tx.select().from(PONDER_CHECKPOINT),
            context,
          );
          // Note: The previous app can be in three possible states:
          // 1. Has no checkpoints, hasn't made it past the setup events
          // 2. Has checkpoints but hasn't made it past the historical backfill
          // 3. Has checkpoints and has made it past the historical backfill
          if (checkpoints.length === 0) {
            return {
              status: "success",
              crashRecoveryCheckpoint: undefined,
            } as const;
          }
          for (const { chainId, finalizedCheckpoint } of checkpoints) {
            const finalizedBlock =
              finalizedBlocks[chains.findIndex((c) => c.id === chainId)]!;
            if (
              hexToBigInt(finalizedBlock.timestamp) <
              decodeCheckpoint(finalizedCheckpoint).blockTimestamp
            ) {
              throw new MigrationError(
                `Finalized block for chain "${chainId}" cannot move backwards`,
              );
            }
          }
          const crashRecoveryCheckpoint = checkpoints.map((c) => ({
            chainId: c.chainId,
            checkpoint: c.safeCheckpoint,
          }));
          // Note: The statements below will not affect chains that are not "live".
          // Remove triggers
          if (preBuild.ordering === "experimental_isolated") {
            for (const { chainId } of checkpoints) {
              await dropTriggers(tx, { tables, chainId });
              await dropLiveQueryTriggers(
                tx,
                { namespaceBuild: namespace, tables, chainId },
                context,
              );
            }
          } else {
            await dropTriggers(tx, { tables });
            await dropLiveQueryTriggers(
              tx,
              { namespaceBuild: namespace, tables },
              context,
            );
          }
          // Remove indexes
          for (const indexStatement of schemaBuild.statements.indexes.json) {
            await tx.wrap(
              (tx) =>
                tx.execute(
                  `DROP INDEX IF EXISTS "${namespace.schema}"."${indexStatement.data.name}"`,
                ),
              context,
            );
          }
          for (const table of tables) {
            await crashRecovery(tx, { table });
          }
          // Note: We don't update the `_ponder_checkpoint` table here, instead we wait for it to be updated
          // in the runtime script.
          await tx.wrap(
            (tx) => tx.update(PONDER_META).set({ value: metadata }),
            context,
          );
          return { status: "success", crashRecoveryCheckpoint } as const;
        });
      let result = await tryAcquireLockAndMigrate();
      if (result.status === "locked") {
        const duration = result.expiry - Date.now();
        common.logger.warn({
          msg: "Schema is locked by a different Ponder app",
          schema: namespace.schema,
          retry_delay: duration,
        });
        await wait(duration);
        result = await tryAcquireLockAndMigrate();
        if (result.status === "locked") {
          const error = new MigrationError(
            `Failed to acquire lock on schema "${namespace.schema}". A different Ponder app is actively using this schema.`,
          );
          error.stack = undefined;
          throw error;
        }
      }
      heartbeatInterval = setInterval(async () => {
        try {
          const heartbeat = Date.now();
          const endClock = startClock();
          await adminQB.wrap({ label: "update_heartbeat" }, (db) =>
            db.update(PONDER_META).set({
              value: sql`jsonb_set(value, '{heartbeat_at}', ${heartbeat})`,
            }),
          );
          common.logger.trace({
            msg: "Updated heartbeat timestamp",
            heartbeat,
            build_id: buildId,
            schema: namespace.schema,
            duration: endClock(),
          });
        } catch (err) {
          const error = err as Error;
          common.logger.error({
            msg: "Failed to update heartbeat timestamp",
            retry_delay: common.options.databaseHeartbeatInterval,
            error,
          });
        }
      }, common.options.databaseHeartbeatInterval);
      return result.crashRecoveryCheckpoint;
    },
  };
};
</file>

<file path="packages/core/src/database/queryBuilder.test.ts">
import { context, setupCommon, setupIsolatedDatabase } from "@/_test/setup.js";
import { NotNullConstraintError } from "@/internal/errors.js";
import { createPool } from "@/utils/pg.js";
import { sql } from "drizzle-orm";
import { drizzle } from "drizzle-orm/node-postgres";
import { Client, Pool } from "pg";
import { beforeEach, expect, test, vi } from "vitest";
import { SCHEMATA } from "./index.js";
import { createQB } from "./queryBuilder.js";
beforeEach(setupCommon);
beforeEach(setupIsolatedDatabase);
test("QB query", async () => {
  if (context.databaseConfig.kind !== "postgres") return;
  const pool = createPool(
    context.databaseConfig.poolConfig,
    context.common.logger,
  );
  const qb = createQB(drizzle(pool, { casing: "snake_case" }), {
    common: context.common,
  });
  await qb.wrap((qb) =>
    qb.execute(sql`SELECT * FROM information_schema.schemata`),
  );
  await qb.wrap({ label: "test" }, (db) => db.select().from(SCHEMATA));
});
test("QB transaction", async () => {
  if (context.databaseConfig.kind !== "postgres") return;
  const pool = createPool(
    context.databaseConfig.poolConfig,
    context.common.logger,
  );
  const qb = createQB(drizzle(pool, { casing: "snake_case" }), {
    common: context.common,
  });
  await qb.transaction({ label: "test1" }, async (tx) => {
    await tx.wrap({ label: "test2" }, (db) => db.select().from(SCHEMATA));
  });
  await qb.transaction(async (tx) => {
    await tx.transaction({ label: "test3" }, async (tx) => {
      await tx.wrap({ label: "test4" }, (db) => db.select().from(SCHEMATA));
    });
  });
});
test("QB wrap retries error", async () => {
  if (context.databaseConfig.kind !== "postgres") return;
  const pool = createPool(
    context.databaseConfig.poolConfig,
    context.common.logger,
  );
  const qb = createQB(drizzle(pool, { casing: "snake_case" }), {
    common: context.common,
  });
  const querySpy = vi.spyOn(pool, "query");
  querySpy.mockRejectedValueOnce(
    new Error("Connection terminated unexpectedly"),
  );
  await qb.wrap({ label: "test1" }, (db) => db.select().from(SCHEMATA));
  expect(querySpy).toHaveBeenCalledTimes(2);
});
test("QB transaction retries error", async () => {
  if (context.databaseConfig.kind !== "postgres") return;
  const pool = createPool(
    context.databaseConfig.poolConfig,
    context.common.logger,
  );
  const connection = await pool.connect();
  const qb = createQB(drizzle(connection, { casing: "snake_case" }), {
    common: context.common,
  });
  const querySpy = vi.spyOn(connection, "query");
  querySpy.mockRejectedValueOnce(
    new Error("Connection terminated unexpectedly"),
  );
  let error = true;
  await qb.transaction({ label: "test1" }, async (tx) => {
    if (error) {
      error = false;
      querySpy.mockRejectedValueOnce(
        new Error("Connection terminated unexpectedly"),
      );
    }
    await tx.wrap({ label: "test2" }, (db) => db.select().from(SCHEMATA));
  });
  // BEGIN, BEGIN, SELECT, ROLLBACK, BEGIN, SELECT, COMMIT
  expect(querySpy).toHaveBeenCalledTimes(7);
  connection.release();
});
test("QB parses error", async () => {
  if (context.databaseConfig.kind !== "postgres") return;
  const pool = createPool(
    context.databaseConfig.poolConfig,
    context.common.logger,
  );
  const qb = createQB(drizzle(pool, { casing: "snake_case" }), {
    common: context.common,
  });
  const querySpy = vi.spyOn(pool, "query");
  querySpy.mockRejectedValueOnce(new Error("violates not-null constraint"));
  const error = await qb
    .wrap({ label: "test1" }, (db) => db.select().from(SCHEMATA))
    .catch((error) => error);
  expect(querySpy).toHaveBeenCalledTimes(1);
  expect(error).toBeInstanceOf(NotNullConstraintError);
});
test("QB client", async () => {
  if (context.databaseConfig.kind !== "postgres") return;
  const pool = createPool(
    context.databaseConfig.poolConfig,
    context.common.logger,
  );
  const qb = createQB(drizzle(pool, { casing: "snake_case" }), {
    common: context.common,
  });
  expect(qb.$dialect).toBe("postgres");
  expect(qb.$client).toBeInstanceOf(Pool);
  await qb.transaction(async (tx) => {
    expect(tx.$dialect).toBe("postgres");
    expect(tx.$client).toBeInstanceOf(Client);
  });
});
</file>

<file path="packages/core/src/database/queryBuilder.ts">
import crypto from "node:crypto";
import type { Common } from "@/internal/common.js";
import { BaseError } from "@/internal/errors.js";
import {
  BigIntSerializationError,
  CheckConstraintError,
  DbConnectionError,
  NonRetryableUserError,
  NotNullConstraintError,
  ShutdownError,
  UniqueConstraintError,
} from "@/internal/errors.js";
import type { Logger } from "@/internal/logger.js";
import type { Schema } from "@/internal/types.js";
import type { Drizzle } from "@/types/db.js";
import { startClock } from "@/utils/timer.js";
import { wait } from "@/utils/wait.js";
import { PGlite } from "@electric-sql/pglite";
import type { ExtractTablesWithRelations } from "drizzle-orm";
import type {
  PgDatabase,
  PgQueryResultHKT,
  PgTransaction,
  PgTransactionConfig,
} from "drizzle-orm/pg-core";
import type pg from "pg";
const RETRY_COUNT = 9;
const BASE_DURATION = 125;
type InnerQB<
  TSchema extends Schema = Schema,
  TClient extends PGlite | pg.Pool | pg.PoolClient =
    | PGlite
    | pg.Pool
    | pg.PoolClient,
> = Omit<Drizzle<TSchema>, "transaction"> & TransactionQB<TSchema, TClient>;
type TransactionQB<
  TSchema extends Schema = Schema,
  TClient extends PGlite | pg.Pool | pg.PoolClient =
    | PGlite
    | pg.Pool
    | pg.PoolClient,
> = {
  /**
   * Transaction with retries, logging, metrics, and error parsing.
   */
  transaction<T>(
    transaction: (tx: QB<TSchema, TClient>) => Promise<T>,
    config?: PgTransactionConfig,
    context?: { logger?: Logger },
  ): Promise<T>;
  transaction<T>(
    { label }: { label: string },
    transaction: (tx: QB<TSchema, TClient>) => Promise<T>,
    config?: PgTransactionConfig,
    context?: { logger?: Logger },
  ): Promise<T>;
};
/**
 * Query builder with built-in retry logic, logging, and metrics.
 */
export type QB<
  TSchema extends Schema = Schema,
  TClient extends PGlite | pg.Pool | pg.PoolClient =
    | PGlite
    | pg.Pool
    | pg.PoolClient,
> = TransactionQB<TSchema, TClient> & {
  raw: Drizzle<TSchema>;
  /**
   * Query with retries, logging, metrics, and error parsing.
   */
  wrap<T>(
    query: (db: InnerQB<TSchema, TClient>) => T,
    context?: { logger?: Logger },
  ): T;
  wrap<T>(
    { label }: { label: string },
    query: (db: InnerQB<TSchema, TClient>) => T,
    context?: { logger?: Logger },
  ): T;
} & (
    | { $dialect: "pglite"; $client: PGlite }
    | { $dialect: "postgres"; $client: pg.Pool | pg.PoolClient }
  );
export const parseDbError = (error: any): Error => {
  const stack = error.stack;
  if (error instanceof BaseError) {
    return error;
  }
  if (error?.message?.includes("violates not-null constraint")) {
    error = new NotNullConstraintError(error.message);
  } else if (error?.message?.includes("violates unique constraint")) {
    error = new UniqueConstraintError(error.message);
  } else if (error?.message?.includes("violates check constraint")) {
    error = new CheckConstraintError(error.message);
  } else if (
    // nodejs error message
    error?.message?.includes("Do not know how to serialize a BigInt") ||
    // bun error message
    error?.message?.includes("cannot serialize BigInt")
  ) {
    error = new BigIntSerializationError(error.message);
    error.meta.push(
      "Hint:\n  The JSON column type does not support BigInt values. Use the replaceBigInts() helper function before inserting into the database. Docs: https://ponder.sh/docs/api-reference/ponder-utils#replacebigints",
    );
  } else if (error?.message?.includes("does not exist")) {
    error = new NonRetryableUserError(error.message);
  } else if (error?.message?.includes("already exists")) {
    error = new NonRetryableUserError(error.message);
  } else if (
    error?.message?.includes(
      "terminating connection due to administrator command",
    ) ||
    error?.message?.includes("connection to client lost") ||
    error?.message?.includes("too many clients already") ||
    error?.message?.includes("Connection terminated unexpectedly") ||
    error?.message?.includes("ECONNRESET") ||
    error?.message?.includes("ETIMEDOUT") ||
    error?.message?.includes("timeout exceeded when trying to connect")
  ) {
    error = new DbConnectionError(error.message);
  }
  error.stack = stack;
  return error;
};
/**
 * Create a query builder.
 *
 * @example
 * ```ts
 * const qb = createQB(drizzle(pool), { casing: "snake_case", common });
 * const result1 = await qb.wrap((db) => db.select().from(accounts));
 * const result2 = await qb.wrap({ label: "label" }, (db) => db.select().from(accounts));
 * ```
 */
export const createQB = <
  TSchema extends Schema = { [name: string]: never },
  TClient extends PGlite | pg.Pool | pg.PoolClient =
    | PGlite
    | pg.Pool
    | pg.PoolClient,
>(
  db: Drizzle<TSchema> & { $client: TClient },
  { common, isAdmin }: { common: Common; isAdmin?: boolean },
): QB<TSchema, TClient> => {
  const dialect = db.$client instanceof PGlite ? "pglite" : "postgres";
  // Retry, logging, metrics, and error parsing wrapper
  const retryLogMetricErrorWrap = async <T>(
    fn: () => Promise<T>,
    {
      label,
      isTransaction,
      isTransactionStatement,
      logger,
    }: {
      label?: string;
      isTransaction: boolean;
      isTransactionStatement: boolean;
      logger: Logger;
    },
  ): Promise<T> => {
    // First error thrown is often the most useful
    let firstError: any;
    let hasError = false;
    for (let i = 0; i <= RETRY_COUNT; i++) {
      const endClock = startClock();
      const id = crypto.randomUUID().slice(0, 8);
      if (label) {
        logger.trace({
          msg: "Started database query",
          query: label,
          query_id: id,
        });
      }
      try {
        if (common.shutdown.isKilled && isAdmin === false) {
          throw new ShutdownError();
        }
        const result = await fn();
        if (label) {
          common.metrics.ponder_database_method_duration.observe(
            { method: label },
            endClock(),
          );
        }
        if (label) {
          logger.trace({
            msg: "Completed database query",
            query: label,
            query_id: id,
            duration: endClock(),
          });
        }
        return result;
      } catch (e) {
        const error = parseDbError(e);
        if (common.shutdown.isKilled) {
          throw new ShutdownError();
        }
        if (label) {
          common.metrics.ponder_database_method_duration.observe(
            { method: label },
            endClock(),
          );
          common.metrics.ponder_database_method_error_total.inc({
            method: label,
          });
        }
        if (!hasError) {
          hasError = true;
          firstError = error;
        }
        // Two types of transaction environments
        // 1. Inside callback (running user statements or control flow statements): Throw error, retry
        // later. We want the error bubbled up out of the callback, so the transaction is properly rolled back.
        // 2. Outside callback (running entire transaction, user statements + control flow statements): Retry immediately.
        if (isTransaction) {
          if (error instanceof NonRetryableUserError) {
            throw error;
          }
        } else if (isTransactionStatement) {
          // Transaction statements are not immediately retried, so the transaction will be properly rolled back.
          logger.warn({
            msg: "Failed database query",
            query: label,
            query_id: id,
            duration: endClock(),
            error,
          });
          throw error;
        } else if (error instanceof NonRetryableUserError) {
          logger.warn({
            msg: "Failed database query",
            query: label,
            query_id: id,
            duration: endClock(),
            error,
          });
          throw error;
        }
        if (i === RETRY_COUNT) {
          logger.warn({
            msg: "Failed database query",
            query: label,
            query_id: id,
            retry_count: i,
            duration: endClock(),
            error,
          });
          throw firstError;
        }
        const duration = BASE_DURATION * 2 ** i;
        logger.warn({
          msg: "Failed database query",
          query: label,
          query_id: id,
          retry_count: i,
          retry_delay: duration,
          duration: endClock(),
          error,
        });
        await wait(duration);
      }
    }
    throw "unreachable";
  };
  // Add QB methods to the transaction object
  const addQBMethods = (db: PgDatabase<PgQueryResultHKT, TSchema>) => {
    const _transaction = db.transaction.bind(db);
    // @ts-ignore
    db.transaction = async (...args) => {
      if (typeof args[0] === "function") {
        const [callback, config, transactionContext] = args as unknown as [
          (
            tx: PgTransaction<
              PgQueryResultHKT,
              TSchema,
              ExtractTablesWithRelations<TSchema>
            >,
          ) => Promise<unknown>,
          PgTransactionConfig | undefined,
          { logger?: Logger } | undefined,
        ];
        // Note: We want to retry errors from `callback` but include
        // the transaction control statements in `_transaction`.
        return retryLogMetricErrorWrap(
          () =>
            _transaction(async (tx) => {
              addQBMethods(tx);
              // @ts-expect-error
              tx.raw = tx;
              Object.assign(tx, { $dialect: dialect });
              // @ts-expect-error
              Object.assign(tx, { $client: tx.session.client });
              // Note: `tx.wrap` should not retry errors, because the transaction will be aborted
              // @ts-ignore
              (tx as unknown as QB<TSchema, TClient>).wrap = (...args) => {
                if (typeof args[0] === "function") {
                  const [query, context] = args as [
                    (db: InnerQB<TSchema, TClient>) => unknown,
                    { logger?: Logger } | undefined,
                  ];
                  return retryLogMetricErrorWrap(
                    async () =>
                      query(tx as unknown as InnerQB<TSchema, TClient>),
                    {
                      isTransaction: false,
                      isTransactionStatement: true,
                      logger:
                        context?.logger ??
                        transactionContext?.logger ??
                        common.logger,
                    },
                  );
                } else {
                  const [{ label }, query, context] = args as [
                    { label: string },
                    (db: InnerQB<TSchema, TClient>) => unknown,
                    { logger?: Logger } | undefined,
                  ];
                  return retryLogMetricErrorWrap(
                    async () =>
                      query(tx as unknown as InnerQB<TSchema, TClient>),
                    {
                      label,
                      isTransaction: false,
                      isTransactionStatement: true,
                      logger:
                        context?.logger ??
                        transactionContext?.logger ??
                        common.logger,
                    },
                  );
                }
              };
              const result = await callback(tx);
              return result;
            }, config),
          {
            isTransaction: true,
            isTransactionStatement: false,
            logger: transactionContext?.logger ?? common.logger,
          },
        );
      } else {
        const [{ label }, callback, config, transactionContext] =
          args as unknown as [
            { label: string },
            (
              tx: PgTransaction<
                PgQueryResultHKT,
                TSchema,
                ExtractTablesWithRelations<TSchema>
              >,
            ) => Promise<unknown>,
            PgTransactionConfig | undefined,
            { logger?: Logger } | undefined,
          ];
        // Note: We want to retry errors from `callback` but include
        // the transaction control statements in `_transaction`.
        return retryLogMetricErrorWrap(
          () =>
            _transaction(async (tx) => {
              addQBMethods(tx);
              // @ts-expect-error
              tx.raw = tx;
              Object.assign(tx, { $dialect: dialect });
              // @ts-expect-error
              Object.assign(tx, { $client: tx.session.client });
              // Note: `tx.wrap` should not retry errors, because the transaction will be aborted
              // @ts-ignore
              (tx as unknown as QB<TSchema, TClient>).wrap = (...args) => {
                if (typeof args[0] === "function") {
                  const [query, context] = args as [
                    (db: InnerQB<TSchema, TClient>) => unknown,
                    { logger?: Logger } | undefined,
                  ];
                  return retryLogMetricErrorWrap(
                    async () =>
                      query(tx as unknown as InnerQB<TSchema, TClient>),
                    {
                      label,
                      isTransaction: false,
                      isTransactionStatement: true,
                      logger:
                        context?.logger ??
                        transactionContext?.logger ??
                        common.logger,
                    },
                  );
                } else {
                  const [{ label }, query, context] = args as [
                    { label: string },
                    (db: InnerQB<TSchema, TClient>) => unknown,
                    { logger?: Logger } | undefined,
                  ];
                  return retryLogMetricErrorWrap(
                    async () =>
                      query(tx as unknown as InnerQB<TSchema, TClient>),
                    {
                      label,
                      isTransaction: false,
                      isTransactionStatement: true,
                      logger:
                        context?.logger ??
                        transactionContext?.logger ??
                        common.logger,
                    },
                  );
                }
              };
              const result = await callback(tx);
              return result;
            }, config),
          {
            label,
            isTransaction: true,
            isTransactionStatement: false,
            logger: transactionContext?.logger ?? common.logger,
          },
        );
      }
    };
  };
  if (dialect === "postgres") {
    addQBMethods(db);
  } else {
    // @ts-ignore
    db.transaction = async (...args) => {
      if (typeof args[0] === "function") {
        const [callback, context] = args as [
          (
            tx: PgTransaction<
              PgQueryResultHKT,
              TSchema,
              ExtractTablesWithRelations<TSchema>
            >,
          ) => Promise<unknown>,
          { logger?: Logger } | undefined,
        ];
        // @ts-expect-error
        return retryLogMetricErrorWrap(() => callback(db), {
          isTransactionStatement: true,
          logger: context?.logger ?? common.logger,
        });
      } else {
        const [{ label }, callback, context] = args as [
          { label: string },
          (
            tx: PgTransaction<
              PgQueryResultHKT,
              TSchema,
              ExtractTablesWithRelations<TSchema>
            >,
          ) => Promise<unknown>,
          { logger?: Logger } | undefined,
        ];
        // @ts-expect-error
        return retryLogMetricErrorWrap(() => callback(db), {
          label,
          isTransactionStatement: true,
          logger: context?.logger ?? common.logger,
        });
      }
    };
  }
  const qb = db as unknown as QB<TSchema, TClient>;
  qb.raw = db;
  qb.$dialect = dialect;
  qb.$client = db.$client;
  qb.wrap = async (...args) => {
    if (typeof args[0] === "function") {
      const [query, context] = args;
      // @ts-expect-error
      return retryLogMetricErrorWrap(() => query(qb), {
        isTransactionStatement: false,
        // @ts-expect-error
        logger: context?.logger ?? common.logger,
      });
    } else {
      const [{ label }, query, context] = args;
      // @ts-expect-error
      return retryLogMetricErrorWrap(() => query(qb), {
        isTransactionStatement: false,
        label,
        // @ts-expect-error
        logger: context?.logger ?? common.logger,
      });
    }
  };
  return qb;
};
</file>

<file path="packages/core/src/drizzle/kit/index.ts">
import { SQL, is, sql } from "drizzle-orm";
import { CasingCache, toCamelCase, toSnakeCase } from "drizzle-orm/casing";
import {
  type AnyPgTable,
  PgColumn,
  PgDialect,
  type PgEnum,
  PgEnumColumn,
  PgMaterializedView,
  PgSchema,
  type PgSequence,
  PgTable,
  PgView,
  getTableConfig,
  getViewConfig,
  index,
  integer,
  isPgEnum,
  isPgSequence,
  pgSchema,
  pgTable,
  varchar,
} from "drizzle-orm/pg-core";
import { getReorgSequenceName, getReorgTableName } from "../onchain.js";
type Dialect = "postgresql";
type CasingType = "snake_case" | "camelCase";
export type SqlStatements = {
  tables: {
    sql: string[];
    json: JsonCreateTableStatement[];
  };
  views: {
    sql: string[];
    json: JsonCreatePgViewStatement[];
  };
  enums: {
    sql: string[];
    json: JsonCreateEnumStatement[];
  };
  indexes: { sql: string[]; json: JsonPgCreateIndexStatement[] };
};
export const getReorgTable = (table: PgTable) => {
  const schema = getTableConfig(table).schema;
  if (schema && schema !== "public") {
    return pgSchema(schema).table(
      getReorgTableName(table),
      {
        operation_id: integer()
          .notNull()
          .primaryKey()
          .default(
            sql.raw(`nextval('"${schema}"."${getReorgSequenceName()}"')`),
          ),
        operation: integer().notNull().$type<0 | 1 | 2>(),
        checkpoint: varchar({ length: 75 }).notNull(),
      },
      (table) => [index().on(table.checkpoint)],
    );
  }
  return pgTable(
    getReorgTableName(table),
    {
      operation_id: integer()
        .notNull()
        .primaryKey()
        .default(sql.raw(`nextval('"${getReorgSequenceName()}"')`)),
      operation: integer().notNull().$type<0 | 1 | 2>(),
      checkpoint: varchar({ length: 75 }).notNull(),
    },
    (table) => [index().on(table.checkpoint)],
  );
};
export const getSql = (schema: { [name: string]: unknown }): SqlStatements => {
  const { tables, views, enums, schemas } = prepareFromExports(schema);
  const json = generatePgSnapshot(tables, views, enums, schemas, "snake_case");
  const squashed = squashPgScheme(json);
  const jsonCreateIndexesForCreatedTables = Object.values(
    squashed.tables,
  ).flatMap((it) => {
    // @ts-ignore
    return preparePgCreateIndexesJson(it.name, it.schema, it.indexes);
  });
  const jsonCreateEnums =
    Object.values(squashed.enums).map((it) => {
      // @ts-ignore
      return prepareCreateEnumJson(it.name, it.schema, it.values);
    }) ?? [];
  const jsonCreateTables = Object.values(squashed.tables).map((it: any) => {
    return preparePgCreateTableJson(it, json);
  });
  const jsonCreateViews = Object.values(squashed.views).map((it: any) => {
    return preparePgCreateViewJson(
      it.name,
      it.schema,
      it.definition!,
      it.materialized,
      it.withNoData,
      it.with,
      it.using,
      it.tablespace,
    );
  });
  const fromJson = (statements: any[]) =>
    statements
      .flatMap((statement) => {
        const filtered = convertors.filter((it) => {
          return it.can(statement, "postgresql");
        });
        const convertor = filtered.length === 1 ? filtered[0] : undefined;
        if (!convertor) {
          return "";
        }
        return convertor.convert(statement);
      })
      .filter((it) => it !== "");
  const combinedTables = jsonCreateTables.flatMap((statement) => [
    statement,
    createReorgTableStatement(statement),
  ]);
  return {
    tables: {
      sql: fromJson(combinedTables),
      json: combinedTables,
    },
    views: { sql: fromJson(jsonCreateViews), json: jsonCreateViews },
    enums: { sql: fromJson(jsonCreateEnums), json: jsonCreateEnums },
    indexes: {
      sql: fromJson(jsonCreateIndexesForCreatedTables),
      json: jsonCreateIndexesForCreatedTables,
    },
  };
};
const createReorgTableStatement = (statement: JsonCreateTableStatement) => {
  const reorgStatement: JsonCreateTableStatement = structuredClone(statement);
  reorgStatement.compositePkName = undefined;
  reorgStatement.compositePKs = [];
  for (const column of reorgStatement.columns) {
    column.primaryKey = false;
  }
  const reorgColumns = Object.values(
    squashPgScheme(
      generatePgSnapshot(
        [
          pgTable("", {
            operation_id: integer()
              .notNull()
              .primaryKey()
              .default(
                sql.raw(
                  `nextval('"${statement.schema === "" ? "public" : statement.schema}"."${getReorgSequenceName()}"')`,
                ),
              ),
            operation: integer().notNull(),
            checkpoint: varchar({ length: 75 }).notNull(),
          }),
        ],
        [],
        [],
        [],
        "snake_case",
      ),
    ).tables,
    //@ts-ignore
  )[0]!.columns;
  reorgStatement.columns.push(...Object.values(reorgColumns));
  reorgStatement.tableName = getReorgTableName(reorgStatement.tableName);
  return reorgStatement;
};
////////
// Serializer
////////
type Index = any;
type Column = any;
type PgSchemaSquashed = any;
type Table = any;
type Enum = any;
type PrimaryKey = any;
type IndexedColumn = any;
type IndexColumnType = any;
const PgSquasher = {
  squashIdx: (idx: Index) => {
    return `${idx.name};${idx.columns
      .map(
        (c: {
          expression: any;
          isExpression: any;
          asc: any;
          nulls: any;
          opclass: any;
        }) =>
          `${c.expression}--${c.isExpression}--${c.asc}--${c.nulls}--${
            c.opclass && ""
          }`,
      )
      .join(
        ",,",
      )};${idx.isUnique};${idx.concurrently};${idx.method};${idx.where};${JSON.stringify(idx.with)}`;
  },
  unsquashIdx: (input: string): Index => {
    const [
      name,
      columnsString,
      isUnique,
      concurrently,
      method,
      where,
      idxWith,
    ] = input.split(";");
    const columnString = columnsString!.split(",,");
    const columns: IndexColumnType[] = [];
    for (const column of columnString) {
      const [expression, isExpression, asc, nulls, opclass] =
        column.split("--");
      columns.push({
        nulls: nulls as IndexColumnType["nulls"],
        isExpression: isExpression === "true",
        asc: asc === "true",
        expression: expression,
        opclass: opclass === "undefined" ? undefined : opclass,
      });
    }
    return {
      name,
      columns,
      isUnique: isUnique === "true",
      concurrently: concurrently === "true",
      method,
      where: where === "undefined" ? undefined : where,
      with:
        !idxWith || idxWith === "undefined" ? undefined : JSON.parse(idxWith),
    };
  },
  squashPK: (pk: PrimaryKey) => {
    return `${pk.columns.join(",")};${pk.name}`;
  },
  unsquashPK: (pk: string): PrimaryKey => {
    const splitted = pk.split(";");
    return { name: splitted[1], columns: splitted[0]!.split(",") };
  },
};
////////
// JSON
////////
interface JsonCreateTableStatement {
  type: "create_table";
  tableName: string;
  schema: string;
  columns: Column[];
  compositePKs: string[];
  compositePkName?: string;
  uniqueConstraints?: string[];
  checkConstraints?: string[];
}
type JsonCreatePgViewStatement = {
  type: "create_view";
  name: string;
  schema: string;
  definition?: string;
  materialized: boolean;
  with: any;
  withNoData?: boolean;
  using?: string;
  tablespace?: string;
};
interface JsonCreateEnumStatement {
  type: "create_type_enum";
  name: string;
  schema: string;
  values: string[];
}
interface JsonCreateIndexStatement {
  type: "create_index";
  tableName: string;
  data: string;
  schema: string;
}
interface JsonPgCreateIndexStatement {
  type: "create_index_pg";
  tableName: string;
  data: Index;
  schema: string;
}
interface JsonCreateUniqueConstraint {
  type: "create_unique_constraint";
  tableName: string;
  data: string;
  schema?: string;
  constraintName?: string;
}
interface JsonCreateCheckConstraint {
  type: "create_check_constraint";
  tableName: string;
  data: string;
  schema?: string;
}
interface JsonCreateCompositePK {
  type: "create_composite_pk";
  tableName: string;
  data: string;
  schema?: string;
  constraintName?: string;
}
interface JsonCreateSchema {
  type: "create_schema";
  name: string;
}
interface JsonCreateReferenceStatement {
  type: "create_reference";
  data: string;
  schema: string;
  tableName: string;
  isMulticolumn?: boolean;
  columnNotNull?: boolean;
  columnDefault?: string;
  columnType?: string;
}
type JsonStatement =
  | JsonCreateTableStatement
  | JsonCreatePgViewStatement
  | JsonCreateEnumStatement
  | JsonCreateIndexStatement
  | JsonPgCreateIndexStatement
  | JsonCreateReferenceStatement
  | JsonCreateCompositePK
  | JsonCreateUniqueConstraint
  | JsonCreateSchema
  | JsonCreateCheckConstraint;
////////
// Generator
////////
const parseType = (schemaPrefix: string, type: string) => {
  const pgNativeTypes = [
    "uuid",
    "smallint",
    "integer",
    "bigint",
    "boolean",
    "text",
    "varchar",
    "serial",
    "bigserial",
    "decimal",
    "numeric",
    "real",
    "json",
    "jsonb",
    "time",
    "time with time zone",
    "time without time zone",
    "time",
    "timestamp",
    "timestamp with time zone",
    "timestamp without time zone",
    "date",
    "interval",
    "bigint",
    "bigserial",
    "double precision",
    "interval year",
    "interval month",
    "interval day",
    "interval hour",
    "interval minute",
    "interval second",
    "interval year to month",
    "interval day to hour",
    "interval day to minute",
    "interval day to second",
    "interval hour to minute",
    "interval hour to second",
    "interval minute to second",
  ];
  const arrayDefinitionRegex = /\[\d*(?:\[\d*\])*\]/g;
  const arrayDefinition = (type.match(arrayDefinitionRegex) ?? []).join("");
  const withoutArrayDefinition = type.replace(arrayDefinitionRegex, "");
  return pgNativeTypes.some((it) => type.startsWith(it))
    ? `${withoutArrayDefinition}${arrayDefinition}`
    : `${schemaPrefix}"${withoutArrayDefinition}"${arrayDefinition}`;
};
abstract class Convertor {
  abstract can(statement: JsonStatement, dialect: Dialect): boolean;
  abstract convert(
    statement: JsonStatement,
    action?: "push",
  ): string | string[];
}
class PgCreateTableConvertor extends Convertor {
  can(statement: JsonStatement, dialect: Dialect): boolean {
    return statement.type === "create_table" && dialect === "postgresql";
  }
  convert(st: JsonCreateTableStatement) {
    const { tableName, schema, columns, compositePKs } = st;
    let statement = "";
    const name = schema ? `"${schema}"."${tableName}"` : `"${tableName}"`;
    statement += `CREATE TABLE ${name} (\n`;
    for (let i = 0; i < columns.length; i++) {
      const column = columns[i];
      const primaryKeyStatement = column.primaryKey ? " PRIMARY KEY" : "";
      const notNullStatement =
        column.notNull && !column.identity ? " NOT NULL" : "";
      const defaultStatement =
        column.default !== undefined ? ` DEFAULT ${column.default}` : "";
      // const uniqueConstraint = column.isUnique
      //   ? ` CONSTRAINT "${column.uniqueName}" UNIQUE${column.nullsNotDistinct ? " NULLS NOT DISTINCT" : ""}`
      //   : "";
      const schemaPrefix =
        column.typeSchema && column.typeSchema !== "public"
          ? `"${column.typeSchema}".`
          : "";
      const type = parseType(schemaPrefix, column.type);
      // const generated = column.generated;
      // const generatedStatement = generated
      //   ? ` GENERATED ALWAYS AS (${generated?.as}) STORED`
      //   : "";
      // const unsquashedIdentity = column.identity
      //   ? PgSquasher.unsquashIdentity(column.identity)
      //   : undefined;
      // const identityWithSchema = schema
      //   ? `"${schema}"."${unsquashedIdentity?.name}"`
      //   : `"${unsquashedIdentity?.name}"`;
      // const identity = unsquashedIdentity
      //   ? ` GENERATED ${
      //       unsquashedIdentity.type === "always" ? "ALWAYS" : "BY DEFAULT"
      //     } AS IDENTITY (sequence name ${identityWithSchema}${
      //       unsquashedIdentity.increment
      //         ? ` INCREMENT BY ${unsquashedIdentity.increment}`
      //         : ""
      //     }${
      //       unsquashedIdentity.minValue
      //         ? ` MINVALUE ${unsquashedIdentity.minValue}`
      //         : ""
      //     }${
      //       unsquashedIdentity.maxValue
      //         ? ` MAXVALUE ${unsquashedIdentity.maxValue}`
      //         : ""
      //     }${
      //       unsquashedIdentity.startWith
      //         ? ` START WITH ${unsquashedIdentity.startWith}`
      //         : ""
      //     }${unsquashedIdentity.cache ? ` CACHE ${unsquashedIdentity.cache}` : ""}${
      //       unsquashedIdentity.cycle ? " CYCLE" : ""
      //     })`
      //   : "";
      statement += `\t"${column.name}" ${type}${primaryKeyStatement}${defaultStatement}${notNullStatement}`;
      statement += i === columns.length - 1 ? "" : ",\n";
    }
    if (typeof compositePKs !== "undefined" && compositePKs.length > 0) {
      statement += ",\n";
      const compositePK = PgSquasher.unsquashPK(compositePKs[0]!);
      statement += `\tCONSTRAINT "${st.compositePkName}" PRIMARY KEY(\"${compositePK.columns.join(`","`)}\")`;
      // statement += `\n`;
    }
    // if (
    //   typeof uniqueConstraints !== "undefined" &&
    //   uniqueConstraints.length > 0
    // ) {
    //   for (const uniqueConstraint of uniqueConstraints) {
    //     statement += ",\n";
    //     const unsquashedUnique = PgSquasher.unsquashUnique(uniqueConstraint);
    //     statement += `\tCONSTRAINT "${unsquashedUnique.name}" UNIQUE${
    //       unsquashedUnique.nullsNotDistinct ? " NULLS NOT DISTINCT" : ""
    //     }(\"${unsquashedUnique.columns.join(`","`)}\")`;
    //     // statement += `\n`;
    //   }
    // }
    // if (
    //   typeof checkConstraints !== "undefined" &&
    //   checkConstraints.length > 0
    // ) {
    //   for (const checkConstraint of checkConstraints) {
    //     statement += ",\n";
    //     const unsquashedCheck = PgSquasher.unsquashCheck(checkConstraint);
    //     statement += `\tCONSTRAINT "${unsquashedCheck.name}" CHECK (${unsquashedCheck.value})`;
    //   }
    // }
    statement += "\n);";
    statement += "\n";
    return statement;
  }
}
class PgCreateViewConvertor extends Convertor {
  can(statement: JsonStatement, dialect: Dialect): boolean {
    return statement.type === "create_view" && dialect === "postgresql";
  }
  convert(st: JsonCreatePgViewStatement) {
    const {
      definition,
      name: viewName,
      schema,
      with: withOption,
      materialized,
      withNoData,
      tablespace,
      using,
    } = st;
    const name = schema ? `"${schema}"."${viewName}"` : `"${viewName}"`;
    let statement = materialized
      ? `CREATE MATERIALIZED VIEW ${name}`
      : `CREATE VIEW ${name}`;
    if (using) statement += ` USING "${using}"`;
    const options: string[] = [];
    if (withOption) {
      statement += " WITH (";
      Object.entries(withOption).forEach(([key, value]) => {
        if (typeof value === "undefined") return;
        options.push(`${toSnakeCase(key)} = ${value}`);
      });
      statement += options.join(", ");
      statement += ")";
    }
    if (tablespace) statement += ` TABLESPACE ${tablespace}`;
    statement += ` AS (${definition})`;
    if (withNoData) statement += " WITH NO DATA";
    statement += ";";
    return statement;
  }
}
class CreateTypeEnumConvertor extends Convertor {
  can(statement: JsonStatement): boolean {
    return statement.type === "create_type_enum";
  }
  convert(st: JsonCreateEnumStatement) {
    const { name, values, schema } = st;
    const enumNameWithSchema = schema ? `"${schema}"."${name}"` : `"${name}"`;
    let valuesStatement = "(";
    valuesStatement += values.map((it) => `'${it}'`).join(", ");
    valuesStatement += ")";
    // TODO do we need this?
    // let statement = 'DO $$ BEGIN';
    // statement += '\n';
    const statement = `CREATE TYPE ${enumNameWithSchema} AS ENUM${valuesStatement};`;
    // statement += '\n';
    // statement += 'EXCEPTION';
    // statement += '\n';
    // statement += ' WHEN duplicate_object THEN null;';
    // statement += '\n';
    // statement += 'END $$;';
    // statement += '\n';
    return statement;
  }
}
class CreatePgIndexConvertor extends Convertor {
  can(statement: JsonStatement, dialect: Dialect): boolean {
    return statement.type === "create_index_pg" && dialect === "postgresql";
  }
  convert(statement: JsonPgCreateIndexStatement): string {
    const {
      name,
      columns,
      isUnique,
      concurrently,
      with: withMap,
      method,
      where,
    } = statement.data;
    // // since postgresql 9.5
    const indexPart = isUnique ? "UNIQUE INDEX" : "INDEX";
    const value = columns
      .map(
        (it: {
          isExpression: any;
          expression: any;
          opclass: any;
          asc: any;
          nulls: string;
        }) =>
          `${it.isExpression ? it.expression : `"${it.expression}"`}${
            it.opclass ? ` ${it.opclass}` : it.asc ? "" : " DESC"
          }${
            (it.asc && it.nulls && it.nulls === "last") || it.opclass
              ? ""
              : ` NULLS ${it.nulls!.toUpperCase()}`
          }`,
      )
      .join(",");
    const tableNameWithSchema = statement.schema
      ? `"${statement.schema}"."${statement.tableName}"`
      : `"${statement.tableName}"`;
    function reverseLogic(mappedWith: Record<string, string>): string {
      let reversedString = "";
      for (const key in mappedWith) {
        // biome-ignore lint/suspicious/noPrototypeBuiltins: <explanation>
        if (mappedWith.hasOwnProperty(key)) {
          reversedString += `${key}=${mappedWith[key]},`;
        }
      }
      reversedString = reversedString.slice(0, -1);
      return reversedString;
    }
    return `CREATE ${indexPart}${
      concurrently ? " CONCURRENTLY" : ""
    } IF NOT EXISTS "${name}" ON ${tableNameWithSchema} USING ${method} (${value})${
      Object.keys(withMap!).length !== 0
        ? ` WITH (${reverseLogic(withMap!)})`
        : ""
    }${where ? ` WHERE ${where}` : ""};`;
  }
}
class PgCreateSchemaConvertor extends Convertor {
  can(statement: JsonStatement, dialect: Dialect): boolean {
    return statement.type === "create_schema" && dialect === "postgresql";
  }
  convert(statement: JsonCreateSchema) {
    const { name } = statement;
    return `CREATE SCHEMA IF NOT EXISTS"${name}";\n`;
  }
}
const convertors: Convertor[] = [];
convertors.push(new PgCreateTableConvertor());
convertors.push(new PgCreateViewConvertor());
convertors.push(new CreateTypeEnumConvertor());
convertors.push(new CreatePgIndexConvertor());
convertors.push(new PgCreateSchemaConvertor());
const preparePgCreateTableJson = (
  table: Table,
  json: PgSchemaSquashed,
): JsonCreateTableStatement => {
  const { name, schema, columns, compositePrimaryKeys } = table;
  const tableKey = `${schema || "public"}.${name}`;
  // TODO: @AndriiSherman. We need this, will add test cases
  const compositePkName =
    Object.values(compositePrimaryKeys).length > 0
      ? json.tables[tableKey].compositePrimaryKeys[
          `${PgSquasher.unsquashPK(Object.values(compositePrimaryKeys)[0]! as string).name}`
        ].name
      : "";
  return {
    type: "create_table",
    tableName: name,
    schema,
    columns: Object.values(columns),
    compositePKs: Object.values(compositePrimaryKeys),
    compositePkName: compositePkName,
  };
};
const preparePgCreateViewJson = (
  name: string,
  schema: string,
  definition: string,
  materialized: boolean,
  withNoData = false,
  withOption?: any,
  using?: string,
  tablespace?: string,
): JsonCreatePgViewStatement => {
  return {
    type: "create_view",
    name: name,
    schema: schema,
    definition: definition,
    with: withOption,
    materialized: materialized,
    withNoData,
    using,
    tablespace,
  };
};
const preparePgCreateIndexesJson = (
  tableName: string,
  schema: string,
  indexes: Record<string, string>,
): JsonPgCreateIndexStatement[] => {
  return Object.values(indexes).map((indexData) => {
    return {
      type: "create_index_pg",
      tableName,
      data: PgSquasher.unsquashIdx(indexData),
      schema,
    };
  });
};
const prepareCreateEnumJson = (
  name: string,
  schema: string,
  values: string[],
): JsonCreateEnumStatement => {
  return {
    type: "create_type_enum",
    name: name,
    schema: schema,
    values,
  };
};
const prepareFromExports = (exports: Record<string, unknown>) => {
  const tables: AnyPgTable[] = [];
  const enums: PgEnum<any>[] = [];
  const schemas: PgSchema[] = [];
  const sequences: PgSequence[] = [];
  const views: PgView[] = [];
  const matViews: PgMaterializedView[] = [];
  const i0values = Object.values(exports);
  i0values.forEach((t) => {
    if (isPgEnum(t)) {
      enums.push(t);
      return;
    }
    if (is(t, PgTable)) {
      tables.push(t);
    }
    if (is(t, PgSchema)) {
      schemas.push(t);
    }
    if (is(t, PgView)) {
      views.push(t);
    }
    if (is(t, PgMaterializedView)) {
      matViews.push(t);
    }
    if (isPgSequence(t)) {
      sequences.push(t);
    }
  });
  return { tables, enums, schemas, sequences, views, matViews };
};
export function getColumnCasing(
  column: { keyAsName: boolean; name: string | undefined },
  casing: CasingType | undefined,
) {
  if (!column.name) return "";
  return !column.keyAsName || casing === undefined
    ? column.name
    : casing === "camelCase"
      ? toCamelCase(column.name)
      : toSnakeCase(column.name);
}
const sqlToStr = (sql: SQL, casing: CasingType | undefined) => {
  return sql.toQuery({
    escapeName: () => {
      throw new Error("we don't support params for `sql` default values");
    },
    escapeParam: () => {
      throw new Error("we don't support params for `sql` default values");
    },
    escapeString: () => {
      throw new Error("we don't support params for `sql` default values");
    },
    casing: new CasingCache(casing),
  }).sql;
};
function isPgArrayType(sqlType: string) {
  return sqlType.match(/.*\[\d*\].*|.*\[\].*/g) !== null;
}
function buildArrayString(array: readonly any[], sqlType: string): string {
  sqlType = sqlType.split("[")[0]!;
  const values = array
    .map((value) => {
      if (typeof value === "number" || typeof value === "bigint") {
        return value.toString();
      } else if (typeof value === "boolean") {
        return value ? "true" : "false";
      } else if (Array.isArray(value)) {
        return buildArrayString(value, sqlType);
      } else if (value instanceof Date) {
        if (sqlType === "date") {
          return `"${value.toISOString().split("T")[0]}"`;
        } else if (sqlType === "timestamp") {
          return `"${value.toISOString().replace("T", " ").slice(0, 23)}"`;
        } else {
          return `"${value.toISOString()}"`;
        }
      } else if (typeof value === "object") {
        return `"${JSON.stringify(value).replaceAll('"', '\\"')}"`;
      }
      return `"${value}"`;
    })
    .join(",");
  return `{${values}}`;
}
const indexName = (tableName: string, columns: string[]) => {
  return `${tableName}_${columns.join("_")}_index`;
};
const generatePgSnapshot = (
  tables: AnyPgTable[],
  views: PgView[],
  enums: PgEnum<any>[],
  schemas: PgSchema[],
  casing: CasingType | undefined,
) => {
  const dialect = new PgDialect({ casing });
  const result: Record<string, Table> = {};
  const resultViews: Record<string, any> = {};
  // This object stores unique names for indexes and will be used to detect if you have the same names for indexes
  // within the same PostgreSQL schema
  const indexesInSchema: Record<string, string[]> = {};
  for (const table of tables) {
    const {
      name: tableName,
      columns,
      indexes,
      schema,
      primaryKeys,
    } = getTableConfig(table);
    const columnsObject: Record<string, Column> = {};
    const indexesObject: Record<string, Index> = {};
    const primaryKeysObject: Record<string, PrimaryKey> = {};
    columns.forEach((column) => {
      const name = getColumnCasing(column, casing);
      const notNull: boolean = column.notNull;
      const primaryKey: boolean = column.primary;
      const sqlTypeLowered = column.getSQLType().toLowerCase();
      const typeSchema = is(column, PgEnumColumn)
        ? column.enum.schema || "public"
        : undefined;
      const columnToSet: Column = {
        name,
        type: column.getSQLType(),
        typeSchema: typeSchema,
        primaryKey,
        notNull,
      };
      if (column.default !== undefined) {
        if (is(column.default, SQL)) {
          columnToSet.default = sqlToStr(column.default, casing);
        } else {
          if (typeof column.default === "string") {
            columnToSet.default = `'${column.default}'`;
          } else {
            if (sqlTypeLowered === "jsonb" || sqlTypeLowered === "json") {
              columnToSet.default = `'${JSON.stringify(column.default)}'::${sqlTypeLowered}`;
            } else if (column.default instanceof Date) {
              if (sqlTypeLowered === "date") {
                columnToSet.default = `'${column.default.toISOString().split("T")[0]}'`;
              } else if (sqlTypeLowered === "timestamp") {
                columnToSet.default = `'${column.default.toISOString().replace("T", " ").slice(0, 23)}'`;
              } else {
                columnToSet.default = `'${column.default.toISOString()}'`;
              }
            } else if (
              isPgArrayType(sqlTypeLowered) &&
              Array.isArray(column.default)
            ) {
              columnToSet.default = `'${buildArrayString(column.default, sqlTypeLowered)}'`;
            } else {
              // Should do for all types
              // columnToSet.default = `'${column.default}'::${sqlTypeLowered}`;
              columnToSet.default = column.default;
            }
          }
        }
      }
      columnsObject[name] = columnToSet;
    });
    primaryKeys.map((pk) => {
      const originalColumnNames = pk.columns.map((c) => c.name);
      const columnNames = pk.columns.map((c) => getColumnCasing(c, casing));
      let name = pk.getName();
      if (casing !== undefined) {
        for (let i = 0; i < originalColumnNames.length; i++) {
          name = name.replace(originalColumnNames[i]!, columnNames[i]!);
        }
      }
      primaryKeysObject[name] = {
        name,
        columns: columnNames,
      };
    });
    indexes.forEach((value) => {
      const columns = value.config.columns;
      const indexColumnNames: string[] = [];
      columns.forEach((it) => {
        const name = getColumnCasing(it as IndexedColumn, casing);
        indexColumnNames.push(name);
      });
      const name = value.config.name
        ? value.config.name
        : indexName(tableName, indexColumnNames);
      const indexColumns: IndexColumnType[] = columns.map(
        (it): IndexColumnType => {
          if (is(it, SQL)) {
            return {
              expression: dialect.sqlToQuery(it, "indexes").sql,
              asc: true,
              isExpression: true,
              nulls: "last",
            };
          } else {
            it = it as IndexedColumn;
            return {
              expression: getColumnCasing(it as IndexedColumn, casing),
              isExpression: false,
              // @ts-ignore
              asc: it.indexConfig?.order === "asc",
              // @ts-ignore
              nulls: it.indexConfig?.nulls
                ? // @ts-ignore
                  it.indexConfig?.nulls
                : // @ts-ignore
                  it.indexConfig?.order === "desc"
                  ? "first"
                  : "last",
              // @ts-ignore
              opclass: it.indexConfig?.opClass,
            };
          }
        },
      );
      // check for index names duplicates
      if (typeof indexesInSchema[schema ?? "public"] !== "undefined") {
        indexesInSchema[schema ?? "public"]!.push(name);
      } else {
        indexesInSchema[schema ?? "public"] = [name];
      }
      indexesObject[name] = {
        name,
        columns: indexColumns,
        isUnique: value.config.unique ?? false,
        where: value.config.where
          ? dialect.sqlToQuery(value.config.where).sql
          : undefined,
        concurrently: value.config.concurrently ?? false,
        method: value.config.method ?? "btree",
        with: value.config.with ?? {},
      };
    });
    const tableKey = `${schema ?? "public"}.${tableName}`;
    result[tableKey] = {
      name: tableName,
      schema: schema ?? "",
      columns: columnsObject,
      indexes: indexesObject,
      compositePrimaryKeys: primaryKeysObject,
    };
  }
  const combinedViews = [...views];
  for (const view of combinedViews) {
    // @ts-ignore
    // biome-ignore lint/suspicious/noImplicitAnyLet: <explanation>
    // biome-ignore lint/style/useConst: <explanation>
    let viewName;
    // @ts-ignore
    // biome-ignore lint/suspicious/noImplicitAnyLet: <explanation>
    // biome-ignore lint/style/useConst: <explanation>
    let schema;
    // @ts-ignore
    // biome-ignore lint/suspicious/noImplicitAnyLet: <explanation>
    // biome-ignore lint/style/useConst: <explanation>
    let query;
    // @ts-ignore
    // biome-ignore lint/suspicious/noImplicitAnyLet: <explanation>
    // biome-ignore lint/style/useConst: <explanation>
    let selectedFields;
    // @ts-ignore
    // biome-ignore lint/suspicious/noImplicitAnyLet: <explanation>
    // biome-ignore lint/style/useConst: <explanation>
    let isExisting;
    // @ts-ignore
    // biome-ignore lint/suspicious/noImplicitAnyLet: <explanation>
    // biome-ignore lint/style/useConst: <explanation>
    let withOption;
    // @ts-ignore
    // biome-ignore lint/suspicious/noImplicitAnyLet: <explanation>
    // biome-ignore lint/style/useConst: <explanation>
    let tablespace;
    // @ts-ignore
    // biome-ignore lint/suspicious/noImplicitAnyLet: <explanation>
    // biome-ignore lint/style/useConst: <explanation>
    let using;
    // @ts-ignore
    // biome-ignore lint/suspicious/noImplicitAnyLet: <explanation>
    // biome-ignore lint/style/useConst: <explanation>
    let withNoData;
    const materialized: boolean = false;
    // if (is(view, PgView)) {
    ({
      name: viewName,
      schema,
      query,
      selectedFields,
      isExisting,
      with: withOption,
    } = getViewConfig(view));
    // } else {
    // 	({ name: viewName, schema, query, selectedFields, isExisting, with: withOption, tablespace, using, withNoData } =
    // 		getMaterializedViewConfig(view));
    // 	materialized = true;
    // }
    const viewSchema = schema ?? "public";
    const viewKey = `${viewSchema}.${viewName}`;
    const columnsObject: Record<string, Column> = {};
    // const uniqueConstraintObject: Record<string, UniqueConstraint> = {};
    // const existingView = resultViews[viewKey];
    // if (typeof existingView !== 'undefined') {
    // 	console.log(
    // 		`\n${
    // 			withStyle.errorWarning(
    // 				`We\'ve found duplicated view name across ${
    // 					chalk.underline.blue(schema ?? 'public')
    // 				} schema. Please rename your view`,
    // 			)
    // 		}`,
    // 	);
    // 	process.exit(1);
    // }
    for (const key in selectedFields) {
      if (is(selectedFields[key], PgColumn)) {
        const column = selectedFields[key] as PgColumn;
        const notNull: boolean = column.notNull;
        const primaryKey: boolean = column.primary;
        const sqlTypeLowered = column.getSQLType().toLowerCase();
        const typeSchema = is(column, PgEnumColumn)
          ? column.enum.schema || "public"
          : undefined;
        // const generated = column.generated;
        // const identity = column.generatedIdentity;
        // const increment = stringFromIdentityProperty(identity?.sequenceOptions?.increment) ?? '1';
        // const minValue = stringFromIdentityProperty(identity?.sequenceOptions?.minValue)
        // 	?? (Number.parseFloat(increment) < 0 ? minRangeForIdentityBasedOn(column.columnType) : '1');
        // const maxValue = stringFromIdentityProperty(identity?.sequenceOptions?.maxValue)
        // 	?? (Number.parseFloat(increment) < 0 ? '-1' : maxRangeForIdentityBasedOn(column.getSQLType()));
        // const startWith = stringFromIdentityProperty(identity?.sequenceOptions?.startWith)
        // 	?? (Number.parseFloat(increment) < 0 ? maxValue : minValue);
        // const cache = stringFromIdentityProperty(identity?.sequenceOptions?.cache) ?? '1';
        const columnToSet: Column = {
          name: column.name,
          type: column.getSQLType(),
          typeSchema: typeSchema,
          primaryKey,
          notNull,
          // generated: generated
          // 	? {
          // 		as: is(generated.as, SQL)
          // 			? dialect.sqlToQuery(generated.as as SQL).sql
          // 			: typeof generated.as === 'function'
          // 			? dialect.sqlToQuery(generated.as() as SQL).sql
          // 			: (generated.as as any),
          // 		type: 'stored',
          // 	}
          // 	: undefined,
          // identity: identity
          // 	? {
          // 		type: identity.type,
          // 		name: identity.sequenceName ?? `${viewName}_${column.name}_seq`,
          // 		schema: schema ?? 'public',
          // 		increment,
          // 		startWith,
          // 		minValue,
          // 		maxValue,
          // 		cache,
          // 		cycle: identity?.sequenceOptions?.cycle ?? false,
          // 	}
          // 	: undefined,
        };
        // if (column.isUnique) {
        // 	// const existingUnique = uniqueConstraintObject[column.uniqueName!];
        // 	// if (typeof existingUnique !== 'undefined') {
        // 	// 	console.log(
        // 	// 		`\n${
        // 	// 			withStyle.errorWarning(
        // 	// 				`We\'ve found duplicated unique constraint names in ${chalk.underline.blue(viewName)} table.
        //   // The unique constraint ${chalk.underline.blue(column.uniqueName)} on the ${
        // 	// 					chalk.underline.blue(
        // 	// 						column.name,
        // 	// 					)
        // 	// 				} column is conflicting with a unique constraint name already defined for ${
        // 	// 					chalk.underline.blue(existingUnique.columns.join(','))
        // 	// 				} columns\n`,
        // 	// 			)
        // 	// 		}`,
        // 	// 	);
        // 	// 	process.exit(1);
        // 	// }
        // 	uniqueConstraintObject[column.uniqueName!] = {
        // 		name: column.uniqueName!,
        // 		nullsNotDistinct: column.uniqueType === 'not distinct',
        // 		columns: [columnToSet.name],
        // 	};
        // }
        if (column.default !== undefined) {
          if (is(column.default, SQL)) {
            columnToSet.default = sqlToStr(column.default, casing);
          } else {
            if (typeof column.default === "string") {
              columnToSet.default = `'${column.default}'`;
            } else {
              if (sqlTypeLowered === "jsonb" || sqlTypeLowered === "json") {
                columnToSet.default = `'${JSON.stringify(column.default)}'::${sqlTypeLowered}`;
              } else if (column.default instanceof Date) {
                if (sqlTypeLowered === "date") {
                  columnToSet.default = `'${column.default.toISOString().split("T")[0]}'`;
                } else if (sqlTypeLowered === "timestamp") {
                  columnToSet.default = `'${column.default.toISOString().replace("T", " ").slice(0, 23)}'`;
                } else {
                  columnToSet.default = `'${column.default.toISOString()}'`;
                }
              } else if (
                isPgArrayType(sqlTypeLowered) &&
                Array.isArray(column.default)
              ) {
                columnToSet.default = `'${buildArrayString(column.default, sqlTypeLowered)}'`;
              } else {
                // Should do for all types
                // columnToSet.default = `'${column.default}'::${sqlTypeLowered}`;
                columnToSet.default = column.default;
              }
            }
          }
        }
        columnsObject[column.name] = columnToSet;
      }
    }
    resultViews[viewKey] = {
      columns: columnsObject,
      definition: isExisting ? undefined : dialect.sqlToQuery(query!).sql,
      name: viewName,
      schema: viewSchema,
      isExisting,
      with: withOption,
      withNoData,
      materialized,
      tablespace,
      using,
    };
  }
  const enumsToReturn: Record<string, Enum> = enums.reduce<{
    [key: string]: Enum;
  }>((map, obj) => {
    const enumSchema = obj.schema || "public";
    const key = `${enumSchema}.${obj.enumName}`;
    map[key] = {
      name: obj.enumName,
      schema: enumSchema,
      values: obj.enumValues,
    };
    return map;
  }, {});
  const schemasObject = Object.fromEntries(
    schemas
      .filter((it) => {
        return it.schemaName !== "public";
      })
      .map((it) => [it.schemaName, it.schemaName]),
  );
  return {
    version: "7",
    dialect: "postgresql",
    tables: result,
    enums: enumsToReturn,
    schemas: schemasObject,
    views: resultViews,
  };
};
const mapValues = <IN, OUT>(
  obj: Record<string, IN>,
  map: (input: IN) => OUT,
): Record<string, OUT> => {
  const result = Object.keys(obj).reduce(
    (result, key) => {
      result[key] = map(obj[key]!);
      return result;
    },
    {} as Record<string, OUT>,
  );
  return result;
};
const squashPgScheme = (json: PgSchemaSquashed): PgSchemaSquashed => {
  const mappedTables = Object.fromEntries(
    Object.entries(json.tables).map((it: [string, any]) => {
      const squashedIndexes = mapValues(it[1]!.indexes, (index) => {
        return PgSquasher.squashIdx(index);
      });
      const squashedPKs = mapValues(it[1]!.compositePrimaryKeys, (pk) => {
        return PgSquasher.squashPK(pk);
      });
      const mappedColumns = Object.fromEntries(
        Object.entries(it[1]!.columns).map((it) => {
          return [
            it[0],
            {
              ...it[1]!,
              identity: undefined,
            },
          ];
        }),
      );
      return [
        it[0],
        {
          name: it[1]!.name,
          schema: it[1]!.schema,
          columns: mappedColumns,
          indexes: squashedIndexes,
          compositePrimaryKeys: squashedPKs,
        },
      ];
    }),
  );
  return {
    version: "7",
    dialect: json.dialect,
    tables: mappedTables,
    enums: json.enums,
    schemas: json.schemas,
    views: json.views,
  };
};
</file>

<file path="packages/core/src/drizzle/bigint.ts">
import {
  type ColumnBaseConfig,
  type ColumnBuilderBaseConfig,
  type ColumnBuilderRuntimeConfig,
  type MakeColumnConfig,
  entityKind,
} from "drizzle-orm";
import {
  type AnyPgTable,
  PgColumn,
  PgColumnBuilder,
} from "drizzle-orm/pg-core";
export type PgBigintBuilderInitial<TName extends string> = PgBigintBuilder<{
  name: TName;
  dataType: "bigint";
  columnType: "PgEvmBigint";
  data: bigint;
  driverParam: string;
  enumValues: undefined;
  generated: undefined;
}>;
export class PgBigintBuilder<
  T extends ColumnBuilderBaseConfig<"bigint", "PgEvmBigint">,
> extends PgColumnBuilder<T> {
  static readonly [entityKind]: string = "PgEvmBigintBuilder";
  constructor(name: T["name"]) {
    super(name, "bigint", "PgEvmBigint");
  }
  /** @internal */
  // @ts-ignore
  override build<TTableName extends string>(
    table: AnyPgTable<{ name: TTableName }>,
  ): PgBigint<MakeColumnConfig<T, TTableName>> {
    return new PgBigint<MakeColumnConfig<T, TTableName>>(
      table,
      this.config as ColumnBuilderRuntimeConfig<any, any>,
    );
  }
}
export class PgBigint<
  T extends ColumnBaseConfig<"bigint", "PgEvmBigint">,
> extends PgColumn<T> {
  static readonly [entityKind]: string = "PgEvmBigint";
  getSQLType(): string {
    return "numeric(78)";
  }
  override mapFromDriverValue(value: string): bigint {
    return BigInt(value);
  }
}
</file>

<file path="packages/core/src/drizzle/bytes.ts">
import {
  type ColumnBaseConfig,
  type ColumnBuilderBaseConfig,
  type ColumnBuilderRuntimeConfig,
  type MakeColumnConfig,
  entityKind,
} from "drizzle-orm";
import {
  type AnyPgTable,
  PgColumn,
  PgColumnBuilder,
} from "drizzle-orm/pg-core";
export type PgBytesBuilderInitial<TName extends string> = PgBytesBuilder<{
  name: TName;
  dataType: "buffer";
  columnType: "PgBytes";
  data: Uint8Array;
  driverParam: string;
  enumValues: undefined;
  generated: undefined;
}>;
export class PgBytesBuilder<
  T extends ColumnBuilderBaseConfig<"buffer", "PgBytes">,
> extends PgColumnBuilder<T> {
  static readonly [entityKind]: string = "PgBytesBuilder";
  constructor(name: T["name"]) {
    super(name, "buffer", "PgBytes");
  }
  /** @internal */
  // @ts-ignore
  override build<TTableName extends string>(
    table: AnyPgTable<{ name: TTableName }>,
  ): PgBytes<MakeColumnConfig<T, TTableName>> {
    return new PgBytes<MakeColumnConfig<T, TTableName>>(
      table,
      this.config as ColumnBuilderRuntimeConfig<any, any>,
    );
  }
  /**
   * @deprecated Bytes columns cannot be used as arrays
   */
  override array(): never {
    throw new Error("bytes().array() is not supported");
  }
}
export class PgBytes<
  T extends ColumnBaseConfig<"buffer", "PgBytes">,
> extends PgColumn<T> {
  static readonly [entityKind]: string = "PgBytes";
  getSQLType(): string {
    return "bytea";
  }
  override mapFromDriverValue(value: Buffer): Uint8Array {
    return new Uint8Array(value);
  }
  override mapToDriverValue(value: Uint8Array): Buffer {
    return Buffer.from(value);
  }
}
</file>

<file path="packages/core/src/drizzle/hex.ts">
import {
  type ColumnBaseConfig,
  type ColumnBuilderBaseConfig,
  type ColumnBuilderRuntimeConfig,
  type MakeColumnConfig,
  entityKind,
} from "drizzle-orm";
import {
  type AnyPgTable,
  PgColumn,
  PgColumnBuilder,
} from "drizzle-orm/pg-core";
export type PgHexBuilderInitial<TName extends string> = PgHexBuilder<{
  name: TName;
  dataType: "string";
  columnType: "PgHex";
  data: `0x${string}`;
  driverParam: string;
  enumValues: undefined;
  generated: undefined;
}>;
export class PgHexBuilder<
  T extends ColumnBuilderBaseConfig<"string", "PgHex">,
> extends PgColumnBuilder<T> {
  static readonly [entityKind]: string = "PgHexBuilder";
  constructor(name: T["name"]) {
    super(name, "string", "PgHex");
  }
  /** @internal */
  // @ts-ignore
  override build<TTableName extends string>(
    table: AnyPgTable<{ name: TTableName }>,
  ): PgHex<MakeColumnConfig<T, TTableName>> {
    return new PgHex<MakeColumnConfig<T, TTableName>>(
      table,
      this.config as ColumnBuilderRuntimeConfig<any, any>,
    );
  }
}
export class PgHex<
  T extends ColumnBaseConfig<"string", "PgHex">,
> extends PgColumn<T> {
  static readonly [entityKind]: string = "PgHex";
  getSQLType(): string {
    return "text";
  }
  override mapToDriverValue(value: `0x${string}`) {
    if (value.length % 2 === 0) return value.toLowerCase() as `0x${string}`;
    return `0x0${value.slice(2)}`.toLowerCase() as `0x${string}`;
  }
}
</file>

<file path="packages/core/src/drizzle/index.test.ts">
import { expect, test } from "vitest";
import { getPrimaryKeyColumns } from "./index.js";
import { onchainTable, primaryKey } from "./onchain.js";
test("getPrimaryKeyColumns()", () => {
  const table = onchainTable("table", (p) => ({
    account: p.hex().primaryKey(),
    balance: p.bigint().notNull(),
  }));
  const primaryKeys = getPrimaryKeyColumns(table);
  expect(primaryKeys).toStrictEqual([{ js: "account", sql: "account" }]);
});
test("getPrimaryKeyColumns() sql", () => {
  const table = onchainTable("table", (p) => ({
    name: p.integer("unique_name").primaryKey(),
  }));
  const primaryKeys = getPrimaryKeyColumns(table);
  expect(primaryKeys).toStrictEqual([{ js: "name", sql: "unique_name" }]);
});
test("getPrimaryKeyColumns() snake case", () => {
  const table = onchainTable("table", (p) => ({
    chainId: p.integer().primaryKey(),
  }));
  const primaryKeys = getPrimaryKeyColumns(table);
  expect(primaryKeys).toStrictEqual([{ js: "chainId", sql: "chain_id" }]);
});
test("getPrimaryKeyColumns() composite", () => {
  const table = onchainTable(
    "table",
    (p) => ({
      name: p.text(),
      age: p.integer(),
      address: p.hex(),
    }),
    (table) => ({
      primaryKeys: primaryKey({ columns: [table.name, table.address] }),
    }),
  );
  const primaryKeys = getPrimaryKeyColumns(table);
  expect(primaryKeys).toStrictEqual([
    { js: "name", sql: "name" },
    { js: "address", sql: "address" },
  ]);
});
</file>

<file path="packages/core/src/drizzle/index.ts">
import { getTableColumns } from "drizzle-orm";
import {
  type PgColumn,
  type PgTable,
  getTableConfig,
} from "drizzle-orm/pg-core";
import { getColumnCasing } from "./kit/index.js";
export const getPrimaryKeyColumns = (
  table: PgTable,
): { sql: string; js: string }[] => {
  const primaryKeys = getTableConfig(table).primaryKeys;
  const findJsName = (column: PgColumn): string => {
    const name = column.name;
    for (const [js, column] of Object.entries(getTableColumns(table))) {
      if (column.name === name) return js;
    }
    throw "unreachable";
  };
  if (primaryKeys.length > 0) {
    return primaryKeys[0]!.columns.map((column) => ({
      sql: getColumnCasing(column, "snake_case"),
      js: findJsName(column),
    }));
  }
  const pkColumn = Object.values(getTableColumns(table)).find(
    (c) => c.primary,
  )!;
  return [
    {
      sql: getColumnCasing(pkColumn, "snake_case"),
      js: findJsName(pkColumn),
    },
  ];
};
</file>

<file path="packages/core/src/drizzle/json.ts">
import { type BaseError, BigIntSerializationError } from "@/internal/errors.js";
import { type ColumnBaseConfig, entityKind } from "drizzle-orm";
import type {
  ColumnBuilderBaseConfig,
  ColumnBuilderRuntimeConfig,
  MakeColumnConfig,
} from "drizzle-orm/column-builder";
import {
  type AnyPgTable,
  PgColumn,
  PgColumnBuilder,
} from "drizzle-orm/pg-core";
export type PgJsonBuilderInitial<TName extends string> = PgJsonBuilder<{
  name: TName;
  dataType: "json";
  columnType: "PgJson";
  data: unknown;
  driverParam: string;
  enumValues: undefined;
}>;
export class PgJsonBuilder<
  T extends ColumnBuilderBaseConfig<"json", "PgJson">,
> extends PgColumnBuilder<T> {
  static readonly [entityKind]: string = "PgJsonBuilder";
  constructor(name: T["name"]) {
    super(name, "json", "PgJson");
  }
  /** @internal */
  // @ts-ignore
  override build<TTableName extends string>(
    table: AnyPgTable<{ name: TTableName }>,
  ): PgJson<MakeColumnConfig<T, TTableName>> {
    return new PgJson<MakeColumnConfig<T, TTableName>>(
      table,
      this.config as ColumnBuilderRuntimeConfig<any, any>,
    );
  }
}
export class PgJson<
  T extends ColumnBaseConfig<"json", "PgJson">,
> extends PgColumn<T> {
  static readonly [entityKind]: string = "PgJson";
  getSQLType(): string {
    return "json";
  }
  override mapToDriverValue(value: T["data"]): string {
    try {
      return JSON.stringify(value);
    } catch (_error) {
      let error = _error as Error;
      if (
        // node error message
        error?.message?.includes("Do not know how to serialize a BigInt") ||
        // bun error message
        error?.message?.includes("cannot serialize BigInt")
      ) {
        error = new BigIntSerializationError(error.message);
        (error as BaseError).meta.push(
          "Hint:\n  The JSON column type does not support BigInt values. Use the replaceBigInts() helper function before inserting into the database. Docs: https://ponder.sh/docs/api-reference/ponder-utils#replacebigints",
        );
      }
      throw error;
    }
  }
  override mapFromDriverValue(value: T["data"] | string): T["data"] {
    if (typeof value === "string") {
      try {
        return JSON.parse(value);
      } catch {
        return value as T["data"];
      }
    }
    return value;
  }
}
export type PgJsonbBuilderInitial<TName extends string> = PgJsonbBuilder<{
  name: TName;
  dataType: "json";
  columnType: "PgJsonb";
  data: unknown;
  driverParam: unknown;
  enumValues: undefined;
}>;
export class PgJsonbBuilder<
  T extends ColumnBuilderBaseConfig<"json", "PgJsonb">,
> extends PgColumnBuilder<T> {
  static override readonly [entityKind]: string = "PgJsonbBuilder";
  constructor(name: T["name"]) {
    super(name, "json", "PgJsonb");
  }
  /** @internal */
  // @ts-ignore
  override build<TTableName extends string>(
    table: AnyPgTable<{ name: TTableName }>,
  ): PgJsonb<MakeColumnConfig<T, TTableName>> {
    return new PgJsonb<MakeColumnConfig<T, TTableName>>(
      table,
      this.config as ColumnBuilderRuntimeConfig<any, any>,
    );
  }
}
export class PgJsonb<
  T extends ColumnBaseConfig<"json", "PgJsonb">,
> extends PgColumn<T> {
  static override readonly [entityKind]: string = "PgJsonb";
  // biome-ignore lint/complexity/noUselessConstructor: <explanation>
  constructor(
    table: AnyPgTable<{ name: T["tableName"] }>,
    config: PgJsonbBuilder<T>["config"],
  ) {
    super(table, config);
  }
  getSQLType(): string {
    return "jsonb";
  }
  override mapToDriverValue(value: T["data"]): string {
    try {
      return JSON.stringify(value);
    } catch (_error) {
      let error = _error as Error;
      if (error?.message?.includes("Do not know how to serialize a BigInt")) {
        error = new BigIntSerializationError(error.message);
        (error as BaseError).meta.push(
          "Hint:\n  The JSON column type does not support BigInt values. Use the replaceBigInts() helper function before inserting into the database. Docs: https://ponder.sh/docs/api-reference/ponder-utils#replacebigints",
        );
      }
      throw error;
    }
  }
  override mapFromDriverValue(value: T["data"] | string): T["data"] {
    if (typeof value === "string") {
      try {
        return JSON.parse(value);
      } catch {
        return value as T["data"];
      }
    }
    return value;
  }
}
</file>

<file path="packages/core/src/drizzle/onchain.ts">
import type { PonderTypeError } from "@/types/utils.js";
import {
  type BuildColumns,
  type ColumnBuilderBase,
  Table,
  type Writable,
  getTableName,
} from "drizzle-orm";
import { toSnakeCase } from "drizzle-orm/casing";
import {
  type AnyPgColumn,
  type PrimaryKeyBuilder as DrizzlePrimaryKeyBuilder,
  type ExtraConfigColumn,
  ManualViewBuilder,
  type PgColumnBuilder,
  type PgColumnBuilderBase,
  PgEnumColumnBuilder,
  type PgEnumColumnBuilderInitial,
  PgTable,
  type PgTableExtraConfig,
  type PgTableWithColumns,
  type PgTextConfig,
  type TableConfig,
  ViewBuilder,
  primaryKey as drizzlePrimaryKey,
} from "drizzle-orm/pg-core";
import {
  type PgColumnsBuilders as _PgColumnsBuilders,
  getPgColumnBuilders,
} from "drizzle-orm/pg-core/columns/all";
import { PgBigintBuilder, type PgBigintBuilderInitial } from "./bigint.js";
import { PgBytesBuilder, type PgBytesBuilderInitial } from "./bytes.js";
import { PgHexBuilder, type PgHexBuilderInitial } from "./hex.js";
import {
  PgJsonBuilder,
  type PgJsonBuilderInitial,
  PgJsonbBuilder,
  type PgJsonbBuilderInitial,
} from "./json.js";
import { PgTextBuilder, type PgTextBuilderInitial } from "./text.js";
// 16 digits for chain ID.
export const MAX_DATABASE_OBJECT_NAME_LENGTH = 45;
// Note: All of these database object names should be less than 63 characters, otherwise they will
// be truncated by postgres.
export const getLiveQueryTriggerName = () => {
  return "live_query";
};
export const getLiveQueryProcedureName = () => {
  return "live_query()";
};
export const getLiveQueryChannelName = (schema: string) => {
  return `${schema}_live_query`;
};
export const getLiveQueryNotifyTriggerName = () => {
  return "live_query_notify";
};
/**
 * Returns the name of the trigger used to notify live queries for the views pattern.
 * @dev The trigger is placed in the base schema, but used to notify in the views schema.
 */
export const getViewsLiveQueryNotifyTriggerName = (viewsSchema: string) => {
  return `${viewsSchema}_live_query_notify`;
};
export const getLiveQueryNotifyProcedureName = () => {
  return "live_query_notify()";
};
export const getLiveQueryTempTableName = () => {
  return "live_query_tables";
};
export const getPartitionName = (table: string | PgTable, chainId: number) => {
  return `${typeof table === "string" ? table : getTableName(table)}_${chainId}`;
};
export const getReorgTableName = (table: string | PgTable) => {
  return `_reorg__${typeof table === "string" ? table : getTableName(table)}`;
};
export const getReorgTriggerName = () => {
  return "reorg";
};
export const getReorgProcedureName = (table: string | PgTable) => {
  return `operation_reorg__${typeof table === "string" ? table : getTableName(table)}()`;
};
export const getReorgSequenceName = () => {
  return "operation_id";
};
/** @internal */
function getColumnNameAndConfig<
  TConfig extends Record<string, any> | undefined,
>(a: string | TConfig | undefined, b: TConfig | undefined) {
  return {
    name: typeof a === "string" && a.length > 0 ? a : ("" as string),
    config: typeof a === "object" ? a : (b as TConfig),
  };
}
// @ts-ignore
export function hex(): PgHexBuilderInitial<"">;
export function hex<name extends string>(
  columnName: name,
): PgHexBuilderInitial<name>;
export function hex(columnName?: string) {
  return new PgHexBuilder(columnName ?? "");
}
// @ts-ignore
export function bigint(): PgBigintBuilderInitial<"">;
export function bigint<name extends string>(
  columnName: name,
): PgBigintBuilderInitial<name>;
export function bigint(columnName?: string) {
  return new PgBigintBuilder(columnName ?? "");
}
export function json(): PgJsonBuilderInitial<"">;
export function json<name extends string>(
  name: name,
): PgJsonBuilderInitial<name>;
export function json(name?: string) {
  return new PgJsonBuilder(name ?? "");
}
export function jsonb(): PgJsonbBuilderInitial<"">;
export function jsonb<name extends string>(
  name: name,
): PgJsonbBuilderInitial<name>;
export function jsonb(name?: string) {
  return new PgJsonbBuilder(name ?? "");
}
// @ts-ignore
export function bytes(): PgBytesBuilderInitial<"">;
export function bytes<name extends string>(
  columnName: name,
): PgBytesBuilderInitial<name>;
export function bytes(columnName?: string) {
  return new PgBytesBuilder(columnName ?? "");
}
export function text(): PgTextBuilderInitial<"", [string, ...string[]]>;
export function text<U extends string, T extends Readonly<[U, ...U[]]>>(
  config?: PgTextConfig<T | Writable<T>>,
): PgTextBuilderInitial<"", Writable<T>>;
export function text<
  TName extends string,
  U extends string,
  T extends Readonly<[U, ...U[]]>,
>(
  name: TName,
  config?: PgTextConfig<T | Writable<T>>,
): PgTextBuilderInitial<TName, Writable<T>>;
export function text(a?: string | PgTextConfig, b: PgTextConfig = {}): any {
  const { name, config } = getColumnNameAndConfig<PgTextConfig>(a, b);
  return new PgTextBuilder(name, config as any);
}
export const onchain = Symbol.for("ponder:onchain");
export type PrimaryKeyBuilder<columnNames extends string = string> =
  DrizzlePrimaryKeyBuilder & { columnNames: columnNames };
export const primaryKey = <
  tableName extends string,
  column extends AnyPgColumn<{ tableName: tableName }> & { " name": string },
  columns extends (AnyPgColumn<{ tableName: tableName }> & {
    " name": string;
  })[],
>({
  name,
  columns,
}: { name?: string; columns: [column, ...columns] }) =>
  drizzlePrimaryKey({ name, columns }) as PrimaryKeyBuilder<
    column[" name"] | columns[number][" name"]
  >;
export type OnchainTable<
  T extends TableConfig & {
    extra: PgTableExtraConfig | undefined;
  } = TableConfig & { extra: PgTableExtraConfig | undefined },
> = PgTable<T> & {
  [Key in keyof T["columns"]]: T["columns"][Key];
} & { [onchain]: true } & {
  enableRLS: () => Omit<OnchainTable<T>, "enableRLS">;
};
export type BuildExtraConfigColumns<
  columns extends Record<string, ColumnBuilderBase>,
> = {
  [key in keyof columns]: ExtraConfigColumn & {
    " name": key;
  };
};
export type PgColumnsBuilders = Omit<
  _PgColumnsBuilders,
  "bigint" | "serial" | "smallserial" | "bigserial" | "json" | "jsonb"
> & {
  /**
   * Create an 8 byte number column.
   */
  int8: _PgColumnsBuilders["bigint"];
  /**
   * Create a column for hex strings.
   *
   * - Docs: https://ponder.sh/docs/api-reference/ponder/schema#onchaintable
   *
   * @example
   * import { hex, onchainTable } from "ponder";
   *
   * export const account = onchainTable("account", (p) => ({
   *   address: p.hex(),
   * }));
   */
  hex: typeof hex;
  /**
   * Create a column for Ethereum integers
   *
   * - Docs: https://ponder.sh/docs/api-reference/ponder/schema#onchaintable
   *
   * @example
   * import { bigint, onchainTable } from "ponder";
   *
   * export const account = onchainTable("account", (p) => ({
   *   balance: p.bigint(),
   * }));
   */
  bigint: typeof bigint;
  /**
   * Create a column for Ethereum bytes
   *
   * - Docs: https://ponder.sh/docs/api-reference/ponder/schema#onchaintable
   *
   * @example
   * import { bytes, onchainTable } from "ponder";
   *
   * export const account = onchainTable("account", (p) => ({
   *   calldata: p.bytes(),
   * }));
   */
  bytes: typeof bytes;
  json: typeof json;
  jsonb: typeof jsonb;
};
/**
 * Create an onchain table.
 *
 * - Docs: https://ponder.sh/docs/api-reference/ponder/schema#onchaintable
 *
 * @example
 * import { onchainTable } from "ponder";
 *
 * export const account = onchainTable("account", (p) => ({
 *   address: p.hex().primaryKey(),
 *   balance: p.bigint().notNull(),
 * }));
 *
 * @param name - The table name in the database.
 * @param columns - The table columns.
 * @param extra - Config such as indexes or composite primary keys.
 * @returns The onchain table.
 */
export const onchainTable = <
  name extends string,
  columns extends Record<string, PgColumnBuilderBase>,
  extra extends PgTableExtraConfig | undefined = undefined,
>(
  name: name extends "" ? PonderTypeError<`Table name cannot be empty`> : name,
  columns: columns | ((columnTypes: PgColumnsBuilders) => columns),
  extraConfig?: (self: BuildExtraConfigColumns<columns>) => extra,
): OnchainTable<{
  name: name;
  schema: undefined;
  columns: BuildColumns<name, columns, "pg">;
  extra: extra;
  dialect: "pg";
}> => {
  const schema = globalThis?.PONDER_NAMESPACE_BUILD?.schema;
  const table = pgTableWithSchema(name, columns, extraConfig as any, schema);
  // @ts-ignore
  table[onchain] = true;
  // @ts-ignore
  return table;
};
/**
 * Create an onchain view.
 *
 * - Docs: https://ponder.sh/docs/api-reference/ponder/schema#onchainview
 *
 * @example
 * import { onchainView } from "ponder";
 *
 * export const accountView = onchainView("account_view").as((qb) =>
 *   qb.select().from(account),
 * );
 *
 * @param name - The view name in the database.
 * @param columns - [Optional] The view columns.
 * @returns The onchain view.
 */
export function onchainView<TName extends string>(
  name: TName,
): ViewBuilder<TName>;
export function onchainView<
  TName extends string,
  TColumns extends Record<string, PgColumnBuilderBase>,
>(name: TName, columns: TColumns): ManualViewBuilder<TName, TColumns>;
export function onchainView(
  name: string,
  columns?: Record<string, PgColumnBuilderBase>,
): ViewBuilder | ManualViewBuilder {
  const schema = globalThis?.PONDER_NAMESPACE_BUILD?.schema;
  const view = pgViewWithSchema(name, columns, schema);
  // @ts-ignore
  view[onchain] = true;
  return view;
}
export const isPgEnumSym = Symbol.for("drizzle:isPgEnum");
export type OnchainEnum<TValues extends [string, ...string[]]> = {
  (): PgEnumColumnBuilderInitial<"", TValues>;
  <TName extends string>(
    name: TName,
  ): PgEnumColumnBuilderInitial<TName, TValues>;
  <TName extends string>(
    name?: TName,
  ): PgEnumColumnBuilderInitial<TName, TValues>;
  readonly enumName: string;
  readonly enumValues: TValues;
  readonly schema: string | undefined;
  /** @internal */
  [isPgEnumSym]: true;
} & { [onchain]: true };
export const onchainEnum = <U extends string, T extends Readonly<[U, ...U[]]>>(
  enumName: string,
  values: T | Writable<T>,
): OnchainEnum<Writable<T>> => {
  const schema = globalThis?.PONDER_NAMESPACE_BUILD?.schema;
  const e = pgEnumWithSchema(enumName, values, schema);
  // @ts-ignore
  e[onchain] = true;
  // @ts-ignore
  return e;
};
/** @see https://github.com/drizzle-team/drizzle-orm/blob/main/drizzle-orm/src/pg-core/table.ts#L51 */
function pgTableWithSchema<
  name extends string,
  schema extends string | undefined,
  columns extends Record<string, PgColumnBuilderBase>,
>(
  name: name,
  columns: columns | ((columnTypes: PgColumnsBuilders) => columns),
  extraConfig:
    | ((self: BuildExtraConfigColumns<columns>) => PgTableExtraConfig)
    | undefined,
  schema: schema,
  baseName = name,
): PgTableWithColumns<{
  name: name;
  schema: schema;
  columns: BuildColumns<name, columns, "pg">;
  dialect: "pg";
}> {
  const rawTable = new PgTable<{
    name: name;
    schema: schema;
    columns: BuildColumns<name, columns, "pg">;
    dialect: "pg";
  }>(name, schema, baseName);
  const { bigint: int8, text: _text, ...restColumns } = getPgColumnBuilders();
  const parsedColumns: columns =
    typeof columns === "function"
      ? columns({ ...restColumns, int8, hex, bigint, bytes, text, json, jsonb })
      : columns;
  const builtColumns = Object.fromEntries(
    Object.entries(parsedColumns).map(([name, colBuilderBase]) => {
      const colBuilder = colBuilderBase;
      // @ts-ignore
      colBuilder.setName(toSnakeCase(name));
      // @ts-ignore
      const column = colBuilder.build(rawTable);
      // @ts-ignore
      rawTable[Symbol.for("drizzle:PgInlineForeignKeys")].push(
        // @ts-ignore
        ...colBuilder.buildForeignKeys(column, rawTable),
      );
      return [name, column];
    }),
  ) as unknown as BuildColumns<name, columns, "pg">;
  const builtColumnsForExtraConfig = Object.fromEntries(
    Object.entries(parsedColumns).map(([name, colBuilderBase]) => {
      const colBuilder = colBuilderBase as PgColumnBuilder;
      //@ts-ignore
      colBuilder.setName(toSnakeCase(name));
      //@ts-ignore
      const column = colBuilder.buildExtraConfigColumn(rawTable);
      return [name, column];
    }),
  ) as unknown as BuildExtraConfigColumns<columns>;
  const table = Object.assign(rawTable, builtColumns);
  //@ts-ignore
  table[Table.Symbol.Columns] = builtColumns;
  //@ts-ignore
  table[Table.Symbol.ExtraConfigColumns] = builtColumnsForExtraConfig;
  if (extraConfig) {
    //@ts-ignore
    table[PgTable.Symbol.ExtraConfigBuilder] = extraConfig as any;
  }
  return Object.assign(table, {
    enableRLS: () => {
      // @ts-ignore
      table[PgTable.Symbol.EnableRLS] = true;
      return table as PgTableWithColumns<{
        name: name;
        schema: schema;
        columns: BuildColumns<name, columns, "pg">;
        dialect: "pg";
      }>;
    },
  });
}
function pgViewWithSchema(
  name: string,
  selection: Record<string, PgColumnBuilderBase> | undefined,
  schema: string | undefined,
): ViewBuilder | ManualViewBuilder {
  if (selection) {
    return new ManualViewBuilder(name, selection, schema);
  }
  return new ViewBuilder(name, schema);
}
function pgEnumWithSchema<U extends string, T extends Readonly<[U, ...U[]]>>(
  enumName: string,
  values: T | Writable<T>,
  schema?: string,
): OnchainEnum<Writable<T>> {
  const enumInstance: OnchainEnum<Writable<T>> = Object.assign(
    <TName extends string>(
      name?: TName,
    ): PgEnumColumnBuilderInitial<TName, Writable<T>> =>
      new PgEnumColumnBuilder(name ?? ("" as TName), enumInstance),
    {
      enumName,
      enumValues: values,
      schema,
      [isPgEnumSym]: true,
      [onchain]: true,
    } as const,
  );
  return enumInstance;
}
</file>

<file path="packages/core/src/drizzle/text.ts">
import {
  type ColumnBaseConfig,
  type ColumnBuilderBaseConfig,
  type ColumnBuilderRuntimeConfig,
  type MakeColumnConfig,
  entityKind,
  getTableName,
} from "drizzle-orm";
import {
  type AnyPgTable,
  PgColumn,
  PgColumnBuilder,
  type PgTextConfig,
} from "drizzle-orm/pg-core";
export type PgTextBuilderInitial<
  TName extends string,
  TEnum extends [string, ...string[]],
> = PgTextBuilder<{
  name: TName;
  dataType: "string";
  columnType: "PgText";
  data: TEnum[number];
  enumValues: TEnum;
  driverParam: string;
}>;
export class PgTextBuilder<
  T extends ColumnBuilderBaseConfig<"string", "PgText">,
> extends PgColumnBuilder<T, { enumValues: T["enumValues"] }> {
  static override readonly [entityKind]: string = "PgTextBuilder";
  constructor(name: T["name"], config: PgTextConfig<T["enumValues"]>) {
    super(name, "string", "PgText");
    this.config.enumValues = config.enum;
  }
  /** @internal */
  // @ts-ignore
  override build<TTableName extends string>(
    table: AnyPgTable<{ name: TTableName }>,
  ): PgText<MakeColumnConfig<T, TTableName>> {
    return new PgText<MakeColumnConfig<T, TTableName>>(
      table,
      this.config as ColumnBuilderRuntimeConfig<any, any>,
    );
  }
}
export class PgText<
  T extends ColumnBaseConfig<"string", "PgText">,
> extends PgColumn<T, { enumValues: T["enumValues"] }> {
  static override readonly [entityKind]: string = "PgText";
  override readonly enumValues = this.config.enumValues;
  getSQLType(): string {
    return "text";
  }
  override mapToDriverValue(value: string) {
    // Note: swallow errors because drizzle will throw a more specific error if the value is invalid
    try {
      if (value.match(/\0/g)) {
        globalThis.PONDER_COMMON?.logger.warn({
          msg: `Detected and removed null byte characters. Postgres "text" columns do not support null byte characters. Please consider handling this case in your indexing logic.`,
          value,
          table: getTableName(this.table),
          column: this.name,
        });
        return value.replace(/\0/g, "");
      }
    } catch {}
    return value;
  }
}
</file>

<file path="packages/core/src/graphql/graphiql.html.ts">
// https://github.com/graphql/graphiql/blob/main/examples/graphiql-cdn/index.html
export const graphiQLHtml = `<!--
 *  Copyright (c) 2021 GraphQL Contributors
 *  All rights reserved.
 *
 *  This source code is licensed under the license found in the
 *  LICENSE file in the root directory of this source tree.
-->
<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Ponder Playground</title>
    <style>
      body {
        height: 100%;
        margin: 0;
        width: 100%;
        overflow: hidden;
      }
      #graphiql {
        height: 100vh;
      }
      *::-webkit-scrollbar {
        height: 0.3rem;
        width: 0.5rem;
      }
      *::-webkit-scrollbar-track {
        -ms-overflow-style: none;
        overflow: -moz-scrollbars-none;
      }
      *::-webkit-scrollbar-thumb {
        -ms-overflow-style: none;
        overflow: -moz-scrollbars-none;
      }
    </style>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/graphiql@3.7.2/graphiql.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@graphiql/plugin-explorer@3.2.3/dist/style.css" />
  </head>
  <body>
    <div id="graphiql">Loading...</div>
    <script crossorigin src="https://cdn.jsdelivr.net/npm/react@18.3.1/umd/react.development.js"></script>1
    <script crossorigin src="https://cdn.jsdelivr.net/npm/react-dom@18.3.1/umd/react-dom.development.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/graphiql@3.7.2/graphiql.min.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@graphiql/plugin-explorer@3.2.3/dist/index.umd.js" crossorigin="anonymous"></script>
    <script>
      const fetcher = GraphiQL.createFetcher({ url: "" });
      const explorerPlugin = GraphiQLPluginExplorer.explorerPlugin();
      const root = ReactDOM.createRoot(document.getElementById("graphiql"));
      root.render(
        React.createElement(GraphiQL, {
          fetcher,
          plugins: [explorerPlugin],
          defaultEditorToolsVisibility: false,
        })
      );
    </script>
  </body>
</html>`;
</file>

<file path="packages/core/src/graphql/index.test.ts">
import {
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { type Database, getPonderCheckpointTable } from "@/database/index.js";
import {
  onchainEnum,
  onchainTable,
  onchainView,
  primaryKey,
} from "@/drizzle/onchain.js";
import { EVENT_TYPES, encodeCheckpoint } from "@/utils/checkpoint.js";
import { count, relations } from "drizzle-orm";
import {
  type ExecutionResult,
  type GraphQLType,
  execute,
  parse,
} from "graphql";
import { toBytes } from "viem";
import { zeroAddress } from "viem";
import { beforeEach, expect, test, vi } from "vitest";
import { buildDataLoaderCache, buildGraphQLSchema } from "./index.js";
beforeEach(setupCommon);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
function buildContextValue(database: Database) {
  const qb = database.readonlyQB;
  const getDataLoader = buildDataLoaderCache(qb);
  return { qb, getDataLoader };
}
test("metadata", async () => {
  const schema = {};
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  const graphqlSchema = buildGraphQLSchema({ schema });
  await database.adminQB.wrap((db) =>
    db.insert(getPonderCheckpointTable()).values({
      chainId: 1,
      chainName: "mainnet",
      latestCheckpoint: encodeCheckpoint({
        blockNumber: 10n,
        chainId: 1n,
        blockTimestamp: 20n,
        transactionIndex: 0n,
        eventType: EVENT_TYPES.blocks,
        eventIndex: 0n,
      }),
      finalizedCheckpoint: encodeCheckpoint({
        blockNumber: 10n,
        chainId: 1n,
        blockTimestamp: 20n,
        transactionIndex: 0n,
        eventType: EVENT_TYPES.blocks,
        eventIndex: 0n,
      }),
      safeCheckpoint: encodeCheckpoint({
        blockNumber: 10n,
        chainId: 1n,
        blockTimestamp: 20n,
        transactionIndex: 0n,
        eventType: EVENT_TYPES.blocks,
        eventIndex: 0n,
      }),
    }),
  );
  const result = await query(`
    query {
      _meta {
        status
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    _meta: {
      status: {
        mainnet: {
          id: 1,
          block: {
            number: 10,
            timestamp: 20,
          },
        },
      },
    },
  });
});
test("scalar, scalar not null, scalar array, scalar array not null", async () => {
  const schema = {
    table: onchainTable("table", (t) => ({
      id: t.text().primaryKey(),
      string: t.text(),
      int: t.integer(),
      float: t.doublePrecision(),
      boolean: t.boolean(),
      hex: t.hex(),
      bigint: t.bigint(),
      bytes: t.bytes(),
      stringNotNull: t.text().notNull(),
      intNotNull: t.integer().notNull(),
      floatNotNull: t.doublePrecision().notNull(),
      booleanNotNull: t.boolean().notNull(),
      hexNotNull: t.hex().notNull(),
      bigintNotNull: t.bigint().notNull(),
      bytesNotNull: t.bytes().notNull(),
      stringArray: t.text().array(),
      intArray: t.integer().array(),
      floatArray: t.doublePrecision().array(),
      booleanArray: t.boolean().array(),
      hexArray: t.hex().array(),
      bigintArray: t.bigint().array(),
      stringArrayNotNull: t.text().array().notNull(),
      intArrayNotNull: t.integer().array().notNull(),
      floatArrayNotNull: t.doublePrecision().array().notNull(),
      booleanArrayNotNull: t.boolean().array().notNull(),
      hexArrayNotNull: t.hex().array().notNull(),
      bigintArrayNotNull: t.bigint().array().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.table).values({
    id: "0",
    string: "0",
    int: 0,
    float: 0,
    boolean: false,
    hex: "0x0",
    bigint: 0n,
    bytes: toBytes(zeroAddress),
    stringNotNull: "0",
    intNotNull: 0,
    floatNotNull: 0,
    booleanNotNull: false,
    hexNotNull: "0x0",
    bigintNotNull: 0n,
    bytesNotNull: toBytes(zeroAddress),
    stringArray: ["0"],
    intArray: [0],
    floatArray: [0],
    booleanArray: [false],
    hexArray: ["0x0"],
    bigintArray: [0n],
    stringArrayNotNull: ["0"],
    intArrayNotNull: [0],
    floatArrayNotNull: [0],
    booleanArrayNotNull: [false],
    hexArrayNotNull: ["0x0"],
    bigintArrayNotNull: [0n],
  });
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      table(id: "0") {
        id
        string
        int
        float
        boolean
        hex
        bigint
        bytes
        stringNotNull
        intNotNull
        floatNotNull
        booleanNotNull
        hexNotNull
        bigintNotNull
        bytesNotNull
        stringArray
        intArray
        floatArray
        booleanArray
        hexArray
        bigintArray
        stringArrayNotNull
        intArrayNotNull
        floatArrayNotNull
        booleanArrayNotNull
        hexArrayNotNull
        bigintArrayNotNull
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    table: {
      id: "0",
      string: "0",
      int: 0,
      float: 0,
      boolean: false,
      hex: "0x00",
      bigint: "0",
      bytes: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
      stringNotNull: "0",
      intNotNull: 0,
      floatNotNull: 0,
      booleanNotNull: false,
      hexNotNull: "0x00",
      bigintNotNull: "0",
      bytesNotNull: [
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
      ],
      stringArray: ["0"],
      intArray: [0],
      floatArray: [0],
      booleanArray: [false],
      hexArray: ["0x00"],
      bigintArray: ["0"],
      stringArrayNotNull: ["0"],
      intArrayNotNull: [0],
      floatArrayNotNull: [0],
      booleanArrayNotNull: [false],
      hexArrayNotNull: ["0x00"],
      bigintArrayNotNull: ["0"],
    },
  });
});
test("enum, enum not null, enum array, enum array not null", async () => {
  const testEnum = onchainEnum("enum", ["A", "B"]);
  const table = onchainTable("table", (t) => ({
    id: t.text().primaryKey(),
    enum: testEnum("enum"),
    enumNotNull: testEnum("enumNotNull").notNull(),
    enumArray: testEnum("enumArray").array(),
    enumArrayNotNull: testEnum("enumArrayNotNull").array().notNull(),
  }));
  const schema = { testEnum, table };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.table).values({
    id: "0",
    enum: null,
    enumNotNull: "A",
    enumArray: null,
    enumArrayNotNull: ["A"],
  });
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      table(id: "0") {
        id
        enum
        enumNotNull
        enumArray
        enumArrayNotNull
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    table: {
      id: "0",
      enum: null,
      enumNotNull: "A",
      enumArray: null,
      enumArrayNotNull: ["A"],
    },
  });
});
test("enum primary key", async () => {
  const testEnum = onchainEnum("enum", ["A", "B"]);
  const table = onchainTable(
    "table",
    (t) => ({
      a: t.text().notNull(),
      enum: testEnum("enum").notNull(),
    }),
    (table) => ({
      pk: primaryKey({ columns: [table.a, table.enum] }),
    }),
  );
  const schema = { testEnum, table };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.table).values({
    a: "0",
    enum: "A",
  });
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      table(a: "0", enum: "A") {
        a
        enum
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    table: {
      a: "0",
      enum: "A",
    },
  });
});
test("json, json not null", async () => {
  const schema = {
    table: onchainTable("table", (t) => ({
      id: t.text().primaryKey(),
      json: t.json(),
      jsonNotNull: t.json().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.table).values({
    id: "0",
    json: null,
    jsonNotNull: { kevin: 52 },
  });
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      table(id: "0") {
        id
        json
        jsonNotNull
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    table: {
      id: "0",
      json: null,
      jsonNotNull: { kevin: 52 },
    },
  });
});
test("singular", async () => {
  const transferEvents = onchainTable("transfer_events", (t) => ({
    id: t.text().primaryKey(),
    amount: t.bigint().notNull(),
  }));
  const allowances = onchainTable(
    "allowances",
    (t) => ({
      owner: t.text().notNull(),
      spender: t.text().notNull(),
      amount: t.bigint().notNull(),
    }),
    (table) => ({
      pk: primaryKey({ columns: [table.owner, table.spender] }),
    }),
  );
  const schema = { transferEvents, allowances };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.transferEvents).values([
    { id: "0", amount: 0n },
    { id: "1", amount: 10n },
  ]);
  await database.userQB.raw.insert(schema.allowances).values([
    { owner: "0", spender: "0", amount: 1n },
    { owner: "0", spender: "1", amount: 10n },
    { owner: "1", spender: "0", amount: 100n },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      transferEvents(id: "0") {
        id
        amount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    transferEvents: { id: "0", amount: "0" },
  });
  result = await query(`
    query {
      transferEvents(id: "1") {
        id
        amount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    transferEvents: { id: "1", amount: "10" },
  });
  result = await query(`
    query {
      allowances(owner: "0", spender: "0") {
        owner
        spender
        amount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    allowances: { owner: "0", spender: "0", amount: "1" },
  });
  result = await query(`
    query {
      allowances(owner: "1", spender: "0") {
        owner
        spender
        amount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    allowances: { owner: "1", spender: "0", amount: "100" },
  });
});
test("singular with one relation", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
    name: t.text(),
  }));
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    ownerId: t.text().notNull(),
    ownerIdNullable: t.text(),
  }));
  // Note that regardless of whether the `fields` column(s) are nullable,
  // the field type corresponding to the `one` relation must be nullable.
  const petRelations = relations(pet, ({ one }) => ({
    owner: one(person, { fields: [pet.ownerId], references: [person.id] }),
    ownerNullable: one(person, {
      fields: [pet.ownerIdNullable],
      references: [person.id],
    }),
  }));
  const schema = { person, pet, petRelations };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.person).values([
    { id: "jake", name: "jake" },
    { id: "kyle", name: "kyle" },
  ]);
  await database.userQB.raw
    .insert(schema.pet)
    .values({ id: "dog1", ownerId: "jake" });
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      pet(id: "dog1") {
        owner {
          id
          name
        }
        ownerNullable {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    pet: {
      owner: {
        id: "jake",
        name: "jake",
      },
      ownerNullable: null,
    },
  });
});
test("singular with one relation using camel case 'references' column name", async () => {
  const person = onchainTable("person", (t) => ({
    camelCaseId: t.text().primaryKey(),
    name: t.text(),
  }));
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    ownerId: t.text(),
  }));
  const petRelations = relations(pet, ({ one }) => ({
    owner: one(person, {
      fields: [pet.ownerId],
      references: [person.camelCaseId],
    }),
  }));
  const schema = { person, pet, petRelations };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.person)
    .values({ camelCaseId: "jake", name: "jake" });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "dog1", ownerId: "jake" },
    { id: "dog2", ownerId: "kyle" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      pet(id: "dog1") {
        owner {
          camelCaseId
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    pet: {
      owner: { camelCaseId: "jake" },
    },
  });
});
test("singular with many relation", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
    name: t.text(),
  }));
  const personRelations = relations(person, ({ many }) => ({
    pets: many(pet),
  }));
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    ownerId: t.text(),
  }));
  const petRelations = relations(pet, ({ one }) => ({
    owner: one(person, { fields: [pet.ownerId], references: [person.id] }),
  }));
  const schema = { person, personRelations, pet, petRelations };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.person)
    .values({ id: "jake", name: "jake" });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "dog1", ownerId: "jake" },
    { id: "dog2", ownerId: "jake" },
    { id: "dog3", ownerId: "kyle" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      person(id: "jake") {
        pets {
          items {
            id
          }
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    person: {
      pets: { items: [{ id: "dog1" }, { id: "dog2" }] },
    },
  });
});
test("singular with many relation using camel case 'references' column name", async () => {
  const person = onchainTable("person", (t) => ({
    camelCaseId: t.text().primaryKey(),
    name: t.text(),
  }));
  const personRelations = relations(person, ({ many }) => ({
    pets: many(pet),
  }));
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    ownerId: t.text(),
  }));
  const petRelations = relations(pet, ({ one }) => ({
    owner: one(person, {
      fields: [pet.ownerId],
      references: [person.camelCaseId],
    }),
  }));
  const schema = { person, personRelations, pet, petRelations };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.person)
    .values({ camelCaseId: "jake", name: "jake" });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "dog1", ownerId: "jake" },
    { id: "dog2", ownerId: "jake" },
    { id: "dog3", ownerId: "kyle" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      person(camelCaseId: "jake") {
        pets {
          items {
            id
          }
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    person: {
      pets: { items: [{ id: "dog1" }, { id: "dog2" }] },
    },
  });
});
test("singular with many relation and extra one relation", async () => {
  const extra = onchainTable("extra", (t) => ({
    id: t.text().primaryKey(),
    name: t.text(),
  }));
  const user = onchainTable("user", (t) => ({
    id: t.text().primaryKey(),
    name: t.text(),
  }));
  const userRelations = relations(user, ({ many }) => ({
    heroes: many(hero),
  }));
  const hero = onchainTable("hero", (t) => ({
    id: t.text().primaryKey(),
    ownerId: t.text(),
    extraId: t.text(),
  }));
  const heroRelations = relations(hero, ({ one }) => ({
    extra: one(extra, { fields: [hero.extraId], references: [extra.id] }),
    owner: one(user, { fields: [hero.ownerId], references: [user.id] }),
  }));
  const extraRelations = relations(extra, ({ one }) => ({
    cuh: one(user, { fields: [extra.id], references: [user.id] }),
  }));
  const schema = {
    extra,
    extraRelations,
    hero,
    heroRelations,
    user,
    userRelations,
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.user)
    .values({ id: "jake", name: "jake" });
  await database.userQB.raw.insert(schema.hero).values([
    { id: "dog1", ownerId: "jake" },
    { id: "dog2", ownerId: "jake" },
    { id: "dog3", ownerId: "kyle" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      user(id: "jake") {
        heroes {
          items {
            id
          }
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    user: {
      heroes: { items: [{ id: "dog1" }, { id: "dog2" }] },
    },
  });
});
test("multiple many relations", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
    name: t.text(),
  }));
  const personRelations = relations(person, ({ many }) => ({
    pets1: many(pet, { relationName: "owner1_relation" }),
    pets2: many(pet, { relationName: "owner2_relation" }),
  }));
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    owner1: t.text(),
    owner2: t.text(),
  }));
  const petRelations = relations(pet, ({ one }) => ({
    owner1Relation: one(person, {
      fields: [pet.owner1],
      relationName: "owner1_relation",
      references: [person.id],
    }),
    owner2Relation: one(person, {
      fields: [pet.owner2],
      relationName: "owner2_relation",
      references: [person.id],
    }),
  }));
  const schema = { person, personRelations, pet, petRelations };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.person)
    .values({ id: "jake", name: "jake" });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "dog1", owner1: "jake", owner2: "jim" },
    { id: "dog2", owner1: "jake", owner2: "kyle" },
    { id: "dog3", owner1: "kyle", owner2: "jim" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      person(id: "jake") {
       pets1 {
          items {
            id
          }
        }
        pets2 {
          items {
            id
          }
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchInlineSnapshot(`
    {
      "person": {
        "pets1": {
          "items": [
            {
              "id": "dog1",
            },
            {
              "id": "dog2",
            },
          ],
        },
        "pets2": {
          "items": [],
        },
      },
    }
  `);
});
test("singular with many relation using filter", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
    name: t.text(),
  }));
  const personRelations = relations(person, ({ many }) => ({
    pets: many(pet),
  }));
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    age: t.integer(),
    ownerId: t.text(),
  }));
  const petRelations = relations(pet, ({ one }) => ({
    owner: one(person, { fields: [pet.ownerId], references: [person.id] }),
  }));
  const schema = { person, personRelations, pet, petRelations };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.person)
    .values({ id: "jake", name: "jake" });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "dog1", age: 1, ownerId: "jake" },
    { id: "dog2", age: 2, ownerId: "jake" },
    { id: "dog3", age: 3, ownerId: "jake" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      person(id: "jake") {
        pets(where: { id: "dog2" }) {
          items {
            id
          }
          totalCount
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    person: {
      pets: {
        items: [{ id: "dog2" }],
        totalCount: 1,
      },
    },
  });
});
test("singular with many relation using order by", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
    name: t.text(),
  }));
  const personRelations = relations(person, ({ many }) => ({
    pets: many(pet),
  }));
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    age: t.integer(),
    ownerId: t.text(),
  }));
  const petRelations = relations(pet, ({ one }) => ({
    owner: one(person, { fields: [pet.ownerId], references: [person.id] }),
  }));
  const schema = { person, personRelations, pet, petRelations };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.person)
    .values({ id: "jake", name: "jake" });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "dog1", age: 1, ownerId: "jake" },
    { id: "dog2", age: 2, ownerId: "jake" },
    { id: "dog3", age: 3, ownerId: "jake" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      person(id: "jake") {
        pets(orderBy: "age", orderDirection: "desc") {
          items {
            id
          }
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    person: {
      pets: {
        items: [{ id: "dog3" }, { id: "dog2" }, { id: "dog1" }],
      },
    },
  });
});
test("plural with one relation uses dataloader", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
    name: t.text(),
  }));
  const personRelations = relations(person, ({ many }) => ({
    pets: many(pet),
  }));
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    ownerId: t.text(),
  }));
  const petRelations = relations(pet, ({ one }) => ({
    owner: one(person, { fields: [pet.ownerId], references: [person.id] }),
  }));
  const schema = { person, personRelations, pet, petRelations };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.person)
    .values({ id: "jake", name: "jake" });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "dog1", ownerId: "jake" },
    { id: "dog2", ownerId: "jake" },
    { id: "dog3", ownerId: "kyle" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const personSelectSpy = vi.spyOn(database.readonlyQB.raw, "select");
  const petSelectSpy = vi.spyOn(database.readonlyQB.raw, "select");
  const result = await query(`
    query {
      pets {
        items {
          id
          owner {
            id
          }
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    pets: {
      items: [
        { id: "dog1", owner: { id: "jake" } },
        { id: "dog2", owner: { id: "jake" } },
        { id: "dog3", owner: null },
      ],
    },
  });
  expect(personSelectSpy).toHaveBeenCalledTimes(1);
  expect(petSelectSpy).toHaveBeenCalledTimes(1);
});
test("filter input type", async () => {
  const simpleEnum = onchainEnum("SimpleEnum", ["VALUE", "ANOTHER_VALUE"]);
  const table = onchainTable("table", (t) => ({
    text: t.text().primaryKey(),
    hex: t.hex(),
    bool: t.boolean(),
    int: t.integer(),
    int8Number: t.int8({ mode: "number" }),
    int8Bigint: t.int8({ mode: "bigint" }),
    real: t.real(),
    doublePrecision: t.doublePrecision(),
    numeric: t.numeric(),
    numericArray: t.numeric().array(),
    bigint: t.bigint(),
    bigintArray: t.bigint().array(),
    enum: simpleEnum(),
    enumArray: simpleEnum().array(),
  }));
  const schema = { simpleEnum, table };
  await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const graphqlSchema = buildGraphQLSchema({ schema });
  const typeMap = graphqlSchema.getTypeMap();
  const tableFilterType = typeMap.tableFilter!;
  const fields = (tableFilterType.toConfig() as any).fields as Record<
    string,
    { name: string; type: GraphQLType }
  >;
  const fieldsPretty = Object.entries(fields).reduce<Record<string, any>>(
    (acc, [key, value]) => {
      acc[key] = value.type.toString();
      return acc;
    },
    {},
  );
  expect(fieldsPretty).toMatchObject({
    AND: "[tableFilter]",
    OR: "[tableFilter]",
    text: "String",
    text_not: "String",
    text_in: "[String]",
    text_not_in: "[String]",
    text_contains: "String",
    text_not_contains: "String",
    text_starts_with: "String",
    text_ends_with: "String",
    text_not_starts_with: "String",
    text_not_ends_with: "String",
    hex: "String",
    hex_not: "String",
    hex_in: "[String]",
    hex_not_in: "[String]",
    hex_contains: "String",
    hex_not_contains: "String",
    hex_starts_with: "String",
    hex_ends_with: "String",
    hex_not_starts_with: "String",
    hex_not_ends_with: "String",
    bool: "Boolean",
    bool_not: "Boolean",
    bool_in: "[Boolean]",
    bool_not_in: "[Boolean]",
    int: "Int",
    int_not: "Int",
    int_in: "[Int]",
    int_not_in: "[Int]",
    int_gt: "Int",
    int_lt: "Int",
    int_gte: "Int",
    int_lte: "Int",
    int8Number: "Int",
    int8Number_not: "Int",
    int8Number_in: "[Int]",
    int8Number_not_in: "[Int]",
    int8Number_gt: "Int",
    int8Number_lt: "Int",
    int8Number_gte: "Int",
    int8Number_lte: "Int",
    int8Bigint: "BigInt",
    int8Bigint_not: "BigInt",
    int8Bigint_in: "[BigInt]",
    int8Bigint_not_in: "[BigInt]",
    int8Bigint_gt: "BigInt",
    int8Bigint_lt: "BigInt",
    int8Bigint_gte: "BigInt",
    int8Bigint_lte: "BigInt",
    real: "Float",
    real_not: "Float",
    real_in: "[Float]",
    real_not_in: "[Float]",
    real_gt: "Float",
    real_lt: "Float",
    real_gte: "Float",
    real_lte: "Float",
    doublePrecision: "Float",
    doublePrecision_not: "Float",
    doublePrecision_in: "[Float]",
    doublePrecision_not_in: "[Float]",
    doublePrecision_gt: "Float",
    doublePrecision_lt: "Float",
    doublePrecision_gte: "Float",
    doublePrecision_lte: "Float",
    numeric: "Numeric",
    numeric_not: "Numeric",
    numeric_in: "[Numeric]",
    numeric_not_in: "[Numeric]",
    numeric_gt: "Numeric",
    numeric_lt: "Numeric",
    numeric_gte: "Numeric",
    numeric_lte: "Numeric",
    numericArray: "[Numeric]",
    numericArray_not: "[Numeric]",
    numericArray_has: "Numeric",
    numericArray_not_has: "Numeric",
    bigint: "BigInt",
    bigint_not: "BigInt",
    bigint_in: "[BigInt]",
    bigint_not_in: "[BigInt]",
    bigint_gt: "BigInt",
    bigint_lt: "BigInt",
    bigint_gte: "BigInt",
    bigint_lte: "BigInt",
    bigintArray: "[BigInt]",
    bigintArray_not: "[BigInt]",
    bigintArray_has: "BigInt",
    bigintArray_not_has: "BigInt",
    enum: "simpleEnum",
    enum_not: "simpleEnum",
    enum_in: "[simpleEnum]",
    enum_not_in: "[simpleEnum]",
    enumArray: "[simpleEnum]",
    enumArray_not: "[simpleEnum]",
    enumArray_has: "simpleEnum",
    enumArray_not_has: "simpleEnum",
  });
});
test("filter universal", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.bigint().primaryKey(),
  }));
  const schema = { person };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.person)
    .values([{ id: 1n }, { id: 2n }, { id: 3n }]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      persons(where: { id: "1" }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({ persons: { items: [{ id: "1" }] } });
  result = await query(`
    query {
      persons(where: { id_not: "1" }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "2" }, { id: "3" }] },
  });
});
test("filter null equality", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.bigint().primaryKey(),
    nullable: t.text(),
  }));
  const schema = { person };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.person)
    .values([{ id: 1n, nullable: "a" }, { id: 2n, nullable: "b" }, { id: 3n }]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      persons(where: { nullable: null }) {
        items {
          id
          nullable
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "3", nullable: null }] },
  });
  result = await query(`
    query {
      persons(where: { nullable_not: null }) {
        items {
          id
          nullable
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: {
      items: [
        { id: "1", nullable: "a" },
        { id: "2", nullable: "b" },
      ],
    },
  });
});
test("filter singular", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.hex().primaryKey(),
  }));
  const schema = { person };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw
    .insert(schema.person)
    .values([{ id: "0x01" }, { id: "0x02" }, { id: "0x03" }]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      persons(where: { id_in: ["0x01", "0x02"] }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "0x01" }, { id: "0x02" }] },
  });
  result = await query(`
    query {
      persons(where: { id_not_in: ["0x01", "0x02"] }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "0x03" }] },
  });
});
test("filter plural", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
    number: t.integer().array().notNull(),
  }));
  const schema = { person };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.person).values([
    { id: "1", number: [1, 2, 3] },
    { id: "2", number: [3, 4, 5] },
    { id: "3", number: [5, 6, 7] },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      persons(where: { number: [1, 2, 3] }) {
        items {
          id
          number
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "1", number: [1, 2, 3] }] },
  });
  result = await query(`
    query {
      persons(where: { number_not: [5] }) {
        items {
          id
          number
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: {
      items: [
        { id: "1", number: [1, 2, 3] },
        { id: "2", number: [3, 4, 5] },
        { id: "3", number: [5, 6, 7] },
      ],
    },
  });
  result = await query(`
    query {
      persons(where: { number_has: 3 }) {
        items {
          id
          number
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: {
      items: [
        { id: "1", number: [1, 2, 3] },
        { id: "2", number: [3, 4, 5] },
      ],
    },
  });
  result = await query(`
    query {
      persons(where: { number_not_has: 4 }) {
        items {
          id
          number
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: {
      items: [
        { id: "1", number: [1, 2, 3] },
        { id: "3", number: [5, 6, 7] },
      ],
    },
  });
});
test("filter numeric", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
    number: t.integer(),
    bigintNumber: t.int8({ mode: "number" }),
    bigintBigint: t.int8({ mode: "bigint" }),
    float: t.doublePrecision(),
    bigint: t.bigint(),
    numeric: t.numeric(),
  }));
  const schema = { person };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.person).values([
    {
      id: "1",
      number: 1,
      bigintNumber: 1,
      bigintBigint: 1n,
      float: 1.5,
      bigint: 1n,
      numeric: "1",
    },
    {
      id: "2",
      number: 2,
      bigintNumber: 2,
      bigintBigint: 2n,
      float: 2.5,
      bigint: 2n,
      numeric: "2",
    },
    {
      id: "3",
      number: 3,
      bigintNumber: 3,
      bigintBigint: 3n,
      float: 3.5,
      bigint: 3n,
      numeric: "3",
    },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      persons(where: { number_gt: 1 }) {
        items {
          id
          number
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "2" }, { id: "3" }] },
  });
  result = await query(`
    query {
      persons(where: { bigintNumber_lte: 1 }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "1" }] },
  });
  result = await query(`
    query {
      persons(where: { bigintBigint_lte: "1" }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "1" }] },
  });
  result = await query(`
    query {
      persons(where: { float_lt: 3.5 }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "1" }, { id: "2" }] },
  });
  result = await query(`
    query {
      persons(where: { bigint_gte: "2" }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "2" }, { id: "3" }] },
  });
  result = await query(`
    query {
      persons(where: { numeric_gte: "2.1" }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "3" }] },
  });
});
test("filter string", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
    text: t.text(),
    hex: t.hex(),
  }));
  const schema = { person };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.person).values([
    { id: "1", text: "one", hex: "0xabc" },
    { id: "2", text: "two", hex: "0xcde" },
    { id: "3", text: "three", hex: "0xef0" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      persons(where: { text_starts_with: "o" }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({ persons: { items: [{ id: "1" }] } });
  result = await query(`
    query {
      persons(where: { text_not_ends_with: "e" }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "2" }] },
  });
  result = await query(`
    query {
      persons(where: { hex_contains: "c" }) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "1" }, { id: "2" }] },
  });
});
test("filter and/or", async () => {
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    name: t.text().notNull(),
    bigAge: t.bigint(),
    age: t.integer(),
  }));
  const schema = { pet };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "id1", name: "Skip", bigAge: 105n },
    { id: "id2", name: "Foo", bigAge: 10n },
    { id: "id3", name: "Bar", bigAge: 190n },
    { id: "id4", name: "Zarbar" },
    { id: "id5", name: "Winston", age: 12 },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      pets(where: { OR: [{ bigAge_gt: "50" }, { AND: [{ name: "Foo" }, { bigAge_lt: "20" }] }] }) {
        items {
          id
          name
          bigAge
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  // @ts-ignore
  expect(result.data.pets.items).toMatchObject([
    { id: "id1", name: "Skip", bigAge: "105" },
    { id: "id2", name: "Foo", bigAge: "10" },
    { id: "id3", name: "Bar", bigAge: "190" },
  ]);
});
test("order by", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
    integer: t.integer(),
    bigintBigint: t.int8({ mode: "bigint" }),
    float: t.doublePrecision(),
    bigint: t.bigint(),
    hex: t.hex(),
  }));
  const schema = { person };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.person).values([
    {
      id: "1",
      integer: 1,
      bigintBigint: 1n,
      float: 1.5,
      bigint: 1n,
      hex: "0xa",
    },
    {
      id: "2",
      integer: 2,
      bigintBigint: 2n,
      float: 2.5,
      bigint: 3n,
      hex: "0xc",
    },
    {
      id: "3",
      integer: 3,
      bigintBigint: 3n,
      float: 3.5,
      bigint: 2n,
      hex: "0xb",
    },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      persons(orderBy: "integer", orderDirection: "desc") {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "3" }, { id: "2" }, { id: "1" }] },
  });
  result = await query(`
    query {
      persons(orderBy: "bigintBigint", orderDirection: "desc") {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "3" }, { id: "2" }, { id: "1" }] },
  });
  result = await query(`
    query {
      persons(orderBy: "float", orderDirection: "desc") {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "3" }, { id: "2" }, { id: "1" }] },
  });
  result = await query(`
    query {
      persons(orderBy: "bigint", orderDirection: "desc") {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "2" }, { id: "3" }, { id: "1" }] },
  });
  result = await query(`
    query {
      persons(orderBy: "hex", orderDirection: "desc") {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    persons: { items: [{ id: "2" }, { id: "3" }, { id: "1" }] },
  });
});
test("limit", async () => {
  const person = onchainTable("person", (t) => ({
    id: t.text().primaryKey(),
  }));
  const schema = { person };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  for (let i = 0; i < 100; i++) {
    await database.userQB.raw.insert(schema.person).values({ id: String(i) });
  }
  const graphqlSchema = buildGraphQLSchema({ schema });
  // Default limit of 50
  let result = await query(`
    query {
      persons {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  // @ts-ignore
  expect(result.data.persons.items).toHaveLength(50);
  // Custom limit (below max)
  result = await query(`
    query {
      persons(limit: 75) {
        items {
          id
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  // @ts-ignore
  expect(result.data.persons.items).toHaveLength(75);
  // Custom limit (above max)
  result = await query(`
    query {
      persons(limit: 1005) {
        items {
          id
        }
      }
    }
  `);
  // @ts-ignore
  expect(result.errors?.[0]?.message).toBe(
    "Invalid limit. Got 1005, expected <=1000.",
  );
});
function clone(result: ExecutionResult): ExecutionResult {
  return JSON.parse(JSON.stringify(result));
}
test("cursor pagination ascending", async () => {
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    name: t.text().notNull(),
  }));
  const schema = { pet };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({
      schema: graphqlSchema,
      contextValue,
      document: parse(source),
    });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "id1", name: "Skip" },
    { id: "id2", name: "Foo" },
    { id: "id3", name: "Bar" },
    { id: "id4", name: "Zarbar" },
    { id: "id5", name: "Winston" },
    { id: "id6", name: "Book" },
    { id: "id7", name: "Shea" },
    { id: "id8", name: "Snack" },
    { id: "id9", name: "Last" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      pets(orderBy: "id", orderDirection: "asc", limit: 5) {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    pets: {
      items: [
        { id: "id1", name: "Skip" },
        { id: "id2", name: "Foo" },
        { id: "id3", name: "Bar" },
        { id: "id4", name: "Zarbar" },
        { id: "id5", name: "Winston" },
      ],
      pageInfo: {
        hasNextPage: true,
        hasPreviousPage: false,
        startCursor: expect.any(String),
        endCursor: expect.any(String),
      },
      totalCount: 9,
    },
  });
  // @ts-ignore
  const endCursor = result.data.pets.pageInfo.endCursor;
  result = await query(`
    query {
      pets(orderBy: "id", orderDirection: "asc", after: "${endCursor}") {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    pets: {
      items: [
        { id: "id6", name: "Book" },
        { id: "id7", name: "Shea" },
        { id: "id8", name: "Snack" },
        { id: "id9", name: "Last" },
      ],
      pageInfo: {
        hasNextPage: false,
        hasPreviousPage: true,
        startCursor: expect.any(String),
        endCursor: expect.any(String),
      },
      totalCount: 9,
    },
  });
  // @ts-ignore
  const startCursor = result.data.pets.pageInfo.startCursor;
  result = await query(`
    query {
      pets(orderBy: "id", orderDirection: "asc", before: "${startCursor}", limit: 2) {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    pets: {
      items: [
        { id: "id4", name: "Zarbar" },
        { id: "id5", name: "Winston" },
      ],
      pageInfo: {
        hasNextPage: true,
        hasPreviousPage: true,
        startCursor: expect.any(String),
        endCursor: expect.any(String),
      },
      totalCount: 9,
    },
  });
});
test("cursor pagination descending", async () => {
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    name: t.text().notNull(),
    bigAge: t.bigint(),
    age: t.integer(),
  }));
  const schema = { pet };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "id1", name: "Skip", bigAge: 105n },
    { id: "id2", name: "Foo", bigAge: 10n },
    { id: "id3", name: "Bar", bigAge: 190n },
    { id: "id4", name: "Zarbar" },
    { id: "id5", name: "Winston", age: 12 },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      pets(orderBy: "name", orderDirection: "desc", limit: 2) {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    pets: {
      items: [
        { id: "id4", name: "Zarbar" },
        { id: "id5", name: "Winston" },
      ],
      pageInfo: {
        hasNextPage: true,
        hasPreviousPage: false,
        startCursor: expect.any(String),
        endCursor: expect.any(String),
      },
      totalCount: 5,
    },
  });
  // @ts-ignore
  const endCursor = result.data.pets.pageInfo.endCursor;
  result = await query(`
    query {
      pets(orderBy: "name", orderDirection: "desc", after: "${endCursor}") {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    pets: {
      items: [
        { id: "id1", name: "Skip" },
        { id: "id2", name: "Foo" },
        { id: "id3", name: "Bar" },
      ],
      pageInfo: {
        hasNextPage: false,
        hasPreviousPage: true,
        startCursor: expect.any(String),
        endCursor: expect.any(String),
      },
      totalCount: 5,
    },
  });
  // @ts-ignore
  const startCursor = result.data.pets.pageInfo.startCursor;
  result = await query(`
    query {
      pets(orderBy: "name", orderDirection: "desc", before: "${startCursor}", limit: 1) {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    pets: {
      items: [{ id: "id5", name: "Winston" }],
      pageInfo: {
        hasNextPage: true,
        hasPreviousPage: true,
        startCursor: expect.any(String),
        endCursor: expect.any(String),
      },
      totalCount: 5,
    },
  });
});
test("cursor pagination start and end cursors", async () => {
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    name: t.text().notNull(),
    bigAge: t.bigint(),
    age: t.integer(),
  }));
  const schema = { pet };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "id1", name: "Skip", bigAge: 105n },
    { id: "id2", name: "Foo", bigAge: 10n },
    { id: "id3", name: "Bar", bigAge: 190n },
    { id: "id4", name: "Zarbar" },
    { id: "id5", name: "Winston", age: 12 },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      pets(orderBy: "name", orderDirection: "asc") {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  // Should return start and end cursors when returning full result
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    pets: {
      items: [
        { id: "id3", name: "Bar" },
        { id: "id2", name: "Foo" },
        { id: "id1", name: "Skip" },
        { id: "id5", name: "Winston" },
        { id: "id4", name: "Zarbar" },
      ],
      pageInfo: {
        startCursor: expect.any(String),
        endCursor: expect.any(String),
        hasPreviousPage: false,
        hasNextPage: false,
      },
      totalCount: 5,
    },
  });
});
test("cursor pagination has previous page", async () => {
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    name: t.text().notNull(),
    bigAge: t.bigint(),
    age: t.integer(),
  }));
  const schema = { pet };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "id1", name: "Skip", bigAge: 105n },
    { id: "id2", name: "Foo", bigAge: 10n },
    { id: "id3", name: "Bar", bigAge: 190n },
    { id: "id4", name: "Zarbar" },
    { id: "id5", name: "Winston", age: 12 },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      pets(orderBy: "name", orderDirection: "asc") {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  // @ts-ignore
  const endCursor = result.data.pets.pageInfo.endCursor;
  result = await query(`
    query {
      pets(orderBy: "name", orderDirection: "asc", after: "${endCursor}") {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    pets: {
      items: [],
      pageInfo: {
        startCursor: null,
        endCursor: null,
        // Should return true even if the current page is empty
        hasPreviousPage: true,
        hasNextPage: false,
      },
      totalCount: 5,
    },
  });
});
test("cursor pagination composite primary key", async () => {
  const allowance = onchainTable(
    "allowance",
    (t) => ({
      owner: t.text().notNull(),
      spender: t.text("speeeeender").notNull(),
      amount: t.bigint().notNull(),
    }),
    (table) => ({
      pk: primaryKey({ columns: [table.owner, table.spender] }),
    }),
  );
  const schema = { allowance };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.allowance).values([
    { owner: "alice", spender: "bob", amount: 100n },
    { owner: "bob", spender: "alice", amount: 400n },
    { owner: "bob", spender: "bill", amount: 500n },
    { owner: "bill", spender: "bill", amount: 600n },
    { owner: "bill", spender: "jenny", amount: 700n },
    { owner: "jenny", spender: "bill", amount: 800n },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      allowances(orderBy: "owner", orderDirection: "asc", limit: 4) {
        items {
          owner
          spender
          amount
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    allowances: {
      items: [
        { owner: "alice", spender: "bob", amount: "100" },
        { owner: "bill", spender: "bill", amount: "600" },
        { owner: "bill", spender: "jenny", amount: "700" },
        { owner: "bob", spender: "alice", amount: "400" },
      ],
      pageInfo: {
        hasNextPage: true,
        hasPreviousPage: false,
        startCursor: expect.any(String),
        endCursor: expect.any(String),
      },
      totalCount: 6,
    },
  });
  // @ts-ignore
  const endCursor = result.data.allowances.pageInfo.endCursor;
  result = await query(`
    query {
      allowances(orderBy: "owner", orderDirection: "asc", after: "${endCursor}") {
        items {
          owner
          spender
          amount
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    allowances: {
      items: [
        { owner: "bob", spender: "bill", amount: "500" },
        { owner: "jenny", spender: "bill", amount: "800" },
      ],
      pageInfo: {
        hasNextPage: false,
        hasPreviousPage: true,
        startCursor: expect.any(String),
        endCursor: expect.any(String),
      },
      totalCount: 6,
    },
  });
  // @ts-ignore
  const startCursor = result.data.allowances.pageInfo.startCursor;
  result = await query(`
    query {
      allowances(orderBy: "owner", orderDirection: "asc", before: "${startCursor}", limit: 2) {
        items {
          owner
          spender
          amount
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchObject({
    allowances: {
      items: [
        { owner: "bill", spender: "jenny", amount: "700" },
        { owner: "bob", spender: "alice", amount: "400" },
      ],
      pageInfo: {
        hasNextPage: true,
        hasPreviousPage: true,
        startCursor: expect.any(String),
        endCursor: expect.any(String),
      },
      totalCount: 6,
    },
  });
});
test("cursor pagination with date order", async () => {
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    name: t.text().notNull(),
    createdAt: t.date({ mode: "date" }).notNull(),
  }));
  const schema = { pet };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "id1", name: "Skip", createdAt: new Date("2021-01-01") },
    { id: "id2", name: "Foo", createdAt: new Date("2021-01-02") },
    { id: "id3", name: "Bar", createdAt: new Date("2021-01-03") },
    { id: "id4", name: "Zarbar", createdAt: new Date("2021-01-04") },
    { id: "id5", name: "Winston", createdAt: new Date("2021-01-05") },
    { id: "id6", name: "Book", createdAt: new Date("2021-01-06") },
    { id: "id7", name: "Shea", createdAt: new Date("2021-01-07") },
    { id: "id8", name: "Snack", createdAt: new Date("2021-01-08") },
    { id: "id9", name: "Last", createdAt: new Date("2021-01-09") },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      pets(orderBy: "createdAt", orderDirection: "asc", limit: 5) {
        items {
          id
          name
          createdAt
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchInlineSnapshot(`
    {
      "pets": {
        "items": [
          {
            "createdAt": "1609459200000",
            "id": "id1",
            "name": "Skip",
          },
          {
            "createdAt": "1609545600000",
            "id": "id2",
            "name": "Foo",
          },
          {
            "createdAt": "1609632000000",
            "id": "id3",
            "name": "Bar",
          },
          {
            "createdAt": "1609718400000",
            "id": "id4",
            "name": "Zarbar",
          },
          {
            "createdAt": "1609804800000",
            "id": "id5",
            "name": "Winston",
          },
        ],
        "pageInfo": {
          "endCursor": "eyJqc29uIjp7ImNyZWF0ZWRBdCI6IjIw...",
          "hasNextPage": true,
          "hasPreviousPage": false,
          "startCursor": "eyJqc29uIjp7ImNyZWF0ZWRBdCI6IjIw...",
        },
        "totalCount": 9,
      },
    }
  `);
  // @ts-ignore
  const endCursor = result.data.pets.pageInfo.endCursor;
  result = await query(`
    query {
      pets(orderBy: "createdAt", orderDirection: "asc", after: "${endCursor}") {
        items {
          id
          name
          createdAt
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
          startCursor
          endCursor
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(clone(result).data).toMatchInlineSnapshot(`
    {
      "pets": {
        "items": [
          {
            "createdAt": "1609891200000",
            "id": "id6",
            "name": "Book",
          },
          {
            "createdAt": "1609977600000",
            "id": "id7",
            "name": "Shea",
          },
          {
            "createdAt": "1610064000000",
            "id": "id8",
            "name": "Snack",
          },
          {
            "createdAt": "1610150400000",
            "id": "id9",
            "name": "Last",
          },
        ],
        "pageInfo": {
          "endCursor": "eyJqc29uIjp7ImNyZWF0ZWRBdCI6IjIw...",
          "hasNextPage": false,
          "hasPreviousPage": true,
          "startCursor": "eyJqc29uIjp7ImNyZWF0ZWRBdCI6IjIw...",
        },
        "totalCount": 9,
      },
    }
  `);
});
test("column casing", async () => {
  const schema = {
    table: onchainTable("table", (t) => ({
      id: t.text().primaryKey(),
      userName: t.text("user_name"),
      camelCase: t.text(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.table).values({
    id: "0",
    userName: "0",
    camelCase: "0",
  });
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      table(id: "0") {
        id
        userName
        camelCase
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    table: {
      id: "0",
      userName: "0",
      camelCase: "0",
    },
  });
});
test("snake case table and column names with where clause", async () => {
  const schema = {
    deposited_token: onchainTable(
      "deposited_token",
      (t) => ({
        chain_id: t.bigint().notNull(),
        token_address: t.hex().notNull(),
        first_seen_at: t.bigint().notNull(),
        total_supply: t.bigint().notNull(),
      }),
      (table) => ({
        pk: primaryKey({ columns: [table.token_address, table.chain_id] }),
      }),
    ),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.deposited_token).values({
    chain_id: 1n,
    token_address: "0x0000000000000000000000000000000000000000",
    first_seen_at: 0n,
    total_supply: 0n,
  });
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      deposited_token(token_address: "0x0000000000000000000000000000000000000000", chain_id: "1") {
        chain_id
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    deposited_token: {
      chain_id: "1",
    },
  });
});
test("singular with hex primary key uses case insensitive where", async () => {
  const account = onchainTable("account", (t) => ({
    address: t.hex().primaryKey(),
  }));
  const schema = { account };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  const CHECKSUM_ADDRESS = "0x67BD7c89B54Fa52826186A57363A9303DB3E7626";
  const LOWERCASE_ADDRESS = "0x67bd7c89b54fa52826186a57363a9303db3e7626";
  await database.userQB.raw
    .insert(schema.account)
    .values({ address: CHECKSUM_ADDRESS });
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      account(address: "${CHECKSUM_ADDRESS}") {
        address
      }
      accounts(where: { address: "${CHECKSUM_ADDRESS}" }) {
        items {
          address
        }
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    account: {
      address: LOWERCASE_ADDRESS,
    },
    accounts: {
      items: [
        {
          address: LOWERCASE_ADDRESS,
        },
      ],
    },
  });
});
test("view", async () => {
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    name: t.text().notNull(),
  }));
  const petView = onchainView("pet_view").as((qb) => qb.select().from(pet));
  const schema = { pet, petView };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "id1", name: "Skip" },
    { id: "id2", name: "Foo" },
    { id: "id3", name: "Bar" },
    { id: "id4", name: "Zarbar" },
    { id: "id5", name: "Winston" },
    { id: "id6", name: "Book" },
    { id: "id7", name: "Shea" },
    { id: "id8", name: "Snack" },
    { id: "id9", name: "Last" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      petViews(orderBy: "id", orderDirection: "asc", limit: 5) {
        items {
          id
          name
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    petViews: {
      items: [
        { id: "id1", name: "Skip" },
        { id: "id2", name: "Foo" },
        { id: "id3", name: "Bar" },
        { id: "id4", name: "Zarbar" },
        { id: "id5", name: "Winston" },
      ],
      totalCount: 9,
    },
  });
});
test("view with alias", async () => {
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    name: t.text().notNull(),
  }));
  const petView = onchainView("pet_view").as((qb) =>
    qb
      .select({
        id: pet.id,
        name: pet.name,
        count: count().as("count"),
      })
      .from(pet)
      .groupBy(pet.id),
  );
  const schema = { pet, petView };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "id1", name: "Skip" },
    { id: "id2", name: "Foo" },
    { id: "id3", name: "Bar" },
    { id: "id4", name: "Zarbar" },
    { id: "id5", name: "Winston" },
    { id: "id6", name: "Book" },
    { id: "id7", name: "Shea" },
    { id: "id8", name: "Snack" },
    { id: "id9", name: "Last" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  const result = await query(`
    query {
      petViews(orderBy: "id", orderDirection: "asc", limit: 5) {
        items {
          id
          name
          count
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    petViews: {
      items: [
        { id: "id1", name: "Skip", count: 1 },
        { id: "id2", name: "Foo", count: 1 },
        { id: "id3", name: "Bar", count: 1 },
        { id: "id4", name: "Zarbar", count: 1 },
        { id: "id5", name: "Winston", count: 1 },
      ],
      totalCount: 9,
    },
  });
});
test("view limit/offset pagination", async () => {
  const pet = onchainTable("pet", (t) => ({
    id: t.text().primaryKey(),
    name: t.text().notNull(),
  }));
  const petView = onchainView("pet_view").as((qb) => qb.select().from(pet));
  const schema = { pet, petView };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const contextValue = buildContextValue(database);
  const query = (source: string) =>
    execute({ schema: graphqlSchema, contextValue, document: parse(source) });
  await database.userQB.raw.insert(schema.pet).values([
    { id: "id1", name: "Skip" },
    { id: "id2", name: "Foo" },
    { id: "id3", name: "Bar" },
    { id: "id4", name: "Zarbar" },
    { id: "id5", name: "Winston" },
    { id: "id6", name: "Book" },
    { id: "id7", name: "Shea" },
    { id: "id8", name: "Snack" },
    { id: "id9", name: "Last" },
  ]);
  const graphqlSchema = buildGraphQLSchema({ schema });
  let result = await query(`
    query {
      petViews(orderBy: "id", orderDirection: "asc", limit: 5) {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    petViews: {
      items: [
        { id: "id1", name: "Skip" },
        { id: "id2", name: "Foo" },
        { id: "id3", name: "Bar" },
        { id: "id4", name: "Zarbar" },
        { id: "id5", name: "Winston" },
      ],
      pageInfo: {
        hasNextPage: true,
        hasPreviousPage: false,
      },
      totalCount: 9,
    },
  });
  result = await query(`
    query {
      petViews(orderBy: "id", orderDirection: "asc", offset: 5) {
        items {
          id
          name
        }
        pageInfo {
          hasNextPage
          hasPreviousPage
        }
        totalCount
      }
    }
  `);
  expect(result.errors?.[0]?.message).toBeUndefined();
  expect(result.data).toMatchObject({
    petViews: {
      items: [
        { id: "id6", name: "Book" },
        { id: "id7", name: "Shea" },
        { id: "id8", name: "Snack" },
        { id: "id9", name: "Last" },
      ],
      pageInfo: {
        hasNextPage: false,
        hasPreviousPage: true,
      },
      totalCount: 9,
    },
  });
});
</file>

<file path="packages/core/src/graphql/index.ts">
import type { QB } from "@/database/queryBuilder.js";
import { getPrimaryKeyColumns } from "@/drizzle/index.js";
import type { OnchainTable } from "@/drizzle/onchain.js";
import { normalizeColumn } from "@/indexing-store/utils.js";
import type { Schema, Status } from "@/internal/types.js";
import { decodeCheckpoint } from "@/utils/checkpoint.js";
import { never } from "@/utils/never.js";
import DataLoader from "dataloader";
import {
  type Column,
  Many,
  One,
  SQL,
  type TableRelationalConfig,
  and,
  arrayContained,
  arrayContains,
  asc,
  count,
  createTableRelationsHelpers,
  desc,
  eq,
  extractTablesRelationalConfig,
  getTableColumns,
  gt,
  gte,
  inArray,
  is,
  isNotNull,
  isNull,
  like,
  lt,
  lte,
  ne,
  not,
  notInArray,
  notLike,
  or,
} from "drizzle-orm";
import {
  PgBigInt53,
  type PgColumn,
  type PgEnum,
  PgEnumColumn,
  PgInteger,
  PgSerial,
  type PgTable,
  PgView,
  getViewConfig,
  isPgEnum,
  isPgView,
} from "drizzle-orm/pg-core";
import {
  GraphQLBoolean,
  GraphQLEnumType,
  type GraphQLFieldConfig,
  type GraphQLFieldConfigMap,
  GraphQLFloat,
  type GraphQLInputFieldConfigMap,
  GraphQLInputObjectType,
  type GraphQLInputType,
  GraphQLInt,
  GraphQLList,
  GraphQLNonNull,
  GraphQLObjectType,
  type GraphQLOutputType,
  type GraphQLResolveInfo,
  GraphQLScalarType,
  GraphQLSchema,
  GraphQLString,
} from "graphql";
import superjson from "superjson";
import { GraphQLJSON } from "./json.js";
type Parent = Record<string, any>;
type Context = {
  qb: QB<{ [key: string]: OnchainTable }>;
  getDataLoader: ReturnType<typeof buildDataLoaderCache>;
};
type PluralArgs = {
  where?: { [key: string]: number | string };
  after?: string;
  before?: string;
  limit?: number;
  offset?: number;
  orderBy?: string;
  orderDirection?: "asc" | "desc";
};
const DEFAULT_LIMIT = 50 as const;
const MAX_LIMIT = 1000 as const;
export function buildGraphQLSchema({
  schema,
}: { schema: Schema }): GraphQLSchema {
  const tablesConfig = extractTablesRelationalConfig(
    schema,
    createTableRelationsHelpers,
  );
  const tables = Object.values(tablesConfig.tables) as TableRelationalConfig[];
  const views = Object.entries(schema).filter((el): el is [string, PgView] =>
    isPgView(el[1]),
  );
  const enums = Object.entries(schema).filter(
    (el): el is [string, PgEnum<[string, ...string[]]>] => isPgEnum(el[1]),
  );
  const enumTypes: Record<string, GraphQLEnumType> = {};
  for (const [enumTsName, enumObject] of enums) {
    // Note that this is keyed by enumName (the SQL name) because that's what is
    // available on the PgEnumColumn type. See `columnToGraphQLCore` for context.
    enumTypes[enumObject.enumName] = new GraphQLEnumType({
      name: enumTsName,
      values: enumObject.enumValues.reduce(
        (acc: Record<string, {}>, cur) => ({ ...acc, [cur]: {} }),
        {},
      ),
    });
  }
  const entityFilterTypes: Record<string, GraphQLInputObjectType> = {};
  for (const table of tables) {
    const filterType = new GraphQLInputObjectType({
      name: `${table.tsName}Filter`,
      fields: () => {
        const filterFields: GraphQLInputFieldConfigMap = {
          // Logical operators
          AND: { type: new GraphQLList(filterType) },
          OR: { type: new GraphQLList(filterType) },
        };
        for (const [columnName, column] of Object.entries(table.columns)) {
          const type = columnToGraphQLCore(column, enumTypes);
          // List fields => universal, plural
          if (type instanceof GraphQLList) {
            const baseType = innerType(type);
            conditionSuffixes.universal.forEach((suffix) => {
              filterFields[`${columnName}${suffix}`] = {
                type: new GraphQLList(baseType),
              };
            });
            conditionSuffixes.plural.forEach((suffix) => {
              filterFields[`${columnName}${suffix}`] = { type: baseType };
            });
          }
          // JSON => no filters.
          // Boolean => universal and singular only.
          // All other scalar => universal, singular, numeric OR string depending on type
          if (
            type instanceof GraphQLScalarType ||
            type instanceof GraphQLEnumType
          ) {
            if (type.name === "JSON") continue;
            conditionSuffixes.universal.forEach((suffix) => {
              filterFields[`${columnName}${suffix}`] = {
                type,
              };
            });
            conditionSuffixes.singular.forEach((suffix) => {
              filterFields[`${columnName}${suffix}`] = {
                type: new GraphQLList(type),
              };
            });
            if (["String", "ID"].includes(type.name)) {
              conditionSuffixes.string.forEach((suffix) => {
                filterFields[`${columnName}${suffix}`] = {
                  type: type,
                };
              });
            }
            if (["Int", "Float", "BigInt", "Numeric"].includes(type.name)) {
              conditionSuffixes.numeric.forEach((suffix) => {
                filterFields[`${columnName}${suffix}`] = {
                  type: type,
                };
              });
            }
          }
        }
        return filterFields;
      },
    });
    entityFilterTypes[table.tsName] = filterType;
  }
  for (const [viewName, view] of views) {
    const viewConfig = getViewConfig(view);
    const filterType = new GraphQLInputObjectType({
      name: `${viewName}Filter`,
      fields: () => {
        const filterFields: GraphQLInputFieldConfigMap = {
          // Logical operators
          AND: { type: new GraphQLList(filterType) },
          OR: { type: new GraphQLList(filterType) },
        };
        for (const [columnName, column] of Object.entries(
          viewConfig.selectedFields,
        )) {
          if (is(column, SQL.Aliased)) {
            // Note: Aliased column filters are not supported by GraphQL because they don't have an associated type.
            continue;
          }
          const type = columnToGraphQLCore(column as PgColumn, enumTypes);
          // List fields => universal, plural
          if (type instanceof GraphQLList) {
            const baseType = innerType(type);
            conditionSuffixes.universal.forEach((suffix) => {
              filterFields[`${columnName}${suffix}`] = {
                type: new GraphQLList(baseType),
              };
            });
            conditionSuffixes.plural.forEach((suffix) => {
              filterFields[`${columnName}${suffix}`] = { type: baseType };
            });
          }
          // JSON => no filters.
          // Boolean => universal and singular only.
          // All other scalar => universal, singular, numeric OR string depending on type
          if (
            type instanceof GraphQLScalarType ||
            type instanceof GraphQLEnumType
          ) {
            if (type.name === "JSON") continue;
            conditionSuffixes.universal.forEach((suffix) => {
              filterFields[`${columnName}${suffix}`] = {
                type,
              };
            });
            conditionSuffixes.singular.forEach((suffix) => {
              filterFields[`${columnName}${suffix}`] = {
                type: new GraphQLList(type),
              };
            });
            if (["String", "ID"].includes(type.name)) {
              conditionSuffixes.string.forEach((suffix) => {
                filterFields[`${columnName}${suffix}`] = {
                  type: type,
                };
              });
            }
            if (["Int", "Float", "BigInt", "Numeric"].includes(type.name)) {
              conditionSuffixes.numeric.forEach((suffix) => {
                filterFields[`${columnName}${suffix}`] = {
                  type: type,
                };
              });
            }
          }
        }
        return filterFields;
      },
    });
    entityFilterTypes[viewName] = filterType;
  }
  const entityTypes: Record<string, GraphQLObjectType<Parent, Context>> = {};
  const entityPageTypes: Record<string, GraphQLObjectType> = {};
  for (const table of tables) {
    entityTypes[table.tsName] = new GraphQLObjectType({
      name: table.tsName,
      fields: () => {
        const fieldConfigMap: GraphQLFieldConfigMap<Parent, Context> = {};
        // Scalar fields
        for (const [columnName, column] of Object.entries(table.columns)) {
          const type = columnToGraphQLCore(column, enumTypes);
          fieldConfigMap[columnName] = {
            type: column.notNull ? new GraphQLNonNull(type) : type,
          };
        }
        // Relations
        const relations = Object.entries(table.relations);
        for (const [relationName, relation] of relations) {
          const referencedTable = tables.find(
            (table) => table.dbName === relation.referencedTableName,
          );
          if (!referencedTable)
            throw new Error(
              `Internal error: Referenced table "${relation.referencedTableName}" not found`,
            );
          const referencedEntityType = entityTypes[referencedTable.tsName];
          const referencedEntityPageType =
            entityPageTypes[referencedTable.tsName];
          const referencedEntityFilterType =
            entityFilterTypes[referencedTable.tsName];
          if (
            referencedEntityType === undefined ||
            referencedEntityPageType === undefined ||
            referencedEntityFilterType === undefined
          )
            throw new Error(
              `Internal error: Referenced entity types not found for table "${referencedTable.tsName}" `,
            );
          if (is(relation, One)) {
            const fields = relation.config?.fields ?? [];
            const references = relation.config?.references ?? [];
            if (fields.length !== references.length) {
              throw new Error(
                "Internal error: Fields and references arrays must be the same length",
              );
            }
            fieldConfigMap[relationName] = {
              // Note: There is a `relation.isNullable` field here but it appears
              // to be internal / incorrect. Until we have support for foreign
              // key constraints, all `one` relations must be nullable.
              type: referencedEntityType,
              resolve: (parent, _args, context) => {
                const loader = context.getDataLoader({
                  table: referencedTable,
                });
                const rowFragment: Record<string, unknown> = {};
                for (let i = 0; i < references.length; i++) {
                  const referenceColumn = references[i]!;
                  const fieldColumn = fields[i]!;
                  const fieldColumnTsName = getColumnTsName(fieldColumn);
                  const referenceColumnTsName =
                    getColumnTsName(referenceColumn);
                  rowFragment[referenceColumnTsName] =
                    parent[fieldColumnTsName];
                }
                const encodedId = encodeRowFragment(rowFragment);
                return loader.load(encodedId);
              },
            };
          } else if (is(relation, Many)) {
            // Search the relations of the referenced table for the corresponding `one` relation.
            // If `relation.relationName` is not provided, use the first `one` relation that references this table.
            let oneRelation: One | undefined;
            // Note: can find the wrong relation if `relationName` is undefined.
            // The first relation will be found.
            if (relation.relationName !== undefined) {
              for (const referencedRelation of Object.values(
                referencedTable.relations,
              )) {
                if (
                  is(referencedRelation, One) &&
                  relation.relationName === referencedRelation.relationName
                ) {
                  oneRelation = referencedRelation;
                  break;
                }
              }
            }
            if (oneRelation === undefined) {
              for (const referencedRelation of Object.values(
                referencedTable.relations,
              )) {
                if (
                  is(referencedRelation, One) &&
                  table.dbName === referencedRelation.referencedTableName
                ) {
                  oneRelation = referencedRelation;
                  break;
                }
              }
            }
            if (oneRelation === undefined) {
              throw new Error(
                `Internal error: Relation "${relationName}" not found in table "${referencedTable.tsName}"`,
              );
            }
            const fields = oneRelation.config?.fields ?? [];
            const references = oneRelation.config?.references ?? [];
            fieldConfigMap[relationName] = {
              type: referencedEntityPageType,
              args: {
                where: { type: referencedEntityFilterType },
                orderBy: { type: GraphQLString },
                orderDirection: { type: GraphQLString },
                before: { type: GraphQLString },
                after: { type: GraphQLString },
                limit: { type: GraphQLInt },
                offset: { type: GraphQLInt },
              },
              resolve: (parent, args: PluralArgs, context, info) => {
                const relationalConditions = [];
                for (let i = 0; i < references.length; i++) {
                  const column = fields[i]!;
                  const value = parent[getColumnTsName(references[i]!)];
                  relationalConditions.push(eq(column, value));
                }
                const includeTotalCount = selectionIncludesField(
                  info,
                  "totalCount",
                );
                return executePluralQuery(
                  schema[referencedTable.tsName] as PgTable,
                  referencedTable.columns,
                  context.qb,
                  args,
                  includeTotalCount,
                  relationalConditions,
                );
              },
            };
          } else {
            throw new Error(
              `Internal error: Relation "${relationName}" is unsupported, expected One or Many`,
            );
          }
        }
        return fieldConfigMap;
      },
    });
    entityPageTypes[table.tsName] = new GraphQLObjectType({
      name: `${table.tsName}Page`,
      fields: () => ({
        items: {
          type: new GraphQLNonNull(
            new GraphQLList(new GraphQLNonNull(entityTypes[table.tsName]!)),
          ),
        },
        pageInfo: { type: new GraphQLNonNull(GraphQLPageInfo) },
        totalCount: { type: new GraphQLNonNull(GraphQLInt) },
      }),
    });
  }
  for (const [viewName, view] of views) {
    entityTypes[viewName] = new GraphQLObjectType({
      name: viewName,
      fields: () => {
        const fieldConfigMap: GraphQLFieldConfigMap<Parent, Context> = {};
        const viewConfig = getViewConfig(view);
        // Scalar fields
        for (const [columnName, column] of Object.entries(
          viewConfig.selectedFields,
        )) {
          if (is(column, SQL.Aliased)) {
            fieldConfigMap[columnName] = { type: GraphQLJSON };
          } else {
            const type = columnToGraphQLCore(column as PgColumn, enumTypes);
            fieldConfigMap[columnName] = {
              type: (column as PgColumn).notNull
                ? new GraphQLNonNull(type)
                : type,
            };
          }
        }
        return fieldConfigMap;
      },
    });
    entityPageTypes[viewName] = new GraphQLObjectType({
      name: `${viewName}Page`,
      fields: () => ({
        items: {
          type: new GraphQLNonNull(
            new GraphQLList(new GraphQLNonNull(entityTypes[viewName]!)),
          ),
        },
        pageInfo: { type: new GraphQLNonNull(GraphQLViewPageInfo) },
        totalCount: { type: new GraphQLNonNull(GraphQLInt) },
      }),
    });
  }
  const queryFields: Record<string, GraphQLFieldConfig<Parent, Context>> = {};
  for (const table of tables) {
    const entityType = entityTypes[table.tsName]!;
    const entityPageType = entityPageTypes[table.tsName]!;
    const entityFilterType = entityFilterTypes[table.tsName]!;
    const singularFieldName =
      table.tsName.charAt(0).toLowerCase() + table.tsName.slice(1);
    const pluralFieldName = `${singularFieldName}s`;
    queryFields[singularFieldName] = {
      type: entityType,
      // Find the primary key columns and GraphQL core types and include them
      // as arguments to the singular query type.
      args: Object.fromEntries(
        table.primaryKey.map((column) => [
          getColumnTsName(column),
          {
            type: new GraphQLNonNull(
              columnToGraphQLCore(column, enumTypes) as GraphQLInputType,
            ),
          },
        ]),
      ),
      resolve: async (_parent, args, context) => {
        const loader = context.getDataLoader({ table });
        // The `args` object here should be a valid `where` argument that
        // uses the `eq` shorthand for each primary key column.
        const encodedId = encodeRowFragment(args);
        return loader.load(encodedId);
      },
    };
    queryFields[pluralFieldName] = {
      type: new GraphQLNonNull(entityPageType),
      args: {
        where: { type: entityFilterType },
        orderBy: { type: GraphQLString },
        orderDirection: { type: GraphQLString },
        before: { type: GraphQLString },
        after: { type: GraphQLString },
        limit: { type: GraphQLInt },
        offset: { type: GraphQLInt },
      },
      resolve: async (_parent, args: PluralArgs, context, info) => {
        const includeTotalCount = selectionIncludesField(info, "totalCount");
        return executePluralQuery(
          schema[table.tsName] as PgTable,
          table.columns,
          context.qb,
          args,
          includeTotalCount,
        );
      },
    };
  }
  for (const [viewName, view] of views) {
    const viewConfig = getViewConfig(view);
    const entityPageType = entityPageTypes[viewName]!;
    const entityFilterType = entityFilterTypes[viewName]!;
    const singularFieldName =
      viewName.charAt(0).toLowerCase() + viewName.slice(1);
    const pluralFieldName = `${singularFieldName}s`;
    queryFields[pluralFieldName] = {
      type: new GraphQLNonNull(entityPageType),
      args: {
        where: { type: entityFilterType },
        orderBy: { type: GraphQLString },
        orderDirection: { type: GraphQLString },
        limit: { type: GraphQLInt },
        offset: { type: GraphQLInt },
      },
      resolve: async (_parent, args: PluralArgs, context, info) => {
        const includeTotalCount = selectionIncludesField(info, "totalCount");
        return executePluralQuery(
          view,
          viewConfig.selectedFields as Record<string, Column>,
          context.qb,
          args,
          includeTotalCount,
        );
      },
    };
  }
  queryFields._meta = {
    type: GraphQLMeta,
    resolve: async (_source, _args, context) => {
      // Note: This is done to avoid non-browser compatible dependencies
      const checkpoints = (await context.qb
        .wrap({ label: "select_checkpoints" }, (db) =>
          db.execute(
            "SELECT chain_name, chain_id, latest_checkpoint, safe_checkpoint from _ponder_checkpoint",
          ),
        )
        .then((res) => res.rows)) as {
        chain_name: string;
        chain_id: string;
        latest_checkpoint: string;
        safe_checkpoint: string;
      }[];
      const status: Status = {};
      for (const {
        chain_name,
        chain_id,
        latest_checkpoint,
      } of checkpoints.sort((a, b) => (a.chain_id > b.chain_id ? 1 : -1))) {
        status[chain_name] = {
          id: Number(chain_id),
          block: {
            number: Number(decodeCheckpoint(latest_checkpoint).blockNumber),
            timestamp: Number(
              decodeCheckpoint(latest_checkpoint).blockTimestamp,
            ),
          },
        };
      }
      return { status };
    },
  };
  return new GraphQLSchema({
    // Include these here so they are listed first in the printed schema.
    types: [
      GraphQLJSON,
      GraphQLBigInt,
      GraphQLPageInfo,
      GraphQLViewPageInfo,
      GraphQLMeta,
    ],
    query: new GraphQLObjectType({
      name: "Query",
      fields: queryFields,
    }),
  });
}
const GraphQLPageInfo = new GraphQLObjectType({
  name: "PageInfo",
  fields: {
    hasNextPage: { type: new GraphQLNonNull(GraphQLBoolean) },
    hasPreviousPage: { type: new GraphQLNonNull(GraphQLBoolean) },
    startCursor: { type: GraphQLString },
    endCursor: { type: GraphQLString },
  },
});
const GraphQLViewPageInfo = new GraphQLObjectType({
  name: "ViewPageInfo",
  fields: {
    hasNextPage: { type: new GraphQLNonNull(GraphQLBoolean) },
    hasPreviousPage: { type: new GraphQLNonNull(GraphQLBoolean) },
  },
});
const GraphQLBigInt = new GraphQLScalarType({
  name: "BigInt",
  serialize: (value) => String(value),
  parseValue: (value) => BigInt(value as any),
  parseLiteral: (value) => {
    if (value.kind === "StringValue") {
      return BigInt(value.value);
    } else {
      throw new Error(
        `Invalid value kind provided for field of type BigInt: ${value.kind}. Expected: StringValue`,
      );
    }
  },
});
const GraphQLNumeric = new GraphQLScalarType({
  name: "Numeric",
  serialize: (value) => String(value),
  parseValue: (value) => String(value as any),
  parseLiteral: (value) => {
    if (value.kind === "StringValue") {
      return String(value.value);
    } else {
      throw new Error(
        `Invalid value kind provided for field of type BigInt: ${value.kind}. Expected: StringValue`,
      );
    }
  },
});
const GraphQLMeta = new GraphQLObjectType({
  name: "Meta",
  fields: { status: { type: GraphQLJSON } },
});
const columnToGraphQLCore = (
  column: Column,
  enumTypes: Record<string, GraphQLEnumType>,
): GraphQLOutputType => {
  if (column.columnType === "PgEvmBigint") {
    return GraphQLBigInt;
  }
  if (column.columnType === "PgNumeric") {
    return GraphQLNumeric;
  }
  if (column instanceof PgEnumColumn) {
    if (column.enum === undefined) {
      throw new Error(
        `Internal error: Expected enum column "${getColumnTsName(column)}" to have an "enum" property`,
      );
    }
    const enumType = enumTypes[column.enum.enumName];
    if (enumType === undefined) {
      throw new Error(
        `Internal error: Expected to find a GraphQL enum named "${column.enum.enumName}". This may happen if "${column.enum.enumName}" has not been exported from your Ponder schema`,
      );
    }
    return enumType;
  }
  switch (column.dataType) {
    case "boolean":
      return GraphQLBoolean;
    case "json":
      return GraphQLJSON;
    case "date":
      return GraphQLString;
    case "string":
      return GraphQLString;
    case "bigint":
      return GraphQLBigInt;
    case "number":
      return is(column, PgInteger) ||
        is(column, PgSerial) ||
        is(column, PgBigInt53)
        ? GraphQLInt
        : GraphQLFloat;
    case "buffer":
      return new GraphQLList(new GraphQLNonNull(GraphQLInt));
    case "array": {
      if (column.columnType === "PgVector") {
        return new GraphQLList(new GraphQLNonNull(GraphQLFloat));
      }
      if (column.columnType === "PgGeometry") {
        return new GraphQLList(new GraphQLNonNull(GraphQLFloat));
      }
      const innerType = columnToGraphQLCore(
        (column as any).baseColumn,
        enumTypes,
      );
      return new GraphQLList(new GraphQLNonNull(innerType));
    }
    default:
      throw new Error(`Type ${column.dataType} is not implemented`);
  }
};
const innerType = (
  type: GraphQLOutputType,
): GraphQLScalarType | GraphQLEnumType => {
  if (type instanceof GraphQLScalarType || type instanceof GraphQLEnumType)
    return type;
  if (type instanceof GraphQLList || type instanceof GraphQLNonNull)
    return innerType(type.ofType);
  throw new Error(`Type ${type.toString()} is not implemented`);
};
async function executePluralQuery(
  relation: PgTable | PgView,
  columns: Record<string, Column>,
  qb: QB<{ [key: string]: OnchainTable }>,
  args: PluralArgs,
  includeTotalCount: boolean,
  extraConditions: (SQL | undefined)[] = [],
) {
  const limit = args.limit ?? DEFAULT_LIMIT;
  const offset = args.offset ?? 0;
  if (limit > MAX_LIMIT) {
    throw new Error(`Invalid limit. Got ${limit}, expected <=${MAX_LIMIT}.`);
  }
  const orderBySchema = buildOrderBySchema(relation, args);
  const orderBy = orderBySchema.map(([columnName, direction]) => {
    const column = columns[columnName];
    if (column === undefined) {
      throw new Error(
        `Unknown column "${columnName}" used in orderBy argument`,
      );
    }
    return direction === "asc" ? asc(column) : desc(column);
  });
  const orderByReversed = orderBySchema.map(([columnName, direction]) => {
    const column = columns[columnName];
    if (column === undefined) {
      throw new Error(
        `Unknown column "${columnName}" used in orderBy argument`,
      );
    }
    return direction === "asc" ? desc(column) : asc(column);
  });
  const whereConditions = buildWhereConditions(args.where, columns);
  const after = args.after ?? null;
  const before = args.before ?? null;
  if (after !== null && before !== null) {
    throw new Error("Cannot specify both before and after cursors.");
  }
  if (after !== null && offset > 0) {
    throw new Error("Cannot specify both after cursor and offset.");
  }
  if (before !== null && offset > 0) {
    throw new Error("Cannot specify both before cursor and offset.");
  }
  let startCursor = null;
  let endCursor = null;
  let hasPreviousPage = false;
  let hasNextPage = false;
  const totalCountPromise = includeTotalCount
    ? qb
        .wrap((db) =>
          db
            .select({ count: count() })
            .from(relation)
            .where(and(...whereConditions, ...extraConditions)),
        )
        .then((rows) => rows[0]?.count ?? null)
    : Promise.resolve(null);
  // Neither cursors are specified, apply the order conditions and execute.
  if (after === null && before === null) {
    const [rows, totalCount] = await Promise.all([
      qb.raw
        .select()
        .from(relation)
        .where(and(...whereConditions, ...extraConditions))
        .orderBy(...orderBy)
        .limit(limit + 1)
        .offset(offset),
      totalCountPromise,
    ]);
    if (rows.length === limit + 1) {
      rows.pop();
      hasNextPage = true;
    }
    if (offset === 0) {
      startCursor =
        rows.length > 0 ? encodeCursor(orderBySchema, rows[0]!) : null;
      endCursor =
        rows.length > 0
          ? encodeCursor(orderBySchema, rows[rows.length - 1]!)
          : null;
    } else {
      hasPreviousPage = true;
    }
    return {
      items: rows,
      totalCount,
      pageInfo: { hasNextPage, hasPreviousPage, startCursor, endCursor },
    };
  }
  if (after !== null) {
    // User specified an 'after' cursor.
    const cursorObject = decodeCursor(after);
    const cursorCondition = buildCursorCondition(
      columns,
      orderBySchema,
      "after",
      cursorObject,
    );
    const [rows, totalCount] = await Promise.all([
      qb.raw
        .select()
        .from(relation)
        .where(and(...whereConditions, cursorCondition, ...extraConditions))
        .orderBy(...orderBy)
        .limit(limit + 2)
        .offset(offset),
      totalCountPromise,
    ]);
    if (rows.length === 0) {
      return {
        items: rows,
        totalCount,
        pageInfo: { hasNextPage, hasPreviousPage, startCursor, endCursor },
      };
    }
    // If the cursor of the first returned record equals the `after` cursor,
    // `hasPreviousPage` is true. Remove that record.
    if (encodeCursor(orderBySchema, rows[0]!) === after) {
      rows.shift();
      hasPreviousPage = true;
    } else {
      // Otherwise, remove the last record.
      rows.pop();
    }
    // Now if the length of the records is still equal to limit + 1,
    // there is a next page.
    if (rows.length === limit + 1) {
      rows.pop();
      hasNextPage = true;
    }
    // Now calculate the cursors.
    startCursor =
      rows.length > 0 ? encodeCursor(orderBySchema, rows[0]!) : null;
    endCursor =
      rows.length > 0
        ? encodeCursor(orderBySchema, rows[rows.length - 1]!)
        : null;
    return {
      items: rows,
      totalCount,
      pageInfo: { hasNextPage, hasPreviousPage, startCursor, endCursor },
    };
  }
  // User specified a 'before' cursor.
  const cursorObject = decodeCursor(before!);
  const cursorCondition = buildCursorCondition(
    columns,
    orderBySchema,
    "before",
    cursorObject,
  );
  // Reverse the order by conditions to get the previous page,
  // then reverse the results back to the original order.
  const [rows, totalCount] = await Promise.all([
    qb.raw
      .select()
      .from(relation)
      .where(and(...whereConditions, cursorCondition, ...extraConditions))
      .orderBy(...orderByReversed)
      .limit(limit + 2)
      .offset(offset)
      .then((rows) => rows.reverse()),
    totalCountPromise,
  ]);
  if (rows.length === 0) {
    return {
      items: rows,
      totalCount,
      pageInfo: { hasNextPage, hasPreviousPage, startCursor, endCursor },
    };
  }
  // If the cursor of the last returned record equals the `before` cursor,
  // `hasNextPage` is true. Remove that record.
  if (encodeCursor(orderBySchema, rows[rows.length - 1]!) === before) {
    rows.pop();
    hasNextPage = true;
  } else {
    // Otherwise, remove the first record.
    rows.shift();
  }
  // Now if the length of the records is equal to limit + 1, we know
  // there is a previous page.
  if (rows.length === limit + 1) {
    rows.shift();
    hasPreviousPage = true;
  }
  // Now calculate the cursors.
  startCursor = rows.length > 0 ? encodeCursor(orderBySchema, rows[0]!) : null;
  endCursor =
    rows.length > 0
      ? encodeCursor(orderBySchema, rows[rows.length - 1]!)
      : null;
  return {
    items: rows,
    totalCount,
    pageInfo: { hasNextPage, hasPreviousPage, startCursor, endCursor },
  };
}
const conditionSuffixes = {
  universal: ["", "_not"],
  singular: ["_in", "_not_in"],
  plural: ["_has", "_not_has"],
  numeric: ["_gt", "_lt", "_gte", "_lte"],
  string: [
    "_contains",
    "_not_contains",
    "_starts_with",
    "_ends_with",
    "_not_starts_with",
    "_not_ends_with",
  ],
} as const;
const conditionSuffixesByLengthDesc = Object.values(conditionSuffixes)
  .flat()
  .sort((a, b) => b.length - a.length);
function buildWhereConditions(
  where: Record<string, any> | undefined,
  columns: Record<string, Column>,
) {
  const conditions: (SQL | undefined)[] = [];
  if (where === undefined) return conditions;
  for (const [whereKey, rawValue] of Object.entries(where)) {
    // Handle the `AND` and `OR` operators
    if (whereKey === "AND" || whereKey === "OR") {
      if (!Array.isArray(rawValue)) {
        throw new Error(
          `Invalid query: Expected an array for the ${whereKey} operator. Got: ${rawValue}`,
        );
      }
      const nestedConditions = rawValue.flatMap((subWhere) =>
        buildWhereConditions(subWhere, columns),
      );
      if (nestedConditions.length > 0) {
        conditions.push(
          whereKey === "AND"
            ? and(...nestedConditions)
            : or(...nestedConditions),
        );
      }
      continue;
    }
    // Search for a valid filter suffix, traversing the list from longest to shortest
    // to avoid ambiguity between cases like `_not_in` and `_in`.
    const conditionSuffix = conditionSuffixesByLengthDesc.find((s) =>
      whereKey.endsWith(s),
    );
    if (conditionSuffix === undefined) {
      throw new Error(
        `Invariant violation: Condition suffix not found for where key ${whereKey}`,
      );
    }
    // Remove the condition suffix and use the remaining string as the column name.
    const columnName = whereKey.slice(
      0,
      whereKey.length - conditionSuffix.length,
    );
    // Validate that the column name is present in the table.
    const column = columns[columnName];
    if (column === undefined) {
      throw new Error(
        `Invalid query: Where clause contains unknown column ${columnName}`,
      );
    }
    switch (conditionSuffix) {
      case "":
        if (column.columnType === "PgArray") {
          conditions.push(
            and(
              arrayContains(column, rawValue),
              arrayContained(column, rawValue),
            ),
          );
        } else {
          if (rawValue === null) {
            conditions.push(isNull(column));
          } else {
            conditions.push(eq(column, rawValue));
          }
        }
        break;
      case "_not":
        if (column.columnType === "PgArray") {
          conditions.push(
            not(
              and(
                arrayContains(column, rawValue),
                arrayContained(column, rawValue),
              )!,
            ),
          );
        } else {
          if (rawValue === null) {
            conditions.push(isNotNull(column));
          } else {
            conditions.push(ne(column, rawValue));
          }
        }
        break;
      case "_in":
        conditions.push(inArray(column, rawValue));
        break;
      case "_not_in":
        conditions.push(notInArray(column, rawValue));
        break;
      case "_has":
        conditions.push(arrayContains(column, [rawValue]));
        break;
      case "_not_has":
        conditions.push(not(arrayContains(column, [rawValue])));
        break;
      case "_gt":
        conditions.push(gt(column, rawValue));
        break;
      case "_lt":
        conditions.push(lt(column, rawValue));
        break;
      case "_gte":
        conditions.push(gte(column, rawValue));
        break;
      case "_lte":
        conditions.push(lte(column, rawValue));
        break;
      case "_contains":
        conditions.push(like(column, `%${rawValue}%`));
        break;
      case "_not_contains":
        conditions.push(notLike(column, `%${rawValue}%`));
        break;
      case "_starts_with":
        conditions.push(like(column, `${rawValue}%`));
        break;
      case "_ends_with":
        conditions.push(like(column, `%${rawValue}`));
        break;
      case "_not_starts_with":
        conditions.push(notLike(column, `${rawValue}%`));
        break;
      case "_not_ends_with":
        conditions.push(notLike(column, `%${rawValue}`));
        break;
      default:
        never(conditionSuffix);
    }
  }
  return conditions;
}
function buildOrderBySchema(relation: PgTable | PgView, args: PluralArgs) {
  // If the user-provided order by does not include the ALL of the ID columns,
  // add any missing ID columns to the end of the order by clause (asc).
  // This ensures a consistent sort order to unblock cursor pagination.
  const userDirection = args.orderDirection ?? "asc";
  const userColumns: [string, "asc" | "desc"][] =
    args.orderBy !== undefined ? [[args.orderBy, userDirection]] : [];
  if (is(relation, PgView)) {
    return userColumns;
  }
  const pkColumns = getPrimaryKeyColumns(relation).map(({ js }) => [
    js,
    userDirection,
  ]);
  const missingPkColumns = pkColumns.filter(
    (pkColumn) =>
      !userColumns.some((userColumn) => userColumn[0] === pkColumn[0]),
  ) as [string, "asc" | "desc"][];
  return [...userColumns, ...missingPkColumns];
}
function encodeCursor(
  orderBySchema: [string, "asc" | "desc"][],
  row: { [k: string]: unknown },
): string {
  const cursorObject = Object.fromEntries(
    orderBySchema.map(([columnName, _]) => [columnName, row[columnName]]),
  );
  return encodeRowFragment(cursorObject);
}
function decodeCursor(cursor: string): { [k: string]: unknown } {
  return decodeRowFragment(cursor);
}
function encodeRowFragment(rowFragment: { [k: string]: unknown }): string {
  return Buffer.from(superjson.stringify(rowFragment)).toString("base64");
}
function decodeRowFragment(encodedRowFragment: string): {
  [k: string]: unknown;
} {
  return superjson.parse(Buffer.from(encodedRowFragment, "base64").toString());
}
function buildCursorCondition(
  columns: Record<string, Column>,
  orderBySchema: [string, "asc" | "desc"][],
  direction: "after" | "before",
  cursorObject: { [k: string]: unknown },
): SQL | undefined {
  const cursorColumns = orderBySchema.map(([columnName, orderDirection]) => {
    const column = columns[columnName];
    if (column === undefined)
      throw new Error(
        `Unknown column "${columnName}" used in orderBy argument`,
      );
    const value = cursorObject[columnName];
    let comparator: typeof gt | typeof lt;
    let comparatorOrEquals: typeof gte | typeof lte;
    if (direction === "after") {
      [comparator, comparatorOrEquals] =
        orderDirection === "asc" ? [gt, gte] : [lt, lte];
    } else {
      [comparator, comparatorOrEquals] =
        orderDirection === "asc" ? [lt, lte] : [gt, gte];
    }
    return { column, value, comparator, comparatorOrEquals };
  });
  const buildCondition = (index: number): SQL | undefined => {
    if (index === cursorColumns.length - 1) {
      const { column, value, comparatorOrEquals } = cursorColumns[index]!;
      return comparatorOrEquals(column, value);
    }
    const currentColumn = cursorColumns[index]!;
    const nextCondition = buildCondition(index + 1);
    return or(
      currentColumn.comparator(currentColumn.column, currentColumn.value),
      and(eq(currentColumn.column, currentColumn.value), nextCondition),
    );
  };
  return buildCondition(0);
}
export function buildDataLoaderCache(qb: QB) {
  const dataLoaderMap = new Map<
    TableRelationalConfig,
    DataLoader<string, any> | undefined
  >();
  return ({ table }: { table: TableRelationalConfig }) => {
    const baseQuery = (qb as QB<{ [key: string]: OnchainTable }>).raw.query[
      table.tsName
    ];
    if (baseQuery === undefined)
      throw new Error(
        `Internal error: Unknown table "${table.tsName}" in data loader cache`,
      );
    let dataLoader = dataLoaderMap.get(table);
    if (dataLoader === undefined) {
      dataLoader = new DataLoader(
        async (encodedIds) => {
          const decodedRowFragments = encodedIds.map(decodeRowFragment);
          // The decoded row fragments should be valid `where` objects
          // which use the `eq` object shorthand for each primary key column.
          const idConditions = decodedRowFragments.map((decodedRowFragment) =>
            and(...buildWhereConditions(decodedRowFragment, table.columns)),
          );
          const rows = await baseQuery.findMany({
            where: or(...idConditions),
            limit: encodedIds.length,
          });
          // Now, we need to order the rows coming out of the database to match
          // the order of the IDs passed in. To accomplish this, we need to do
          // a comparison of the decoded row PK fragments with the database rows.
          // This is tricky because the decoded row PK fragments are not normalized,
          // so some comparisons will fail (eg for our PgHex column type).
          // To fix this, we need to normalize the values before doing the comparison.
          return (
            decodedRowFragments
              // Normalize the decoded row fragments
              .map((fragment) =>
                Object.fromEntries(
                  Object.entries(fragment).map(([col, val]) => {
                    const column = table.columns[col];
                    if (column === undefined) {
                      throw new Error(
                        `Unknown column '${table.tsName}.${col}' used in dataloader row ID fragment`,
                      );
                    }
                    return [col, normalizeColumn(column, val, false)];
                  }),
                ),
              )
              // Find the database row corresponding to each normalized row fragment
              .map((fragment) =>
                rows.find((row) =>
                  Object.entries(fragment).every(
                    ([col, val]) => row[col] === val,
                  ),
                ),
              )
          );
        },
        { maxBatchSize: 1_000 },
      );
      dataLoaderMap.set(table, dataLoader);
    }
    return dataLoader;
  };
}
function getColumnTsName(column: Column) {
  const tableColumns = getTableColumns(column.table);
  return Object.entries(tableColumns).find(
    ([_, c]) => c.name === column.name,
  )![0];
}
/**
 * Returns `true` if the query includes a specific field.
 * Does not consider nested selections; only works one "layer" deep.
 */
function selectionIncludesField(
  info: GraphQLResolveInfo,
  fieldName: string,
): boolean {
  for (const fieldNode of info.fieldNodes) {
    for (const selection of fieldNode.selectionSet?.selections ?? []) {
      if (selection.kind === "Field" && selection.name.value === fieldName) {
        return true;
      }
    }
  }
  return false;
}
</file>

<file path="packages/core/src/graphql/json.ts">
// Modified from https://github.com/taion/graphql-type-json/blob/master/src/index.js
import {
  type GraphQLScalarLiteralParser,
  GraphQLScalarType,
  Kind,
  type ObjectValueNode,
  type ValueNode,
  print,
} from "graphql";
export const GraphQLJSON = new GraphQLScalarType({
  name: "JSON",
  description:
    "The `JSON` scalar type represents JSON values as specified by [ECMA-404](http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf).",
  serialize: (x) => x,
  parseValue: (x) => x,
  parseLiteral: (ast, variables) => {
    if (ast.kind !== Kind.OBJECT) {
      throw new TypeError(
        `JSONObject cannot represent non-object value: ${print(ast)}`,
      );
    }
    return parseObject(ast, variables);
  },
});
const parseLiteral = (
  ast: ValueNode,
  variables: Parameters<GraphQLScalarType["parseLiteral"]>[1],
): ReturnType<GraphQLScalarLiteralParser<unknown>> => {
  switch (ast.kind) {
    case Kind.STRING:
    case Kind.BOOLEAN:
      return ast.value;
    case Kind.INT:
    case Kind.FLOAT:
      return Number.parseFloat(ast.value);
    case Kind.OBJECT:
      return parseObject(ast, variables);
    case Kind.LIST:
      return ast.values.map((n) => parseLiteral(n, variables));
    case Kind.NULL:
      return null;
    case Kind.VARIABLE:
      return variables ? variables[ast.name.value] : undefined;
    default:
      throw new TypeError(`JSON cannot represent value: ${print(ast)}`);
  }
};
const parseObject = (
  ast: ObjectValueNode,
  variables: Parameters<GraphQLScalarType["parseLiteral"]>[1],
) => {
  const value = Object.create(null);
  ast.fields.forEach((field) => {
    value[field.name.value] = parseLiteral(field.value, variables);
  });
  return value;
};
</file>

<file path="packages/core/src/graphql/middleware.test.ts">
import {
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { onchainTable } from "@/drizzle/onchain.js";
import { Hono } from "hono";
import { beforeEach, expect, test } from "vitest";
import { graphql } from "./middleware.js";
beforeEach(setupCommon);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
test("middleware serves request", async () => {
  const schema = {
    table: onchainTable("table", (t) => ({
      id: t.text().primaryKey(),
      string: t.text(),
      int: t.integer(),
      float: t.doublePrecision(),
      boolean: t.boolean(),
      hex: t.hex(),
      bigint: t.bigint(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  globalThis.PONDER_DATABASE = database;
  await database.userQB.raw.insert(schema.table).values({
    id: "0",
    string: "0",
    int: 0,
    float: 0,
    boolean: false,
    hex: "0x0",
    bigint: 0n,
  });
  const app = new Hono().use(
    "/graphql",
    graphql({ schema, db: database.readonlyQB.raw }),
  );
  const response = await app.request("/graphql", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      query: `
      query {
        table(id: "0") {
          id
          string
          int
          float
          boolean
          hex
          bigint
        }
      }
    `,
    }),
  });
  expect(response.status).toBe(200);
  expect(JSON.parse(await response.text())).toMatchObject({
    data: {
      table: {
        id: "0",
        string: "0",
        int: 0,
        float: 0,
        boolean: false,
        hex: "0x00",
        bigint: "0",
      },
    },
  });
});
test("middleware supports path other than /graphql using hono routing", async () => {
  const schema = {
    table: onchainTable("table", (t) => ({ id: t.text().primaryKey() })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  globalThis.PONDER_DATABASE = database;
  await database.userQB.raw.insert(schema.table).values({
    id: "0",
  });
  const app = new Hono().use(
    "/not-graphql/**",
    graphql({ schema, db: database.readonlyQB.raw }),
  );
  const response = await app.request("/not-graphql/at-all", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      query: `query { table(id: "0") { id } }`,
    }),
  });
  expect(response.status).toBe(200);
  expect(await response.json()).toMatchObject({
    data: { table: { id: "0" } },
  });
});
test("middleware throws error when extra filter is applied", async () => {
  const schema = {
    table: onchainTable("table", (t) => ({
      id: t.text().primaryKey(),
      string: t.text(),
      int: t.integer(),
      float: t.doublePrecision(),
      boolean: t.boolean(),
      hex: t.hex(),
      bigint: t.bigint(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    "/graphql",
    graphql({ schema, db: database.readonlyQB.raw }),
  );
  const response = await app.request("/graphql", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      query: `
        {
          table(id: "0", doesntExist: "kevin") {
            id
            string
            int
            float
            boolean
            hex
            bigint
          }
        }
      `,
    }),
  });
  expect(response.status).toBe(200);
  const body = await response.json();
  expect(body.errors[0].message).toBe(
    'Unknown argument "doesntExist" on field "Query.table".',
  );
});
test("graphQLMiddleware throws error for token limit", async () => {
  const schema = {
    table: onchainTable("table", (t) => ({
      id: t.text().primaryKey(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    "/graphql",
    graphql({ schema, db: database.readonlyQB.raw }, { maxOperationTokens: 3 }),
  );
  const response = await app.request("/graphql", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      query: `
        {
          __schema {
            types {
              fields {
                type {
                  fields {
                    type {
                      description
                    }              
                  }
                }
              }
            }
          }
        }
      `,
    }),
  });
  expect(response.status).toBe(200);
  const body = await response.json();
  expect(body.errors[0].message).toBe(
    "Syntax Error: Token limit of 3 exceeded.",
  );
});
test("graphQLMiddleware throws error for depth limit", async () => {
  const schema = {
    table: onchainTable("table", (t) => ({
      id: t.text().primaryKey(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    "/graphql",
    graphql({ schema, db: database.readonlyQB.raw }, { maxOperationDepth: 5 }),
  );
  const response = await app.request("/graphql", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      query: `
        {
          __schema {
            types {
              fields {
                type {
                  fields {
                    type {
                      description
                    }              
                  }
                }
              }
            }
          }
        }
      `,
    }),
  });
  expect(response.status).toBe(200);
  const body = await response.json();
  expect(body.errors[0].message).toBe(
    "Syntax Error: Query depth limit of 5 exceeded, found 7.",
  );
});
test("graphQLMiddleware throws error for max aliases", async () => {
  const schema = {
    table: onchainTable("table", (t) => ({
      id: t.text().primaryKey(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    "/graphql",
    graphql(
      { schema, db: database.readonlyQB.raw },
      { maxOperationAliases: 2 },
    ),
  );
  const response = await app.request("/graphql", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      query: `
        {
          __schema {
            types {
              fields {
                type {
                  alias1: fields {
                    type {
                      description
                    }
                  }
                  alias2: fields {
                    type {
                      description
                    }
                  }
                  alias3: fields {
                    type {
                      description
                    }
                  }
                }
              }
            }
          }
        }
      `,
    }),
  });
  expect(response.status).toBe(200);
  const body = await response.json();
  expect(body.errors[0].message).toBe(
    "Syntax Error: Aliases limit of 2 exceeded, found 3.",
  );
});
test("graphQLMiddleware interactive", async () => {
  const schema = {};
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  globalThis.PONDER_DATABASE = database;
  const app = new Hono().use(
    "/graphql",
    graphql(
      { schema, db: database.readonlyQB.raw },
      { maxOperationAliases: 2 },
    ),
  );
  const response = await app.request("/graphql");
  expect(response.status).toBe(200);
});
</file>

<file path="packages/core/src/graphql/middleware.ts">
import { graphiQLHtml } from "@/graphql/graphiql.html.js";
import type { Schema } from "@/internal/types.js";
import type { ReadonlyDrizzle } from "@/types/db.js";
import { maxAliasesPlugin } from "@escape.tech/graphql-armor-max-aliases";
import { maxDepthPlugin } from "@escape.tech/graphql-armor-max-depth";
import { maxTokensPlugin } from "@escape.tech/graphql-armor-max-tokens";
import { type GraphQLSchema, printSchema } from "graphql";
import { createYoga } from "graphql-yoga";
import { createMiddleware } from "hono/factory";
import { buildDataLoaderCache, buildGraphQLSchema } from "./index.js";
/**
 * Middleware for GraphQL with an interactive web view.
 *
 * - Docs: https://ponder.sh/docs/api-reference/ponder/api-endpoints#graphql
 *
 * @example
 * import { db } from "ponder:api";
 * import schema from "ponder:schema";
 * import { graphql } from "@/index.js";
 * import { Hono } from "hono";
 *
 * const app = new Hono();
 *
 * app.use("/graphql", graphql({ db, schema }));
 *
 * export default app;
 *
 */
export const graphql = (
  { schema }: { db: ReadonlyDrizzle<Schema>; schema: Schema },
  {
    maxOperationTokens = 1000,
    maxOperationDepth = 100,
    maxOperationAliases = 30,
  }: {
    maxOperationTokens?: number;
    maxOperationDepth?: number;
    maxOperationAliases?: number;
  } = {
    // Default limits are from Apollo:
    // https://www.apollographql.com/blog/prevent-graph-misuse-with-operation-size-and-complexity-limits
    maxOperationTokens: 1000,
    maxOperationDepth: 100,
    maxOperationAliases: 30,
  },
) => {
  if (globalThis.PONDER_DATABASE === undefined) {
    throw new Error(
      "graphql() middleware cannot be initialized outside of a Ponder project",
    );
  }
  const graphqlSchema = buildGraphQLSchema({ schema });
  generateSchema({ graphqlSchema }).catch(() => {});
  const yoga = createYoga({
    graphqlEndpoint: "*", // Disable built-in route validation, use Hono routing instead
    schema: graphqlSchema,
    context: () => {
      const getDataLoader = buildDataLoaderCache(
        globalThis.PONDER_DATABASE.readonlyQB,
      );
      return { qb: globalThis.PONDER_DATABASE.readonlyQB, getDataLoader };
    },
    maskedErrors: process.env.NODE_ENV === "production",
    logging: false,
    graphiql: false,
    parserAndValidationCache: false,
    plugins: [
      maxTokensPlugin({ n: maxOperationTokens }),
      maxDepthPlugin({ n: maxOperationDepth, ignoreIntrospection: false }),
      maxAliasesPlugin({ n: maxOperationAliases, allowList: [] }),
    ],
  });
  return createMiddleware(async (c) => {
    if (c.req.method === "GET") {
      return c.html(graphiQLHtml);
    }
    const response = await yoga.handle(c.req.raw);
    if (response.status === 200) return response;
    // set response status to 200, must construct a new response object because Response.status is readonly
    // TODO: Figure out why Yoga is returning 500 status codes for GraphQL errors.
    const body = response.body;
    const headers = new Headers(response.headers);
    return new Response(body, {
      status: 200,
      statusText: "OK",
      headers,
    });
  });
};
async function generateSchema({
  graphqlSchema,
}: { graphqlSchema: GraphQLSchema }) {
  const fs = await import(/* webpackIgnore: true */ "node:fs");
  const path = await import(/* webpackIgnore: true */ "node:path");
  fs.mkdirSync(path.join(process.cwd(), "generated"), { recursive: true });
  fs.writeFileSync(
    path.join(process.cwd(), "generated", "schema.graphql"),
    printSchema(graphqlSchema),
    "utf-8",
  );
  globalThis.PONDER_COMMON?.logger.debug({
    msg: `Wrote file "generated/schema.graphql"`,
  });
}
</file>

<file path="packages/core/src/indexing/addStackTrace.ts">
import { readFileSync } from "node:fs";
import type { Options } from "@/internal/options.js";
import { codeFrameColumns } from "@babel/code-frame";
import { type StackFrame, parse as parseStackTrace } from "stacktrace-parser";
// Note: this currently works for both indexing functions and api
// routes only because the api route dir is a subdir of the indexing function
// dir.
export const addStackTrace = (error: Error, options: Options) => {
  if (!error.stack) return;
  const stackTrace = parseStackTrace(error.stack);
  let codeFrame: string | undefined;
  let userStackTrace: StackFrame[];
  // Find first frame that occurred within user code.
  const firstUserFrameIndex = stackTrace.findIndex((frame) =>
    frame.file?.includes(options.indexingDir),
  );
  if (firstUserFrameIndex >= 0) {
    userStackTrace = stackTrace.filter((frame) =>
      frame.file?.includes(options.indexingDir),
    );
    const firstUserFrame = stackTrace[firstUserFrameIndex];
    if (firstUserFrame?.file && firstUserFrame?.lineNumber) {
      try {
        const sourceContent = readFileSync(firstUserFrame.file, {
          encoding: "utf-8",
        });
        codeFrame = codeFrameColumns(
          sourceContent,
          {
            start: {
              line: firstUserFrame.lineNumber,
              column: firstUserFrame.column ?? undefined,
            },
          },
          { highlightCode: true },
        );
      } catch (err) {
        // Ignore errors here.
      }
    }
  } else {
    userStackTrace = stackTrace;
  }
  const formattedStackTrace = [
    `${error.name}: ${error.message}`,
    ...userStackTrace.map(({ file, lineNumber, column, methodName }) => {
      const prefix = "    at";
      const path = `${file}${lineNumber !== null ? `:${lineNumber}` : ""}${
        column !== null ? `:${column}` : ""
      }`;
      if (methodName === null || methodName === "<unknown>") {
        return `${prefix} ${path}`;
      } else {
        return `${prefix} ${methodName} (${path})`;
      }
    }),
    codeFrame,
  ].join("\n");
  error.stack = formattedStackTrace;
};
</file>

<file path="packages/core/src/indexing/client.test.ts">
import { ALICE } from "@/_test/constants.js";
import { erc20ABI, revertABI } from "@/_test/generated.js";
import {
  context,
  setupAnvil,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import {
  deployErc20,
  deployMulticall,
  deployRevert,
  mintErc20,
  simulateBlock,
} from "@/_test/simulate.js";
import {
  getBlocksIndexingBuild,
  getChain,
  getErc20IndexingBuild,
  getSimulatedEvent,
  publicClient,
} from "@/_test/utils.js";
import { createRpc } from "@/rpc/index.js";
import {
  type Hex,
  decodeFunctionResult,
  encodeFunctionData,
  multicall3Abi,
  parseEther,
} from "viem";
import { toHex } from "viem";
import { beforeEach, expect, test, vi } from "vitest";
import { createCachedViemClient } from "./client.js";
import { getEventCount } from "./index.js";
beforeEach(setupCommon);
beforeEach(setupAnvil);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
test("request() block dependent method", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const blockData = await simulateBlock();
  const { eventCallbacks, indexingFunctions } = getBlocksIndexingBuild({
    interval: 1,
  });
  const { syncStore } = await setupDatabaseServices();
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount: getEventCount(indexingFunctions),
  });
  cachedViemClient.event = event;
  const request = cachedViemClient.getClient(chain).request;
  const response1 = await request({
    method: "eth_getBlockByNumber",
    params: [toHex(blockData.block.number), false],
  });
  expect(response1).toBeDefined();
  const insertSpy = vi.spyOn(syncStore, "insertRpcRequestResults");
  const getSpy = vi.spyOn(syncStore, "getRpcRequestResults");
  const response2 = await request({
    method: "eth_getBlockByNumber",
    params: [toHex(blockData.block.number), false],
  });
  expect(response1).toStrictEqual(response2);
  expect(insertSpy).toHaveBeenCalledTimes(0);
  expect(getSpy).toHaveBeenCalledTimes(1);
});
test("request() non-block dependent method", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const { syncStore } = await setupDatabaseServices();
  const blockNumber = await publicClient.getBlockNumber();
  const block = await publicClient.getBlock({ blockNumber: blockNumber });
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount: getEventCount(indexingFunctions),
  });
  cachedViemClient.event = event;
  const request = cachedViemClient.getClient(chain).request;
  const response1 = await request({
    method: "eth_getTransactionByHash",
    params: [block.transactions[0]!],
  });
  expect(response1).toBeDefined;
  const insertSpy = vi.spyOn(syncStore, "insertRpcRequestResults");
  const getSpy = vi.spyOn(syncStore, "getRpcRequestResults");
  const response2 = await request({
    method: "eth_getTransactionByHash",
    params: [block.transactions[0]!],
  });
  expect(response1).toStrictEqual(response2);
  expect(insertSpy).toHaveBeenCalledTimes(0);
  expect(getSpy).toHaveBeenCalledTimes(1);
});
test("request() non-cached method", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const blockData = await simulateBlock();
  const { eventCallbacks, indexingFunctions } = getBlocksIndexingBuild({
    interval: 1,
  });
  const { syncStore } = await setupDatabaseServices();
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount: getEventCount(indexingFunctions),
  });
  cachedViemClient.event = event;
  const request = cachedViemClient.getClient(chain).request;
  const insertSpy = vi.spyOn(syncStore, "insertRpcRequestResults");
  const getSpy = vi.spyOn(syncStore, "getRpcRequestResults");
  expect(await request({ method: "eth_blockNumber" })).toBeDefined();
  expect(insertSpy).toHaveBeenCalledTimes(0);
  expect(getSpy).toHaveBeenCalledTimes(0);
});
test("request() multicall", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const blockData = await simulateBlock();
  const { eventCallbacks, indexingFunctions } = getBlocksIndexingBuild({
    interval: 1,
  });
  const { syncStore } = await setupDatabaseServices();
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount: getEventCount(indexingFunctions),
  });
  cachedViemClient.event = event;
  const request = cachedViemClient.getClient(chain).request;
  const { address: multicall } = await deployMulticall({ sender: ALICE });
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const response1 = await request({
    method: "eth_call",
    params: [
      {
        to: multicall,
        data: encodeFunctionData({
          abi: multicall3Abi,
          functionName: "aggregate3",
          args: [
            [
              {
                target: address,
                allowFailure: false,
                callData: encodeFunctionData({
                  abi: erc20ABI,
                  functionName: "totalSupply",
                }),
              },
            ],
          ],
        }),
      },
      "latest",
    ],
  });
  expect(response1).toBeDefined();
  const insertSpy = vi.spyOn(syncStore, "insertRpcRequestResults");
  const getSpy = vi.spyOn(syncStore, "getRpcRequestResults");
  const requestSpy = vi.spyOn(rpc, "request");
  let result = decodeFunctionResult({
    abi: multicall3Abi,
    functionName: "aggregate3",
    data: response1 as Hex,
  });
  expect(result).toMatchInlineSnapshot(`
    [
      {
        "returnData": "0x0000000000000000000000000000000000000000000000000de0b6b3a7640000",
        "success": true,
      },
    ]
  `);
  const response2 = await request({
    method: "eth_call",
    params: [
      {
        to: multicall,
        data: encodeFunctionData({
          abi: multicall3Abi,
          functionName: "aggregate3",
          args: [
            [
              {
                target: address,
                allowFailure: false,
                callData: encodeFunctionData({
                  abi: erc20ABI,
                  functionName: "totalSupply",
                }),
              },
              {
                target: address,
                allowFailure: false,
                callData: encodeFunctionData({
                  abi: erc20ABI,
                  functionName: "balanceOf",
                  args: [ALICE],
                }),
              },
            ],
          ],
        }),
      },
      "latest",
    ],
  });
  result = decodeFunctionResult({
    abi: multicall3Abi,
    functionName: "aggregate3",
    data: response2 as Hex,
  });
  expect(result).toMatchInlineSnapshot(`
    [
      {
        "returnData": "0x0000000000000000000000000000000000000000000000000de0b6b3a7640000",
        "success": true,
      },
      {
        "returnData": "0x0000000000000000000000000000000000000000000000000de0b6b3a7640000",
        "success": true,
      },
    ]
  `);
  expect(insertSpy).toHaveBeenCalledTimes(1);
  expect(getSpy).toHaveBeenCalledTimes(1);
  expect(requestSpy).toHaveBeenCalledTimes(1);
});
test("request() multicall empty", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const blockData = await simulateBlock();
  const { eventCallbacks, indexingFunctions } = getBlocksIndexingBuild({
    interval: 1,
  });
  const { syncStore } = await setupDatabaseServices();
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount: getEventCount(indexingFunctions),
  });
  cachedViemClient.event = event;
  const request = cachedViemClient.getClient(chain).request;
  const { address: multicall } = await deployMulticall({ sender: ALICE });
  const response = await request({
    method: "eth_call",
    params: [
      {
        to: multicall,
        data: encodeFunctionData({
          abi: multicall3Abi,
          functionName: "aggregate3",
          args: [[]],
        }),
      },
      "latest",
    ],
  });
  expect(response).toBeDefined();
  const result = decodeFunctionResult({
    abi: multicall3Abi,
    functionName: "aggregate3",
    data: response as Hex,
  });
  expect(result).toMatchInlineSnapshot("[]");
});
test("prefetch() uses profile metadata", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const eventCount = getEventCount(indexingFunctions);
  eventCount[
    "Erc20:Transfer(address indexed from, address indexed to, uint256 amount)"
  ] = 1;
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  cachedViemClient.event = event;
  let totalSupply = await cachedViemClient.getClient(chain).readContract({
    abi: erc20ABI,
    functionName: "totalSupply",
    address,
  });
  expect(totalSupply).toBe(parseEther("1"));
  event.event.block.number = 2n;
  cachedViemClient.event = event;
  await cachedViemClient.prefetch({
    events: [event],
  });
  const requestSpy = vi.spyOn(rpc, "request");
  const getSpy = vi.spyOn(syncStore, "getRpcRequestResults");
  totalSupply = await cachedViemClient.getClient(chain).readContract({
    abi: erc20ABI,
    functionName: "totalSupply",
    address,
  });
  expect(totalSupply).toBe(parseEther("1"));
  expect(requestSpy).toHaveBeenCalledTimes(0);
  expect(getSpy).toHaveBeenCalledTimes(0);
});
test("request() revert", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployRevert({ sender: ALICE });
  const { address: erc20 } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { syncStore } = await setupDatabaseServices();
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address: erc20,
  });
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount: getEventCount(indexingFunctions),
  });
  cachedViemClient.event = event;
  const request = cachedViemClient.getClient(chain).request;
  const response1 = await request({
    method: "eth_call",
    params: [
      {
        to: address,
        data: encodeFunctionData({
          abi: revertABI,
          functionName: "revert",
          args: [true],
        }),
      },
      "0x1",
    ],
  }).catch((error) => error);
  expect(response1).toBeInstanceOf(Error);
});
test("readContract() action retry", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { syncStore } = await setupDatabaseServices();
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const requestSpy = vi.spyOn(rpc, "request");
  requestSpy.mockReturnValueOnce(Promise.resolve("0x"));
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount: getEventCount(indexingFunctions),
  });
  cachedViemClient.event = event;
  await cachedViemClient.getClient(chain).readContract({
    abi: erc20ABI,
    functionName: "totalSupply",
    address,
  });
  expect(requestSpy).toHaveBeenCalledTimes(2);
});
test("readContract() with immutable cache", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { syncStore } = await setupDatabaseServices();
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const requestSpy = vi.spyOn(rpc, "request");
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount: getEventCount(indexingFunctions),
  });
  cachedViemClient.event = event;
  const result = await cachedViemClient.getClient(chain).readContract({
    abi: erc20ABI,
    functionName: "totalSupply",
    address,
    cache: "immutable",
  });
  expect(result).toMatchInlineSnapshot("1000000000000000000n");
  expect(requestSpy).toBeCalledWith(
    {
      method: "eth_call",
      params: [expect.any(Object), "latest"],
    },
    expect.any(Object),
  );
});
test("readContract() with no retry empty response", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { syncStore } = await setupDatabaseServices();
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const requestSpy = vi.spyOn(rpc, "request");
  requestSpy.mockReturnValueOnce(Promise.resolve("0x"));
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount: getEventCount(indexingFunctions),
  });
  cachedViemClient.event = event;
  await expect(
    cachedViemClient.getClient(chain).readContract({
      abi: erc20ABI,
      functionName: "totalSupply",
      address,
      retryEmptyResponse: false,
    }),
  ).rejects.toThrow();
});
test("getBlock() action retry", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const blockData = await simulateBlock();
  const { syncStore } = await setupDatabaseServices();
  const { eventCallbacks, indexingFunctions } = getBlocksIndexingBuild({
    interval: 1,
  });
  const requestSpy = vi.spyOn(rpc, "request");
  requestSpy.mockReturnValueOnce(Promise.resolve(null));
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common: context.common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount: getEventCount(indexingFunctions),
  });
  cachedViemClient.event = event;
  await cachedViemClient.getClient(chain).getBlock({ blockNumber: 1n });
  expect(requestSpy).toHaveBeenCalledTimes(2);
});
</file>

<file path="packages/core/src/indexing/client.ts">
import type { Common } from "@/internal/common.js";
import type { Chain, IndexingBuild, SetupEvent } from "@/internal/types.js";
import type { Event } from "@/internal/types.js";
import type { RequestParameters, Rpc } from "@/rpc/index.js";
import type { SyncStore } from "@/sync-store/index.js";
import { dedupe } from "@/utils/dedupe.js";
import { toLowerCase } from "@/utils/lowercase.js";
import { orderObject } from "@/utils/order.js";
import { startClock } from "@/utils/timer.js";
import { wait } from "@/utils/wait.js";
import {
  type Abi,
  type Account,
  BlockNotFoundError,
  type Client,
  type ContractFunctionArgs,
  type ContractFunctionName,
  type ContractFunctionParameters,
  type GetBlockReturnType,
  type GetTransactionConfirmationsParameters,
  type GetTransactionConfirmationsReturnType,
  type GetTransactionParameters,
  type GetTransactionReceiptParameters,
  type GetTransactionReceiptReturnType,
  type GetTransactionReturnType,
  type Hash,
  type Hex,
  type MulticallParameters,
  type MulticallReturnType,
  type Prettify,
  type PublicActions,
  type PublicRpcSchema,
  type ReadContractParameters,
  type ReadContractReturnType,
  type SimulateContractParameters,
  type SimulateContractReturnType,
  TransactionNotFoundError,
  TransactionReceiptNotFoundError,
  type Transport,
  type Chain as ViemChain,
  createClient,
  custom,
  decodeFunctionData,
  decodeFunctionResult,
  encodeFunctionData,
  encodeFunctionResult,
  getAbiItem,
  hexToNumber,
  multicall3Abi,
  publicActions,
  toFunctionSelector,
  toHex,
} from "viem";
import {
  getProfilePatternKey,
  recordProfilePattern,
  recoverProfilePattern,
} from "./profile.js";
export type CachedViemClient = {
  getClient: (chain: Chain) => ReadonlyClient;
  prefetch: (params: {
    events: Event[];
  }) => Promise<void>;
  clear: () => void;
  event: Event | SetupEvent | undefined;
};
const MULTICALL_SELECTOR = toFunctionSelector(
  getAbiItem({ abi: multicall3Abi, name: "aggregate3" }),
);
const SAMPLING_RATE = 10;
const DB_PREDICTION_THRESHOLD = 0.2;
const RPC_PREDICTION_THRESHOLD = 0.8;
const MAX_CONSTANT_PATTERN_COUNT = 10;
/**
 * RPC responses that are not cached. These are valid responses
 * that are sometimes erroneously returned by the RPC.
 *
 * `"0x"` is returned by `eth_call` and causes the `ContractFunctionZeroDataError`.
 * `null` is returned by `eth_getBlockByNumber` and `eth_getBlockByHash` and causes the `BlockNotFoundError`.
 */
const UNCACHED_RESPONSES = ["0x", null] as any[];
/** RPC methods that reference a block number. */
const blockDependentMethods = new Set([
  "eth_getBalance",
  "eth_getTransactionCount",
  "eth_getBlockByNumber",
  "eth_getBlockTransactionCountByNumber",
  "eth_getTransactionByBlockNumberAndIndex",
  "eth_call",
  "eth_estimateGas",
  "eth_feeHistory",
  "eth_getProof",
  "eth_getCode",
  "eth_getStorageAt",
  "eth_getUncleByBlockNumberAndIndex",
  "debug_traceBlockByNumber",
]);
/** RPC methods that don't reference a block number. */
const nonBlockDependentMethods = new Set([
  "eth_getBlockByHash",
  "eth_getTransactionByHash",
  "eth_getBlockTransactionCountByHash",
  "eth_getTransactionByBlockHashAndIndex",
  "eth_getTransactionConfirmations",
  "eth_getTransactionReceipt",
  "eth_getUncleByBlockHashAndIndex",
  "eth_getUncleCountByBlockHash",
  "debug_traceBlockByHash",
  "debug_traceTransaction",
  "debug_traceCall",
]);
/** Viem actions where the `block` property is optional and implicit. */
const blockDependentActions = [
  "getBalance",
  "call",
  "estimateGas",
  "getFeeHistory",
  "getProof",
  "getCode",
  "getStorageAt",
  "getEnsAddress",
  "getEnsAvatar",
  "getEnsName",
  "getEnsResolver",
  "getEnsText",
  "readContract",
  "multicall",
  "simulateContract",
] as const satisfies readonly (keyof ReturnType<typeof publicActions>)[];
/** Viem actions where the `block` property is required. */
const blockRequiredActions = [
  "getBlock",
  "getTransactionCount",
  "getBlockTransactionCount",
] as const satisfies readonly (keyof ReturnType<typeof publicActions>)[];
/** Viem actions where the `block` property is non-existent. */
const nonBlockDependentActions = [
  "getTransaction",
  "getTransactionReceipt",
  "getTransactionConfirmations",
] as const satisfies readonly (keyof ReturnType<typeof publicActions>)[];
/** Viem actions that should be retried if they fail. */
const retryableActions = [
  "readContract",
  "simulateContract",
  "multicall",
  "getBlock",
  "getTransaction",
  "getTransactionReceipt",
  "getTransactionConfirmations",
] as const satisfies readonly (keyof ReturnType<typeof publicActions>)[];
type BlockOptions =
  | {
      cache?: undefined;
      blockNumber?: undefined;
    }
  | {
      cache: "immutable";
      blockNumber?: undefined;
    }
  | {
      cache?: undefined;
      blockNumber: bigint;
    };
type RequiredBlockOptions =
  | {
      /** Hash of the block. */
      blockHash: Hash;
      blockNumber?: undefined;
    }
  | {
      blockHash?: undefined;
      /** The block number. */
      blockNumber: bigint;
    };
type RetryableOptions = {
  /**
   * Whether or not to retry the action if the response is empty.
   *
   * @default true
   */
  retryEmptyResponse?: boolean;
};
type BlockDependentAction<
  fn extends (client: any, args: any) => unknown,
  ///
  params = Parameters<fn>[0],
  returnType = ReturnType<fn>,
> = (
  args: Omit<params, "blockTag" | "blockNumber"> & BlockOptions,
) => returnType;
export type PonderActions = Omit<
  {
    [action in (typeof blockDependentActions)[number]]: BlockDependentAction<
      ReturnType<typeof publicActions>[action]
    >;
  } & Pick<PublicActions, (typeof nonBlockDependentActions)[number]> &
    Pick<PublicActions, (typeof blockRequiredActions)[number]>,
  (typeof retryableActions)[number]
> & {
  // Types for `retryableActions` are manually defined.
  readContract: <
    const abi extends Abi | readonly unknown[],
    functionName extends ContractFunctionName<abi, "pure" | "view">,
    const args extends ContractFunctionArgs<abi, "pure" | "view", functionName>,
  >(
    args: Omit<
      ReadContractParameters<abi, functionName, args>,
      "blockTag" | "blockNumber"
    > &
      BlockOptions &
      RetryableOptions,
  ) => Promise<ReadContractReturnType<abi, functionName, args>>;
  simulateContract: <
    const abi extends Abi | readonly unknown[],
    functionName extends ContractFunctionName<abi, "nonpayable" | "payable">,
    const args extends ContractFunctionArgs<
      abi,
      "nonpayable" | "payable",
      functionName
    >,
  >(
    args: Omit<
      SimulateContractParameters<abi, functionName, args>,
      "blockTag" | "blockNumber"
    > &
      BlockOptions &
      RetryableOptions,
  ) => Promise<SimulateContractReturnType<abi, functionName, args>>;
  multicall: <
    const contracts extends readonly unknown[],
    allowFailure extends boolean = true,
  >(
    args: Omit<
      MulticallParameters<contracts, allowFailure>,
      "blockTag" | "blockNumber"
    > &
      BlockOptions &
      RetryableOptions,
  ) => Promise<MulticallReturnType<contracts, allowFailure>>;
  getBlock: <includeTransactions extends boolean = false>(
    args: {
      /** Whether or not to include transaction data in the response. */
      includeTransactions?: includeTransactions | undefined;
    } & RequiredBlockOptions &
      RetryableOptions,
  ) => Promise<GetBlockReturnType<ViemChain | undefined, includeTransactions>>;
  getTransaction: (
    args: GetTransactionParameters & RetryableOptions,
  ) => Promise<GetTransactionReturnType>;
  getTransactionReceipt: (
    args: GetTransactionReceiptParameters & RetryableOptions,
  ) => Promise<GetTransactionReceiptReturnType>;
  getTransactionConfirmations: (
    args: GetTransactionConfirmationsParameters & RetryableOptions,
  ) => Promise<GetTransactionConfirmationsReturnType>;
};
export type ReadonlyClient<
  transport extends Transport = Transport,
  chain extends ViemChain | undefined = ViemChain | undefined,
> = Prettify<
  Omit<
    Client<transport, chain, undefined, PublicRpcSchema, PonderActions>,
    | "extend"
    | "key"
    | "batch"
    | "cacheTime"
    | "account"
    | "type"
    | "uid"
    | "chain"
    | "name"
    | "pollingInterval"
    | "transport"
    | "ccipRead"
  >
>;
/**
 * RPC request.
 */
export type Request = Pick<
  ReadContractParameters,
  "abi" | "address" | "functionName" | "args"
> & { blockNumber: bigint | "latest"; chainId: number };
/**
 * Serialized RPC request for uniquely identifying a request.
 *
 * @dev Encoded from {@link Request} using `abi`.
 *
 * @example
 * "{
 *   "method": "eth_call",
 *   "params": [{"data": "0x123", "to": "0x456"}, "0x789"]
 * }"
 */
type CacheKey = string;
/**
 * Response of an RPC request.
 *
 * @example
 * "0x123"
 *
 * @example
 * ""0x123456789""
 */
type Response = string;
/**
 * Recorded RPC request pattern.
 *
 * @example
 * {
 *   "address": ["args", "from"],
 *   "abi": [...],
 *   "functionName": "balanceOf",
 *   "args": ["log", "address"],
 * }
 */
export type ProfilePattern = Pick<
  ReadContractParameters,
  "abi" | "functionName"
> & {
  address:
    | { type: "constant"; value: unknown }
    | { type: "derived"; value: string[] };
  args?: (
    | { type: "constant"; value: unknown }
    | { type: "derived"; value: string[] }
  )[];
  cache?: "immutable";
};
/**
 * Serialized {@link ProfilePattern} for unique identification.
 *
 * @example
 * "{
 *   "address": ["args", "from"],
 *   "args": ["log", "address"],
 *   "functionName": "balanceOf",
 * }"
 */
type ProfileKey = string;
/**
 * Event name.
 *
 * @example
 * "Erc20:Transfer"
 *
 * @example
 * "Erc20.mint()"
 */
type EventName = string;
/**
 * Metadata about RPC request patterns for each event.
 *
 * @dev Only profile "eth_call" requests.
 */
type Profile = Map<
  EventName,
  Map<
    ProfileKey,
    { pattern: ProfilePattern; hasConstant: boolean; count: number }
  >
>;
/**
 * LRU cache of {@link ProfilePattern} in {@link Profile} with constant args.
 *
 * @dev Used to determine which {@link ProfilePattern} should be evicted.
 */
type ProfileConstantLRU = Map<EventName, Set<ProfileKey>>;
/**
 * Cache of RPC responses.
 */
type Cache = Map<number, Map<CacheKey, Promise<Response | Error> | Response>>;
export const getCacheKey = (request: RequestParameters) => {
  return toLowerCase(JSON.stringify(orderObject(request)));
};
export const encodeRequest = (request: Request) =>
  ({
    method: "eth_call",
    params: [
      {
        to: request.address,
        data: encodeFunctionData({
          abi: request.abi,
          functionName: request.functionName,
          args: request.args,
        }),
      },
      request.blockNumber === "latest" ? "latest" : toHex(request.blockNumber),
    ],
  }) satisfies RequestParameters;
export const decodeResponse = (response: Response) => {
  // Note: I don't actually remember why we had to add the try catch.
  try {
    return JSON.parse(response);
  } catch (error) {
    return response;
  }
};
export const createCachedViemClient = ({
  common,
  indexingBuild,
  syncStore,
  eventCount,
}: {
  common: Common;
  indexingBuild: Pick<IndexingBuild, "chains" | "rpcs">;
  syncStore: SyncStore;
  eventCount: { [eventName: string]: number };
}): CachedViemClient => {
  let event: Event | SetupEvent = undefined!;
  const cache: Cache = new Map();
  const profile: Profile = new Map();
  const profileConstantLRU: ProfileConstantLRU = new Map();
  for (const chain of indexingBuild.chains) {
    cache.set(chain.id, new Map());
  }
  const ponderActions = <
    TTransport extends Transport = Transport,
    TChain extends ViemChain | undefined = ViemChain | undefined,
    TAccount extends Account | undefined = Account | undefined,
  >(
    client: Client<TTransport, TChain, TAccount>,
  ): PonderActions => {
    const actions = {} as PonderActions;
    const _publicActions = publicActions(client);
    const addProfilePattern = ({
      pattern,
      hasConstant,
    }: { pattern: ProfilePattern; hasConstant: boolean }) => {
      const profilePatternKey = getProfilePatternKey(pattern);
      const eventName = (event as Event).eventCallback.name;
      if (profile.get(eventName)!.has(profilePatternKey)) {
        profile.get(eventName)!.get(profilePatternKey)!.count++;
        if (hasConstant) {
          profileConstantLRU.get(eventName)!.delete(profilePatternKey);
          profileConstantLRU.get(eventName)!.add(profilePatternKey);
        }
      } else {
        profile
          .get(eventName)!
          .set(profilePatternKey, { pattern, hasConstant, count: 1 });
        if (hasConstant) {
          profileConstantLRU.get(eventName)!.add(profilePatternKey);
          if (
            profileConstantLRU.get(eventName)!.size > MAX_CONSTANT_PATTERN_COUNT
          ) {
            const firstKey = profileConstantLRU
              .get(eventName)!
              .keys()
              .next().value;
            if (firstKey) {
              profile.get(eventName)!.delete(firstKey);
              profileConstantLRU.get(eventName)!.delete(firstKey);
            }
          }
        }
      }
    };
    const getPonderAction = <
      action extends (typeof blockDependentActions)[number],
    >(
      action: action,
    ) => {
      return ({
        cache,
        blockNumber: userBlockNumber,
        ...args
      }: Parameters<PonderActions[action]>[0]) => {
        // Note: prediction only possible when block number is managed by Ponder.
        if (
          event.type !== "setup" &&
          userBlockNumber === undefined &&
          eventCount[event.eventCallback.name]! % SAMPLING_RATE === 1
        ) {
          const eventName = event.eventCallback.name;
          if (profile.has(eventName) === false) {
            profile.set(eventName, new Map());
            profileConstantLRU.set(eventName, new Set());
          }
          // profile "readContract" and "multicall" actions
          if (action === "readContract") {
            const recordPatternResult = recordProfilePattern({
              event: event,
              args: { ...args, cache } as Parameters<
                PonderActions["readContract"]
              >[0],
              hints: Array.from(profile.get(eventName)!.values()),
            });
            if (recordPatternResult) {
              addProfilePattern(recordPatternResult);
            }
          } else if (action === "multicall") {
            const contracts = (
              { ...args, cache } as Parameters<PonderActions["multicall"]>[0]
            ).contracts as ContractFunctionParameters[];
            if (contracts.length < 10) {
              for (const contract of contracts) {
                const recordPatternResult = recordProfilePattern({
                  event: event,
                  args: contract,
                  hints: Array.from(profile.get(eventName)!.values()),
                });
                if (recordPatternResult) {
                  addProfilePattern(recordPatternResult);
                }
              }
            }
          }
        }
        const blockNumber =
          event.type === "setup" ? event.block : event.event.block.number;
        // @ts-expect-error
        return _publicActions[action]({
          ...args,
          ...(cache === "immutable"
            ? { blockTag: "latest" }
            : { blockNumber: userBlockNumber ?? blockNumber }),
        } as Parameters<ReturnType<typeof publicActions>[action]>[0]);
      };
    };
    const getRetryAction = (
      action: PonderActions[keyof PonderActions],
      actionName: keyof PonderActions,
    ) => {
      return async (...args: Parameters<typeof action>) => {
        const RETRY_COUNT = 9;
        const BASE_DURATION = 125;
        for (let i = 0; i <= RETRY_COUNT; i++) {
          try {
            // @ts-ignore
            return await action(...args);
          } catch (error) {
            const eventName =
              event.type === "setup"
                ? event.setupCallback.name
                : event.eventCallback.name;
            if (
              (error instanceof BlockNotFoundError === false &&
                error instanceof TransactionNotFoundError === false &&
                error instanceof TransactionReceiptNotFoundError === false &&
                // Note: Another way to catch this error is:
                // `error instanceof ContractFunctionExecutionError && error.cause instanceOf ContractFunctionZeroDataError`
                (error as Error)?.message?.includes("returned no data") ===
                  false) ||
              i === RETRY_COUNT ||
              (args[0] as RetryableOptions).retryEmptyResponse === false
            ) {
              const chain = indexingBuild.chains.find(
                (n) => n.id === event.chain.id,
              )!;
              common.logger.warn({
                msg: "Failed 'context.client' action",
                action: actionName,
                event: eventName,
                chain: chain.name,
                chain_id: chain.id,
                retry_count: i,
                error: error as Error,
              });
              throw error;
            }
            const duration = BASE_DURATION * 2 ** i;
            const chain = indexingBuild.chains.find(
              (n) => n.id === event.chain.id,
            )!;
            common.logger.warn({
              msg: "Failed 'context.client' action",
              action: actionName,
              event: eventName,
              chain: chain.name,
              chain_id: chain.id,
              retry_count: i,
              retry_delay: duration,
              error: error as Error,
            });
            await wait(duration);
          }
        }
      };
    };
    for (const action of blockDependentActions) {
      actions[action] = getPonderAction(action);
    }
    for (const action of nonBlockDependentActions) {
      // @ts-ignore
      actions[action] = _publicActions[action];
    }
    for (const action of blockRequiredActions) {
      // @ts-ignore
      actions[action] = _publicActions[action];
    }
    for (const action of retryableActions) {
      // @ts-ignore
      actions[action] = getRetryAction(actions[action], action);
    }
    const actionsWithMetrics = {} as PonderActions;
    for (const [action, actionFn] of Object.entries(actions)) {
      // @ts-ignore
      actionsWithMetrics[action] = async (
        ...args: Parameters<PonderActions[keyof PonderActions]>
      ) => {
        const endClock = startClock();
        try {
          // @ts-ignore
          return await actionFn(...args);
        } finally {
          common.metrics.ponder_indexing_rpc_action_duration.observe(
            { action },
            endClock(),
          );
        }
      };
    }
    return actionsWithMetrics;
  };
  return {
    getClient(chain) {
      const rpc =
        indexingBuild.rpcs[indexingBuild.chains.findIndex((n) => n === chain)]!;
      return createClient({
        transport: cachedTransport({
          common,
          chain,
          rpc,
          syncStore,
          cache,
          event: () => event,
        }),
        chain: chain.viemChain,
        // @ts-expect-error overriding `readContract` is not supported by viem
      }).extend(ponderActions);
    },
    async prefetch({ events }) {
      const context = {
        logger: common.logger.child({ action: "prefetch_rpc_requests" }),
      };
      const prefetchEndClock = startClock();
      // Use profiling metadata + next event batch to determine which
      // rpc requests are going to be made, and preload them into the cache.
      const prediction: { ev: number; request: Request }[] = [];
      for (const event of events) {
        if (profile.has(event.eventCallback.name)) {
          for (const [, { pattern, count }] of profile.get(
            event.eventCallback.name,
          )!) {
            // Expected value of times the prediction will be used.
            const ev =
              (count * SAMPLING_RATE) / eventCount[event.eventCallback.name]!;
            prediction.push({
              ev,
              request: recoverProfilePattern(pattern, event),
            });
          }
        }
      }
      const chainRequests: Map<
        number,
        { ev: number; request: RequestParameters }[]
      > = new Map();
      for (const chain of indexingBuild.chains) {
        chainRequests.set(chain.id, []);
      }
      for (const { ev, request } of dedupe(prediction, ({ request }) =>
        getCacheKey(encodeRequest(request)),
      )) {
        chainRequests.get(request.chainId)!.push({
          ev,
          request: encodeRequest(request),
        });
      }
      await Promise.all(
        Array.from(chainRequests.entries()).map(async ([chainId, requests]) => {
          const i = indexingBuild.chains.findIndex((n) => n.id === chainId);
          const chain = indexingBuild.chains[i]!;
          const rpc = indexingBuild.rpcs[i]!;
          const dbRequests = requests.filter(
            ({ ev }) => ev > DB_PREDICTION_THRESHOLD,
          );
          common.metrics.ponder_indexing_rpc_prefetch_total.inc(
            {
              chain: chain.name,
              method: "eth_call",
              type: "database",
            },
            dbRequests.length,
          );
          const cachedResults = await syncStore.getRpcRequestResults(
            {
              requests: dbRequests.map(({ request }) => request),
              chainId,
            },
            context,
          );
          for (let i = 0; i < dbRequests.length; i++) {
            const request = dbRequests[i]!;
            const cachedResult = cachedResults[i]!;
            if (cachedResult !== undefined) {
              cache
                .get(chainId)!
                .set(getCacheKey(request.request), cachedResult);
            } else if (request.ev > RPC_PREDICTION_THRESHOLD) {
              const resultPromise = rpc
                .request(request.request, context)
                .then((result) => JSON.stringify(result))
                .catch((error) => error as Error);
              common.metrics.ponder_indexing_rpc_prefetch_total.inc({
                chain: chain.name,
                method: "eth_call",
                type: "rpc",
              });
              // Note: Unawaited request added to cache
              cache
                .get(chainId)!
                .set(getCacheKey(request.request), resultPromise);
            }
          }
          if (dbRequests.length > 0) {
            common.logger.debug({
              msg: "Prefetched JSON-RPC requests",
              chain: chain.name,
              chain_id: chain.id,
              request_count: dbRequests.length,
              duration: prefetchEndClock(),
            });
          }
        }),
      );
    },
    clear() {
      for (const chain of indexingBuild.chains) {
        cache.get(chain.id)!.clear();
      }
    },
    set event(_event: Event | SetupEvent) {
      event = _event;
    },
  };
};
export const cachedTransport =
  ({
    common,
    chain,
    rpc,
    syncStore,
    cache,
    event,
  }: {
    common: Common;
    chain: Chain;
    rpc: Rpc;
    syncStore: SyncStore;
    cache: Cache;
    event: () => Event | SetupEvent;
  }): Transport =>
  ({ chain: viemChain }) =>
    custom({
      async request({ method, params }) {
        const _event = event();
        const context = {
          logger: common.logger.child({
            action: "cache JSON-RPC request",
            event:
              _event.type === "setup"
                ? _event.setupCallback.name
                : _event.eventCallback.name,
          }),
        };
        const body = { method, params };
        // multicall
        if (
          method === "eth_call" &&
          params[0]?.data?.startsWith(MULTICALL_SELECTOR)
        ) {
          let blockNumber: Hex | "latest" | undefined = undefined;
          [, blockNumber] = params;
          const multicallRequests = decodeFunctionData({
            abi: multicall3Abi,
            data: params[0]!.data,
          }).args[0];
          if (multicallRequests.length === 0) {
            // empty multicall result
            return "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000000";
          }
          const requests = multicallRequests.map(
            (call) =>
              ({
                method: "eth_call",
                params: [
                  {
                    to: call.target,
                    data: call.callData,
                  },
                  blockNumber ?? "latest",
                ],
              }) as const satisfies RequestParameters,
          );
          const results = new Map<
            RequestParameters,
            {
              success: boolean;
              returnData: `0x${string}`;
            }
          >();
          const requestsToInsert = new Set<RequestParameters>();
          for (const request of requests) {
            const cacheKey = getCacheKey(request);
            if (cache.get(chain.id)!.has(cacheKey)) {
              const cachedResult = cache.get(chain.id)!.get(cacheKey)!;
              if (cachedResult instanceof Promise) {
                common.metrics.ponder_indexing_rpc_requests_total.inc({
                  chain: chain.name,
                  method,
                  type: "prefetch_rpc",
                });
                const result = await cachedResult;
                // Note: we don't attempt to cache or prefetch errors, instead relying on the eventual RPC request.
                if (result instanceof Error) continue;
                if (UNCACHED_RESPONSES.includes(result) === false) {
                  requestsToInsert.add(request);
                }
                results.set(request, {
                  success: true,
                  returnData: decodeResponse(result),
                });
              } else {
                common.metrics.ponder_indexing_rpc_requests_total.inc({
                  chain: chain.name,
                  method,
                  type: "prefetch_database",
                });
                results.set(request, {
                  success: true,
                  returnData: decodeResponse(cachedResult),
                });
              }
            }
          }
          const dbRequests = requests.filter(
            (request) => results.has(request) === false,
          );
          const dbResults = await syncStore.getRpcRequestResults(
            { requests: dbRequests, chainId: chain.id },
            context,
          );
          for (let i = 0; i < dbRequests.length; i++) {
            const request = dbRequests[i]!;
            const result = dbResults[i]!;
            if (result !== undefined) {
              common.metrics.ponder_indexing_rpc_requests_total.inc({
                chain: chain.name,
                method,
                type: "database",
              });
              results.set(request, {
                success: true,
                returnData: decodeResponse(result),
              });
            }
          }
          if (results.size < requests.length) {
            const _requests = requests.filter(
              (request) => results.has(request) === false,
            );
            const multicallResult = await rpc
              .request(
                {
                  method: "eth_call",
                  params: [
                    {
                      to: params[0]!.to,
                      data: encodeFunctionData({
                        abi: multicall3Abi,
                        functionName: "aggregate3",
                        args: [
                          multicallRequests.filter(
                            (_, i) => results.has(requests[i]!) === false,
                          ),
                        ],
                      }),
                    },
                    blockNumber!,
                  ],
                },
                context,
              )
              .then((result) =>
                decodeFunctionResult({
                  abi: multicall3Abi,
                  functionName: "aggregate3",
                  data: result,
                }),
              );
            for (let i = 0; i < _requests.length; i++) {
              const request = _requests[i]!;
              const result = multicallResult[i]!;
              if (
                result.success &&
                UNCACHED_RESPONSES.includes(result.returnData) === false
              ) {
                requestsToInsert.add(request);
              }
              common.metrics.ponder_indexing_rpc_requests_total.inc({
                chain: chain.name,
                method,
                type: "rpc",
              });
              results.set(request, result);
            }
          }
          const encodedBlockNumber =
            blockNumber === undefined
              ? undefined
              : blockNumber === "latest"
                ? 0
                : hexToNumber(blockNumber);
          // Note: insertRpcRequestResults errors can be ignored and not awaited, since
          // the response is already fetched.
          syncStore
            .insertRpcRequestResults(
              {
                requests: Array.from(requestsToInsert).map((request) => ({
                  request,
                  blockNumber: encodedBlockNumber,
                  result: JSON.stringify(results.get(request)!.returnData),
                })),
                chainId: chain.id,
              },
              context,
            )
            .catch(() => {});
          // Note: at this point, it is an invariant that either `allowFailure` is true or
          // there are no failed requests.
          // Note: viem <= 2.23.6 had a bug with `encodeFunctionResult` which can be worked around by adding
          // another layer of array nesting.
          // Fixed by this commit https://github.com/wevm/viem/commit/9c442de0ff3...
          const resultsToEncode = requests.map(
            (request) => results.get(request)!,
          );
          try {
            return encodeFunctionResult({
              abi: multicall3Abi,
              functionName: "aggregate3",
              result: resultsToEncode,
            });
          } catch (e) {
            return encodeFunctionResult({
              abi: multicall3Abi,
              functionName: "aggregate3",
              result: [
                // @ts-expect-error known issue in viem <= 2.23.6
                resultsToEncode,
              ],
            });
          }
        } else if (
          blockDependentMethods.has(method) ||
          nonBlockDependentMethods.has(method)
        ) {
          const blockNumber = extractBlockNumberParam(body);
          const encodedBlockNumber =
            blockNumber === undefined
              ? undefined
              : blockNumber === "latest"
                ? 0
                : hexToNumber(blockNumber);
          const cacheKey = getCacheKey(body);
          if (cache.get(chain.id)!.has(cacheKey)) {
            const cachedResult = cache.get(chain.id)!.get(cacheKey)!;
            // `cachedResult` is a Promise if the request had to be fetched from the RPC.
            if (cachedResult instanceof Promise) {
              common.metrics.ponder_indexing_rpc_requests_total.inc({
                chain: chain.name,
                method,
                type: "prefetch_rpc",
              });
              const result = await cachedResult;
              if (result instanceof Error) throw result;
              if (UNCACHED_RESPONSES.includes(result) === false) {
                // Note: insertRpcRequestResults errors can be ignored and not awaited, since
                // the response is already fetched.
                syncStore
                  .insertRpcRequestResults(
                    {
                      requests: [
                        {
                          request: body,
                          blockNumber: encodedBlockNumber,
                          result,
                        },
                      ],
                      chainId: chain.id,
                    },
                    context,
                  )
                  .catch(() => {});
              }
              return decodeResponse(result);
            } else {
              common.metrics.ponder_indexing_rpc_requests_total.inc({
                chain: chain.name,
                method,
                type: "prefetch_database",
              });
            }
            return decodeResponse(cachedResult);
          }
          const [cachedResult] = await syncStore.getRpcRequestResults(
            { requests: [body], chainId: chain.id },
            context,
          );
          if (cachedResult !== undefined) {
            common.metrics.ponder_indexing_rpc_requests_total.inc({
              chain: chain.name,
              method,
              type: "database",
            });
            return decodeResponse(cachedResult);
          }
          common.metrics.ponder_indexing_rpc_requests_total.inc({
            chain: chain.name,
            method,
            type: "rpc",
          });
          const response = await rpc.request(body, context);
          if (UNCACHED_RESPONSES.includes(response) === false) {
            // Note: insertRpcRequestResults errors can be ignored and not awaited, since
            // the response is already fetched.
            syncStore
              .insertRpcRequestResults(
                {
                  requests: [
                    {
                      request: body,
                      blockNumber: encodedBlockNumber,
                      result: JSON.stringify(response),
                    },
                  ],
                  chainId: chain.id,
                },
                context,
              )
              .catch(() => {});
          }
          return response;
        } else {
          return rpc.request(body, context);
        }
      },
    })({ chain: viemChain, retryCount: 0 });
export const extractBlockNumberParam = (request: RequestParameters) => {
  let blockNumber: Hex | "latest" | undefined = undefined;
  switch (request.method) {
    case "eth_getBlockByNumber":
    case "eth_getBlockTransactionCountByNumber":
    case "eth_getTransactionByBlockNumberAndIndex":
    case "eth_getUncleByBlockNumberAndIndex":
    case "debug_traceBlockByNumber":
      // @ts-expect-error
      [blockNumber] = request.params;
      break;
    case "eth_getBalance":
    case "eth_call":
    case "eth_getCode":
    case "eth_estimateGas":
    case "eth_feeHistory":
    case "eth_getTransactionCount":
      // @ts-expect-error
      [, blockNumber] = request.params;
      break;
    case "eth_getProof":
    case "eth_getStorageAt":
      // @ts-expect-error
      [, , blockNumber] = request.params;
      break;
  }
  return blockNumber;
};
</file>

<file path="packages/core/src/indexing/index.test.ts">
import { ALICE, BOB } from "@/_test/constants.js";
import { erc20ABI } from "@/_test/generated.js";
import {
  context,
  setupAnvil,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { deployErc20, deployMulticall, mintErc20 } from "@/_test/simulate.js";
import {
  getChain,
  getErc20IndexingBuild,
  getSimulatedEvent,
} from "@/_test/utils.js";
import { onchainTable } from "@/drizzle/onchain.js";
import { createIndexingCache } from "@/indexing-store/cache.js";
import { createIndexingStore } from "@/indexing-store/index.js";
import { createCachedViemClient } from "@/indexing/client.js";
import {
  InvalidEventAccessError,
  type RetryableError,
} from "@/internal/errors.js";
import type { IndexingErrorHandler } from "@/internal/types.js";
import { createRpc } from "@/rpc/index.js";
import { parseEther, toHex, zeroAddress } from "viem";
import { beforeEach, expect, test, vi } from "vitest";
import {
  type Context,
  createColumnAccessPattern,
  createIndexing,
  getEventCount,
} from "./index.js";
beforeEach(setupCommon);
beforeEach(setupAnvil);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
// if ("bun" in process.versions) afterEach(cleanupAnvil);
const account = onchainTable("account", (p) => ({
  address: p.hex().primaryKey(),
  balance: p.bigint().notNull(),
}));
const schema = { account };
const indexingErrorHandler: IndexingErrorHandler = {
  getRetryableError: () => {
    return indexingErrorHandler.error;
  },
  setRetryableError: (error: RetryableError) => {
    indexingErrorHandler.error = error;
  },
  clearRetryableError: () => {
    indexingErrorHandler.error = undefined;
  },
  error: undefined as RetryableError | undefined,
};
test("createIndexing()", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address: zeroAddress,
    });
  const eventCount = getEventCount(indexingFunctions);
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  expect(indexing).toBeDefined();
});
test("processSetupEvents() empty", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { indexingFunctions, contracts } = getErc20IndexingBuild({
    address: zeroAddress,
  });
  const eventCount = getEventCount(indexingFunctions);
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [[]],
      setupCallbacks: [[]],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  await indexing.processSetupEvents();
});
test("processSetupEvents()", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address: zeroAddress,
    });
  const eventCount = getEventCount(indexingFunctions);
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  await indexing.processSetupEvents();
  expect(setupCallbacks[0]!.fn).toHaveBeenCalledOnce();
  expect(setupCallbacks[0]!.fn).toHaveBeenCalledWith({
    event: undefined,
    context: {
      chain: { id: 1, name: "mainnet" },
      contracts: {
        Erc20: {
          abi: expect.any(Object),
          address: zeroAddress,
          startBlock: undefined,
          endBlock: undefined,
        },
      },
      client: expect.any(Object),
      db: expect.any(Object),
    },
  });
});
test("processEvent()", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address,
    });
  const eventCount = getEventCount(indexingFunctions);
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  await indexing.processRealtimeEvents({ events: [event] });
  expect(eventCallbacks[0]!.fn).toHaveBeenCalledTimes(1);
  expect(eventCallbacks[0]!.fn).toHaveBeenCalledWith({
    event: {
      id: expect.any(String),
      args: expect.any(Object),
      log: expect.any(Object),
      block: expect.any(Object),
      transaction: expect.any(Object),
      transactionReceipt: undefined,
    },
    context: {
      chain: { id: 1, name: "mainnet" },
      contracts: {
        Erc20: {
          abi: expect.any(Object),
          // @ts-ignore
          address,
          startBlock: undefined,
          endBlock: undefined,
        },
      },
      client: expect.any(Object),
      db: expect.any(Object),
    },
  });
});
test("processEvents eventCount", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address,
    });
  const eventCount = getEventCount(indexingFunctions);
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  await indexing.processRealtimeEvents({ events: [event] });
  const metrics = await common.metrics.ponder_indexing_completed_events.get();
  expect(metrics.values).toMatchInlineSnapshot(`
    [
      {
        "labels": {
          "event": "Erc20:Transfer(address indexed from, address indexed to, uint256 amount)",
        },
        "value": 1,
      },
    ]
  `);
});
test("executeSetup() context.client", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address: zeroAddress,
    });
  const eventCount = getEventCount(indexingFunctions);
  setupCallbacks[0]!.fn = async ({ context }: { context: Context }) => {
    await context.client.getBalance({ address: BOB });
  };
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const getBalanceSpy = vi.spyOn(rpc, "request");
  await indexing.processSetupEvents();
  expect(getBalanceSpy).toHaveBeenCalledOnce();
  expect(getBalanceSpy).toHaveBeenCalledWith(
    {
      method: "eth_getBalance",
      params: ["0x70997970C51812dc3A010C7d01b50e0d17dc79C8", "0x0"],
    },
    expect.any(Object),
  );
});
test("executeSetup() context.db", async () => {
  const { common } = context;
  const { database, syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address: zeroAddress,
    });
  setupCallbacks[0]!.fn = async ({ context }: { context: Context }) => {
    await context.db
      .insert(account)
      .values({ address: zeroAddress, balance: 10n });
  };
  const eventCount = getEventCount(indexingFunctions);
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  indexingStore.qb = database.userQB;
  indexingCache.qb = database.userQB;
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const insertSpy = vi.spyOn(indexingStore.db, "insert");
  await indexing.processSetupEvents();
  expect(insertSpy).toHaveBeenCalledOnce();
  const supply = await indexingStore.db.find(account, { address: zeroAddress });
  expect(supply).toMatchObject({
    address: zeroAddress,
    balance: 10n,
  });
});
test("executeSetup() metrics", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address: zeroAddress,
    });
  const eventCount = getEventCount(indexingFunctions);
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  await indexing.processSetupEvents();
  const metrics = await common.metrics.ponder_indexing_function_duration.get();
  expect(metrics.values).toBeDefined();
});
test("executeSetup() error", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address: zeroAddress,
    });
  const eventCount = getEventCount(indexingFunctions);
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  // @ts-ignore
  setupCallbacks[0]!.fn.mockRejectedValue(new Error());
  await expect(indexing.processSetupEvents()).rejects.toThrowError();
  expect(setupCallbacks[0]!.fn).toHaveBeenCalledTimes(1);
});
test("processEvents() context.client", async () => {
  // console.log("pausing test...");
  // await setTimeout(1200000);
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address,
    });
  const eventCount = getEventCount(indexingFunctions);
  eventCallbacks[0]!.fn = async ({ context }: { context: Context }) => {
    await context.client.getBalance({ address: BOB });
  };
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const getBalanceSpy = vi.spyOn(rpc, "request");
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  await indexing.processRealtimeEvents({ events: [event] });
  expect(getBalanceSpy).toHaveBeenCalledTimes(1);
  expect(getBalanceSpy).toHaveBeenCalledWith(
    {
      method: "eth_getBalance",
      params: ["0x70997970C51812dc3A010C7d01b50e0d17dc79C8", "0x2"],
    },
    expect.any(Object),
  );
});
test("processEvents() context.db", async () => {
  const { common } = context;
  const { database, syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address,
    });
  const eventCount = getEventCount(indexingFunctions);
  let i = 0;
  eventCallbacks[0]!.fn = async ({ context }: { context: Context }) => {
    await context.db.insert(account).values({
      address: `0x000000000000000000000000000000000000000${i++}`,
      balance: 10n,
    });
  };
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  indexingStore.qb = database.userQB;
  indexingCache.qb = database.userQB;
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const insertSpy = vi.spyOn(indexingStore.db, "insert");
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  await indexing.processRealtimeEvents({ events: [event] });
  expect(insertSpy).toHaveBeenCalledTimes(1);
});
test("processEvents() metrics", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address,
    });
  const eventCount = getEventCount(indexingFunctions);
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  await indexing.processRealtimeEvents({ events: [event] });
  const metrics = await common.metrics.ponder_indexing_function_duration.get();
  expect(metrics.values).toBeDefined();
});
test("processEvents() error", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address,
    });
  const eventCount = getEventCount(indexingFunctions);
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  // @ts-ignore
  eventCallbacks[0]!.fn.mockRejectedValue(new Error());
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  await expect(
    indexing.processRealtimeEvents({ events: [event] }),
  ).rejects.toThrowError();
  expect(eventCallbacks[0]!.fn).toHaveBeenCalledTimes(1);
});
test("processEvents() error with missing event object properties", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address,
    });
  const eventCount = getEventCount(indexingFunctions);
  eventCallbacks[0]!.fn = async ({
    event,
  }: { event: any; context: Context }) => {
    // biome-ignore lint/performance/noDelete: <explanation>
    delete event.transaction;
    throw new Error("empty transaction");
  };
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  await expect(
    indexing.processRealtimeEvents({ events: [event] }),
  ).rejects.toThrowError();
});
test("processEvents() column selection", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, setupCallbacks, indexingFunctions, contracts } =
    getErc20IndexingBuild({
      address,
    });
  const eventCount = getEventCount(indexingFunctions);
  let count = 0;
  eventCallbacks[0]!.fn = async ({
    event,
  }: { event: any; context: Context }) => {
    event.transaction.gas;
    event.transaction.maxFeePerGas;
    if (count++ === 1001) {
      event.transaction.maxPriorityFeePerGas;
    }
  };
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild: { indexingFunctions },
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  const indexingCache = createIndexingCache({
    common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild: {
      eventCallbacks: [eventCallbacks],
      setupCallbacks: [setupCallbacks],
      chains: [chain],
      contracts: [contracts],
      indexingFunctions,
    },
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const events = Array.from({ length: 1001 }).map(() => event);
  await indexing.processHistoricalEvents({
    events,
    updateIndexingSeconds: vi.fn(),
  });
  expect(eventCallbacks[0]!.filter.include).toMatchInlineSnapshot(`
    [
      "transaction.gas",
      "transaction.maxFeePerGas",
      "log.address",
      "log.data",
      "log.logIndex",
      "log.removed",
      "log.topics",
      "transaction.transactionIndex",
      "transaction.from",
      "transaction.to",
      "transaction.hash",
      "transaction.type",
      "block.timestamp",
      "block.number",
      "block.hash",
    ]
  `);
  // Remove accessed property to simulate resolved column selection
  // @ts-ignore
  // biome-ignore lint/performance/noDelete: <explanation>
  delete event.event.transaction.maxPriorityFeePerGas;
  await expect(
    indexing.processHistoricalEvents({
      events: [event],
      updateIndexingSeconds: vi.fn(),
    }),
  ).rejects.toThrowError(
    new InvalidEventAccessError("transaction.maxPriorityFeePerGas"),
  );
});
test("ponderActions getBalance()", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const eventCount = getEventCount(indexingFunctions);
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  cachedViemClient.event = event;
  const client = cachedViemClient.getClient(chain);
  const balance = await client.getBalance({ address: BOB });
  expect(balance).toBe(parseEther("10000"));
});
test("ponderActions getCode()", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const eventCount = getEventCount(indexingFunctions);
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  cachedViemClient.event = event;
  const client = cachedViemClient.getClient(chain);
  const bytecode = await client.getCode({ address });
  expect(bytecode).toBeTruthy();
});
test("ponderActions getStorageAt()", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const eventCount = getEventCount(indexingFunctions);
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  cachedViemClient.event = event;
  const client = cachedViemClient.getClient(chain);
  const storage = await client.getStorageAt({
    address,
    // totalSupply is in the third storage slot
    slot: toHex(2),
  });
  expect(BigInt(storage!)).toBe(parseEther("1"));
});
test("ponderActions readContract()", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const eventCount = getEventCount(indexingFunctions);
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  cachedViemClient.event = event;
  const client = cachedViemClient.getClient(chain);
  const totalSupply = await client.readContract({
    abi: erc20ABI,
    functionName: "totalSupply",
    address,
  });
  expect(totalSupply).toMatchInlineSnapshot("1000000000000000000n");
});
test("ponderActions readContract() blockNumber", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const eventCount = getEventCount(indexingFunctions);
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  cachedViemClient.event = event;
  const client = cachedViemClient.getClient(chain);
  const totalSupply = await client.readContract({
    abi: erc20ABI,
    functionName: "totalSupply",
    address,
    blockNumber: 1n,
  });
  expect(totalSupply).toMatchInlineSnapshot("0n");
});
test("ponderActions readContract() ContractFunctionZeroDataError", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const eventCount = getEventCount(indexingFunctions);
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  // Mock requestQueue.request to throw ContractFunctionZeroDataError
  const requestSpy = vi.spyOn(rpc, "request");
  requestSpy.mockResolvedValueOnce("0x");
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  cachedViemClient.event = event;
  const client = cachedViemClient.getClient(chain);
  const totalSupply = await client.readContract({
    abi: erc20ABI,
    functionName: "totalSupply",
    address,
  });
  expect(totalSupply).toBe(parseEther("1"));
  expect(requestSpy).toHaveBeenCalledTimes(2);
});
test("ponderActions multicall()", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address: multicall } = await deployMulticall({ sender: ALICE });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const eventCount = getEventCount(indexingFunctions);
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  cachedViemClient.event = event;
  const client = cachedViemClient.getClient(chain);
  const [totalSupply] = await client.multicall({
    allowFailure: false,
    multicallAddress: multicall,
    contracts: [
      {
        abi: erc20ABI,
        functionName: "totalSupply",
        address,
      },
      {
        abi: erc20ABI,
        functionName: "totalSupply",
        address,
      },
    ],
  });
  expect(totalSupply).toMatchInlineSnapshot("1000000000000000000n");
});
test("ponderActions multicall() allowFailure", async () => {
  const { common } = context;
  const { syncStore } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address: multicall } = await deployMulticall({ sender: ALICE });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const eventCount = getEventCount(indexingFunctions);
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0],
    blockData,
  });
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild: { chains: [chain], rpcs: [rpc] },
    syncStore,
    eventCount,
  });
  cachedViemClient.event = event;
  const client = cachedViemClient.getClient(chain);
  const result = await client.multicall({
    allowFailure: true,
    multicallAddress: multicall,
    contracts: [
      {
        abi: erc20ABI,
        functionName: "totalSupply",
        address,
      },
      {
        abi: erc20ABI,
        functionName: "totalSupply",
        address,
      },
    ],
  });
  expect(result).toMatchInlineSnapshot(`
    [
      {
        "result": 1000000000000000000n,
        "status": "success",
      },
      {
        "result": 1000000000000000000n,
        "status": "success",
      },
    ]
  `);
});
</file>

<file path="packages/core/src/indexing/index.ts">
import util from "node:util";
import type { IndexingCache } from "@/indexing-store/cache.js";
import type { IndexingStore } from "@/indexing-store/index.js";
import type { CachedViemClient } from "@/indexing/client.js";
import type { Common } from "@/internal/common.js";
import {
  BaseError,
  IndexingFunctionError,
  InvalidEventAccessError,
  ShutdownError,
} from "@/internal/errors.js";
import type {
  Chain,
  Contract,
  Event,
  Filter,
  IndexingBuild,
  IndexingErrorHandler,
  Schema,
  SetupEvent,
  UserBlock,
  UserLog,
  UserTrace,
  UserTransaction,
} from "@/internal/types.js";
import {
  defaultBlockFilterInclude,
  defaultBlockInclude,
  defaultLogFilterInclude,
  defaultTraceFilterInclude,
  defaultTraceInclude,
  defaultTransactionFilterInclude,
  defaultTransactionInclude,
  defaultTransactionReceiptInclude,
  defaultTransferFilterInclude,
  requiredBlockFilterInclude,
  requiredLogFilterInclude,
  requiredTraceFilterInclude,
  requiredTransactionFilterInclude,
  requiredTransactionReceiptInclude,
  requiredTransferFilterInclude,
} from "@/runtime/filter.js";
import type { Db } from "@/types/db.js";
import type {
  Block,
  Trace,
  Transaction,
  TransactionReceipt,
} from "@/types/eth.js";
import type { DeepPartial } from "@/types/utils.js";
import {
  ZERO_CHECKPOINT,
  decodeCheckpoint,
  encodeCheckpoint,
} from "@/utils/checkpoint.js";
import { dedupe } from "@/utils/dedupe.js";
import { prettyPrint } from "@/utils/print.js";
import { startClock } from "@/utils/timer.js";
import type { Abi, Address } from "viem";
import { addStackTrace } from "./addStackTrace.js";
import type { ReadonlyClient } from "./client.js";
declare global {
  var DISABLE_EVENT_PROXY: boolean;
}
globalThis.DISABLE_EVENT_PROXY = false;
const EVENT_LOOP_UPDATE_INTERVAL = 25;
const METRICS_UPDATE_INTERVAL = 100;
export type Context = {
  chain: { id: number; name: string };
  client: ReadonlyClient;
  db: Db<Schema>;
  contracts: Record<
    string,
    {
      abi: Abi;
      address?: Address | readonly Address[];
      startBlock?: number;
      endBlock?: number;
    }
  >;
};
export type Indexing = {
  processSetupEvents: () => Promise<void>;
  processHistoricalEvents: (params: {
    events: Event[];
    updateIndexingSeconds: (event: Event, chain: Chain) => void;
  }) => Promise<void>;
  processRealtimeEvents: (params: { events: Event[] }) => Promise<void>;
};
export const getEventCount = (
  indexingFunctions: IndexingBuild["indexingFunctions"],
) => {
  const eventCount: { [eventName: string]: number } = {};
  for (const { name: eventName } of indexingFunctions) {
    eventCount[eventName] = 0;
  }
  return eventCount;
};
export type ColumnAccessProfile = {
  block: Set<keyof Block>;
  trace: Set<keyof Trace>;
  transaction: Set<keyof Transaction>;
  transactionReceipt: Set<keyof TransactionReceipt>;
  resolved: boolean;
  count: number;
};
export type ColumnAccessPattern = Map<string, ColumnAccessProfile>;
export const createColumnAccessPattern = ({
  indexingBuild,
}: {
  indexingBuild: Pick<IndexingBuild, "indexingFunctions">;
}): ColumnAccessPattern => {
  const columnAccessPattern = new Map<string, ColumnAccessProfile>();
  for (const { name: eventName } of indexingBuild.indexingFunctions) {
    columnAccessPattern.set(eventName, {
      block: new Set(),
      trace: new Set(),
      transaction: new Set(),
      transactionReceipt: new Set(),
      resolved: false,
      count: 0,
    });
  }
  return columnAccessPattern;
};
export const createIndexing = ({
  common,
  indexingBuild: {
    eventCallbacks,
    setupCallbacks,
    chains,
    contracts,
    indexingFunctions,
  },
  indexingStore,
  indexingCache,
  client,
  indexingErrorHandler,
  columnAccessPattern,
  eventCount,
}: {
  common: Common;
  indexingBuild: Pick<
    IndexingBuild,
    | "eventCallbacks"
    | "setupCallbacks"
    | "chains"
    | "contracts"
    | "indexingFunctions"
  >;
  indexingStore: IndexingStore;
  indexingCache: IndexingCache;
  client: CachedViemClient;
  indexingErrorHandler: IndexingErrorHandler;
  columnAccessPattern: ColumnAccessPattern;
  eventCount: { [eventName: string]: number };
}): Indexing => {
  const indexingFunctionArg = {
    event: undefined as Event | SetupEvent | undefined,
    context: {
      chain: { name: undefined!, id: undefined! },
      contracts: undefined!,
      client: undefined!,
      db: indexingStore.db,
    } as Context,
  };
  let lastChainId: number | undefined;
  const clientByChainId: { [chainId: number]: ReadonlyClient } = {};
  const contractsByChainId: {
    [chainId: number]: { [name: string]: Contract };
  } = {};
  for (const chain of chains) {
    clientByChainId[chain.id] = client.getClient(chain);
  }
  for (let i = 0; i < chains.length; i++) {
    const chain = chains[i]!;
    contractsByChainId[chain.id] = contracts[i]!;
  }
  const metricLabels: { [event: string]: { event: string } } = {};
  for (const { name } of indexingFunctions) {
    metricLabels[name] = { event: name };
  }
  const executeSetup = async (event: SetupEvent): Promise<void> => {
    try {
      if (event.chain.id !== lastChainId) {
        indexingFunctionArg.context.chain.id = event.chain.id;
        indexingFunctionArg.context.chain.name = event.chain.name;
        indexingFunctionArg.context.contracts =
          contractsByChainId[event.chain.id]!;
        indexingFunctionArg.context.client = clientByChainId[event.chain.id]!;
        lastChainId = event.chain.id;
      }
      const endClock = startClock();
      await event.setupCallback.fn(indexingFunctionArg);
      // Note: Check `getRetryableError` to handle user-code catching errors
      // from the indexing store.
      if (indexingErrorHandler.getRetryableError()) {
        const retryableError = indexingErrorHandler.getRetryableError()!;
        indexingErrorHandler.clearRetryableError();
        throw retryableError;
      }
      common.metrics.ponder_indexing_function_duration.observe(
        metricLabels[event.setupCallback.name]!,
        endClock(),
      );
    } catch (_error) {
      let error = _error instanceof Error ? _error : new Error(String(_error));
      // Note: Use `getRetryableError` rather than `error` to avoid
      // issues with the user-code augmenting errors from the indexing store.
      if (indexingErrorHandler.getRetryableError()) {
        const retryableError = indexingErrorHandler.getRetryableError()!;
        indexingErrorHandler.clearRetryableError();
        error = retryableError;
      }
      if (common.shutdown.isKilled) {
        throw new ShutdownError();
      }
      addStackTrace(error, common.options);
      addErrorMeta(error, toErrorMeta(event));
      const decodedCheckpoint = decodeCheckpoint(event.checkpoint);
      common.logger.error({
        msg: "Error while processing event",
        event: event.setupCallback.name,
        chain: event.chain.name,
        chain_id: event.chain.id,
        block_number: decodedCheckpoint.blockNumber,
        error,
      });
      common.metrics.hasError = true;
      if (error instanceof BaseError === false) {
        error = new IndexingFunctionError(error.message);
      }
      throw error;
    }
  };
  // metric label for "ponder_indexing_function_duration"
  const executeEvent = async (event: Event): Promise<void> => {
    try {
      if (event.chain.id !== lastChainId) {
        indexingFunctionArg.context.chain.id = event.chain.id;
        indexingFunctionArg.context.chain.name = event.chain.name;
        indexingFunctionArg.context.contracts =
          contractsByChainId[event.chain.id]!;
        indexingFunctionArg.context.client = clientByChainId[event.chain.id]!;
        lastChainId = event.chain.id;
      }
      // @ts-ignore
      indexingFunctionArg.event = event.event;
      const endClock = startClock();
      await event.eventCallback.fn(indexingFunctionArg);
      common.metrics.ponder_indexing_function_duration.observe(
        metricLabels[event.eventCallback.name]!,
        endClock(),
      );
      // Note: Check `getRetryableError` to handle user-code catching errors
      // from the indexing store.
      if (indexingErrorHandler.getRetryableError()) {
        const retryableError = indexingErrorHandler.getRetryableError()!;
        indexingErrorHandler.clearRetryableError();
        throw retryableError;
      }
    } catch (_error) {
      let error = _error instanceof Error ? _error : new Error(String(_error));
      // Note: Use `getRetryableError` rather than `error` to avoid
      // issues with the user-code augmenting errors from the indexing store.
      if (indexingErrorHandler.getRetryableError()) {
        const retryableError = indexingErrorHandler.getRetryableError()!;
        indexingErrorHandler.clearRetryableError();
        error = retryableError;
      }
      if (common.shutdown.isKilled) {
        throw new ShutdownError();
      }
      if (error instanceof InvalidEventAccessError) {
        throw error;
      }
      addStackTrace(error, common.options);
      addErrorMeta(error, toErrorMeta(event));
      const decodedCheckpoint = decodeCheckpoint(event.checkpoint);
      common.logger.error({
        msg: "Error while processing event",
        event: event.eventCallback.name,
        chain: event.chain.name,
        chain_id: event.chain.id,
        block_number: decodedCheckpoint.blockNumber,
        error,
      });
      common.metrics.hasError = true;
      if (error instanceof BaseError === false) {
        error = new IndexingFunctionError(error.message);
      }
      throw error;
    }
  };
  const resetFilterInclude = (eventName: string) => {
    const filters = perEventFilters.get(eventName)!;
    let include: Filter["include"];
    // Note: It's an invariant that all filters have the same type.
    switch (filters[0]!.type) {
      case "block": {
        include = defaultBlockFilterInclude;
        break;
      }
      case "transaction": {
        include = defaultTransactionFilterInclude;
        break;
      }
      case "trace": {
        include = defaultTraceFilterInclude.concat(
          filters[0]!.hasTransactionReceipt
            ? defaultTransactionReceiptInclude.map(
                (value) => `transactionReceipt.${value}` as const,
              )
            : [],
        );
        break;
      }
      case "log": {
        include = defaultLogFilterInclude.concat(
          filters[0]!.hasTransactionReceipt
            ? defaultTransactionReceiptInclude.map(
                (value) => `transactionReceipt.${value}` as const,
              )
            : [],
        );
        break;
      }
      case "transfer": {
        include = defaultTransferFilterInclude.concat(
          filters[0]!.hasTransactionReceipt
            ? defaultTransactionReceiptInclude.map(
                (value) => `transactionReceipt.${value}` as const,
              )
            : [],
        );
        break;
      }
    }
    for (const filter of filters) {
      isFilterResolved.set(filter, false);
      filter.include = include;
    }
    columnAccessPattern.get(eventName)!.count = 0;
  };
  const blockProxy = createEventProxy<Block>(
    columnAccessPattern,
    "block",
    indexingErrorHandler,
    resetFilterInclude,
  );
  const transactionProxy = createEventProxy<Transaction>(
    columnAccessPattern,
    "transaction",
    indexingErrorHandler,
    resetFilterInclude,
  );
  const transactionReceiptProxy = createEventProxy<TransactionReceipt>(
    columnAccessPattern,
    "transactionReceipt",
    indexingErrorHandler,
    resetFilterInclude,
  );
  const traceProxy = createEventProxy<Trace>(
    columnAccessPattern,
    "trace",
    indexingErrorHandler,
    resetFilterInclude,
  );
  // Note: There is no `log` proxy because all log columns are required.
  // Note: Indexing functions map to one or more filters.
  const perEventFilters = new Map<string, Filter[]>();
  const isFilterResolved = new Map<Filter, boolean>();
  for (const eventCallback of eventCallbacks.flat()) {
    if (perEventFilters.has(eventCallback.name) === false) {
      perEventFilters.set(eventCallback.name, [eventCallback.filter]);
    } else {
      perEventFilters.get(eventCallback.name)!.push(eventCallback.filter);
    }
    isFilterResolved.set(eventCallback.filter, false);
  }
  return {
    async processSetupEvents() {
      for (const setupCallback of setupCallbacks.flat()) {
        const event = {
          type: "setup",
          chain: setupCallback.chain,
          setupCallback,
          checkpoint: encodeCheckpoint({
            ...ZERO_CHECKPOINT,
            chainId: BigInt(setupCallback.chain.id),
            blockNumber: BigInt(setupCallback.block ?? 0),
          }),
          block: BigInt(setupCallback.block ?? 0),
        } satisfies SetupEvent;
        client.event = event;
        await executeSetup(event);
      }
    },
    async processHistoricalEvents({ events, updateIndexingSeconds }) {
      let lastEventLoopUpdate = performance.now();
      let lastMetricsUpdate = performance.now();
      for (let i = 0; i < events.length; i++) {
        const event = events[i]!;
        client.event = event;
        indexingCache.event = event;
        // Note: Create a new event object instead of mutuating the original one because
        // the event object could be reused across multiple indexing functions.
        const proxyEvent: typeof event.event = { ...event.event };
        switch (event.type) {
          case "block": {
            blockProxy.eventName = event.eventCallback.name;
            blockProxy.underlying = event.event.block as Block;
            proxyEvent.block = blockProxy.proxy;
            break;
          }
          case "transaction": {
            blockProxy.eventName = event.eventCallback.name;
            blockProxy.underlying = event.event.block as Block;
            proxyEvent.block = blockProxy.proxy;
            transactionProxy.eventName = event.eventCallback.name;
            transactionProxy.underlying = event.event
              .transaction as Transaction;
            // @ts-expect-error
            proxyEvent.transaction = transactionProxy.proxy;
            if (event.event.transactionReceipt !== undefined) {
              transactionReceiptProxy.eventName = event.eventCallback.name;
              transactionReceiptProxy.underlying = event.event
                .transactionReceipt as TransactionReceipt;
              // @ts-expect-error
              proxyEvent.transactionReceipt = transactionReceiptProxy.proxy;
            }
            break;
          }
          case "trace":
          case "transfer": {
            blockProxy.eventName = event.eventCallback.name;
            blockProxy.underlying = event.event.block as Block;
            proxyEvent.block = blockProxy.proxy;
            transactionProxy.eventName = event.eventCallback.name;
            transactionProxy.underlying = event.event
              .transaction as Transaction;
            // @ts-expect-error
            proxyEvent.transaction = transactionProxy.proxy;
            if (event.event.transactionReceipt !== undefined) {
              transactionReceiptProxy.eventName = event.eventCallback.name;
              transactionReceiptProxy.underlying = event.event
                .transactionReceipt as TransactionReceipt;
              // @ts-expect-error
              proxyEvent.transactionReceipt = transactionReceiptProxy.proxy;
            }
            traceProxy.eventName = event.eventCallback.name;
            traceProxy.underlying = event.event.trace as Trace;
            // @ts-expect-error
            proxyEvent.trace = traceProxy.proxy;
            break;
          }
          case "log": {
            blockProxy.eventName = event.eventCallback.name;
            blockProxy.underlying = event.event.block as Block;
            proxyEvent.block = blockProxy.proxy;
            if (event.event.transaction !== undefined) {
              transactionProxy.eventName = event.eventCallback.name;
              transactionProxy.underlying = event.event
                .transaction as Transaction;
              // @ts-expect-error
              proxyEvent.transaction = transactionProxy.proxy;
            }
            if (event.event.transactionReceipt !== undefined) {
              transactionReceiptProxy.eventName = event.eventCallback.name;
              transactionReceiptProxy.underlying = event.event
                .transactionReceipt as TransactionReceipt;
              // @ts-expect-error
              proxyEvent.transactionReceipt = transactionReceiptProxy.proxy;
            }
            break;
          }
        }
        // @ts-expect-error
        await executeEvent({ ...event, event: proxyEvent });
        common.metrics.ponder_indexing_completed_events.inc(
          { event: event.eventCallback.name },
          1,
        );
        columnAccessPattern.get(event.eventCallback.name)!.count++;
        eventCount[event.eventCallback.name]++;
        const now = performance.now();
        if (now - lastEventLoopUpdate > EVENT_LOOP_UPDATE_INTERVAL) {
          lastEventLoopUpdate = now;
          await new Promise(setImmediate);
        }
        if (now - lastMetricsUpdate > METRICS_UPDATE_INTERVAL) {
          lastMetricsUpdate = now;
          updateIndexingSeconds(event, event.chain);
        }
      }
      let isEveryFilterResolvedBefore = true;
      let isEveryFilterResolvedAfter = true;
      for (const eventCallback of eventCallbacks.flat()) {
        if (isFilterResolved.get(eventCallback.filter)) continue;
        isEveryFilterResolvedBefore = false;
        if (columnAccessPattern.get(eventCallback.name)!.count < 100) {
          isEveryFilterResolvedAfter = false;
          continue;
        }
        isFilterResolved.set(eventCallback.filter, true);
        const filterInclude: Filter["include"] = [];
        const columnAccessProfile = columnAccessPattern.get(
          eventCallback.name,
        )!;
        columnAccessProfile.resolved = true;
        for (const column of columnAccessProfile.block) {
          filterInclude.push(`block.${column}` as const);
        }
        for (const column of columnAccessProfile.transaction) {
          // @ts-expect-error
          filterInclude.push(`transaction.${column}` as const);
        }
        for (const column of columnAccessProfile.transactionReceipt) {
          // @ts-expect-error
          filterInclude.push(`transactionReceipt.${column}` as const);
        }
        for (const column of columnAccessProfile.trace) {
          // @ts-expect-error
          filterInclude.push(`trace.${column}` as const);
        }
        switch (eventCallback.filter.type) {
          case "block": {
            filterInclude.push(...requiredBlockFilterInclude);
            break;
          }
          case "transaction": {
            // @ts-expect-error
            filterInclude.push(...requiredTransactionFilterInclude);
            break;
          }
          case "trace": {
            // @ts-expect-error
            filterInclude.push(...requiredTraceFilterInclude);
            if (eventCallback.filter.hasTransactionReceipt) {
              filterInclude.push(
                // @ts-expect-error
                ...requiredTransactionReceiptInclude.map(
                  (value) => `transactionReceipt.${value}` as const,
                ),
              );
            }
            break;
          }
          case "log": {
            // @ts-expect-error
            filterInclude.push(...requiredLogFilterInclude);
            if (eventCallback.filter.hasTransactionReceipt) {
              filterInclude.push(
                // @ts-expect-error
                ...requiredTransactionReceiptInclude.map(
                  (value) => `transactionReceipt.${value}` as const,
                ),
              );
            }
            break;
          }
          case "transfer": {
            // @ts-expect-error
            filterInclude.push(...requiredTransferFilterInclude);
            if (eventCallback.filter.hasTransactionReceipt) {
              filterInclude.push(
                // @ts-expect-error
                ...requiredTransactionReceiptInclude.map(
                  (value) => `transactionReceipt.${value}` as const,
                ),
              );
            }
            break;
          }
        }
        // @ts-expect-error
        eventCallback.filter.include = dedupe(filterInclude);
      }
      if (isEveryFilterResolvedBefore === false && isEveryFilterResolvedAfter) {
        const blockInclude = new Set<keyof Block>();
        const transactionInclude = new Set<keyof Transaction>();
        const transactionReceiptInclude = new Set<keyof TransactionReceipt>();
        const traceInclude = new Set<keyof Trace>();
        for (const [_, columnAccessProfile] of columnAccessPattern) {
          for (const blockAccess of columnAccessProfile.block) {
            blockInclude.add(blockAccess);
          }
          for (const transactionAccess of columnAccessProfile.transaction) {
            transactionInclude.add(transactionAccess);
          }
          for (const transactionReceiptAccess of columnAccessProfile.transactionReceipt) {
            transactionReceiptInclude.add(transactionReceiptAccess);
          }
          for (const traceAccess of columnAccessProfile.trace) {
            traceInclude.add(traceAccess);
          }
        }
        common.logger.debug(
          {
            msg: "Resolved event property access",
            total_access_count:
              blockInclude.size +
              transactionInclude.size +
              transactionReceiptInclude.size +
              traceInclude.size,
            block_count: blockInclude.size,
            transaction_count: transactionInclude.size,
            transaction_receipt_count: transactionReceiptInclude.size,
            trace_count: traceInclude.size,
          },
          ["total_access_count"],
        );
      }
      await new Promise(setImmediate);
      if (events.length > 0) {
        updateIndexingSeconds(
          events[events.length - 1]!,
          events[events.length - 1]!.chain,
        );
      }
    },
    async processRealtimeEvents({ events }) {
      for (let i = 0; i < events.length; i++) {
        const event = events[i]!;
        client.event = event;
        await executeEvent(event);
        common.metrics.ponder_indexing_completed_events.inc(
          { event: event.eventCallback.name },
          1,
        );
        eventCount[event.eventCallback.name]++;
      }
    },
  };
};
export const createEventProxy = <
  T extends Block | Transaction | TransactionReceipt | Trace,
>(
  columnAccessPattern: ColumnAccessPattern,
  type: "block" | "trace" | "transaction" | "transactionReceipt",
  indexingErrorHandler: IndexingErrorHandler,
  resetFilterInclude: (eventName: string) => void,
): { proxy: T; underlying: T; eventName: string } => {
  let underlying: T = undefined!;
  let eventName: string = undefined!;
  // Note: We rely on the fact that `default[type]Include` is the entire set of possible columns.
  let defaultInclude: Set<keyof T>;
  if (type === "block") {
    // @ts-expect-error
    defaultInclude = new Set(defaultBlockInclude);
  } else if (type === "trace") {
    // @ts-expect-error
    defaultInclude = new Set(defaultTraceInclude);
  } else if (type === "transaction") {
    // @ts-expect-error
    defaultInclude = new Set(defaultTransactionInclude);
  } else if (type === "transactionReceipt") {
    // @ts-expect-error
    defaultInclude = new Set(defaultTransactionReceiptInclude);
  }
  const proxy = new Proxy<T>(
    // @ts-expect-error
    {
      [util.inspect.custom]: (): T => {
        const printableObject = {} as T;
        for (const prop of defaultInclude) {
          printableObject[prop] = proxy[prop];
        }
        return printableObject;
      },
    },
    {
      deleteProperty(_, prop) {
        if (
          // @ts-expect-error
          defaultInclude.has(prop) === false ||
          globalThis.DISABLE_EVENT_PROXY
        ) {
          return Reflect.deleteProperty(underlying, prop);
        }
        const profile = columnAccessPattern.get(eventName)!;
        const isInvalidAccess = prop in underlying === false;
        // @ts-expect-error
        profile[type].add(prop);
        if (isInvalidAccess) {
          profile.resolved = false;
          resetFilterInclude(eventName);
          // @ts-expect-error
          const error = new InvalidEventAccessError(`${type}.${prop}`);
          indexingErrorHandler.setRetryableError(error);
          throw error;
        }
        return Reflect.deleteProperty(underlying, prop);
      },
      has(_, prop) {
        // @ts-expect-error
        return defaultInclude.has(prop);
      },
      ownKeys() {
        return Array.from(defaultInclude);
      },
      set(_, prop, value) {
        if (
          // @ts-expect-error
          defaultInclude.has(prop) === false ||
          globalThis.DISABLE_EVENT_PROXY
        ) {
          return Reflect.set(underlying, prop, value);
        }
        const profile = columnAccessPattern.get(eventName)!;
        const isInvalidAccess = prop in underlying === false;
        // @ts-expect-error
        profile[type].add(prop);
        if (isInvalidAccess) {
          profile.resolved = false;
          resetFilterInclude(eventName);
          // @ts-expect-error
          const error = new InvalidEventAccessError(`${type}.${prop}`);
          indexingErrorHandler.setRetryableError(error);
          throw error;
        }
        return Reflect.set(underlying, prop, value);
      },
      get(_, prop, receiver) {
        if (
          // @ts-expect-error
          defaultInclude.has(prop) === false ||
          globalThis.DISABLE_EVENT_PROXY
        ) {
          return Reflect.get(underlying, prop, receiver);
        }
        const profile = columnAccessPattern.get(eventName)!;
        const isInvalidAccess = prop in underlying === false;
        // @ts-expect-error
        profile[type].add(prop);
        if (isInvalidAccess) {
          profile.resolved = false;
          resetFilterInclude(eventName);
          // @ts-expect-error
          const error = new InvalidEventAccessError(`${type}.${prop}`);
          indexingErrorHandler.setRetryableError(error);
          throw error;
        }
        return Reflect.get(underlying, prop, receiver);
      },
    },
  );
  return {
    proxy,
    set underlying(_underlying: T) {
      underlying = _underlying;
    },
    set eventName(_eventName: string) {
      eventName = _eventName;
    },
  };
};
export const toErrorMeta = (
  event: DeepPartial<Event> | DeepPartial<SetupEvent>,
) => {
  globalThis.DISABLE_EVENT_PROXY = true;
  switch (event?.type) {
    case "setup": {
      const meta = `Block:\n${prettyPrint({
        number: event?.block,
      })}`;
      globalThis.DISABLE_EVENT_PROXY = false;
      return meta;
    }
    case "log": {
      const meta = [
        `Event arguments:\n${prettyPrint(Array.isArray(event.event?.args) ? undefined : event.event?.args)}`,
        logText(event?.event?.log),
        transactionText(event?.event?.transaction),
        blockText(event?.event?.block),
      ].join("\n");
      globalThis.DISABLE_EVENT_PROXY = false;
      return meta;
    }
    case "trace": {
      const meta = [
        `Call trace arguments:\n${prettyPrint(Array.isArray(event.event?.args) ? undefined : event.event?.args)}`,
        traceText(event?.event?.trace),
        transactionText(event?.event?.transaction),
        blockText(event?.event?.block),
      ].join("\n");
      globalThis.DISABLE_EVENT_PROXY = false;
      return meta;
    }
    case "transfer": {
      const meta = [
        `Transfer arguments:\n${prettyPrint(event?.event?.transfer)}`,
        traceText(event?.event?.trace),
        transactionText(event?.event?.transaction),
        blockText(event?.event?.block),
      ].join("\n");
      globalThis.DISABLE_EVENT_PROXY = false;
      return meta;
    }
    case "block": {
      const meta = blockText(event?.event?.block);
      globalThis.DISABLE_EVENT_PROXY = false;
      return meta;
    }
    case "transaction": {
      const meta = [
        transactionText(event?.event?.transaction),
        blockText(event?.event?.block),
      ].join("\n");
      globalThis.DISABLE_EVENT_PROXY = false;
      return meta;
    }
    default: {
      return undefined;
    }
  }
};
export const addErrorMeta = (error: unknown, meta: string | undefined) => {
  // If error isn't an object we can modify, do nothing
  if (typeof error !== "object" || error === null) return;
  if (meta === undefined) return;
  try {
    const errorObj = error as { meta?: unknown };
    // If meta exists and is an array, try to add to it
    if (Array.isArray(errorObj.meta)) {
      errorObj.meta = [...errorObj.meta, meta];
    } else {
      // Otherwise set meta to be a new array with the meta string
      errorObj.meta = [meta];
    }
  } catch {
    // Ignore errors
  }
};
const blockText = (block?: DeepPartial<UserBlock>) =>
  `Block:\n${prettyPrint({
    hash: block?.hash,
    number: block?.number,
    timestamp: block?.timestamp,
  })}`;
const transactionText = (transaction?: DeepPartial<UserTransaction>) =>
  `Transaction:\n${prettyPrint({
    hash: transaction?.hash,
    from: transaction?.from,
    to: transaction?.to,
  })}`;
const logText = (log?: DeepPartial<UserLog>) =>
  `Log:\n${prettyPrint({
    index: log?.logIndex,
    address: log?.address,
  })}`;
const traceText = (trace?: DeepPartial<UserTrace>) =>
  `Trace:\n${prettyPrint({
    traceIndex: trace?.traceIndex,
    from: trace?.from,
    to: trace?.to,
  })}`;
</file>

<file path="packages/core/src/indexing/profile.test.ts">
import { ALICE } from "@/_test/constants.js";
import { erc20ABI } from "@/_test/generated.js";
import { getBlocksIndexingBuild, getChain } from "@/_test/utils.js";
import type { BlockEvent, LogEvent, TraceEvent } from "@/internal/types.js";
import { ZERO_CHECKPOINT_STRING } from "@/utils/checkpoint.js";
import { getAbiItem, zeroAddress } from "viem";
import { expect, test } from "vitest";
import { recordProfilePattern, recoverProfilePattern } from "./profile.js";
test("recordProfilePattern() with undefined log event args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: undefined,
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { number: 5n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const pattern = recordProfilePattern({
    event,
    args: {
      address: zeroAddress,
      abi: [getAbiItem({ abi: erc20ABI, name: "balanceOf" })],
      functionName: "balanceOf",
      args: [ALICE],
    },
    hints: [],
  });
  expect(pattern).toMatchInlineSnapshot(`
    {
      "hasConstant": true,
      "pattern": {
        "abi": [
          {
            "inputs": [
              {
                "internalType": "address",
                "name": "",
                "type": "address",
              },
            ],
            "name": "balanceOf",
            "outputs": [
              {
                "internalType": "uint256",
                "name": "",
                "type": "uint256",
              },
            ],
            "stateMutability": "view",
            "type": "function",
          },
        ],
        "address": {
          "type": "constant",
          "value": "0x0000000000000000000000000000000000000000",
        },
        "args": [
          {
            "type": "constant",
            "value": "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
          },
        ],
        "cache": undefined,
        "functionName": "balanceOf",
      },
    }
  `);
  expect(recoverProfilePattern(pattern!.pattern, event)).toMatchInlineSnapshot(`
    {
      "abi": [
        {
          "inputs": [
            {
              "internalType": "address",
              "name": "",
              "type": "address",
            },
          ],
          "name": "balanceOf",
          "outputs": [
            {
              "internalType": "uint256",
              "name": "",
              "type": "uint256",
            },
          ],
          "stateMutability": "view",
          "type": "function",
        },
      ],
      "address": "0x0000000000000000000000000000000000000000",
      "args": [
        "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
      ],
      "blockNumber": 5n,
      "chainId": 1,
      "functionName": "balanceOf",
    }
  `);
});
test("recordProfilePattern() with undefined trace event args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "trace",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: undefined,
      result: undefined,
      trace: {} as TraceEvent["event"]["trace"],
      transaction: {} as TraceEvent["event"]["transaction"],
      block: { number: 5n } as BlockEvent["event"]["block"],
    },
  } satisfies TraceEvent;
  const pattern = recordProfilePattern({
    event,
    args: {
      address: zeroAddress,
      abi: [getAbiItem({ abi: erc20ABI, name: "balanceOf" })],
      functionName: "balanceOf",
      args: [ALICE],
    },
    hints: [],
  });
  expect(pattern).toMatchInlineSnapshot(`
    {
      "hasConstant": true,
      "pattern": {
        "abi": [
          {
            "inputs": [
              {
                "internalType": "address",
                "name": "",
                "type": "address",
              },
            ],
            "name": "balanceOf",
            "outputs": [
              {
                "internalType": "uint256",
                "name": "",
                "type": "uint256",
              },
            ],
            "stateMutability": "view",
            "type": "function",
          },
        ],
        "address": {
          "type": "constant",
          "value": "0x0000000000000000000000000000000000000000",
        },
        "args": [
          {
            "type": "constant",
            "value": "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
          },
        ],
        "cache": undefined,
        "functionName": "balanceOf",
      },
    }
  `);
  expect(recoverProfilePattern(pattern!.pattern, event)).toMatchInlineSnapshot(`
    {
      "abi": [
        {
          "inputs": [
            {
              "internalType": "address",
              "name": "",
              "type": "address",
            },
          ],
          "name": "balanceOf",
          "outputs": [
            {
              "internalType": "uint256",
              "name": "",
              "type": "uint256",
            },
          ],
          "stateMutability": "view",
          "type": "function",
        },
      ],
      "address": "0x0000000000000000000000000000000000000000",
      "args": [
        "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
      ],
      "blockNumber": 5n,
      "chainId": 1,
      "functionName": "balanceOf",
    }
  `);
});
test("recordProfilePattern() with array log event args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: [],
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { number: 5n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const pattern = recordProfilePattern({
    event,
    args: {
      address: zeroAddress,
      abi: [getAbiItem({ abi: erc20ABI, name: "balanceOf" })],
      functionName: "balanceOf",
      args: [ALICE],
    },
    hints: [],
  });
  expect(pattern).toMatchInlineSnapshot(`
    {
      "hasConstant": true,
      "pattern": {
        "abi": [
          {
            "inputs": [
              {
                "internalType": "address",
                "name": "",
                "type": "address",
              },
            ],
            "name": "balanceOf",
            "outputs": [
              {
                "internalType": "uint256",
                "name": "",
                "type": "uint256",
              },
            ],
            "stateMutability": "view",
            "type": "function",
          },
        ],
        "address": {
          "type": "constant",
          "value": "0x0000000000000000000000000000000000000000",
        },
        "args": [
          {
            "type": "constant",
            "value": "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
          },
        ],
        "cache": undefined,
        "functionName": "balanceOf",
      },
    }
  `);
  expect(recoverProfilePattern(pattern!.pattern, event)).toMatchInlineSnapshot(`
    {
      "abi": [
        {
          "inputs": [
            {
              "internalType": "address",
              "name": "",
              "type": "address",
            },
          ],
          "name": "balanceOf",
          "outputs": [
            {
              "internalType": "uint256",
              "name": "",
              "type": "uint256",
            },
          ],
          "stateMutability": "view",
          "type": "function",
        },
      ],
      "address": "0x0000000000000000000000000000000000000000",
      "args": [
        "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
      ],
      "blockNumber": 5n,
      "chainId": 1,
      "functionName": "balanceOf",
    }
  `);
});
test("recordProfilePattern() with array trace event args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "trace",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: [],
      result: [],
      trace: {} as TraceEvent["event"]["trace"],
      transaction: {} as TraceEvent["event"]["transaction"],
      block: { number: 5n } as BlockEvent["event"]["block"],
    },
  } satisfies TraceEvent;
  const pattern = recordProfilePattern({
    event,
    args: {
      address: zeroAddress,
      abi: [getAbiItem({ abi: erc20ABI, name: "balanceOf" })],
      functionName: "balanceOf",
      args: [ALICE],
    },
    hints: [],
  });
  expect(pattern).toMatchInlineSnapshot(`
    {
      "hasConstant": true,
      "pattern": {
        "abi": [
          {
            "inputs": [
              {
                "internalType": "address",
                "name": "",
                "type": "address",
              },
            ],
            "name": "balanceOf",
            "outputs": [
              {
                "internalType": "uint256",
                "name": "",
                "type": "uint256",
              },
            ],
            "stateMutability": "view",
            "type": "function",
          },
        ],
        "address": {
          "type": "constant",
          "value": "0x0000000000000000000000000000000000000000",
        },
        "args": [
          {
            "type": "constant",
            "value": "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
          },
        ],
        "cache": undefined,
        "functionName": "balanceOf",
      },
    }
  `);
  expect(recoverProfilePattern(pattern!.pattern, event)).toMatchInlineSnapshot(`
    {
      "abi": [
        {
          "inputs": [
            {
              "internalType": "address",
              "name": "",
              "type": "address",
            },
          ],
          "name": "balanceOf",
          "outputs": [
            {
              "internalType": "uint256",
              "name": "",
              "type": "uint256",
            },
          ],
          "stateMutability": "view",
          "type": "function",
        },
      ],
      "address": "0x0000000000000000000000000000000000000000",
      "args": [
        "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
      ],
      "blockNumber": 5n,
      "chainId": 1,
      "functionName": "balanceOf",
    }
  `);
});
test("recordProfilePattern() address", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: { address: zeroAddress },
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: {
        number: 1n,
      } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const pattern = recordProfilePattern({
    event,
    args: {
      address: zeroAddress,
      abi: [getAbiItem({ abi: erc20ABI, name: "balanceOf" })],
      functionName: "totalSupply",
    },
    hints: [],
  });
  expect(pattern).toMatchInlineSnapshot(`
    {
      "hasConstant": false,
      "pattern": {
        "abi": [
          {
            "inputs": [
              {
                "internalType": "address",
                "name": "",
                "type": "address",
              },
            ],
            "name": "balanceOf",
            "outputs": [
              {
                "internalType": "uint256",
                "name": "",
                "type": "uint256",
              },
            ],
            "stateMutability": "view",
            "type": "function",
          },
        ],
        "address": {
          "type": "derived",
          "value": [
            "args",
            "address",
          ],
        },
        "args": undefined,
        "cache": undefined,
        "functionName": "totalSupply",
      },
    }
  `);
  expect(recoverProfilePattern(pattern!.pattern, event)).toMatchInlineSnapshot(`
    {
      "abi": [
        {
          "inputs": [
            {
              "internalType": "address",
              "name": "",
              "type": "address",
            },
          ],
          "name": "balanceOf",
          "outputs": [
            {
              "internalType": "uint256",
              "name": "",
              "type": "uint256",
            },
          ],
          "stateMutability": "view",
          "type": "function",
        },
      ],
      "address": "0x0000000000000000000000000000000000000000",
      "args": undefined,
      "blockNumber": 1n,
      "chainId": 1,
      "functionName": "totalSupply",
    }
  `);
});
test("recordProfilePattern() args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: { address: zeroAddress },
      log: {
        address: ALICE,
      } as unknown as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { number: 5n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const pattern = recordProfilePattern({
    event,
    args: {
      address: zeroAddress,
      abi: [getAbiItem({ abi: erc20ABI, name: "balanceOf" })],
      functionName: "balanceOf",
      args: [ALICE],
    },
    hints: [],
  });
  expect(pattern).toMatchInlineSnapshot(`
    {
      "hasConstant": false,
      "pattern": {
        "abi": [
          {
            "inputs": [
              {
                "internalType": "address",
                "name": "",
                "type": "address",
              },
            ],
            "name": "balanceOf",
            "outputs": [
              {
                "internalType": "uint256",
                "name": "",
                "type": "uint256",
              },
            ],
            "stateMutability": "view",
            "type": "function",
          },
        ],
        "address": {
          "type": "derived",
          "value": [
            "args",
            "address",
          ],
        },
        "args": [
          {
            "type": "derived",
            "value": [
              "log",
              "address",
            ],
          },
        ],
        "cache": undefined,
        "functionName": "balanceOf",
      },
    }
  `);
  expect(recoverProfilePattern(pattern!.pattern, event)).toMatchInlineSnapshot(`
    {
      "abi": [
        {
          "inputs": [
            {
              "internalType": "address",
              "name": "",
              "type": "address",
            },
          ],
          "name": "balanceOf",
          "outputs": [
            {
              "internalType": "uint256",
              "name": "",
              "type": "uint256",
            },
          ],
          "stateMutability": "view",
          "type": "function",
        },
      ],
      "address": "0x0000000000000000000000000000000000000000",
      "args": [
        "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
      ],
      "blockNumber": 5n,
      "chainId": 1,
      "functionName": "balanceOf",
    }
  `);
});
test("recordProfilePattern() constants", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: { address: zeroAddress },
      log: {} as unknown as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { number: 5n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const pattern = recordProfilePattern({
    event,
    args: {
      address: zeroAddress,
      abi: [getAbiItem({ abi: erc20ABI, name: "balanceOf" })],
      functionName: "balanceOf",
      args: [ALICE],
    },
    hints: [],
  });
  expect(pattern).toMatchInlineSnapshot(`
    {
      "hasConstant": true,
      "pattern": {
        "abi": [
          {
            "inputs": [
              {
                "internalType": "address",
                "name": "",
                "type": "address",
              },
            ],
            "name": "balanceOf",
            "outputs": [
              {
                "internalType": "uint256",
                "name": "",
                "type": "uint256",
              },
            ],
            "stateMutability": "view",
            "type": "function",
          },
        ],
        "address": {
          "type": "derived",
          "value": [
            "args",
            "address",
          ],
        },
        "args": [
          {
            "type": "constant",
            "value": "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
          },
        ],
        "cache": undefined,
        "functionName": "balanceOf",
      },
    }
  `);
  expect(recoverProfilePattern(pattern!.pattern, event)).toMatchInlineSnapshot(`
    {
      "abi": [
        {
          "inputs": [
            {
              "internalType": "address",
              "name": "",
              "type": "address",
            },
          ],
          "name": "balanceOf",
          "outputs": [
            {
              "internalType": "uint256",
              "name": "",
              "type": "uint256",
            },
          ],
          "stateMutability": "view",
          "type": "function",
        },
      ],
      "address": "0x0000000000000000000000000000000000000000",
      "args": [
        "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
      ],
      "blockNumber": 5n,
      "chainId": 1,
      "functionName": "balanceOf",
    }
  `);
});
test("recordProfilePattern() hint", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: { address: zeroAddress },
      log: {} as unknown as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { number: 5n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  let pattern = recordProfilePattern({
    event,
    args: {
      address: zeroAddress,
      abi: [getAbiItem({ abi: erc20ABI, name: "balanceOf" })],
      functionName: "balanceOf",
      args: [ALICE],
    },
    hints: [],
  });
  pattern = recordProfilePattern({
    event,
    args: {
      address: zeroAddress,
      abi: [getAbiItem({ abi: erc20ABI, name: "balanceOf" })],
      functionName: "balanceOf",
      args: [ALICE],
    },
    hints: [pattern!],
  });
  expect(pattern).toMatchInlineSnapshot(`
    {
      "hasConstant": true,
      "pattern": {
        "abi": [
          {
            "inputs": [
              {
                "internalType": "address",
                "name": "",
                "type": "address",
              },
            ],
            "name": "balanceOf",
            "outputs": [
              {
                "internalType": "uint256",
                "name": "",
                "type": "uint256",
              },
            ],
            "stateMutability": "view",
            "type": "function",
          },
        ],
        "address": {
          "type": "derived",
          "value": [
            "args",
            "address",
          ],
        },
        "args": [
          {
            "type": "constant",
            "value": "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
          },
        ],
        "cache": undefined,
        "functionName": "balanceOf",
      },
    }
  `);
  expect(recoverProfilePattern(pattern!.pattern, event)).toMatchInlineSnapshot(`
    {
      "abi": [
        {
          "inputs": [
            {
              "internalType": "address",
              "name": "",
              "type": "address",
            },
          ],
          "name": "balanceOf",
          "outputs": [
            {
              "internalType": "uint256",
              "name": "",
              "type": "uint256",
            },
          ],
          "stateMutability": "view",
          "type": "function",
        },
      ],
      "address": "0x0000000000000000000000000000000000000000",
      "args": [
        "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
      ],
      "blockNumber": 5n,
      "chainId": 1,
      "functionName": "balanceOf",
    }
  `);
});
test("recordProfilePattern() cache immutable", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: undefined,
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { number: 5n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const pattern = recordProfilePattern({
    event,
    args: {
      address: zeroAddress,
      abi: [getAbiItem({ abi: erc20ABI, name: "balanceOf" })],
      functionName: "balanceOf",
      args: [ALICE],
      cache: "immutable",
    },
    hints: [],
  });
  expect(pattern).toMatchInlineSnapshot(`
    {
      "hasConstant": true,
      "pattern": {
        "abi": [
          {
            "inputs": [
              {
                "internalType": "address",
                "name": "",
                "type": "address",
              },
            ],
            "name": "balanceOf",
            "outputs": [
              {
                "internalType": "uint256",
                "name": "",
                "type": "uint256",
              },
            ],
            "stateMutability": "view",
            "type": "function",
          },
        ],
        "address": {
          "type": "constant",
          "value": "0x0000000000000000000000000000000000000000",
        },
        "args": [
          {
            "type": "constant",
            "value": "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
          },
        ],
        "cache": "immutable",
        "functionName": "balanceOf",
      },
    }
  `);
  expect(recoverProfilePattern(pattern!.pattern, event)).toMatchInlineSnapshot(`
    {
      "abi": [
        {
          "inputs": [
            {
              "internalType": "address",
              "name": "",
              "type": "address",
            },
          ],
          "name": "balanceOf",
          "outputs": [
            {
              "internalType": "uint256",
              "name": "",
              "type": "uint256",
            },
          ],
          "stateMutability": "view",
          "type": "function",
        },
      ],
      "address": "0x0000000000000000000000000000000000000000",
      "args": [
        "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
      ],
      "blockNumber": "latest",
      "chainId": 1,
      "functionName": "balanceOf",
    }
  `);
});
</file>

<file path="packages/core/src/indexing/profile.ts">
import type { Event } from "@/internal/types.js";
import { orderObject } from "@/utils/order.js";
import type { Abi } from "viem";
import type { PonderActions, ProfilePattern, Request } from "./client.js";
export const getProfilePatternKey = (pattern: ProfilePattern): string => {
  return JSON.stringify(
    orderObject({
      address: pattern.address,
      functionName: pattern.functionName,
      args: pattern.args,
    }),
    (_, value) => {
      if (typeof value === "bigint") {
        return value.toString();
      }
      return value;
    },
  );
};
const eq = (target: bigint | string | number | boolean, value: any) => {
  if (target === value) return true;
  if (target && value && target.toString() === value.toString()) return true;
  return false;
};
export const recordProfilePattern = ({
  event,
  args,
  hints,
}: {
  event: Event;
  args: Parameters<PonderActions["readContract"]>[0];
  hints: { pattern: ProfilePattern; hasConstant: boolean }[];
}): { pattern: ProfilePattern; hasConstant: boolean } | undefined => {
  globalThis.DISABLE_EVENT_PROXY = true;
  for (const hint of hints) {
    const request = recoverProfilePattern(hint.pattern, event);
    if (
      request.functionName === args.functionName &&
      request.address === args.address
    ) {
      if (request.args === undefined && args.args === undefined) {
        globalThis.DISABLE_EVENT_PROXY = false;
        return hint;
      }
      if (request.args === undefined || args.args === undefined) continue;
      for (let i = 0; i < request.args.length; i++) {
        if (eq(request.args[i] as any, args.args[i]) === false) continue;
      }
      if ((request.blockNumber === "latest") !== (args.cache === "immutable")) {
        continue;
      }
      globalThis.DISABLE_EVENT_PROXY = false;
      return hint;
    }
  }
  let resultAddress: ProfilePattern["address"] | undefined;
  let hasConstant = false;
  // address
  switch (event.type) {
    case "block": {
      if (
        event.event.block.miner &&
        eq(event.event.block.miner, args.address)
      ) {
        resultAddress = { type: "derived", value: ["block", "miner"] };
        break;
      }
      break;
    }
    case "transaction": {
      if (
        event.event.block.miner &&
        eq(event.event.block.miner, args.address)
      ) {
        resultAddress = { type: "derived", value: ["block", "miner"] };
        break;
      }
      if (eq(event.event.transaction.from, args.address)) {
        resultAddress = { type: "derived", value: ["transaction", "from"] };
        break;
      }
      if (
        event.event.transaction.to &&
        eq(event.event.transaction.to, args.address)
      ) {
        resultAddress = { type: "derived", value: ["transaction", "to"] };
        break;
      }
      if (
        event.event.transactionReceipt?.contractAddress &&
        eq(event.event.transactionReceipt.contractAddress, args.address)
      ) {
        resultAddress = {
          type: "derived",
          value: ["transactionReceipt", "contractAddress"],
        };
        break;
      }
      break;
    }
    case "log": {
      // Note: explicitly skip profiling args if they are an array
      if (
        event.event.args !== undefined &&
        Array.isArray(event.event.args) === false
      ) {
        let hasMatch = false;
        for (const argKey of Object.keys(event.event.args)) {
          const argValue = (event.event.args as { [key: string]: unknown })[
            argKey
          ] as string | bigint | number | boolean;
          if (typeof argValue !== "object" && eq(argValue, args.address)) {
            resultAddress = { type: "derived", value: ["args", argKey] };
            hasMatch = true;
            break;
          }
        }
        if (hasMatch) break;
      }
      if (eq(event.event.log.address, args.address)) {
        resultAddress = { type: "derived", value: ["log", "address"] };
        break;
      }
      if (
        event.event.block.miner &&
        eq(event.event.block.miner, args.address)
      ) {
        resultAddress = { type: "derived", value: ["block", "miner"] };
        break;
      }
      if (eq(event.event.transaction.from, args.address)) {
        resultAddress = { type: "derived", value: ["transaction", "from"] };
        break;
      }
      if (
        event.event.transaction.to &&
        eq(event.event.transaction.to, args.address)
      ) {
        resultAddress = { type: "derived", value: ["transaction", "to"] };
        break;
      }
      if (
        event.event.transactionReceipt?.contractAddress &&
        eq(event.event.transactionReceipt.contractAddress, args.address)
      ) {
        resultAddress = {
          type: "derived",
          value: ["transactionReceipt", "contractAddress"],
        };
        break;
      }
      break;
    }
    case "trace": {
      let hasMatch = false;
      // Note: explicitly skip profiling args if they are an array
      if (
        event.event.args !== undefined &&
        Array.isArray(event.event.args) === false
      ) {
        for (const argKey of Object.keys(event.event.args)) {
          const argValue = (event.event.args as { [key: string]: unknown })[
            argKey
          ] as string | bigint | number | boolean;
          if (typeof argValue !== "object" && eq(argValue, args.address)) {
            resultAddress = { type: "derived", value: ["args", argKey] };
            hasMatch = true;
            break;
          }
        }
      }
      // Note: explicitly skip profiling result if it is an array
      if (
        event.event.result !== undefined &&
        Array.isArray(event.event.result) === false
      ) {
        for (const argKey of Object.keys(event.event.result)) {
          const argValue = (event.event.result as { [key: string]: unknown })[
            argKey
          ] as string | bigint | number | boolean;
          if (typeof argValue !== "object" && eq(argValue, args.address)) {
            resultAddress = { type: "derived", value: ["result", argKey] };
            hasMatch = true;
            break;
          }
        }
      }
      if (hasMatch) break;
      if (eq(event.event.trace.from, args.address)) {
        resultAddress = { type: "derived", value: ["trace", "from"] };
        break;
      }
      if (event.event.trace.to && eq(event.event.trace.to, args.address)) {
        resultAddress = { type: "derived", value: ["trace", "to"] };
        break;
      }
      if (
        event.event.block.miner &&
        eq(event.event.block.miner, args.address)
      ) {
        resultAddress = { type: "derived", value: ["block", "miner"] };
        break;
      }
      if (eq(event.event.transaction.from, args.address)) {
        resultAddress = { type: "derived", value: ["transaction", "from"] };
        break;
      }
      if (
        event.event.transaction.to &&
        eq(event.event.transaction.to, args.address)
      ) {
        resultAddress = { type: "derived", value: ["transaction", "to"] };
        break;
      }
      if (
        event.event.transactionReceipt?.contractAddress &&
        eq(event.event.transactionReceipt.contractAddress, args.address)
      ) {
        resultAddress = {
          type: "derived",
          value: ["transactionReceipt", "contractAddress"],
        };
        break;
      }
      break;
    }
    case "transfer": {
      if (eq(event.event.transfer.from, args.address)) {
        resultAddress = { type: "derived", value: ["transfer", "from"] };
        break;
      }
      if (eq(event.event.transfer.to, args.address)) {
        resultAddress = { type: "derived", value: ["transfer", "to"] };
        break;
      }
      if (eq(event.event.trace.from, args.address)) {
        resultAddress = { type: "derived", value: ["trace", "from"] };
        break;
      }
      if (event.event.trace.to && eq(event.event.trace.to, args.address)) {
        resultAddress = { type: "derived", value: ["trace", "to"] };
        break;
      }
      if (
        event.event.block.miner &&
        eq(event.event.block.miner, args.address)
      ) {
        resultAddress = { type: "derived", value: ["block", "miner"] };
        break;
      }
      if (eq(event.event.transaction.from, args.address)) {
        resultAddress = { type: "derived", value: ["transaction", "from"] };
        break;
      }
      if (
        event.event.transaction.to &&
        eq(event.event.transaction.to, args.address)
      ) {
        resultAddress = { type: "derived", value: ["transaction", "to"] };
        break;
      }
      if (
        event.event.transactionReceipt?.contractAddress &&
        eq(event.event.transactionReceipt.contractAddress, args.address)
      ) {
        resultAddress = {
          type: "derived",
          value: ["transactionReceipt", "contractAddress"],
        };
        break;
      }
      break;
    }
  }
  if (resultAddress === undefined) {
    resultAddress = { type: "constant", value: args.address };
    hasConstant = true;
  }
  if (args.args === undefined || args.args.length === 0) {
    globalThis.DISABLE_EVENT_PROXY = false;
    return {
      pattern: {
        address: resultAddress,
        abi: args.abi as Abi,
        functionName: args.functionName,
        args: undefined,
        cache: args.cache,
      },
      hasConstant,
    };
  }
  const resultArgs: NonNullable<ProfilePattern["args"]> = [];
  // args
  for (const arg of args.args) {
    if (typeof arg === "object") {
      globalThis.DISABLE_EVENT_PROXY = false;
      return undefined;
    }
    switch (event.type) {
      case "block": {
        if (eq(event.event.block.hash, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "hash"] });
          continue;
        }
        if (eq(event.event.block.number, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "number"] });
          continue;
        }
        if (eq(event.event.block.timestamp, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "timestamp"] });
          continue;
        }
        if (event.event.block.miner && eq(event.event.block.miner, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "miner"] });
          continue;
        }
        break;
      }
      case "transaction": {
        if (eq(event.event.block.hash, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "hash"] });
          continue;
        }
        if (eq(event.event.block.number, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "number"] });
          continue;
        }
        if (eq(event.event.block.timestamp, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "timestamp"] });
          continue;
        }
        if (event.event.block.miner && eq(event.event.block.miner, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "miner"] });
          continue;
        }
        if (eq(event.event.transaction.hash, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "hash"] });
          continue;
        }
        if (eq(event.event.transaction.from, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "from"] });
          continue;
        }
        if (event.event.transaction.to && eq(event.event.transaction.to, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "to"] });
          continue;
        }
        if (eq(event.event.transaction.transactionIndex, arg)) {
          resultArgs.push({
            type: "derived",
            value: ["transaction", "transactionIndex"],
          });
          continue;
        }
        if (
          event.event.transactionReceipt?.contractAddress &&
          eq(event.event.transactionReceipt.contractAddress, arg)
        ) {
          resultArgs.push({
            type: "derived",
            value: ["transactionReceipt", "contractAddress"],
          });
          continue;
        }
        break;
      }
      case "log": {
        // Note: explicitly skip profiling args if they are an array
        if (
          event.event.args !== undefined &&
          Array.isArray(event.event.args) === false
        ) {
          let hasMatch = false;
          for (const argKey of Object.keys(event.event.args)) {
            const argValue = (event.event.args as { [key: string]: unknown })[
              argKey
            ] as string | bigint | number | boolean;
            if (typeof argValue !== "object" && eq(argValue, arg)) {
              resultArgs.push({ type: "derived", value: ["args", argKey] });
              hasMatch = true;
              break;
            }
          }
          if (hasMatch) continue;
        }
        if (eq(event.event.log.address, arg)) {
          resultArgs.push({ type: "derived", value: ["log", "address"] });
          continue;
        }
        if (eq(event.event.log.logIndex, arg)) {
          resultArgs.push({ type: "derived", value: ["log", "logIndex"] });
          continue;
        }
        if (eq(event.event.block.hash, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "hash"] });
          continue;
        }
        if (eq(event.event.block.number, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "number"] });
          continue;
        }
        if (eq(event.event.block.timestamp, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "timestamp"] });
          continue;
        }
        if (event.event.block.miner && eq(event.event.block.miner, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "miner"] });
          continue;
        }
        if (eq(event.event.transaction.hash, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "hash"] });
          continue;
        }
        if (eq(event.event.transaction.from, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "from"] });
          continue;
        }
        if (event.event.transaction.to && eq(event.event.transaction.to, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "to"] });
          continue;
        }
        if (eq(event.event.transaction.transactionIndex, arg)) {
          resultArgs.push({
            type: "derived",
            value: ["transaction", "transactionIndex"],
          });
          continue;
        }
        if (
          event.event.transactionReceipt?.contractAddress &&
          eq(event.event.transactionReceipt.contractAddress, arg)
        ) {
          resultArgs.push({
            type: "derived",
            value: ["transactionReceipt", "contractAddress"],
          });
          continue;
        }
        break;
      }
      case "trace": {
        let hasMatch = false;
        // Note: explicitly skip profiling args if they are an array
        if (
          event.event.args !== undefined &&
          Array.isArray(event.event.args) === false
        ) {
          for (const argKey of Object.keys(event.event.args)) {
            const argValue = (event.event.args as { [key: string]: unknown })[
              argKey
            ] as string | bigint | number | boolean;
            if (typeof argValue !== "object" && eq(argValue, arg)) {
              resultArgs.push({ type: "derived", value: ["args", argKey] });
              hasMatch = true;
              break;
            }
          }
        }
        // Note: explicitly skip profiling result if it is an array
        if (
          event.event.result !== undefined &&
          Array.isArray(event.event.result) === false
        ) {
          for (const argKey of Object.keys(event.event.result)) {
            const argValue = (event.event.result as { [key: string]: unknown })[
              argKey
            ] as string | bigint | number | boolean;
            if (typeof argValue !== "object" && eq(argValue, arg)) {
              resultArgs.push({ type: "derived", value: ["result", argKey] });
              hasMatch = true;
              break;
            }
          }
        }
        if (hasMatch) continue;
        if (eq(event.event.trace.from, arg)) {
          resultArgs.push({ type: "derived", value: ["trace", "from"] });
          continue;
        }
        if (event.event.trace.to && eq(event.event.trace.to, arg)) {
          resultArgs.push({ type: "derived", value: ["trace", "to"] });
          continue;
        }
        if (eq(event.event.block.hash, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "hash"] });
          continue;
        }
        if (eq(event.event.block.number, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "number"] });
          continue;
        }
        if (eq(event.event.block.timestamp, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "timestamp"] });
          continue;
        }
        if (event.event.block.miner && eq(event.event.block.miner, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "miner"] });
          continue;
        }
        if (eq(event.event.transaction.hash, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "hash"] });
          continue;
        }
        if (eq(event.event.transaction.from, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "from"] });
          continue;
        }
        if (event.event.transaction.to && eq(event.event.transaction.to, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "to"] });
          continue;
        }
        if (eq(event.event.transaction.transactionIndex, arg)) {
          resultArgs.push({
            type: "derived",
            value: ["transaction", "transactionIndex"],
          });
          continue;
        }
        if (
          event.event.transactionReceipt?.contractAddress &&
          eq(event.event.transactionReceipt.contractAddress, arg)
        ) {
          resultArgs.push({
            type: "derived",
            value: ["transactionReceipt", "contractAddress"],
          });
          continue;
        }
        break;
      }
      case "transfer": {
        if (eq(event.event.transfer.from, arg)) {
          resultArgs.push({ type: "derived", value: ["transfer", "from"] });
          continue;
        }
        if (eq(event.event.transfer.to, arg)) {
          resultArgs.push({ type: "derived", value: ["transfer", "to"] });
          continue;
        }
        if (eq(event.event.trace.from, arg)) {
          resultArgs.push({ type: "derived", value: ["trace", "from"] });
          continue;
        }
        if (event.event.trace.to && eq(event.event.trace.to, arg)) {
          resultArgs.push({ type: "derived", value: ["trace", "to"] });
          continue;
        }
        if (eq(event.event.block.hash, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "hash"] });
          continue;
        }
        if (eq(event.event.block.number, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "number"] });
          continue;
        }
        if (eq(event.event.block.timestamp, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "timestamp"] });
          continue;
        }
        if (event.event.block.miner && eq(event.event.block.miner, arg)) {
          resultArgs.push({ type: "derived", value: ["block", "miner"] });
          continue;
        }
        if (eq(event.event.transaction.hash, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "hash"] });
          continue;
        }
        if (eq(event.event.transaction.from, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "from"] });
          continue;
        }
        if (event.event.transaction.to && eq(event.event.transaction.to, arg)) {
          resultArgs.push({ type: "derived", value: ["transaction", "to"] });
          continue;
        }
        if (eq(event.event.transaction.transactionIndex, arg)) {
          resultArgs.push({
            type: "derived",
            value: ["transaction", "transactionIndex"],
          });
          continue;
        }
        if (
          event.event.transactionReceipt?.contractAddress &&
          eq(event.event.transactionReceipt.contractAddress, arg)
        ) {
          resultArgs.push({
            type: "derived",
            value: ["transactionReceipt", "contractAddress"],
          });
          continue;
        }
        break;
      }
    }
    resultArgs.push({ type: "constant", value: arg });
    hasConstant = true;
  }
  globalThis.DISABLE_EVENT_PROXY = false;
  return {
    pattern: {
      address: resultAddress!,
      abi: args.abi as Abi,
      functionName: args.functionName,
      args: resultArgs,
      cache: args.cache,
    },
    hasConstant,
  };
};
export const recoverProfilePattern = (
  pattern: ProfilePattern,
  event: Event,
): Request => {
  let address: `0x${string}`;
  if (pattern.address.type === "constant") {
    address = pattern.address.value as `0x${string}`;
  } else {
    let _result: unknown = event.event;
    for (const prop of pattern.address.value) {
      // @ts-ignore
      _result = _result[prop];
    }
    address = _result as `0x${string}`;
  }
  let args: unknown[] | undefined;
  if (pattern.args) {
    args = [];
    for (const arg of pattern.args) {
      if (arg.type === "constant") {
        args.push(arg.value);
      } else {
        let _result: unknown = event.event;
        for (const prop of arg.value) {
          // @ts-ignore
          _result = _result[prop];
        }
        args.push(_result);
      }
    }
  }
  return {
    address,
    abi: pattern.abi,
    functionName: pattern.functionName,
    args,
    blockNumber:
      pattern.cache === "immutable" ? "latest" : event.event.block.number,
    chainId: event.chain.id,
  };
};
</file>

<file path="packages/core/src/indexing-store/cache.bench.ts">
import { onchainTable } from "@/index.js";
import { bench, run } from "mitata";
import { getCopyText } from "./cache.js";
const table = onchainTable("account", (t) => ({
  address: t.hex().primaryKey(),
  balance: t.bigint(),
}));
const rows = [
  { address: "0x123", balance: 100n },
  { address: "0x456", balance: 200n },
  { address: "0x123", balance: 100n },
  { address: "0x456", balance: 200n },
  { address: "0x123", balance: 100n },
  { address: "0x456", balance: 200n },
  { address: "0x123", balance: 100n },
  { address: "0x456", balance: 200n },
  { address: "0x123", balance: 100n },
  { address: "0x456", balance: 200n },
];
bench("getCopyText", () => {
  getCopyText(table, rows);
}).gc("inner");
run();
</file>

<file path="packages/core/src/indexing-store/cache.test.ts">
import { ALICE, BOB } from "@/_test/constants.js";
import {
  context,
  setupAnvil,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { deployErc20, mintErc20 } from "@/_test/simulate.js";
import { getErc20IndexingBuild, getSimulatedEvent } from "@/_test/utils.js";
import { onchainEnum, onchainTable } from "@/drizzle/onchain.js";
import { getEventCount } from "@/indexing/index.js";
import type { RetryableError } from "@/internal/errors.js";
import type { IndexingErrorHandler } from "@/internal/types.js";
import { parseEther, zeroAddress } from "viem";
import { beforeEach, expect, test } from "vitest";
import { createIndexingCache } from "./cache.js";
import { createIndexingStore } from "./index.js";
beforeEach(setupCommon);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
beforeEach(setupAnvil);
const indexingErrorHandler: IndexingErrorHandler = {
  getRetryableError: () => {
    return indexingErrorHandler.error;
  },
  setRetryableError: (error: RetryableError) => {
    indexingErrorHandler.error = error;
  },
  clearRetryableError: () => {
    indexingErrorHandler.error = undefined;
  },
  error: undefined as RetryableError | undefined,
};
test("flush() insert", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      balance: 10n,
    });
    await indexingCache.flush();
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 10n,
    });
  });
});
test("flush() update", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // mutate the cache to skip hot loops
    indexingCache.invalidate();
    await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      balance: 10n,
    });
    // first flush takes "insert" path
    await indexingCache.flush();
    await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      .set({
        balance: 12n,
      });
    // second flush takes "update" path
    await indexingCache.flush();
    let result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 12n,
    });
    // flush again to make sure temp tables are cleaned up
    await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      .set({
        balance: 12n,
      });
    await indexingCache.flush();
    result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 12n,
    });
  });
});
test("flush() recovers error", async () => {
  if (context.databaseConfig.kind !== "postgres") {
    return;
  }
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      balance: 10n,
    });
    await indexingCache.flush();
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      balance: 10n,
    });
    await expect(indexingCache.flush()).rejects.toThrowError(
      `duplicate key value violates unique constraint "account_pkey"`,
    );
  });
});
test("flush() encoding", async () => {
  const e = onchainEnum("e", ["a", "b", "c"]);
  const schema = {
    e,
    test: onchainTable("test", (p) => ({
      hex: p.hex().primaryKey(),
      bigint: p.bigint().notNull(),
      e: e().notNull(),
      array: p.integer().array().notNull(),
      json: p.json().notNull(),
      null: p.text(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.test).values({
      hex: zeroAddress,
      bigint: 10n,
      e: "a",
      array: [1, 2, 4],
      json: { a: 1, b: 2 },
      null: null,
    });
    await indexingCache.flush();
    indexingCache.clear();
    const result = await indexingStore.db.sql.select().from(schema.test);
    expect(result).toMatchInlineSnapshot(`
      [
        {
          "array": [
            1,
            2,
            4,
          ],
          "bigint": 10n,
          "e": "a",
          "hex": "0x0000000000000000000000000000000000000000",
          "json": {
            "a": 1,
            "b": 2,
          },
          "null": null,
        },
      ]
    `);
  });
});
test("flush() encoding escape", async () => {
  const schema = {
    test: onchainTable("test", (p) => ({
      backslash: p.text().primaryKey(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    const values = [
      { backslash: "\\\\" },
      { backslash: "\\b" },
      { backslash: "\\f" },
      { backslash: "\\n" },
      { backslash: "\\r" },
      { backslash: "\\t" },
      { backslash: "\\v" },
      { backslash: "\\00" },
      { backslash: "\\x00" },
      { backslash: "\\" },
      { backslash: "\b" },
      { backslash: "\f" },
      { backslash: "\n" },
      { backslash: "\r" },
      { backslash: "\t" },
      { backslash: "\v" },
      // { backslash: "\00" },
      // { backslash: "\x00" },
    ];
    await indexingStore.db.insert(schema.test).values(values);
    await indexingCache.flush();
    indexingCache.clear();
    const result = await indexingStore.db.sql.select().from(schema.test);
    expect(result).toStrictEqual(values);
  });
});
test("prefetch() uses profile metadata", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks, indexingFunctions } = getErc20IndexingBuild({
    address,
  });
  const event = getSimulatedEvent({
    eventCallback: eventCallbacks[0]!,
    blockData,
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: getEventCount(indexingFunctions),
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  indexingCache.event = event;
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db
      .insert(schema.account)
      .values({
        address: ALICE,
        balance: parseEther("1"),
      })
      .onConflictDoNothing();
    // @ts-ignore
    event.event.args.to = BOB;
    await indexingCache.flush();
    await indexingCache.prefetch({ events: [event] });
    const result = indexingCache.has({
      table: schema.account,
      key: { address: BOB },
    });
    expect(result).toBe(true);
  });
});
test("prefetch() evicts rows", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  // skip hot loop
  indexingCache.invalidate();
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      balance: 10n,
    });
    await indexingCache.flush();
    // prefetch() should evict rows from the cache to free memory
    await indexingCache.prefetch({ events: [] });
    await indexingCache.prefetch({ events: [] });
    const result = indexingCache.has({
      table: schema.account,
      key: { address: zeroAddress },
    });
    expect(result).toBe(false);
  });
});
</file>

<file path="packages/core/src/indexing-store/cache.ts">
import type { QB } from "@/database/queryBuilder.js";
import { getPrimaryKeyColumns } from "@/drizzle/index.js";
import { getColumnCasing } from "@/drizzle/kit/index.js";
import { getPartitionName } from "@/drizzle/onchain.js";
import { addErrorMeta, toErrorMeta } from "@/indexing/index.js";
import type { Common } from "@/internal/common.js";
import {
  CopyFlushError,
  DelayedInsertError,
  ShutdownError,
} from "@/internal/errors.js";
import type {
  CrashRecoveryCheckpoint,
  Event,
  SchemaBuild,
} from "@/internal/types.js";
import { dedupe } from "@/utils/dedupe.js";
import { prettyPrint } from "@/utils/print.js";
import { promiseAllSettledWithThrow } from "@/utils/promiseAllSettledWithThrow.js";
import { startClock } from "@/utils/timer.js";
import {
  type Table,
  getTableColumns,
  getTableName,
  isTable,
  or,
  sql,
} from "drizzle-orm";
import { getTableConfig } from "drizzle-orm/pg-core";
import copy from "pg-copy-streams";
import {
  getProfilePatternKey,
  recordProfilePattern,
  recoverProfilePattern,
} from "./profile.js";
import {
  getCacheKey,
  getPrimaryKeyCache,
  getWhereCondition,
  normalizeRow,
} from "./utils.js";
export type IndexingCache = {
  /**
   * Returns true if the cache has an entry for `table` with `key`.
   */
  has: (params: { table: Table; key: object }) => boolean;
  /**
   * Returns the entry for `table` with `key`.
   */
  get: (params: { table: Table; key: object }) =>
    | Row
    | null
    | Promise<Row | null>;
  /**
   * Sets the entry for `table` with `key` to `row`.
   */
  set: (params: {
    table: Table;
    key: object;
    row: Row;
    isUpdate: boolean;
  }) => Row;
  /**
   * Deletes the entry for `table` with `key`.
   */
  delete: (params: { table: Table; key: object }) => boolean | Promise<boolean>;
  /**
   * Writes all temporary data to the database.
   *
   * @param params.tableNames - If provided, only flush the tables in the set.
   */
  flush: (params?: { tableNames?: Set<string> }) => Promise<void>;
  /**
   * Predict and load rows that will be accessed in the next event batch.
   */
  prefetch: (params: { events: Event[] }) => Promise<void>;
  /**
   * Marks the cache as incomplete.
   */
  invalidate: () => void;
  /**
   * Deletes all entries from the cache.
   */
  clear: () => void;
  event: Event | undefined;
  qb: QB;
};
const SAMPLING_RATE = 10;
const PREDICTION_THRESHOLD = 0.25;
const LOW_BATCH_THRESHOLD = 20;
/**
 * Database row.
 *
 * @example
 * {
 *   "owner": "0x123",
 *   "spender": "0x456",
 *   "amount": 100n,
 * }
 */
export type Row = { [key: string]: unknown };
/**
 * Serialized primary key values for uniquely identifying a database row.
 *
 * @example
 * "0x123_0x456"
 */
type CacheKey = string;
/**
 * Event name.
 *
 * @example
 * "Erc20:Transfer"
 *
 * @example
 * "Erc20.mint()"
 */
type EventName = string;
/**
 * Recorded database access pattern.
 *
 * @example
 * {
 *   "owner": ["args", "from"],
 *   "spender": ["log", "address"],
 * }
 */
export type ProfilePattern = {
  [key: string]:
    | {
        type: "derived";
        value: string[];
        fn?: (value: unknown) => unknown;
      }
    | {
        type: "delimeter";
        values: { value: string[]; fn?: (value: unknown) => unknown }[];
        delimiter: string;
      };
};
/**
 * Serialized for uniquely identifying a {@link ProfilePattern}.
 *
 * @example
 * "{
 *   "owner": ["args", "from"],
 *   "spender": ["log", "address"],
 * }"
 */
type ProfileKey = string;
/**
 * Cache of database rows.
 */
type Cache = Map<
  Table,
  {
    cache: Map<CacheKey, Row | null>;
    /** Cached keys that were prefetched. */
    prefetched: Set<CacheKey>;
    /** Cached keys that were not prefetched but were accessed anyway. */
    spillover: Set<CacheKey>;
    /** `true` if the cache completely mirrors the database. */
    isCacheComplete: boolean;
    /**
     * Estimated size of the cache in bytes.
     *
     * Note: this stops getting updated once `isCacheComplete = false`.
     */
    bytes: number;
    /** Number of times `get` missed the cached and read from the database. */
    diskReads: number;
  }
>;
/**
 * Buffer of database rows that will be flushed to the database.
 */
type Buffer = Map<
  Table,
  Map<
    CacheKey,
    {
      row: Row;
      metadata: { event: Event | undefined };
    }
  >
>;
/**
 * Metadata about database access patterns for each event.
 */
type Profile = Map<
  EventName,
  Map<Table, Map<ProfileKey, { pattern: ProfilePattern; count: number }>>
>;
const getBytes = (value: unknown) => {
  let size = 0;
  if (typeof value === "number") {
    size += 8;
  } else if (typeof value === "string") {
    size += 2 * value.length;
  } else if (typeof value === "boolean") {
    size += 4;
  } else if (typeof value === "bigint") {
    size += 48;
  } else if (value === null || value === undefined) {
    size += 8;
  } else if (Array.isArray(value)) {
    size += 24; // NodeJs object overhead (3 * 8 bytes)
    for (const e of value) {
      // value + 8 bytes for the key
      size += getBytes(e) + 8;
    }
  } else {
    size += 24; // NodeJs object overhead (3 * 8 bytes)
    for (const col of Object.values(value)) {
      // value + 8 bytes for the key
      size += getBytes(col) + 8;
    }
  }
  return size;
};
const ESCAPE_REGEX = /([\\\b\f\n\r\t\v])/g;
export const getCopyText = (table: Table, rows: Row[]) => {
  let result = "";
  const columns = Object.entries(getTableColumns(table));
  for (let i = 0; i < rows.length; i++) {
    const isLastRow = i === rows.length - 1;
    const row = rows[i]!;
    for (let j = 0; j < columns.length; j++) {
      const isLastColumn = j === columns.length - 1;
      const [columnName, column] = columns[j]!;
      let value = row[columnName];
      if (isLastColumn) {
        if (value === null || value === undefined) {
          result += "\\N";
        } else {
          if (column.mapToDriverValue !== undefined) {
            value = column.mapToDriverValue(value);
            if (value === null || value === undefined) {
              result += "\\N";
            } else {
              result += `${String(value).replace(ESCAPE_REGEX, "\\$1")}`;
            }
          }
        }
      } else {
        if (value === null || value === undefined) {
          result += "\\N\t";
        } else {
          if (column.mapToDriverValue !== undefined) {
            value = column.mapToDriverValue(value);
          }
          if (value === null || value === undefined) {
            result += "\\N\t";
          } else {
            result += `${String(value).replace(ESCAPE_REGEX, "\\$1")}\t`;
          }
        }
      }
    }
    if (isLastRow === false) {
      result += "\n";
    }
  }
  return result;
};
export const getCopyHelper = (qb: QB, chainId?: number) => {
  if (qb.$dialect === "pglite") {
    return async (table: Table, text: string, includeSchema = true) => {
      const target = includeSchema
        ? `"${getTableConfig(table).schema ?? "public"}"."${chainId === undefined ? getTableName(table) : getPartitionName(table, chainId)}"`
        : `"${chainId === undefined ? getTableName(table) : getPartitionName(table, chainId)}"`;
      await qb.$client
        .query(`COPY ${target} FROM '/dev/blob'`, [], {
          blob: new Blob([text]),
        })
        // Note: `TransactionError` is applied because the query
        // uses the low-level `$client.query` method.
        .catch((error) => {
          throw new CopyFlushError(error.message);
        });
    };
  } else {
    return async (table: Table, text: string, includeSchema = true) => {
      const target = includeSchema
        ? `"${getTableConfig(table).schema ?? "public"}"."${chainId === undefined ? getTableName(table) : getPartitionName(table, chainId)}"`
        : `"${chainId === undefined ? getTableName(table) : getPartitionName(table, chainId)}"`;
      const copyStream = qb.$client.query(
        copy.from(`COPY ${target} FROM STDIN`),
      );
      await new Promise((resolve, reject) => {
        copyStream.on("finish", resolve);
        copyStream.on("error", reject);
        copyStream.write(text);
        copyStream.end();
      }).catch((error) => {
        throw new CopyFlushError(error.message);
      });
    };
  }
};
export const recoverBatchError = async <T>(
  values: T[],
  callback: (values: T[]) => Promise<unknown>,
): Promise<
  { status: "success" } | { status: "error"; error: Error; value: T }
> => {
  try {
    await callback(values);
    return { status: "success" };
  } catch (error) {
    if (values.length === 1) {
      return { status: "error", error: error as Error, value: values[0]! };
    }
    const left = values.slice(0, Math.floor(values.length / 2));
    const right = values.slice(Math.floor(values.length / 2));
    const resultLeft = await recoverBatchError(left, callback);
    if (resultLeft.status === "error") {
      return resultLeft;
    }
    const resultRight = await recoverBatchError(right, callback);
    if (resultRight.status === "error") {
      return resultRight;
    }
    return { status: "success" };
  }
};
export const createIndexingCache = ({
  common,
  schemaBuild: { schema },
  crashRecoveryCheckpoint,
  eventCount,
  chainId,
}: {
  common: Common;
  schemaBuild: Pick<SchemaBuild, "schema">;
  crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  eventCount: { [eventName: string]: number };
  chainId?: number;
}): IndexingCache => {
  let event: Event | undefined;
  let qb: QB = undefined!;
  const cache: Cache = new Map();
  const insertBuffer: Buffer = new Map();
  const updateBuffer: Buffer = new Map();
  /** Profiling data about access patterns for each event. */
  const profile: Profile = new Map();
  const tables = Object.values(schema).filter(isTable);
  const primaryKeyCache = getPrimaryKeyCache(tables);
  for (const table of tables) {
    cache.set(table, {
      cache: new Map(),
      prefetched: new Set(),
      spillover: new Set(),
      isCacheComplete: crashRecoveryCheckpoint === undefined,
      bytes: 0,
      diskReads: 0,
    });
    insertBuffer.set(table, new Map());
    updateBuffer.set(table, new Map());
  }
  return {
    has({ table, key }) {
      if (cache.get(table)!.isCacheComplete) return true;
      const ck = getCacheKey(table, key, primaryKeyCache);
      return (
        cache.get(table)!.cache.has(ck) ??
        insertBuffer.get(table)!.has(ck) ??
        updateBuffer.get(table)!.has(ck)
      );
    },
    async get({ table, key }) {
      if (
        event &&
        eventCount[event.eventCallback.name]! % SAMPLING_RATE === 1
      ) {
        if (profile.has(event.eventCallback.name) === false) {
          profile.set(event.eventCallback.name, new Map());
          for (const table of tables) {
            profile.get(event.eventCallback.name)!.set(table, new Map());
          }
        }
        const pattern = recordProfilePattern(
          event,
          table,
          key,
          Array.from(
            profile.get(event.eventCallback.name)!.get(table)!.values(),
          ).map(({ pattern }) => pattern),
          primaryKeyCache,
        );
        if (pattern) {
          const key = getProfilePatternKey(pattern);
          if (profile.get(event.eventCallback.name)!.get(table)!.has(key)) {
            profile.get(event.eventCallback.name)!.get(table)!.get(key)!
              .count++;
          } else {
            profile
              .get(event.eventCallback.name)!
              .get(table)!
              .set(key, { pattern, count: 1 });
          }
        }
      }
      const ck = getCacheKey(table, key, primaryKeyCache);
      // Note: order is important, it is an invariant that update entries
      // are prioritized over insert entries
      const bufferEntry =
        updateBuffer.get(table)!.get(ck) ?? insertBuffer.get(table)!.get(ck);
      if (bufferEntry) {
        common.metrics.ponder_indexing_cache_requests_total.inc({
          table: getTableName(table),
          type: cache.get(table)!.isCacheComplete ? "complete" : "hit",
        });
        return bufferEntry.row;
      }
      const entry = cache.get(table)!.cache.get(ck);
      if (entry !== undefined) {
        if (
          cache.get(table)!.prefetched.has(ck) === false &&
          cache.get(table)!.isCacheComplete === false
        ) {
          cache.get(table)!.spillover.add(ck);
        }
        common.metrics.ponder_indexing_cache_requests_total.inc({
          table: getTableName(table),
          type: cache.get(table)!.isCacheComplete ? "complete" : "hit",
        });
        return entry;
      }
      cache.get(table)!.diskReads++;
      if (cache.get(table)!.isCacheComplete) {
        common.metrics.ponder_indexing_cache_requests_total.inc({
          table: getTableName(table),
          type: "complete",
        });
        return null;
      }
      cache.get(table)!.spillover.add(ck);
      common.metrics.ponder_indexing_cache_requests_total.inc({
        table: getTableName(table),
        type: "miss",
      });
      const endClock = startClock();
      const result = await qb
        .wrap((db) =>
          db.select().from(table).where(getWhereCondition(table, key)),
        )
        .then((res) => (res.length === 0 ? null : res[0]!))
        .then((row) => {
          cache.get(table)!.cache.set(ck, row);
          // Note: the size is not recorded because it is not possible
          // to miss the cache when in the "full in-memory" mode
          return row;
        });
      common.metrics.ponder_indexing_cache_query_duration.observe(
        {
          table: getTableName(table),
          method: "find",
        },
        endClock(),
      );
      return result;
    },
    set({ table, key, row: _row, isUpdate }) {
      const row = normalizeRow(table, _row, isUpdate);
      const ck = getCacheKey(table, key, primaryKeyCache);
      if (isUpdate) {
        updateBuffer.get(table)!.set(ck, { row, metadata: { event } });
      } else {
        insertBuffer.get(table)!.set(ck, { row, metadata: { event } });
      }
      return row;
    },
    async delete({ table, key }) {
      const ck = getCacheKey(table, key, primaryKeyCache);
      const inInsertBuffer = insertBuffer.get(table)!.delete(ck);
      const inUpdateBuffer = updateBuffer.get(table)!.delete(ck);
      cache.get(table)!.cache.delete(ck);
      const inDb = await qb
        .wrap((db) =>
          db.delete(table).where(getWhereCondition(table, key)).returning(),
        )
        .then((result) => result.length > 0);
      return inInsertBuffer || inUpdateBuffer || inDb;
    },
    async flush({ tableNames } = {}) {
      const context = {
        logger: common.logger.child({ action: "flush_database_rows" }),
      };
      const flushEndClock = startClock();
      const copy = getCopyHelper(qb, chainId);
      const flushTable = async (table: Table) => {
        const shouldRecordBytes = cache.get(table)!.isCacheComplete;
        if (
          tableNames !== undefined &&
          tableNames.has(getTableName(table)) === false
        ) {
          return;
        }
        const tableCache = cache.get(table)!;
        const target =
          chainId === undefined
            ? getTableName(table)
            : getPartitionName(table, chainId);
        const insertValues = Array.from(insertBuffer.get(table)!.values());
        const updateValues = Array.from(updateBuffer.get(table)!.values());
        if (insertValues.length > 0) {
          const endClock = startClock();
          if (insertValues.length > LOW_BATCH_THRESHOLD) {
            const text = getCopyText(
              table,
              insertValues.map(({ row }) => row),
            );
            await new Promise(setImmediate);
            await copy(table, text);
          } else {
            await qb.wrap(
              (db) =>
                db.insert(table).values(insertValues.map(({ row }) => row)),
              context,
            );
          }
          common.metrics.ponder_indexing_cache_query_duration.observe(
            {
              table: getTableName(table),
              method: "flush",
            },
            endClock(),
          );
          let bytes = 0;
          for (const [key, entry] of insertBuffer.get(table)!) {
            if (shouldRecordBytes && tableCache.cache.has(key) === false) {
              bytes += getBytes(entry.row) + getBytes(key);
            }
            tableCache.cache.set(key, entry.row);
          }
          tableCache.bytes += bytes;
          insertBuffer.get(table)!.clear();
          await new Promise(setImmediate);
        }
        if (updateValues.length > 0) {
          const primaryKeys = getPrimaryKeyColumns(table);
          const endClock = startClock();
          if (updateValues.length > LOW_BATCH_THRESHOLD) {
            // Steps for flushing "update" entries:
            // 1. Create temp table
            // 2. Copy into temp table
            // 3. Update target table with data from temp
            const createTempTableQuery = `
              CREATE TEMP TABLE IF NOT EXISTS "${target}" 
              AS SELECT * FROM "${getTableConfig(table).schema ?? "public"}"."${target}"
              WITH NO DATA;`;
            const updateQuery = `
              UPDATE "${getTableConfig(table).schema ?? "public"}"."${target}" as target
              SET ${Object.values(getTableColumns(table))
                .map(
                  (column) =>
                    `"${getColumnCasing(
                      column,
                      "snake_case",
                    )}" = source."${getColumnCasing(column, "snake_case")}"`,
                )
                .join(",\n")}
              FROM "${target}" source
              WHERE ${primaryKeys
                .map(({ sql }) => `target."${sql}" = source."${sql}"`)
                .join(" AND ")};`;
            await qb.wrap((db) => db.execute(createTempTableQuery), context);
            const text = getCopyText(
              table,
              updateValues.map(({ row }) => row),
            );
            await new Promise(setImmediate);
            await copy(table, text, false);
            await qb.wrap((db) => db.execute(updateQuery), context);
            await qb.wrap(
              (db) => db.execute(`TRUNCATE TABLE "${target}"`),
              context,
            );
          } else {
            await qb.wrap(
              (db) =>
                db
                  .insert(table)
                  .values(updateValues.map(({ row }) => row))
                  .onConflictDoUpdate({
                    // @ts-ignore
                    target: primaryKeys.map(({ js }) => table[js]!),
                    set: Object.fromEntries(
                      Object.entries(getTableColumns(table)).map(
                        ([columnName, column]) => [
                          columnName,
                          sql.raw(
                            `excluded."${getColumnCasing(column, "snake_case")}"`,
                          ),
                        ],
                      ),
                    ),
                  }),
              context,
            );
          }
          common.metrics.ponder_indexing_cache_query_duration.observe(
            {
              table: getTableName(table),
              method: "flush",
            },
            endClock(),
          );
          let bytes = 0;
          for (const [key, entry] of updateBuffer.get(table)!) {
            if (shouldRecordBytes && tableCache.cache.has(key) === false) {
              bytes += getBytes(entry.row) + getBytes(key);
            }
            tableCache.cache.set(key, entry.row);
          }
          tableCache.bytes += bytes;
          updateBuffer.get(table)!.clear();
          await new Promise(setImmediate);
        }
        if (insertValues.length > 0 || updateValues.length > 0) {
          common.logger.debug({
            msg: "Wrote database rows",
            table: target,
            row_count: insertValues.length + updateValues.length,
            duration: flushEndClock(),
          });
        }
      };
      try {
        if (qb.$dialect === "postgres") {
          await qb.wrap((db) => db.execute("SAVEPOINT flush"), context);
          await promiseAllSettledWithThrow(
            Array.from(cache.keys()).map(flushTable),
          );
          await qb.wrap((db) => db.execute("RELEASE flush"), context);
        } else {
          // Note: pglite must run sequentially
          for (const table of cache.keys()) {
            await flushTable(table);
          }
        }
      } catch (_error) {
        let error = _error as Error;
        if (error instanceof ShutdownError || qb.$dialect === "pglite") {
          throw error;
        }
        // Note `isFlushRetry` is true when the previous flush failed. When `isFlushRetry` is false, this
        // function takes an optimized fast path, with support for small batch sizes. PGlite always takes
        // the fast path because it doesn't support delayed insert errors.
        await qb.wrap((db) => db.execute("ROLLBACK to flush"), context);
        for (const table of cache.keys()) {
          if (
            tableNames !== undefined &&
            tableNames.has(getTableName(table)) === false
          ) {
            continue;
          }
          const target =
            chainId === undefined
              ? getTableName(table)
              : getPartitionName(table, chainId);
          const insertValues = Array.from(insertBuffer.get(table)!.values());
          const updateValues = Array.from(updateBuffer.get(table)!.values());
          if (insertValues.length > 0) {
            const endClock = startClock();
            await qb.wrap((db) => db.execute("SAVEPOINT flush"), context);
            const result = await recoverBatchError(
              insertValues,
              async (values) => {
                await qb.wrap((db) => db.execute("ROLLBACK to flush"), context);
                const text = getCopyText(
                  table,
                  values.map(({ row }) => row),
                );
                await copy(table, text);
                await qb.wrap((db) => db.execute("SAVEPOINT flush"), context);
              },
            );
            if (result.status === "error") {
              error = new DelayedInsertError(result.error.message);
              error.stack = undefined;
              addErrorMeta(
                error,
                `db.insert arguments:\n${prettyPrint(result.value.row)}`,
              );
              if (result.value.metadata.event) {
                addErrorMeta(error, toErrorMeta(result.value.metadata.event));
                common.logger.warn({
                  msg: "Failed to write cached database rows",
                  event: result.value.metadata.event.eventCallback.name,
                  type: "insert",
                  table: getTableName(table),
                  row_count: insertValues.length,
                  duration: endClock(),
                  error,
                });
              } else {
                common.logger.warn({
                  msg: "Failed to write cached database rows",
                  type: "insert",
                  table: getTableName(table),
                  row_count: insertValues.length,
                  duration: endClock(),
                  error,
                });
              }
              throw error;
            }
          }
          if (updateValues.length > 0) {
            // Steps for flushing "update" entries:
            // 1. Create temp table
            // 2. Copy into temp table
            // 3. Update target table with data from temp
            const createTempTableQuery = `
              CREATE TEMP TABLE IF NOT EXISTS "${target}"
              AS SELECT * FROM "${getTableConfig(table).schema ?? "public"}"."${target}"
              WITH NO DATA;
            `;
            const endClock = startClock();
            await qb.wrap((db) => db.execute(createTempTableQuery), context);
            await qb.wrap((db) => db.execute("SAVEPOINT flush"), context);
            const result = await recoverBatchError(
              updateValues,
              async (values) => {
                await qb.wrap((db) => db.execute("ROLLBACK to flush"), context);
                const text = getCopyText(
                  table,
                  values.map(({ row }) => row),
                );
                await copy(table, text, false);
                await qb.wrap((db) => db.execute("SAVEPOINT flush"), context);
              },
            );
            await qb.wrap(
              (db) => db.execute(`TRUNCATE TABLE "${target}"`),
              context,
            );
            if (result.status === "error") {
              error = new DelayedInsertError(result.error.message);
              error.stack = undefined;
              addErrorMeta(
                error,
                `db.update arguments:\n${prettyPrint(result.value.row)}`,
              );
              if (result.value.metadata.event) {
                addErrorMeta(error, toErrorMeta(result.value.metadata.event));
                common.logger.warn({
                  msg: "Failed to write cached database rows",
                  event: result.value.metadata.event.eventCallback.name,
                  type: "update",
                  table: getTableName(table),
                  row_count: updateValues.length,
                  duration: endClock(),
                  error,
                });
              } else {
                common.logger.warn({
                  msg: "Failed to write cached database rows",
                  type: "update",
                  table: getTableName(table),
                  row_count: updateValues.length,
                  duration: endClock(),
                  error,
                });
              }
              throw error;
            }
          }
        }
        // Note: if we weren't able to find the exact row that caused the error,
        // throw the original error.
        throw error;
      }
    },
    async prefetch({ events }) {
      const context = {
        logger: common.logger.child({ action: "prefetch_database_rows" }),
      };
      let totalBytes = 0;
      for (const table of tables) {
        totalBytes += cache.get(table)!.bytes;
      }
      // If data from the cache needs to be evicted, start with the
      // table with the least disk reads.
      if (totalBytes > common.options.indexingCacheMaxBytes) {
        for (const table of tables.sort(
          (a, b) => cache.get(a)!.diskReads - cache.get(b)!.diskReads,
        )) {
          if (cache.get(table)!.isCacheComplete === false) continue;
          totalBytes -= cache.get(table)!.bytes;
          cache.get(table)!.bytes = 0;
          cache.get(table)!.cache.clear();
          cache.get(table)!.isCacheComplete = false;
          // Note: spillover is not cleared because it is an invariant
          // it is empty
          common.logger.debug({
            msg: "Evicted cached database rows",
            table: getTableName(table),
            row_count: cache.get(table)!.cache.size,
          });
          if (totalBytes < common.options.indexingCacheMaxBytes) break;
        }
      }
      if (tables.every((table) => cache.get(table)!.isCacheComplete)) {
        return;
      }
      // Use historical accesses + next event batch to determine which
      // rows are going to be accessed, and preload them into the cache.
      const prediction = new Map<Table, Map<CacheKey, Row>>();
      for (const table of tables) {
        prediction.set(table, new Map());
      }
      for (const event of events) {
        if (profile.has(event.eventCallback.name)) {
          for (const table of tables) {
            if (cache.get(table)!.isCacheComplete) continue;
            for (const [, { count, pattern }] of profile
              .get(event.eventCallback.name)!
              .get(table)!) {
              // Expected value of times the prediction will be used.
              const ev =
                (count * SAMPLING_RATE) / eventCount[event.eventCallback.name]!;
              if (ev > PREDICTION_THRESHOLD) {
                const row = recoverProfilePattern(pattern, event);
                const key = getCacheKey(table, row, primaryKeyCache);
                prediction.get(table)!.set(key, row);
              }
            }
          }
        }
      }
      for (const [table, tableCache] of cache) {
        if (cache.get(table)!.isCacheComplete) continue;
        for (const key of tableCache.cache.keys()) {
          if (
            tableCache.spillover.has(key) ||
            prediction.get(table)!.has(key)
          ) {
            prediction.get(table)!.delete(key);
          } else {
            tableCache.cache.delete(key);
          }
        }
      }
      for (const table of tables) {
        cache.get(table)!.spillover.clear();
        cache.get(table)!.prefetched.clear();
      }
      for (const [table, tablePredictions] of prediction) {
        common.metrics.ponder_indexing_cache_requests_total.inc(
          {
            table: getTableName(table),
            type: "prefetch",
          },
          tablePredictions.size,
        );
      }
      await Promise.all(
        Array.from(prediction.entries())
          .filter(([, tablePredictions]) => tablePredictions.size > 0)
          .map(async ([table, tablePredictions]) => {
            for (const [key] of tablePredictions) {
              cache.get(table)!.prefetched.add(key);
            }
            const conditions = dedupe(
              Array.from(tablePredictions),
              ([key]) => key,
            ).map(([, prediction]) => getWhereCondition(table, prediction));
            if (conditions.length === 0) return;
            const endClock = startClock();
            await qb
              .wrap(
                (db) =>
                  db
                    .select()
                    .from(table)
                    .where(or(...conditions)),
                context,
              )
              .then((results) => {
                const resultsPerKey = new Map<CacheKey, Row>();
                for (const result of results) {
                  const ck = getCacheKey(table, result, primaryKeyCache);
                  resultsPerKey.set(ck, result);
                }
                const tableCache = cache.get(table)!;
                for (const key of tablePredictions.keys()) {
                  if (resultsPerKey.has(key)) {
                    tableCache.cache.set(key, resultsPerKey.get(key)!);
                  } else {
                    tableCache.cache.set(key, null);
                  }
                }
                common.logger.debug({
                  msg: "Pre-queried database rows",
                  table: getTableName(table),
                  row_count: results.length,
                  duration: endClock(),
                });
              });
            common.metrics.ponder_indexing_cache_query_duration.observe(
              {
                table: getTableName(table),
                method: "load",
              },
              endClock(),
            );
          }),
      );
    },
    invalidate() {
      for (const tableCache of cache.values()) {
        tableCache.isCacheComplete = false;
      }
    },
    clear() {
      for (const tableCache of cache.values()) {
        tableCache.cache.clear();
        tableCache.spillover.clear();
      }
      for (const tableBuffer of insertBuffer.values()) {
        tableBuffer.clear();
      }
      for (const tableBuffer of updateBuffer.values()) {
        tableBuffer.clear();
      }
    },
    set event(_event: Event | undefined) {
      event = _event;
    },
    set qb(_qb: QB) {
      qb = _qb;
    },
  };
};
</file>

<file path="packages/core/src/indexing-store/index.bench.ts">
import { onchainTable } from "@/index.js";
import { bench, run } from "mitata";
import { checkOnchainTable, validateUpdateSet } from "./index.js";
import { getPrimaryKeyCache } from "./utils.js";
const table = onchainTable("account", (t) => ({
  address: t.hex().primaryKey(),
  balance: t.bigint(),
}));
const primaryKeyCache = getPrimaryKeyCache([table]);
// 446.03 ps/iter
bench("checkOnchainTable", () => {
  checkOnchainTable(table, "find");
}).gc("inner");
// 8.89 ns/iter
bench("validateUpdateSet", () => {
  validateUpdateSet(
    table,
    { address: "0x123" },
    { address: "0x123" },
    primaryKeyCache,
  );
}).gc("inner");
run();
</file>

<file path="packages/core/src/indexing-store/index.test.ts">
import { ALICE } from "@/_test/constants.js";
import {
  context,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { getRejectionValue } from "@/_test/utils.js";
import { onchainEnum, onchainTable } from "@/drizzle/onchain.js";
import {
  BigIntSerializationError,
  NonRetryableUserError,
  RawSqlError,
  type RetryableError,
} from "@/internal/errors.js";
import type { IndexingErrorHandler } from "@/internal/types.js";
import { eq } from "drizzle-orm";
import { pgTable } from "drizzle-orm/pg-core";
import { toBytes, zeroAddress } from "viem";
import { beforeEach, expect, test } from "vitest";
import { createIndexingCache } from "./cache.js";
import { createIndexingStore } from "./index.js";
beforeEach(setupCommon);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
const indexingErrorHandler: IndexingErrorHandler = {
  getRetryableError: () => {
    return indexingErrorHandler.error;
  },
  setRetryableError: (error: RetryableError) => {
    indexingErrorHandler.error = error;
  },
  clearRetryableError: () => {
    indexingErrorHandler.error = undefined;
  },
  error: undefined as RetryableError | undefined,
};
test("find", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // empty
    let result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toBe(null);
    // with entry
    await indexingStore.db
      .insert(schema.account)
      .values({ address: zeroAddress, balance: 10n });
    result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 10n,
    });
    // force db query
    indexingCache.clear();
    indexingCache.invalidate();
    result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toBe(null);
  });
});
test("insert", async () => {
  const { database } = await setupDatabaseServices();
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // single
    let result: any = await indexingStore.db
      .insert(schema.account)
      .values({ address: zeroAddress, balance: 10n });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 10n,
    });
    result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 10n,
    });
    // multiple
    result = await indexingStore.db.insert(schema.account).values([
      { address: "0x0000000000000000000000000000000000000001", balance: 12n },
      { address: "0x0000000000000000000000000000000000000002", balance: 52n },
    ]);
    expect(result).toMatchObject([
      {
        address: "0x0000000000000000000000000000000000000001",
        balance: 12n,
      },
      {
        address: "0x0000000000000000000000000000000000000002",
        balance: 52n,
      },
    ]);
    result = await indexingStore.db.find(schema.account, {
      address: "0x0000000000000000000000000000000000000001",
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000001",
      balance: 12n,
    });
    result = await indexingStore.db.find(schema.account, {
      address: "0x0000000000000000000000000000000000000002",
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000002",
      balance: 52n,
    });
    // on conflict do nothing
    result = await indexingStore.db
      .insert(schema.account)
      .values({
        address: "0x0000000000000000000000000000000000000001",
        balance: 44n,
      })
      .onConflictDoNothing();
    expect(result).toBe(null);
    result = await indexingStore.db.find(schema.account, {
      address: "0x0000000000000000000000000000000000000001",
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000001",
      balance: 12n,
    });
    result = await indexingStore.db
      .insert(schema.account)
      .values([
        { address: "0x0000000000000000000000000000000000000001", balance: 44n },
        { address: "0x0000000000000000000000000000000000000003", balance: 0n },
      ])
      .onConflictDoNothing();
    expect(result).toMatchObject([
      null,
      {
        address: "0x0000000000000000000000000000000000000003",
        balance: 0n,
      },
    ]);
    result = await indexingStore.db.find(schema.account, {
      address: "0x0000000000000000000000000000000000000001",
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000001",
      balance: 12n,
    });
    // on conflict do update
    await indexingStore.db
      .insert(schema.account)
      .values({
        address: "0x0000000000000000000000000000000000000001",
        balance: 90n,
      })
      .onConflictDoUpdate({
        balance: 16n,
      });
    result = await indexingStore.db.find(schema.account, {
      address: "0x0000000000000000000000000000000000000001",
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000001",
      balance: 16n,
    });
    await indexingStore.db
      .insert(schema.account)
      .values([
        { address: "0x0000000000000000000000000000000000000001", balance: 44n },
        { address: "0x0000000000000000000000000000000000000002", balance: 0n },
      ])
      .onConflictDoUpdate((row) => ({
        balance: row.balance + 16n,
      }));
    result = await indexingStore.db.find(schema.account, {
      address: "0x0000000000000000000000000000000000000001",
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000001",
      balance: 32n,
    });
  });
});
test("update", async () => {
  const { database } = await setupDatabaseServices();
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // setup
    await indexingStore.db
      .insert(schema.account)
      .values({ address: zeroAddress, balance: 10n });
    // no function
    let result: any = await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      .set({ balance: 12n });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 12n,
    });
    result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 12n,
    });
    // function
    result = await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      .set((row) => ({ balance: row.balance + 10n }));
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 22n,
    });
    result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 22n,
    });
    // undefined
    result = await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      .set({ balance: undefined });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 22n,
    });
    result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 22n,
    });
  });
});
test("update throw error when primary key is updated", async () => {
  const { database } = await setupDatabaseServices();
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // setup
    await indexingStore.db
      .insert(schema.account)
      .values({ address: zeroAddress, balance: 10n });
    // no function
    let error: any = await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      // @ts-expect-error
      .set({ address: ALICE })
      .catch((error) => error);
    expect(error).toBeInstanceOf(NonRetryableUserError);
    // function
    error = await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      .set(() => ({ address: ALICE }))
      .catch((error) => error);
    expect(error).toBeInstanceOf(NonRetryableUserError);
    // update same primary key no function
    let row: any = await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      // @ts-expect-error
      .set({ address: zeroAddress, balance: 20n })
      .catch((error) => error);
    expect(row.address).toBe(zeroAddress);
    expect(row.balance).toBe(20n);
    // update same primary key function
    row = await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      .set(() => ({ address: zeroAddress, balance: 30n }))
      .catch((error) => error);
    expect(row.address).toBe(zeroAddress);
    expect(row.balance).toBe(30n);
  });
});
test("delete", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // no entry
    let deleted = await indexingStore.db.delete(schema.account, {
      address: zeroAddress,
    });
    expect(deleted).toBe(false);
    // entry
    await indexingStore.db
      .insert(schema.account)
      .values({ address: zeroAddress, balance: 12n });
    await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      .set({ balance: 12n });
    deleted = await indexingStore.db.delete(schema.account, {
      address: zeroAddress,
    });
    expect(deleted).toBe(true);
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toBe(null);
  });
});
test("sql", async () => {
  if (context.databaseConfig.kind === "pglite_test") {
    return;
  }
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // setup
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      balance: 10n,
    });
    // select
    const result = await indexingStore.db.sql
      .select()
      .from(schema.account)
      .where(eq(schema.account.address, zeroAddress));
    expect(result).toMatchObject([
      {
        address: "0x0000000000000000000000000000000000000000",
        balance: 10n,
      },
    ]);
    // non-null constraint
    await tx.wrap((db) => db.execute("SAVEPOINT test"));
    expect(
      await getRejectionValue(
        async () =>
          // @ts-ignore
          await indexingStore.db.sql.insert(schema.account).values({
            address: "0x0000000000000000000000000000000000000001",
            balance: undefined,
          }),
      ),
    ).toBeInstanceOf(RawSqlError);
    // TODO(kyle) check constraint
    // unique constraint
    await tx.wrap((db) => db.execute("ROLLBACK TO test"));
    expect(
      await getRejectionValue(
        async () =>
          await indexingStore.db.sql
            .insert(schema.account)
            .values({ address: zeroAddress, balance: 10n }),
      ),
    ).toBeInstanceOf(RawSqlError);
  });
});
test("sql followed by find", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.sql
      .insert(schema.account)
      .values({ address: zeroAddress, balance: 10n });
    const row = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(row).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 10n,
    });
  });
});
test("sql with error", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // error
    const error = await indexingStore.db.sql
      .execute("SELECT * FROM does_not_exist")
      .catch((error) => error);
    expect(error).toBeInstanceOf(RawSqlError);
    // next query doesn't error
    await indexingStore.db.sql
      .select()
      .from(schema.account)
      .where(eq(schema.account.address, zeroAddress));
  });
});
test("onchain table", async () => {
  const { database } = await setupDatabaseServices();
  const schema = {
    account: pgTable("account", (p) => ({
      address: p.text().primaryKey(),
      balance: p.integer().notNull(),
    })),
  };
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // check error
    expect(
      await getRejectionValue(
        async () =>
          await indexingStore.db
            // @ts-ignore
            .find(schema.account, { address: zeroAddress }),
      ),
    ).toBeTruthy();
  });
});
test("missing rows", async () => {
  const { database } = await setupDatabaseServices();
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // error
    expect(
      await getRejectionValue(
        // @ts-ignore
        async () =>
          await indexingStore.db
            .insert(schema.account)
            // @ts-ignore
            .values({ address: zeroAddress }),
      ),
    ).toBeTruthy();
  });
});
test("unawaited promise", async () => {
  const { database } = await setupDatabaseServices();
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  indexingCache.qb = database.userQB;
  indexingStore.qb = database.userQB;
  indexingStore.isProcessingEvents = false;
  const promise = indexingStore.db
    .insert(schema.account)
    .values({
      address: "0x0000000000000000000000000000000000000001",
      balance: 90n,
    })
    .onConflictDoUpdate({
      balance: 16n,
    });
  expect(await getRejectionValue(async () => await promise)).toBeTruthy();
});
test("notNull", async () => {
  const { database } = await setupDatabaseServices();
  let schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint(),
    })),
  };
  let indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  let indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // insert
    let result = await indexingStore.db
      .insert(schema.account)
      .values({ address: zeroAddress });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: null,
    });
    result = await indexingStore.db
      .find(schema.account, {
        address: zeroAddress,
      })
      .then((result) => result!);
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: null,
    });
    // update
    result = await indexingStore.db
      .update(schema.account, { address: zeroAddress })
      .set({});
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: null,
    });
    // error
    schema = {
      // @ts-ignore
      account: onchainTable("account", (p) => ({
        address: p.hex().primaryKey(),
        balance: p.bigint().notNull(),
      })),
    };
    indexingCache = createIndexingCache({
      common: context.common,
      schemaBuild: { schema },
      crashRecoveryCheckpoint: undefined,
      eventCount: {},
    });
    indexingStore = createIndexingStore({
      common: context.common,
      schemaBuild: { schema },
      indexingCache,
      indexingErrorHandler,
    });
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    expect(
      await getRejectionValue(
        async () =>
          await indexingStore.db
            .insert(schema.account)
            .values({ address: zeroAddress }),
      ),
    ).toBeTruthy();
    expect(
      await getRejectionValue(
        async () =>
          await indexingStore.db
            .insert(schema.account)
            .values({ address: zeroAddress, balance: null }),
      ),
    ).toBeTruthy();
  });
});
test("default", async () => {
  const { database } = await setupDatabaseServices();
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.integer().default(0),
    })),
  };
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db
      .insert(schema.account)
      .values({ address: zeroAddress });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 0,
    });
  });
});
test("$default", async () => {
  const { database } = await setupDatabaseServices();
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().$default(() => 10n),
    })),
  };
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db
      .insert(schema.account)
      .values({ address: zeroAddress });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 10n,
    });
  });
});
test("$onUpdateFn", async () => {
  const { database } = await setupDatabaseServices();
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p
        .bigint()
        .notNull()
        .$onUpdateFn(() => 10n),
    })),
  };
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    // insert
    await indexingStore.db
      .insert(schema.account)
      .values({ address: zeroAddress });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 10n,
    });
    // update
  });
});
test("basic columns", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
      int2: p.smallint().notNull(),
      int8n: p.int8({ mode: "number" }).notNull(),
      int8b: p.int8({ mode: "bigint" }).notNull(),
      boolean: p.boolean().notNull(),
      text: p.text().notNull(),
      varchar: p.varchar().notNull(),
      char: p.char().notNull(),
      numeric: p.numeric().notNull(),
      real: p.real().notNull(),
      doublePrecision: p.doublePrecision().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      balance: 20n,
      int2: 20,
      int8n: 20,
      int8b: 20n,
      boolean: true,
      text: "20",
      varchar: "20",
      char: "2",
      numeric: "20",
      real: 20,
      doublePrecision: 20,
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balance: 20n,
      boolean: true,
      char: "2",
      doublePrecision: 20,
      int2: 20,
      int8b: 20n,
      int8n: 20,
      numeric: "20",
      real: 20,
      text: "20",
      varchar: "20",
    });
    await indexingCache.flush();
  });
});
test("array", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balances: p.bigint().array().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  // dynamic size
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      balances: [20n],
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      balances: [20n],
    });
    await indexingCache.flush();
    // TODO(kyle) fixed size
  });
});
test("text array", async () => {
  const schema = {
    test: onchainTable("test", (p) => ({
      address: p.hex().primaryKey(),
      textArray: p.text().array().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    const STRING_ARRAY_VALUE = "//U_W_U\\\\";
    await indexingStore.db.insert(schema.test).values({
      address: zeroAddress,
      textArray: [STRING_ARRAY_VALUE],
    });
    const result = await indexingStore.db.find(schema.test, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      textArray: ["//U_W_U\\\\"],
    });
    await indexingCache.flush();
  });
});
test("enum", async () => {
  const moodEnum = onchainEnum("mood", ["sad", "ok", "happy"]);
  const schema = {
    moodEnum,
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      mood: moodEnum(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      mood: "ok",
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      mood: "ok",
    });
    await indexingCache.flush();
    // TODO(kyle) error
  });
});
test("json bigint", async () => {
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      metadata: p.json().$type<{ balance: bigint }>(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    expect(
      await getRejectionValue(
        async () =>
          await indexingStore.db
            .insert(schema.account)
            .values({ address: zeroAddress, metadata: { balance: 10n } }),
      ),
    ).toBeInstanceOf(BigIntSerializationError);
  });
});
test("bytes", async () => {
  const schema = {
    account: onchainTable("account", (t) => ({
      address: t.hex().primaryKey(),
      calldata: t.bytes().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      calldata: toBytes(zeroAddress),
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      calldata: new Uint8Array(20),
    });
    await indexingCache.flush();
  });
});
test("text with null bytes", async () => {
  const schema = {
    account: onchainTable("account", (t) => ({
      address: t.hex().primaryKey(),
      name: t.text().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      name: "tencentclub\x00\x00\x00\x00\x00\x00",
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      name: "tencentclub",
    });
    await indexingCache.flush();
  });
});
test.skip("time", async () => {
  const schema = {
    account: onchainTable("account", (t) => ({
      address: t.hex().primaryKey(),
      time: t.time().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      time: "04:05:06",
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchInlineSnapshot();
    await indexingCache.flush();
  });
});
test("timestamp", async () => {
  const schema = {
    account: onchainTable("account", (t) => ({
      address: t.hex().primaryKey(),
      timestamp: t.timestamp().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      timestamp: new Date(1742925862000),
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      timestamp: new Date(1742925862000),
    });
    await indexingCache.flush();
  });
});
test.skip("date", async () => {
  const schema = {
    account: onchainTable("account", (t) => ({
      address: t.hex().primaryKey(),
      date: t.date({ mode: "date" }).notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      date: new Date(1742925862000),
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchInlineSnapshot();
    await indexingCache.flush();
  });
});
test.skip("interval", async () => {
  const schema = {
    account: onchainTable("account", (t) => ({
      address: t.hex().primaryKey(),
      interval: t.interval().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      interval: "1 day",
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchInlineSnapshot();
    await indexingCache.flush();
  });
});
test("point", async () => {
  const schema = {
    account: onchainTable("account", (t) => ({
      address: t.hex().primaryKey(),
      point: t.point().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      point: [1, 2],
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      point: [1, 2],
    });
    await indexingCache.flush();
  });
});
test("line", async () => {
  const schema = {
    account: onchainTable("account", (t) => ({
      address: t.hex().primaryKey(),
      line: t.line().notNull(),
    })),
  };
  const { database } = await setupDatabaseServices({
    schemaBuild: { schema },
  });
  const indexingCache = createIndexingCache({
    common: context.common,
    schemaBuild: { schema },
    crashRecoveryCheckpoint: undefined,
    eventCount: {},
  });
  const indexingStore = createIndexingStore({
    common: context.common,
    schemaBuild: { schema },
    indexingCache,
    indexingErrorHandler,
  });
  await database.userQB.transaction(async (tx) => {
    indexingCache.qb = tx;
    indexingStore.qb = tx;
    await indexingStore.db.insert(schema.account).values({
      address: zeroAddress,
      line: [1, 2, 3],
    });
    const result = await indexingStore.db.find(schema.account, {
      address: zeroAddress,
    });
    expect(result).toMatchObject({
      address: "0x0000000000000000000000000000000000000000",
      line: [1, 2, 3],
    });
    await indexingCache.flush();
  });
});
</file>

<file path="packages/core/src/indexing-store/index.ts">
import type { QB } from "@/database/queryBuilder.js";
import { onchain } from "@/drizzle/onchain.js";
import type { Common } from "@/internal/common.js";
import {
  DbConnectionError,
  InvalidStoreAccessError,
  InvalidStoreMethodError,
  NonRetryableUserError,
  RawSqlError,
  RecordNotFoundError,
  RetryableError,
  UndefinedTableError,
  UniqueConstraintError,
} from "@/internal/errors.js";
import type { Schema } from "@/internal/types.js";
import type { IndexingErrorHandler, SchemaBuild } from "@/internal/types.js";
import type { Db } from "@/types/db.js";
import { copy, copyOnWrite } from "@/utils/copy.js";
import { createLock } from "@/utils/mutex.js";
import { prettyPrint } from "@/utils/print.js";
import { getSQLQueryRelations, isReadonlySQLQuery } from "@/utils/sql-parse.js";
import { startClock } from "@/utils/timer.js";
import {
  type Column,
  type QueryWithTypings,
  type Table,
  getTableName,
  getViewName,
  isTable,
  isView,
} from "drizzle-orm";
import { getTableConfig } from "drizzle-orm/pg-core";
import { drizzle } from "drizzle-orm/pg-proxy";
import type { IndexingCache, Row } from "./cache.js";
import { getPrimaryKeyCache } from "./utils.js";
export type IndexingStore = {
  db: Db<Schema>;
  qb: QB;
  isProcessingEvents: boolean;
};
export const validateUpdateSet = (
  table: Table,
  set: Object,
  prev: Row,
  cache: Map<Table, [string, Column][]>,
) => {
  const primaryKeys = cache.get(table)!;
  for (let i = 0; i < primaryKeys.length; i++) {
    const [js] = primaryKeys[i]!;
    if (js in set) {
      // Note: Noop on the primary keys if they are identical, otherwise throw an error.
      if ((set as Row)[js] !== prev[js]) {
        throw new NonRetryableUserError(
          `Primary key column '${js}' cannot be updated`,
        );
      }
    }
  }
};
/** Throw an error if `table` is not an `onchainTable`. */
export const checkOnchainTable = (
  table: Table,
  method: "find" | "insert" | "update" | "delete",
) => {
  if (table === undefined)
    throw new UndefinedTableError(
      `Table object passed to db.${method}() is undefined`,
    );
  if (onchain in table) return;
  throw new InvalidStoreMethodError(
    method === "find"
      ? `db.find() can only be used with onchain tables, and '${getTableConfig(table).name}' is an offchain table or a view.`
      : `Indexing functions can only write to onchain tables, and '${getTableConfig(table).name}' is an offchain table or a view.`,
  );
};
export const checkTableAccess = (
  table: Table,
  method: "find" | "insert" | "update" | "delete",
  key: object,
  chainId?: number,
) => {
  if (chainId === undefined) return;
  if ("chainId" in key && String(key.chainId) === String(chainId)) return;
  throw new InvalidStoreAccessError(
    "chainId" in key
      ? `db.${method}(${getTableConfig(table).name}) cannot access rows on different chains when ordering is 'isolated'.`
      : `db.${method}(${getTableConfig(table).name}) must specify 'chainId' when ordering is 'isolated'.`,
  );
};
export const createIndexingStore = ({
  common,
  schemaBuild: { schema },
  indexingCache,
  indexingErrorHandler,
  chainId,
}: {
  common: Common;
  schemaBuild: Pick<SchemaBuild, "schema">;
  indexingCache: IndexingCache;
  indexingErrorHandler: IndexingErrorHandler;
  chainId?: number;
}): IndexingStore => {
  let qb: QB = undefined!;
  let isProcessingEvents = true;
  const tables = Object.values(schema).filter(isTable);
  const views = Object.values(schema).filter(isView);
  const primaryKeyCache = getPrimaryKeyCache(tables);
  const lock = createLock();
  const storeMethodWrapper = (fn: (...args: any[]) => Promise<any>) => {
    return async (...args: any[]) => {
      try {
        if (isProcessingEvents === false) {
          throw new NonRetryableUserError(
            "A store API method (find, update, insert, delete) was called after the indexing function returned. Hint: Did you forget to await the store API method call (an unawaited promise)?",
          );
        }
        await lock.lock();
        const result = await fn(...args);
        // @ts-expect-error typescript bug lol
        if (isProcessingEvents === false) {
          throw new NonRetryableUserError(
            "A store API method (find, update, insert, delete) was called after the indexing function returned. Hint: Did you forget to await the store API method call (an unawaited promise)?",
          );
        }
        return result;
      } catch (error) {
        if (isProcessingEvents === false) {
          throw new NonRetryableUserError(
            "A store API method (find, update, insert, delete) was called after the indexing function returned. Hint: Did you forget to await the store API method call (an unawaited promise)?",
          );
        }
        if (error instanceof RetryableError) {
          indexingErrorHandler.setRetryableError(error);
        }
        throw error;
      } finally {
        lock.unlock();
      }
    };
  };
  // Note: the naming convention is used to separate user space from ponder space.
  // "user" prefix is user space, "ponder" prefix is ponder space.
  return {
    db: {
      // @ts-ignore
      find: storeMethodWrapper(async (table: Table, key) => {
        common.metrics.ponder_indexing_store_queries_total.inc({
          table: getTableName(table),
          method: "find",
        });
        checkOnchainTable(table, "find");
        checkTableAccess(table, "find", key, chainId);
        const ponderRow = await indexingCache.get({ table, key });
        const userRow = ponderRow === null ? null : copyOnWrite(ponderRow);
        return userRow;
      }),
      // @ts-ignore
      insert(table: Table) {
        return {
          values: (userValues: any) => {
            // @ts-ignore
            const inner = {
              onConflictDoNothing: storeMethodWrapper(async () => {
                common.metrics.ponder_indexing_store_queries_total.inc({
                  table: getTableName(table),
                  method: "insert",
                });
                checkOnchainTable(table, "insert");
                const ponderValues = copy(userValues);
                if (Array.isArray(ponderValues)) {
                  const ponderRows = [];
                  for (const value of ponderValues) {
                    checkTableAccess(table, "insert", value, chainId);
                    const row = await indexingCache.get({ table, key: value });
                    if (row) {
                      ponderRows.push(null);
                    } else {
                      ponderRows.push(
                        indexingCache.set({
                          table,
                          key: value,
                          row: value,
                          isUpdate: false,
                        }),
                      );
                    }
                  }
                  const userRows = ponderRows.map((row) =>
                    row === null ? row : copyOnWrite(row),
                  );
                  return userRows;
                } else {
                  checkTableAccess(table, "insert", ponderValues, chainId);
                  const row = await indexingCache.get({
                    table,
                    key: ponderValues,
                  });
                  if (row) {
                    return null;
                  }
                  const ponderRow = indexingCache.set({
                    table,
                    key: ponderValues,
                    row: ponderValues,
                    isUpdate: false,
                  });
                  const userRow = copyOnWrite(ponderRow);
                  return userRow;
                }
              }),
              onConflictDoUpdate: storeMethodWrapper(
                async (userUpdateValues: any) => {
                  common.metrics.ponder_indexing_store_queries_total.inc({
                    table: getTableName(table),
                    method: "insert",
                  });
                  checkOnchainTable(table, "insert");
                  if (Array.isArray(userValues)) {
                    const ponderRows: Row[] = [];
                    for (const value of userValues) {
                      checkTableAccess(table, "insert", value, chainId);
                      const ponderRowUpdate = await indexingCache.get({
                        table,
                        key: value,
                      });
                      if (ponderRowUpdate) {
                        if (typeof userUpdateValues === "function") {
                          const userRowUpdate = copyOnWrite(ponderRowUpdate);
                          const userSet = userUpdateValues(userRowUpdate);
                          const ponderSet = copy(userSet);
                          validateUpdateSet(
                            table,
                            ponderSet,
                            ponderRowUpdate,
                            primaryKeyCache,
                          );
                          for (const [key, value] of Object.entries(
                            ponderSet,
                          )) {
                            if (value === undefined) continue;
                            ponderRowUpdate[key] = value;
                          }
                        } else {
                          const userSet = userUpdateValues;
                          const ponderSet = copy(userSet);
                          validateUpdateSet(
                            table,
                            ponderSet,
                            ponderRowUpdate,
                            primaryKeyCache,
                          );
                          for (const [key, value] of Object.entries(
                            ponderSet,
                          )) {
                            if (value === undefined) continue;
                            ponderRowUpdate[key] = value;
                          }
                        }
                        ponderRows.push(
                          indexingCache.set({
                            table,
                            key: ponderRowUpdate,
                            row: ponderRowUpdate,
                            isUpdate: true,
                          }),
                        );
                      } else {
                        const ponderValue = copy(value);
                        ponderRows.push(
                          indexingCache.set({
                            table,
                            key: ponderValue,
                            row: ponderValue,
                            isUpdate: false,
                          }),
                        );
                      }
                    }
                    const userRows = ponderRows.map((row) =>
                      row === null ? row : copyOnWrite(row),
                    );
                    return userRows;
                  } else {
                    checkTableAccess(table, "insert", userValues, chainId);
                    const ponderRowUpdate = await indexingCache.get({
                      table,
                      key: userValues,
                    });
                    if (ponderRowUpdate) {
                      if (typeof userUpdateValues === "function") {
                        const userRowUpdate = copyOnWrite(ponderRowUpdate);
                        const userSet = userUpdateValues(userRowUpdate);
                        const ponderSet = copy(userSet);
                        validateUpdateSet(
                          table,
                          ponderSet,
                          ponderRowUpdate,
                          primaryKeyCache,
                        );
                        for (const [key, value] of Object.entries(ponderSet)) {
                          if (value === undefined) continue;
                          ponderRowUpdate[key] = value;
                        }
                      } else {
                        const userSet = userUpdateValues;
                        const ponderSet = copy(userSet);
                        validateUpdateSet(
                          table,
                          ponderSet,
                          ponderRowUpdate,
                          primaryKeyCache,
                        );
                        for (const [key, value] of Object.entries(ponderSet)) {
                          if (value === undefined) continue;
                          ponderRowUpdate[key] = value;
                        }
                      }
                      const ponderRow = indexingCache.set({
                        table,
                        key: ponderRowUpdate,
                        row: ponderRowUpdate,
                        isUpdate: true,
                      });
                      const userRow = copyOnWrite(ponderRow);
                      return userRow;
                    }
                    const ponderValues = copy(userValues);
                    const ponderRowInsert = indexingCache.set({
                      table,
                      key: ponderValues,
                      row: ponderValues,
                      isUpdate: false,
                    });
                    const userRow = copyOnWrite(ponderRowInsert);
                    return userRow;
                  }
                },
              ),
              // biome-ignore lint/suspicious/noThenProperty: <explanation>
              then: (onFulfilled, onRejected) =>
                storeMethodWrapper(async () => {
                  common.metrics.ponder_indexing_store_queries_total.inc({
                    table: getTableName(table),
                    method: "insert",
                  });
                  checkOnchainTable(table, "insert");
                  const ponderValues = copy(userValues);
                  if (Array.isArray(ponderValues)) {
                    const ponderRows = [];
                    for (const value of ponderValues) {
                      checkTableAccess(table, "insert", value, chainId);
                      if (qb.$dialect === "pglite") {
                        const row = await indexingCache.get({
                          table,
                          key: value,
                        });
                        if (row) {
                          throw new UniqueConstraintError(
                            `Primary key conflict in table '${getTableName(table)}'`,
                          );
                        } else {
                          ponderRows.push(
                            indexingCache.set({
                              table,
                              key: value,
                              row: value,
                              isUpdate: false,
                            }),
                          );
                        }
                      } else {
                        // Note: optimistic assumption that no conflict exists
                        // because error is recovered at flush time
                        ponderRows.push(
                          indexingCache.set({
                            table,
                            key: value,
                            row: value,
                            isUpdate: false,
                          }),
                        );
                      }
                    }
                    const userRows = ponderRows.map((row) =>
                      row === null ? row : copyOnWrite(row),
                    );
                    return Promise.resolve(userRows).then(
                      onFulfilled,
                      onRejected,
                    );
                  } else {
                    checkTableAccess(table, "insert", ponderValues, chainId);
                    let ponderRow: Row;
                    if (qb.$dialect === "pglite") {
                      const row = await indexingCache.get({
                        table,
                        key: ponderValues,
                      });
                      if (row) {
                        throw new UniqueConstraintError(
                          `Primary key conflict in table '${getTableName(table)}'`,
                        );
                      } else {
                        ponderRow = indexingCache.set({
                          table,
                          key: ponderValues,
                          row: ponderValues,
                          isUpdate: false,
                        });
                      }
                    } else {
                      // Note: optimistic assumption that no conflict exists
                      // because error is recovered at flush time
                      ponderRow = indexingCache.set({
                        table,
                        key: ponderValues,
                        row: ponderValues,
                        isUpdate: false,
                      });
                    }
                    const userRow = copyOnWrite(ponderRow);
                    return Promise.resolve(userRow).then(
                      onFulfilled,
                      onRejected,
                    );
                  }
                })().then(onFulfilled, onRejected),
              catch: (onRejected) => inner.then(undefined, onRejected),
              finally: (onFinally) =>
                inner.then(
                  (value: any) => {
                    onFinally?.();
                    return value;
                  },
                  (reason: any) => {
                    onFinally?.();
                    throw reason;
                  },
                ),
              // @ts-ignore
            } satisfies ReturnType<
              ReturnType<IndexingStore["db"]["insert"]>["values"]
            >;
            return inner;
          },
        };
      },
      // @ts-ignore
      update(table: Table, key) {
        return {
          set: storeMethodWrapper(async (userValues: any) => {
            common.metrics.ponder_indexing_store_queries_total.inc({
              table: getTableName(table),
              method: "update",
            });
            checkOnchainTable(table, "update");
            checkTableAccess(table, "update", key, chainId);
            const ponderRowUpdate = await indexingCache.get({ table, key });
            if (ponderRowUpdate === null) {
              const error = new RecordNotFoundError(
                `No existing record found in table '${getTableName(table)}'`,
              );
              error.meta.push(`db.update arguments:\n${prettyPrint(key)}`);
              throw error;
            }
            if (typeof userValues === "function") {
              const userRow = copyOnWrite(ponderRowUpdate);
              const userSet = userValues(userRow);
              const ponderSet = copy(userSet);
              validateUpdateSet(
                table,
                ponderSet,
                ponderRowUpdate,
                primaryKeyCache,
              );
              for (const [key, value] of Object.entries(ponderSet)) {
                if (value === undefined) continue;
                ponderRowUpdate[key] = value;
              }
            } else {
              const userSet = userValues;
              const ponderSet = copy(userSet);
              validateUpdateSet(
                table,
                ponderSet,
                ponderRowUpdate,
                primaryKeyCache,
              );
              for (const [key, value] of Object.entries(ponderSet)) {
                if (value === undefined) continue;
                ponderRowUpdate[key] = value;
              }
            }
            const ponderRow = indexingCache.set({
              table,
              key: ponderRowUpdate,
              row: ponderRowUpdate,
              isUpdate: true,
            });
            const userRow = copyOnWrite(ponderRow);
            return userRow;
          }),
        };
      },
      // @ts-ignore
      delete: storeMethodWrapper(async (table: Table, key) => {
        common.metrics.ponder_indexing_store_queries_total.inc({
          table: getTableName(table),
          method: "delete",
        });
        checkOnchainTable(table, "delete");
        checkTableAccess(table, "delete", key, chainId);
        return indexingCache.delete({ table, key });
      }),
      // @ts-ignore
      sql: drizzle(
        storeMethodWrapper(async (_sql, params, method, typings) => {
          const isSelectOnly = await isReadonlySQLQuery(_sql);
          if (isSelectOnly === false) {
            await indexingCache.flush();
            indexingCache.invalidate();
            indexingCache.clear();
          } else {
            // Note: Not all nodes are implemented in the parser,
            // so we need to try/catch to avoid throwing an error.
            let relations: Set<string> | undefined;
            try {
              relations = await getSQLQueryRelations(_sql);
            } catch {}
            if (
              Array.from(relations ?? []).some((refName) =>
                views.some((view) => getViewName(view) === refName),
              )
            ) {
              await indexingCache.flush();
            } else {
              await indexingCache.flush({ tableNames: relations });
            }
          }
          const query: QueryWithTypings = { sql: _sql, params, typings };
          const endClock = startClock();
          try {
            // Note: Use transaction so that user-land queries don't affect the
            // in-progress transaction.
            return await qb.transaction(async (tx) => {
              const result = await tx.wrap((tx) =>
                tx._.session
                  .prepareQuery(query, undefined, undefined, method === "all")
                  .execute(),
              );
              if (method === "all") {
                return {
                  // @ts-ignore
                  ...result,
                  // @ts-ignore
                  rows: result.rows.map((row) => Object.values(row)),
                };
              }
              return result;
            });
          } catch (error) {
            if (error instanceof DbConnectionError) {
              throw error;
            }
            throw new RawSqlError((error as Error).message);
          } finally {
            common.metrics.ponder_indexing_store_raw_sql_duration.observe(
              endClock(),
            );
          }
        }),
        { schema, casing: "snake_case" },
      ),
    },
    set qb(_qb: QB) {
      qb = _qb;
    },
    set isProcessingEvents(_isProcessingEvents: boolean) {
      isProcessingEvents = _isProcessingEvents;
    },
  };
};
</file>

<file path="packages/core/src/indexing-store/profile.test.ts">
import { getBlocksIndexingBuild, getChain } from "@/_test/utils.js";
import { onchainTable } from "@/drizzle/onchain.js";
import type { BlockEvent, LogEvent, TraceEvent } from "@/internal/types.js";
import { ZERO_CHECKPOINT_STRING } from "@/utils/checkpoint.js";
import type { Column, Table } from "drizzle-orm";
import { zeroAddress } from "viem";
import { expect, test } from "vitest";
import { recordProfilePattern, recoverProfilePattern } from "./profile.js";
test("recordProfilePattern() no pattern", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "block",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      block: { timestamp: 1n } as BlockEvent["event"]["block"],
    },
  } satisfies BlockEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["address", schema.account.address]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { address: zeroAddress },
    [],
    primaryKeyCache,
  );
  expect(pattern).toBeUndefined();
});
test("recordProfilePattern() with undefined log event args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: undefined,
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { timestamp: 1n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["address", schema.account.address]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { address: zeroAddress },
    [],
    primaryKeyCache,
  );
  expect(pattern).toBeUndefined();
});
test("recordProfilePattern() with undefined trace event args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "trace",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: undefined,
      result: undefined,
      trace: {} as TraceEvent["event"]["trace"],
      transaction: {} as TraceEvent["event"]["transaction"],
      block: { timestamp: 1n } as BlockEvent["event"]["block"],
    },
  } satisfies TraceEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["address", schema.account.address]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { address: zeroAddress },
    [],
    primaryKeyCache,
  );
  expect(pattern).toBeUndefined();
});
test("recordProfilePattern() with array log event args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: [],
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { timestamp: 1n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["address", schema.account.address]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { address: zeroAddress },
    [],
    primaryKeyCache,
  );
  expect(pattern).toBeUndefined();
});
test("recordProfilePattern() with array trace event args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "trace",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: [],
      result: [],
      trace: {} as TraceEvent["event"]["trace"],
      transaction: {} as TraceEvent["event"]["transaction"],
      block: { timestamp: 1n } as BlockEvent["event"]["block"],
    },
  } satisfies TraceEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["address", schema.account.address]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { address: zeroAddress },
    [],
    primaryKeyCache,
  );
  expect(pattern).toBeUndefined();
});
test("recordProfilePattern() chainId", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "block",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      block: { timestamp: 1n } as BlockEvent["event"]["block"],
    },
  } satisfies BlockEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.integer().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["address", schema.account.address]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { address: 1 },
    [],
    primaryKeyCache,
  );
  expect(pattern).toMatchInlineSnapshot(`
    {
      "address": {
        "type": "derived",
        "value": [
          "chainId",
        ],
      },
    }
  `);
  expect(recoverProfilePattern(pattern!, event)).toMatchInlineSnapshot(`
    {
      "address": 1,
    }
  `);
});
test("recordProfilePattern() log args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: {
        address: zeroAddress,
      },
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { timestamp: 1n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["address", schema.account.address]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { address: zeroAddress },
    [],
    primaryKeyCache,
  );
  expect(pattern).toMatchInlineSnapshot(`
    {
      "address": {
        "type": "derived",
        "value": [
          "args",
          "address",
        ],
      },
    }
  `);
  expect(recoverProfilePattern(pattern!, event)).toMatchInlineSnapshot(`
    {
      "address": "0x0000000000000000000000000000000000000000",
    }
  `);
});
test("recordProfilePattern() block", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "block",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      block: { number: 3n, timestamp: 1n } as BlockEvent["event"]["block"],
    },
  } satisfies BlockEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.bigint().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["address", schema.account.address]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { address: 3n },
    [],
    primaryKeyCache,
  );
  expect(pattern).toMatchInlineSnapshot(`
    {
      "address": {
        "type": "derived",
        "value": [
          "block",
          "number",
        ],
      },
    }
  `);
  expect(recoverProfilePattern(pattern!, event)).toMatchInlineSnapshot(`
    {
      "address": 3n,
    }
  `);
});
test("recordProfilePattern() hint", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "block",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      block: { number: 3n, timestamp: 1n } as BlockEvent["event"]["block"],
    },
  } satisfies BlockEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.bigint().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["address", schema.account.address]]);
  let pattern = recordProfilePattern(
    event,
    schema.account,
    { address: 3n },
    [],
    primaryKeyCache,
  );
  pattern = recordProfilePattern(
    event,
    schema.account,
    { address: 3n },
    [pattern!],
    primaryKeyCache,
  );
  expect(pattern).toMatchInlineSnapshot(`
    {
      "address": {
        "type": "derived",
        "value": [
          "block",
          "number",
        ],
      },
    }
  `);
  expect(recoverProfilePattern(pattern!, event)).toMatchInlineSnapshot(`
    {
      "address": 3n,
    }
  `);
});
test("recordProfilePattern() object args", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: {
        address: [zeroAddress],
      },
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { timestamp: 1n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      address: p.hex().primaryKey(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["address", schema.account.address]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { address: zeroAddress },
    [],
    primaryKeyCache,
  );
  expect(pattern).toBe(undefined);
});
test("recordProfilePattern() string concat", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: {
        address: zeroAddress,
      },
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { timestamp: 26n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      id: p.text().primaryKey(),
      address: p.bigint().notNull(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["id", schema.account.id]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { id: `${1}-${zeroAddress}` },
    [],
    primaryKeyCache,
  );
  expect(pattern).toMatchInlineSnapshot(`
    {
      "id": {
        "delimiter": "-",
        "type": "delimeter",
        "values": [
          {
            "type": "derived",
            "value": [
              "chainId",
            ],
          },
          {
            "type": "derived",
            "value": [
              "args",
              "address",
            ],
          },
        ],
      },
    }
  `);
  expect(recoverProfilePattern(pattern!, event)).toMatchInlineSnapshot(`
    {
      "id": "1-0x0000000000000000000000000000000000000000",
    }
  `);
});
test("recordProfilePattern() string concat mixed delimiters", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: {
        address: zeroAddress,
      },
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { timestamp: 26n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      id: p.text().primaryKey(),
      address: p.bigint().notNull(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["id", schema.account.id]]);
  const pattern = recordProfilePattern(
    event,
    schema.account,
    { id: `${1}-${zeroAddress}_${zeroAddress}` },
    [],
    primaryKeyCache,
  );
  expect(pattern).toBe(undefined);
});
test("recordProfilePattern() string concat hint", () => {
  const { eventCallbacks } = getBlocksIndexingBuild({ interval: 1 });
  const event = {
    type: "log",
    chain: getChain(),
    checkpoint: ZERO_CHECKPOINT_STRING,
    eventCallback: eventCallbacks[0]!,
    event: {
      id: ZERO_CHECKPOINT_STRING,
      args: {
        address: zeroAddress,
      },
      log: {} as LogEvent["event"]["log"],
      transaction: {} as LogEvent["event"]["transaction"],
      block: { timestamp: 26n } as BlockEvent["event"]["block"],
    },
  } satisfies LogEvent;
  const schema = {
    account: onchainTable("account", (p) => ({
      id: p.text().primaryKey(),
      address: p.bigint().notNull(),
      balance: p.bigint().notNull(),
    })),
  };
  const primaryKeyCache = new Map<Table, [string, Column][]>();
  primaryKeyCache.set(schema.account, [["id", schema.account.id]]);
  let pattern = recordProfilePattern(
    event,
    schema.account,
    { id: `${1}-${zeroAddress}` },
    [],
    primaryKeyCache,
  );
  pattern = recordProfilePattern(
    event,
    schema.account,
    { id: `${1}-${zeroAddress}` },
    [pattern!],
    primaryKeyCache,
  );
  expect(pattern).toMatchInlineSnapshot(`
    {
      "id": {
        "delimiter": "-",
        "type": "delimeter",
        "values": [
          {
            "type": "derived",
            "value": [
              "chainId",
            ],
          },
          {
            "type": "derived",
            "value": [
              "args",
              "address",
            ],
          },
        ],
      },
    }
  `);
  expect(recoverProfilePattern(pattern!, event)).toMatchInlineSnapshot(`
    {
      "id": "1-0x0000000000000000000000000000000000000000",
    }
  `);
});
</file>

<file path="packages/core/src/indexing-store/profile.ts">
import type { Event } from "@/internal/types.js";
import { orderObject } from "@/utils/order.js";
import type { Column, Table } from "drizzle-orm";
import type { ProfilePattern, Row } from "./cache.js";
import { getCacheKey } from "./utils.js";
export const getProfilePatternKey = (pattern: ProfilePattern): string => {
  return JSON.stringify(orderObject(pattern), (_, value) => {
    if (typeof value === "bigint") {
      return value.toString();
    }
    return value;
  });
};
const eq = (target: bigint | string | number | boolean, value: any) => {
  if (target === value) return true;
  if (target && value && target.toString() === value.toString()) return true;
  return false;
};
const delimiters = ["-", "_", ":", "#", "$"];
export const recordProfilePattern = (
  event: Event,
  table: Table,
  key: object,
  hints: ProfilePattern[],
  cache: Map<Table, [string, Column][]>,
): ProfilePattern | undefined => {
  globalThis.DISABLE_EVENT_PROXY = true;
  for (const hint of hints) {
    if (
      getCacheKey(table, key, cache) ===
      getCacheKey(table, recoverProfilePattern(hint, event), cache)
    ) {
      globalThis.DISABLE_EVENT_PROXY = false;
      return hint;
    }
  }
  const result: ProfilePattern = {};
  for (const [js] of cache.get(table)!) {
    // @ts-ignore
    const value = key[js]!;
    const pattern = matchEventParameters(event, value);
    if (pattern === undefined) {
      globalThis.DISABLE_EVENT_PROXY = false;
      return undefined;
    }
    result[js] = pattern;
  }
  globalThis.DISABLE_EVENT_PROXY = false;
  return result;
};
const matchEventParameters = (
  event: Event,
  value: any,
): ProfilePattern[keyof ProfilePattern] | undefined => {
  if (eq(event.chain.id, value)) {
    return { type: "derived", value: ["chainId"] };
  }
  if (eq(event.event.id, value)) {
    return { type: "derived", value: ["id"] };
  }
  switch (event.type) {
    case "block": {
      if (eq(event.event.block.hash, value)) {
        return { type: "derived", value: ["block", "hash"] };
      }
      if (eq(event.event.block.number, value)) {
        return { type: "derived", value: ["block", "number"] };
      }
      if (eq(event.event.block.timestamp, value)) {
        return { type: "derived", value: ["block", "timestamp"] };
      }
      if (eq(event.event.block.timestamp / 60n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 60n,
        };
      }
      if (eq(event.event.block.timestamp / 3600n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 3600n,
        };
      }
      if (eq(event.event.block.timestamp / 86400n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 86400n,
        };
      }
      if (event.event.block.miner && eq(event.event.block.miner, value)) {
        return { type: "derived", value: ["block", "miner"] };
      }
      break;
    }
    case "transaction": {
      if (eq(event.event.block.hash, value)) {
        return { type: "derived", value: ["block", "hash"] };
      }
      if (eq(event.event.block.number, value)) {
        return { type: "derived", value: ["block", "number"] };
      }
      if (eq(event.event.block.timestamp, value)) {
        return { type: "derived", value: ["block", "timestamp"] };
      }
      if (eq(event.event.block.timestamp / 60n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 60n,
        };
      }
      if (eq(event.event.block.timestamp / 3600n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 3600n,
        };
      }
      if (eq(event.event.block.timestamp / 86400n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 86400n,
        };
      }
      if (event.event.block.miner && eq(event.event.block.miner, value)) {
        return { type: "derived", value: ["block", "miner"] };
      }
      if (eq(event.event.transaction.hash, value)) {
        return { type: "derived", value: ["transaction", "hash"] };
      }
      if (eq(event.event.transaction.from, value)) {
        return { type: "derived", value: ["transaction", "from"] };
      }
      if (event.event.transaction.to && eq(event.event.transaction.to, value)) {
        return { type: "derived", value: ["transaction", "to"] };
      }
      if (eq(event.event.transaction.transactionIndex, value)) {
        return { type: "derived", value: ["transaction", "transactionIndex"] };
      }
      if (
        event.event.transactionReceipt?.contractAddress &&
        eq(event.event.transactionReceipt.contractAddress, value)
      ) {
        return {
          type: "derived",
          value: ["transactionReceipt", "contractAddress"],
        };
      }
      break;
    }
    case "log": {
      // Note: explicitly skip profiling args if they are an array
      if (
        event.event.args !== undefined &&
        Array.isArray(event.event.args) === false
      ) {
        for (const argKey of Object.keys(event.event.args)) {
          const argValue = (event.event.args as { [key: string]: unknown })[
            argKey
          ] as string | bigint | number | boolean;
          if (typeof argValue !== "object" && eq(argValue, value)) {
            return { type: "derived", value: ["args", argKey] };
          }
        }
      }
      if (eq(event.event.log.address, value)) {
        return { type: "derived", value: ["log", "address"] };
      }
      if (eq(event.event.log.logIndex, value)) {
        return { type: "derived", value: ["log", "logIndex"] };
      }
      if (eq(event.event.block.hash, value)) {
        return { type: "derived", value: ["block", "hash"] };
      }
      if (eq(event.event.block.number, value)) {
        return { type: "derived", value: ["block", "number"] };
      }
      if (eq(event.event.block.timestamp, value)) {
        return { type: "derived", value: ["block", "timestamp"] };
      }
      if (eq(event.event.block.timestamp / 60n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 60n,
        };
      }
      if (eq(event.event.block.timestamp / 3600n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 3600n,
        };
      }
      if (eq(event.event.block.timestamp / 86400n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 86400n,
        };
      }
      if (event.event.block.miner && eq(event.event.block.miner, value)) {
        return { type: "derived", value: ["block", "miner"] };
      }
      if (eq(event.event.transaction.hash, value)) {
        return { type: "derived", value: ["transaction", "hash"] };
      }
      if (eq(event.event.transaction.from, value)) {
        return { type: "derived", value: ["transaction", "from"] };
      }
      if (event.event.transaction.to && eq(event.event.transaction.to, value)) {
        return { type: "derived", value: ["transaction", "to"] };
      }
      if (eq(event.event.transaction.transactionIndex, value)) {
        return { type: "derived", value: ["transaction", "transactionIndex"] };
      }
      if (
        event.event.transactionReceipt?.contractAddress &&
        eq(event.event.transactionReceipt.contractAddress, value)
      ) {
        return {
          type: "derived",
          value: ["transactionReceipt", "contractAddress"],
        };
      }
      break;
    }
    case "trace": {
      // Note: explicitly skip profiling args if they are an array
      if (
        event.event.args !== undefined &&
        Array.isArray(event.event.args) === false
      ) {
        for (const argKey of Object.keys(event.event.args)) {
          const argValue = (event.event.args as { [key: string]: unknown })[
            argKey
          ] as string | bigint | number | boolean;
          if (typeof argValue !== "object" && eq(argValue, value)) {
            return { type: "derived", value: ["args", argKey] };
          }
        }
      }
      // Note: explicitly skip profiling result if it is an array
      if (
        event.event.result !== undefined &&
        Array.isArray(event.event.result) === false
      ) {
        for (const argKey of Object.keys(event.event.result)) {
          const argValue = (event.event.result as { [key: string]: unknown })[
            argKey
          ] as string | bigint | number | boolean;
          if (typeof argValue !== "object" && eq(argValue, value)) {
            return { type: "derived", value: ["result", argKey] };
          }
        }
      }
      if (eq(event.event.trace.from, value)) {
        return { type: "derived", value: ["trace", "from"] };
      }
      if (event.event.trace.to && eq(event.event.trace.to, value)) {
        return { type: "derived", value: ["trace", "to"] };
      }
      if (eq(event.event.block.hash, value)) {
        return { type: "derived", value: ["block", "hash"] };
      }
      if (eq(event.event.block.number, value)) {
        return { type: "derived", value: ["block", "number"] };
      }
      if (eq(event.event.block.timestamp, value)) {
        return { type: "derived", value: ["block", "timestamp"] };
      }
      if (eq(event.event.block.timestamp / 60n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 60n,
        };
      }
      if (eq(event.event.block.timestamp / 3600n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 3600n,
        };
      }
      if (eq(event.event.block.timestamp / 86400n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 86400n,
        };
      }
      if (event.event.block.miner && eq(event.event.block.miner, value)) {
        return { type: "derived", value: ["block", "miner"] };
      }
      if (eq(event.event.transaction.hash, value)) {
        return { type: "derived", value: ["transaction", "hash"] };
      }
      if (eq(event.event.transaction.from, value)) {
        return { type: "derived", value: ["transaction", "from"] };
      }
      if (event.event.transaction.to && eq(event.event.transaction.to, value)) {
        return { type: "derived", value: ["transaction", "to"] };
      }
      if (eq(event.event.transaction.transactionIndex, value)) {
        return { type: "derived", value: ["transaction", "transactionIndex"] };
      }
      if (
        event.event.transactionReceipt?.contractAddress &&
        eq(event.event.transactionReceipt.contractAddress, value)
      ) {
        return {
          type: "derived",
          value: ["transactionReceipt", "contractAddress"],
        };
      }
      break;
    }
    case "transfer": {
      if (eq(event.event.transfer.from, value)) {
        return { type: "derived", value: ["transfer", "from"] };
      }
      if (eq(event.event.transfer.to, value)) {
        return { type: "derived", value: ["transfer", "to"] };
      }
      if (eq(event.event.trace.from, value)) {
        return { type: "derived", value: ["trace", "from"] };
      }
      if (event.event.trace.to && eq(event.event.trace.to, value)) {
        return { type: "derived", value: ["trace", "to"] };
      }
      if (eq(event.event.block.hash, value)) {
        return { type: "derived", value: ["block", "hash"] };
      }
      if (eq(event.event.block.number, value)) {
        return { type: "derived", value: ["block", "number"] };
      }
      if (eq(event.event.block.timestamp, value)) {
        return { type: "derived", value: ["block", "timestamp"] };
      }
      if (eq(event.event.block.timestamp / 60n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 60n,
        };
      }
      if (eq(event.event.block.timestamp / 3600n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 3600n,
        };
      }
      if (eq(event.event.block.timestamp / 86400n, value)) {
        return {
          type: "derived",
          value: ["block", "timestamp"],
          fn: (value) => (value as bigint) / 86400n,
        };
      }
      if (event.event.block.miner && eq(event.event.block.miner, value)) {
        return { type: "derived", value: ["block", "miner"] };
      }
      if (eq(event.event.transaction.hash, value)) {
        return { type: "derived", value: ["transaction", "hash"] };
      }
      if (eq(event.event.transaction.from, value)) {
        return { type: "derived", value: ["transaction", "from"] };
      }
      if (event.event.transaction.to && eq(event.event.transaction.to, value)) {
        return { type: "derived", value: ["transaction", "to"] };
      }
      if (eq(event.event.transaction.transactionIndex, value)) {
        return { type: "derived", value: ["transaction", "transactionIndex"] };
      }
      if (
        event.event.transactionReceipt?.contractAddress &&
        eq(event.event.transactionReceipt.contractAddress, value)
      ) {
        return {
          type: "derived",
          value: ["transactionReceipt", "contractAddress"],
        };
      }
      break;
    }
  }
  if (typeof value === "string") {
    del: for (const delimiter of delimiters) {
      const subValues = value.split(delimiter);
      if (subValues.length > 1) {
        const result: ProfilePattern[keyof ProfilePattern] = {
          type: "delimeter",
          values: [],
          delimiter,
        };
        for (const subValue of subValues) {
          const match = matchEventParameters(event, subValue);
          if (match?.type === "derived") {
            result.values.push(match);
            continue;
          }
          continue del;
        }
        return result;
      }
    }
  }
  return undefined;
};
export const recoverProfilePattern = (
  pattern: ProfilePattern,
  event: Event,
): Row => {
  const result: Row = {};
  for (const [key, _pattern] of Object.entries(pattern)) {
    if (_pattern.type === "derived") {
      const { value, fn } = _pattern;
      if (value[0] === "chainId") {
        result[key] = event.chain.id;
      } else {
        let _result: unknown = event.event;
        for (const prop of value) {
          // @ts-ignore
          _result = _result[prop];
        }
        if (fn) {
          _result = fn(_result);
        }
        result[key] = _result;
      }
    } else {
      const { values, delimiter } = _pattern;
      result[key] = values
        .map(({ value, fn }) => {
          if (value[0] === "chainId") {
            return event.chain.id;
          } else {
            let _result: unknown = event.event;
            for (const prop of value) {
              // @ts-ignore
              _result = _result[prop];
            }
            if (fn) {
              _result = fn(_result);
            }
            return _result;
          }
        })
        .join(delimiter);
    }
  }
  return result;
};
</file>

<file path="packages/core/src/indexing-store/utils.bench.ts">
import { onchainTable } from "@/index.js";
import { bench, run } from "mitata";
import {
  getCacheKey,
  getPrimaryKeyCache,
  normalizeColumn,
  normalizeRow,
} from "./utils.js";
const table = onchainTable("account", (t) => ({
  address: t.hex().primaryKey(),
  balance: t.bigint(),
}));
const primaryKeyCache = getPrimaryKeyCache([table]);
// 7.47 ns/iter
bench("normalizeColumn", () => {
  normalizeColumn(table.address, "0x123", false);
}).gc("inner");
// 97.69 ns/iter
bench("normalizeRow", () => {
  normalizeRow(table, { address: "0x123" }, false);
}).gc("inner");
// 39.77 ns/iter
bench("getCacheKey", () => {
  getCacheKey(table, { address: "0x123" }, primaryKeyCache);
}).gc("inner");
run();
</file>

<file path="packages/core/src/indexing-store/utils.test.ts">
import { onchainTable } from "@/drizzle/onchain.js";
import { expect, test } from "vitest";
import { normalizeColumn } from "./utils.js";
test("normalize smallint", () => {
  const column = onchainTable("account", (t) => ({
    address: t.hex().primaryKey(),
    balance: t.smallint(),
  }));
  const value = 123;
  const result = normalizeColumn(column.balance, value, false);
  expect(result).toBe(value);
});
</file>

<file path="packages/core/src/indexing-store/utils.ts">
import { getPrimaryKeyColumns } from "@/drizzle/index.js";
import {
  BigIntSerializationError,
  NotNullConstraintError,
} from "@/internal/errors.js";
import { prettyPrint } from "@/utils/print.js";
import {
  type Column,
  type SQL,
  type SQLWrapper,
  type Table,
  and,
  eq,
  getTableColumns,
  getTableName,
} from "drizzle-orm";
import { PgArray } from "drizzle-orm/pg-core";
/**
 * Returns true if the column has a "default" value that is used when no value is passed.
 * Handles `.default`, `.$defaultFn()`, `.$onUpdateFn()`.
 */
export const hasEmptyValue = (column: Column) => {
  return column.hasDefault;
};
/** Returns the "default" value for `column`. */
export const getEmptyValue = (column: Column, isUpdate: boolean) => {
  if (isUpdate && column.onUpdateFn) {
    return column.onUpdateFn();
  }
  if (column.default !== undefined) return column.default;
  if (column.defaultFn !== undefined) return column.defaultFn();
  if (column.onUpdateFn !== undefined) return column.onUpdateFn();
  // TODO(kyle) is it an invariant that it doesn't get here
  return undefined;
};
export const getPrimaryKeyCache = (tables: Table[]) => {
  const cache = new Map<Table, [string, Column][]>();
  for (const table of tables) {
    cache.set(table, []);
    for (const { js } of getPrimaryKeyColumns(table)) {
      // @ts-expect-error
      cache.get(table)!.push([js, table[js]!]);
    }
  }
  return cache;
};
export const normalizeColumn = (
  column: Column,
  value: unknown,
  isUpdate: boolean,
): unknown => {
  if (value === undefined) {
    if (hasEmptyValue(column)) return getEmptyValue(column, isUpdate);
    return null;
  }
  if (value === null) return null;
  if (column.mapToDriverValue === undefined) return value;
  try {
    if (Array.isArray(value) && column instanceof PgArray) {
      return value.map((v) => {
        if (column.baseColumn.columnType === "PgTimestamp") {
          return v;
        }
        return column.baseColumn.mapFromDriverValue(
          column.baseColumn.mapToDriverValue(v),
        );
      });
    }
    if (column.columnType === "PgTimestamp") {
      return value;
    }
    return column.mapFromDriverValue(column.mapToDriverValue(value));
  } catch (e) {
    if (
      (e as Error)?.message?.includes("Do not know how to serialize a BigInt")
    ) {
      const error = new BigIntSerializationError((e as Error).message);
      error.meta.push(
        "Hint:\n  The JSON column type does not support BigInt values. Use the replaceBigInts() helper function before inserting into the database. Docs: https://ponder.sh/docs/api-reference/ponder-utils#replacebigints",
      );
      throw error;
    }
    throw e;
  }
};
export const normalizeRow = (
  table: Table,
  row: { [key: string]: unknown },
  isUpdate: boolean,
) => {
  for (const [columnName, column] of Object.entries(getTableColumns(table))) {
    // not-null constraint
    if (
      isUpdate === false &&
      (row[columnName] === undefined || row[columnName] === null) &&
      column.notNull &&
      hasEmptyValue(column) === false
    ) {
      const error = new NotNullConstraintError(
        `Column '${getTableName(
          table,
        )}.${columnName}' violates not-null constraint.`,
      );
      error.meta.push(`db.insert arguments:\n${prettyPrint(row)}`);
      throw error;
    }
    row[columnName] = normalizeColumn(column, row[columnName], isUpdate);
  }
  return row;
};
export const getCacheKey = (
  table: Table,
  key: object,
  cache: Map<Table, [string, Column][]>,
): string => {
  const primaryKeys = cache.get(table)!;
  if (primaryKeys.length === 1) {
    const [pk, col] = primaryKeys[0]!;
    // @ts-expect-error
    return String(normalizeColumn(col, key[pk]!, false));
  }
  let result = "";
  for (let i = 0; i < primaryKeys.length; i++) {
    const [pk, col] = primaryKeys[i]!;
    // @ts-expect-error
    const value = normalizeColumn(col, key[pk]!, false);
    result += `${String(value)}_`;
  }
  return result;
};
/** Returns an sql where condition for `table` with `key`. */
export const getWhereCondition = (table: Table, key: Object): SQL<unknown> => {
  const conditions: SQLWrapper[] = [];
  for (const { js } of getPrimaryKeyColumns(table)) {
    // @ts-ignore
    conditions.push(eq(table[js]!, key[js]));
  }
  return and(...conditions)!;
};
</file>

<file path="packages/core/src/internal/common.ts">
import type { Logger } from "./logger.js";
import type { MetricsService } from "./metrics.js";
import type { Options } from "./options.js";
import type { Shutdown } from "./shutdown.js";
import type { Telemetry } from "./telemetry.js";
export type Common = {
  options: Options;
  logger: Logger;
  metrics: MetricsService;
  telemetry: Telemetry;
  shutdown: Shutdown;
  apiShutdown: Shutdown;
  buildShutdown: Shutdown;
};
</file>

<file path="packages/core/src/internal/errors.ts">
/** Base class for all known errors. */
export class BaseError extends Error {
  override name = "BaseError";
  meta: string[] = [];
  constructor(message?: string | undefined, { cause }: { cause?: Error } = {}) {
    super(message, { cause });
    Object.setPrototypeOf(this, BaseError.prototype);
  }
}
/** Error caused by user code. Should not be retried. */
export class NonRetryableUserError extends BaseError {
  override name = "NonRetryableUserError";
  constructor(message?: string | undefined, { cause }: { cause?: Error } = {}) {
    super(message, { cause });
    Object.setPrototypeOf(this, NonRetryableUserError.prototype);
  }
}
/** Error that may succeed if tried again. */
export class RetryableError extends BaseError {
  override name = "RetryableError";
  constructor(message?: string | undefined, { cause }: { cause?: Error } = {}) {
    super(message, { cause });
    Object.setPrototypeOf(this, RetryableError.prototype);
  }
}
export class ShutdownError extends NonRetryableUserError {
  override name = "ShutdownError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, ShutdownError.prototype);
  }
}
export class BuildError extends NonRetryableUserError {
  override name = "BuildError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, BuildError.prototype);
  }
}
export class MigrationError extends NonRetryableUserError {
  override name = "MigrationError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, MigrationError.prototype);
  }
}
// Non-retryable database errors
export class UniqueConstraintError extends NonRetryableUserError {
  override name = "UniqueConstraintError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, UniqueConstraintError.prototype);
  }
}
export class NotNullConstraintError extends NonRetryableUserError {
  override name = "NotNullConstraintError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, NotNullConstraintError.prototype);
  }
}
export class InvalidStoreAccessError extends NonRetryableUserError {
  override name = "InvalidStoreAccessError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, InvalidStoreAccessError.prototype);
  }
}
export class RecordNotFoundError extends NonRetryableUserError {
  override name = "RecordNotFoundError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, RecordNotFoundError.prototype);
  }
}
export class CheckConstraintError extends NonRetryableUserError {
  override name = "CheckConstraintError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, CheckConstraintError.prototype);
  }
}
// Retryable database errors
export class DbConnectionError extends RetryableError {
  override name = "DbConnectionError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, DbConnectionError.prototype);
  }
}
export class TransactionStatementError extends RetryableError {
  override name = "TransactionStatementError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, TransactionStatementError.prototype);
  }
}
export class CopyFlushError extends RetryableError {
  override name = "CopyFlushError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, CopyFlushError.prototype);
  }
}
export class InvalidEventAccessError extends RetryableError {
  override name = "InvalidEventAccessError";
  key: string;
  constructor(key: string, message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, InvalidEventAccessError.prototype);
    this.key = key;
  }
}
// Non-retryable indexing store errors
export class InvalidStoreMethodError extends NonRetryableUserError {
  override name = "InvalidStoreMethodError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, InvalidStoreMethodError.prototype);
  }
}
export class UndefinedTableError extends NonRetryableUserError {
  override name = "UndefinedTableError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, UndefinedTableError.prototype);
  }
}
export class BigIntSerializationError extends NonRetryableUserError {
  override name = "BigIntSerializationError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, BigIntSerializationError.prototype);
  }
}
export class DelayedInsertError extends NonRetryableUserError {
  override name = "DelayedInsertError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, DelayedInsertError.prototype);
  }
}
export class RawSqlError extends NonRetryableUserError {
  override name = "RawSqlError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, RawSqlError.prototype);
  }
}
export class IndexingFunctionError extends NonRetryableUserError {
  override name = "IndexingFunctionError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, IndexingFunctionError.prototype);
  }
}
export class RpcProviderError extends BaseError {
  override name = "RpcProviderError";
  constructor(message?: string | undefined) {
    super(message);
    Object.setPrototypeOf(this, RpcProviderError.prototype);
  }
}
export const nonRetryableUserErrorNames = [
  ShutdownError,
  BuildError,
  MigrationError,
  UniqueConstraintError,
  NotNullConstraintError,
  InvalidStoreAccessError,
  RecordNotFoundError,
  CheckConstraintError,
  InvalidStoreMethodError,
  UndefinedTableError,
  BigIntSerializationError,
  DelayedInsertError,
  RawSqlError,
  IndexingFunctionError,
].map((err) => err.name);
</file>

<file path="packages/core/src/internal/logger.ts">
import type { Prettify } from "@/types/utils.js";
import { formatEta } from "@/utils/format.js";
import pc from "picocolors";
import { type DestinationStream, type LevelWithSilent, pino } from "pino";
export type LogMode = "pretty" | "json";
export type LogLevel = Prettify<LevelWithSilent>;
export type Logger = {
  error<T extends Omit<Log, "level" | "time">>(
    options: T,
    printKeys?: (keyof T)[],
  ): void;
  warn<T extends Omit<Log, "level" | "time">>(
    options: T,
    printKeys?: (keyof T)[],
  ): void;
  info<T extends Omit<Log, "level" | "time">>(
    options: T,
    printKeys?: (keyof T)[],
  ): void;
  debug<T extends Omit<Log, "level" | "time">>(
    options: T,
    printKeys?: (keyof T)[],
  ): void;
  trace<T extends Omit<Log, "level" | "time">>(
    options: T,
    printKeys?: (keyof T)[],
  ): void;
  child: (bindings: Record<string, unknown>) => Logger;
  flush(): Promise<void>;
};
type Log = {
  // Pino properties
  level: 50 | 40 | 30 | 20 | 10;
  time: number;
  msg: string;
  duration?: number;
  error?: Error;
} & Record<string, unknown>;
const PRINT_KEYS = "PRINT_KEYS";
const INTERNAL_KEYS = [
  "level",
  "time",
  "msg",
  "duration",
  "error",
  "chain_id",
  PRINT_KEYS,
];
export function createLogger({
  level,
  mode = "pretty",
}: { level: LogLevel; mode?: LogMode }) {
  const stream: DestinationStream = {
    write(logString: string) {
      const log = JSON.parse(logString) as Log;
      const prettyLog = format(log);
      console.log(prettyLog);
    },
  };
  const _createLogger = (logger: pino.Logger): Logger => {
    return {
      error<T extends Omit<Log, "level" | "time">>(
        options: T,
        printKeys?: (keyof T)[],
      ) {
        if (mode === "pretty" && printKeys) {
          // @ts-ignore
          options[PRINT_KEYS] = printKeys;
        }
        logger.error(options);
      },
      warn<T extends Omit<Log, "level" | "time">>(
        options: T,
        printKeys?: (keyof T)[],
      ) {
        if (mode === "pretty" && printKeys) {
          // @ts-ignore
          options[PRINT_KEYS] = printKeys;
        }
        logger.warn(options);
      },
      info<T extends Omit<Log, "level" | "time">>(
        options: T,
        printKeys?: (keyof T)[],
      ) {
        if (mode === "pretty" && printKeys) {
          // @ts-ignore
          options[PRINT_KEYS] = printKeys;
        }
        logger.info(options);
      },
      debug<T extends Omit<Log, "level" | "time">>(
        options: T,
        printKeys?: (keyof T)[],
      ) {
        if (mode === "pretty" && printKeys) {
          // @ts-ignore
          options[PRINT_KEYS] = printKeys;
        }
        logger.debug(options);
      },
      trace<T extends Omit<Log, "level" | "time">>(
        options: T,
        printKeys?: (keyof T)[],
      ) {
        if (mode === "pretty" && printKeys) {
          // @ts-ignore
          options[PRINT_KEYS] = printKeys;
        }
        logger.trace(options);
      },
      child: (bindings) => _createLogger(logger.child(bindings)),
      // @ts-expect-error
      flush: () => new Promise<void>(logger.flush),
    };
  };
  const errorSerializer = pino.stdSerializers.wrapErrorSerializer((error) => {
    error.meta = Array.isArray(error.meta) ? error.meta.join("\n") : error.meta;
    // @ts-ignore
    error.type = undefined;
    return error;
  });
  let logger: pino.Logger;
  if (mode === "pretty") {
    logger = pino(
      {
        level,
        serializers: { error: errorSerializer },
        // Removes "pid" and "hostname" properties from the log.
        base: undefined,
      },
      stream,
    );
  } else {
    logger = pino({
      level,
      serializers: { error: errorSerializer },
      // Removes "pid" and "hostname" properties from the log.
      base: undefined,
    });
  }
  return _createLogger(logger);
}
export function createNoopLogger(
  _args: { level?: LogLevel; mode?: LogMode } = {},
) {
  return {
    error(_options: Omit<Log, "level" | "time">) {},
    warn(_options: Omit<Log, "level" | "time">) {},
    info(_options: Omit<Log, "level" | "time">) {},
    debug(_options: Omit<Log, "level" | "time">) {},
    trace(_options: Omit<Log, "level" | "time">) {},
    flush: () => new Promise<unknown>((resolve) => resolve(undefined)),
  };
}
const levels = {
  50: { label: "ERROR", colorLabel: pc.red("ERROR") },
  40: { label: "WARN ", colorLabel: pc.yellow("WARN ") },
  30: { label: "INFO ", colorLabel: pc.green("INFO ") },
  20: { label: "DEBUG", colorLabel: pc.blue("DEBUG") },
  10: { label: "TRACE", colorLabel: pc.gray("TRACE") },
} as const;
const timeFormatter = new Intl.DateTimeFormat(undefined, {
  hour: "2-digit",
  minute: "2-digit",
  second: "2-digit",
  fractionalSecondDigits: 3,
  hour12: false,
});
const format = (log: Log) => {
  const time = timeFormatter.format(new Date(log.time));
  const levelObject = levels[log.level ?? 30];
  let prettyLog: string[];
  if (pc.isColorSupported) {
    const level = levelObject.colorLabel;
    const messageText = pc.reset(log.msg);
    let keyText = "";
    if (PRINT_KEYS in log) {
      for (const key of log[PRINT_KEYS] as (keyof Log)[]) {
        keyText += ` ${key}=${log[key]}`;
      }
    } else {
      for (const key of Object.keys(log)) {
        if (INTERNAL_KEYS.includes(key)) continue;
        keyText += ` ${key}=${log[key]}`;
      }
    }
    let durationText = "";
    if (log.duration) {
      durationText = ` ${pc.gray(`(${formatEta(log.duration)})`)}`;
    }
    prettyLog = [
      `${pc.dim(time)} ${level} ${messageText}${pc.dim(keyText)}${durationText}`,
    ];
  } else {
    const level = levelObject.label;
    let keyText = "";
    if (PRINT_KEYS in log) {
      keyText = (log[PRINT_KEYS] as (keyof Log)[])
        .map((key) => ` ${key}=${log[key]}`)
        .join("");
    } else {
      for (const key of Object.keys(log)) {
        if (INTERNAL_KEYS.includes(key)) continue;
        keyText += ` ${key}=${log[key]}`;
      }
    }
    let durationText = "";
    if (log.duration) {
      durationText = ` (${formatEta(log.duration)})`;
    }
    prettyLog = [`${time} ${level} ${log.msg}${keyText}${durationText}`];
  }
  if (log.error) {
    if (log.error.stack) {
      prettyLog.push(log.error.stack);
    } else {
      prettyLog.push(`${log.error.name}: ${log.error.message}`);
    }
    if (typeof log.error === "object" && "where" in log.error) {
      prettyLog.push(`where: ${log.error.where as string}`);
    }
    if (typeof log.error === "object" && "meta" in log.error) {
      prettyLog.push(log.error.meta as string);
    }
  }
  return prettyLog.join("\n");
};
</file>

<file path="packages/core/src/internal/metrics.ts">
import { type Worker, parentPort } from "node:worker_threads";
import {
  type PromiseWithResolvers,
  promiseWithResolvers,
} from "@/utils/promiseWithResolvers.js";
import { truncate } from "@/utils/truncate.js";
import { getTableName, isTable } from "drizzle-orm";
import prometheus from "prom-client";
import type { IndexingBuild, PreBuild, SchemaBuild } from "./types.js";
const sometimesIODurationMs = [
  0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1_000, 5_000,
  10_000, 50_000, 100_000,
];
const alwaysIODurationMs = [
  1, 5, 10, 50, 100, 500, 1_000, 5_000, 10_000, 50_000, 100_000, 500_000,
];
const httpRequestSizeBytes = [
  10, 100, 1_000, 5_000, 10_000, 50_000, 100_000, 500_000, 1_000_000, 5_000_000,
  10_000_000,
];
const GET_METRICS_REQ = "prom-client:getMetricsReq";
const GET_METRICS_RES = "prom-client:getMetricsRes";
export class MetricsService {
  registry: prometheus.Registry;
  start_timestamp: number;
  progressMetadata: {
    [chain: string]: {
      batches: {
        elapsedSeconds: number;
        completedSeconds: number;
      }[];
      previousTimestamp: number;
      previousCompletedSeconds: number;
      rate: number;
    };
  };
  hasError: boolean;
  port: number | undefined;
  rps: { [chain: string]: { count: number; timestamp: number }[] };
  ponder_version_info: prometheus.Gauge<
    "version" | "major" | "minor" | "patch"
  >;
  ponder_settings_info: prometheus.Gauge<"ordering" | "database" | "command">;
  ponder_historical_concurrency_group_duration: prometheus.Gauge<"group">;
  ponder_historical_extract_duration: prometheus.Gauge<"step">;
  ponder_historical_transform_duration: prometheus.Gauge<"step">;
  ponder_historical_start_timestamp_seconds: prometheus.Gauge<"chain">;
  ponder_historical_end_timestamp_seconds: prometheus.Gauge<"chain">;
  ponder_historical_total_indexing_seconds: prometheus.Gauge<"chain">;
  ponder_historical_cached_indexing_seconds: prometheus.Gauge<"chain">;
  ponder_historical_completed_indexing_seconds: prometheus.Gauge<"chain">;
  ponder_indexing_timestamp: prometheus.Gauge<"chain">;
  ponder_indexing_completed_events: prometheus.Gauge<"event">;
  ponder_indexing_function_duration: prometheus.Histogram<"event">;
  ponder_indexing_cache_requests_total: prometheus.Counter<"table" | "type">;
  ponder_indexing_cache_query_duration: prometheus.Histogram<
    "table" | "method"
  >;
  ponder_indexing_rpc_action_duration: prometheus.Histogram<"action">;
  ponder_indexing_rpc_prefetch_total: prometheus.Counter<
    "chain" | "method" | "type"
  >;
  ponder_indexing_rpc_requests_total: prometheus.Counter<
    "chain" | "method" | "type"
  >;
  ponder_indexing_store_queries_total: prometheus.Counter<"table" | "method">;
  ponder_indexing_store_raw_sql_duration: prometheus.Histogram;
  ponder_sync_block: prometheus.Gauge<"chain">;
  ponder_sync_block_timestamp: prometheus.Gauge<"chain">;
  ponder_sync_is_realtime: prometheus.Gauge<"chain">;
  ponder_sync_is_complete: prometheus.Gauge<"chain">;
  ponder_historical_total_blocks: prometheus.Gauge<"chain">;
  ponder_historical_cached_blocks: prometheus.Gauge<"chain">;
  ponder_historical_completed_blocks: prometheus.Gauge<"chain">;
  ponder_realtime_reorg_total: prometheus.Counter<"chain">;
  ponder_realtime_latency: prometheus.Histogram<"chain">;
  ponder_realtime_block_arrival_latency: prometheus.Histogram<"chain">;
  ponder_database_method_duration: prometheus.Histogram<"service" | "method">;
  ponder_database_method_error_total: prometheus.Counter<"service" | "method">;
  ponder_http_server_active_requests: prometheus.Gauge<"method" | "path">;
  ponder_http_server_request_duration_ms: prometheus.Histogram<
    "method" | "path" | "status"
  >;
  ponder_http_server_request_size_bytes: prometheus.Histogram<
    "method" | "path" | "status"
  >;
  ponder_http_server_response_size_bytes: prometheus.Histogram<
    "method" | "path" | "status"
  >;
  ponder_rpc_request_duration: prometheus.Histogram<"chain" | "method">;
  ponder_rpc_request_error_total: prometheus.Counter<"chain" | "method">;
  ponder_postgres_query_total: prometheus.Counter<"pool">;
  ponder_postgres_query_queue_size: prometheus.Gauge<"pool"> = null!;
  ponder_postgres_pool_connections: prometheus.Gauge<"pool" | "kind"> = null!;
  constructor() {
    this.registry = new prometheus.Registry();
    this.start_timestamp = Date.now();
    this.progressMetadata = {
      general: {
        batches: [{ elapsedSeconds: 0, completedSeconds: 0 }],
        previousTimestamp: Date.now(),
        previousCompletedSeconds: 0,
        rate: 0,
      },
    };
    this.port = undefined!;
    this.hasError = false;
    this.rps = {};
    this.ponder_version_info = new prometheus.Gauge({
      name: "ponder_version_info",
      help: "Ponder version information",
      labelNames: ["version", "major", "minor", "patch"] as const,
      registers: [this.registry],
      aggregator: "first",
    });
    this.ponder_settings_info = new prometheus.Gauge({
      name: "ponder_settings_info",
      help: "Ponder settings information",
      labelNames: ["ordering", "database", "command"] as const,
      registers: [this.registry],
      aggregator: "first",
    });
    this.ponder_historical_concurrency_group_duration = new prometheus.Gauge({
      name: "ponder_historical_concurrency_group_duration",
      help: "Duration of historical concurrency groups",
      labelNames: ["group"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_historical_extract_duration = new prometheus.Gauge({
      name: "ponder_historical_extract_duration",
      help: "Duration of historical extract phase",
      labelNames: ["step"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_historical_transform_duration = new prometheus.Gauge({
      name: "ponder_historical_transform_duration",
      help: "Duration of historical transform phase",
      labelNames: ["step"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_historical_start_timestamp_seconds = new prometheus.Gauge({
      name: "ponder_historical_start_timestamp_seconds",
      help: "Timestamp at which historical indexing started",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "min",
    });
    this.ponder_historical_end_timestamp_seconds = new prometheus.Gauge({
      name: "ponder_historical_end_timestamp_seconds",
      help: "Timestamp at which historical indexing ended",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "max",
    });
    this.ponder_historical_total_indexing_seconds = new prometheus.Gauge({
      name: "ponder_historical_total_indexing_seconds",
      help: "Total number of seconds that are required",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_historical_cached_indexing_seconds = new prometheus.Gauge({
      name: "ponder_historical_cached_indexing_seconds",
      help: "Number of seconds that have been cached",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_historical_completed_indexing_seconds = new prometheus.Gauge({
      name: "ponder_historical_completed_indexing_seconds",
      help: "Number of seconds that have been completed",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_indexing_completed_events = new prometheus.Gauge({
      name: "ponder_indexing_completed_events",
      help: "Number of events that have been processed",
      labelNames: ["chain", "event"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_indexing_timestamp = new prometheus.Gauge({
      name: "ponder_indexing_timestamp",
      help: "Timestamp through which all events have been completed",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "first",
    });
    this.ponder_indexing_function_duration = new prometheus.Histogram({
      name: "ponder_indexing_function_duration",
      help: "Duration of indexing function execution",
      labelNames: ["chain", "event"] as const,
      buckets: sometimesIODurationMs,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_indexing_cache_query_duration = new prometheus.Histogram({
      name: "ponder_indexing_cache_query_duration",
      help: "Duration of cache operations",
      labelNames: ["table", "method"] as const,
      buckets: alwaysIODurationMs,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_indexing_rpc_action_duration = new prometheus.Histogram({
      name: "ponder_indexing_rpc_action_duration",
      help: "Duration of RPC actions",
      labelNames: ["action"] as const,
      buckets: sometimesIODurationMs,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_indexing_rpc_prefetch_total = new prometheus.Counter({
      name: "ponder_indexing_rpc_prefetch_total",
      help: "Number of RPC prefetches",
      labelNames: ["chain", "method", "type"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_indexing_rpc_requests_total = new prometheus.Counter({
      name: "ponder_indexing_rpc_requests_total",
      help: "Number of RPC requests",
      labelNames: ["chain", "method", "type"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_indexing_cache_requests_total = new prometheus.Counter({
      name: "ponder_indexing_cache_requests_total",
      help: "Number of cache accesses",
      labelNames: ["table", "type"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_indexing_store_queries_total = new prometheus.Counter({
      name: "ponder_indexing_store_queries_total",
      help: "Number of indexing store operations",
      labelNames: ["table", "method"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_indexing_store_raw_sql_duration = new prometheus.Histogram({
      name: "ponder_indexing_store_raw_sql_duration",
      help: "Duration of raw SQL store operations",
      buckets: alwaysIODurationMs,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_sync_block = new prometheus.Gauge({
      name: "ponder_sync_block",
      help: "Closest-to-tip synced block number",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "max",
    });
    this.ponder_sync_block_timestamp = new prometheus.Gauge({
      name: "ponder_sync_block_timestamp",
      help: "Closest-to-tip synced block timestamp",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "max",
    });
    this.ponder_sync_is_realtime = new prometheus.Gauge({
      name: "ponder_sync_is_realtime",
      help: "Boolean (0 or 1) indicating if the sync is realtime mode",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "max",
    });
    this.ponder_sync_is_complete = new prometheus.Gauge({
      name: "ponder_sync_is_complete",
      help: "Boolean (0 or 1) indicating if the sync has synced all blocks",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "max",
    });
    this.ponder_historical_total_blocks = new prometheus.Gauge({
      name: "ponder_historical_total_blocks",
      help: "Number of blocks required for the historical sync",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "max",
    });
    this.ponder_historical_cached_blocks = new prometheus.Gauge({
      name: "ponder_historical_cached_blocks",
      help: "Number of blocks that were found in the cache for the historical sync",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "max",
    });
    this.ponder_historical_completed_blocks = new prometheus.Gauge({
      name: "ponder_historical_completed_blocks",
      help: "Number of blocks that have been processed for the historical sync",
      labelNames: ["chain", "source", "type"] as const,
      registers: [this.registry],
      aggregator: "max",
    });
    this.ponder_realtime_reorg_total = new prometheus.Counter({
      name: "ponder_realtime_reorg_total",
      help: "Count of how many re-orgs have occurred",
      labelNames: ["chain"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_realtime_latency = new prometheus.Histogram({
      name: "ponder_realtime_latency",
      help: "Time elapsed between receiving a block and fully processing it",
      labelNames: ["chain"] as const,
      buckets: [
        1, 5, 10, 50, 100, 500, 1_000, 5_000, 10_000, 50_000, 100_000, 500_000,
        1_000_000,
      ],
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_realtime_block_arrival_latency = new prometheus.Histogram({
      name: "ponder_realtime_block_arrival_latency",
      help: "Time elapsed between mining a block and being received by the realtime sync",
      labelNames: ["chain"] as const,
      buckets: [
        1, 5, 10, 50, 100, 500, 1_000, 5_000, 10_000, 50_000, 100_000, 500_000,
        1_000_000,
      ],
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_database_method_duration = new prometheus.Histogram({
      name: "ponder_database_method_duration",
      help: "Duration of database operations",
      labelNames: ["service", "method"] as const,
      buckets: alwaysIODurationMs,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_database_method_error_total = new prometheus.Counter({
      name: "ponder_database_method_error_total",
      help: "Total number of errors encountered during database operations",
      labelNames: ["service", "method"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_http_server_active_requests = new prometheus.Gauge({
      name: "ponder_http_server_active_requests",
      help: "Number of active HTTP server requests",
      labelNames: ["method", "path"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_http_server_request_duration_ms = new prometheus.Histogram({
      name: "ponder_http_server_request_duration_ms",
      help: "Duration of HTTP responses served the server",
      labelNames: ["method", "path", "status"] as const,
      buckets: alwaysIODurationMs,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_http_server_request_size_bytes = new prometheus.Histogram({
      name: "ponder_http_server_request_size_bytes",
      help: "Size of HTTP requests received by the server",
      labelNames: ["method", "path", "status"] as const,
      buckets: httpRequestSizeBytes,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_http_server_response_size_bytes = new prometheus.Histogram({
      name: "ponder_http_server_response_size_bytes",
      help: "Size of HTTP responses served the server",
      labelNames: ["method", "path", "status"] as const,
      buckets: httpRequestSizeBytes,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_rpc_request_duration = new prometheus.Histogram({
      name: "ponder_rpc_request_duration",
      help: "Duration of successful RPC requests",
      labelNames: ["chain", "method"] as const,
      buckets: alwaysIODurationMs,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_rpc_request_error_total = new prometheus.Counter({
      name: "ponder_rpc_request_error_total",
      help: "Total count of failed RPC requests",
      labelNames: ["chain", "method"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    this.ponder_postgres_query_total = new prometheus.Counter({
      name: "ponder_postgres_query_total",
      help: "Total number of queries submitted to the database",
      labelNames: ["pool"] as const,
      registers: [this.registry],
      aggregator: "sum",
    });
    if (!("bun" in process.versions))
      prometheus.collectDefaultMetrics({
        register: this.registry,
        eventLoopMonitoringPrecision: 1,
        gcDurationBuckets: [
          0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10,
        ],
      });
  }
  /**
   * Get string representation for all metrics.
   * @returns Metrics encoded using Prometheus v0.0.4 format.
   */
  getMetrics() {
    return this.registry.metrics();
  }
  async getRegistry() {
    return this.registry;
  }
  initializeIndexingMetrics({
    indexingBuild,
    schemaBuild,
  }: {
    indexingBuild: Pick<IndexingBuild, "indexingFunctions">;
    schemaBuild: SchemaBuild;
  }) {
    const tables = Object.values(schemaBuild.schema).filter(isTable);
    for (const { name: eventName } of indexingBuild.indexingFunctions) {
      this.ponder_indexing_completed_events.inc({ event: eventName }, 0);
    }
    for (const table of tables) {
      for (const type of ["complete", "hit", "miss", "prefetch"]) {
        this.ponder_indexing_cache_requests_total.inc(
          { table: getTableName(table), type },
          0,
        );
      }
      for (const method of ["find", "insert", "update", "delete"]) {
        this.ponder_indexing_store_queries_total.inc(
          { table: getTableName(table), method },
          0,
        );
      }
    }
  }
  resetIndexingMetrics() {
    this.start_timestamp = Date.now();
    this.rps = {};
    this.progressMetadata = {
      general: {
        batches: [{ elapsedSeconds: 0, completedSeconds: 0 }],
        previousTimestamp: Date.now(),
        previousCompletedSeconds: 0,
        rate: 0,
      },
    };
    this.hasError = false;
    this.ponder_settings_info.reset();
    this.ponder_historical_start_timestamp_seconds.reset();
    this.ponder_historical_end_timestamp_seconds.reset();
    this.ponder_historical_total_indexing_seconds.reset();
    this.ponder_historical_cached_indexing_seconds.reset();
    this.ponder_historical_completed_indexing_seconds.reset();
    this.ponder_indexing_completed_events.reset();
    this.ponder_indexing_timestamp.reset();
    this.ponder_indexing_function_duration.reset();
    this.ponder_sync_block.reset();
    this.ponder_sync_is_realtime.reset();
    this.ponder_sync_is_complete.reset();
    this.ponder_historical_total_blocks.reset();
    this.ponder_historical_cached_blocks.reset();
    this.ponder_historical_completed_blocks.reset();
    this.ponder_realtime_reorg_total.reset();
    this.ponder_rpc_request_duration.reset();
    this.ponder_rpc_request_error_total.reset();
    // Note: These are used by both indexing and API services.
    this.ponder_database_method_duration.reset();
    this.ponder_database_method_error_total.reset();
    this.ponder_postgres_pool_connections?.reset();
    this.ponder_postgres_query_queue_size?.reset();
    this.ponder_postgres_query_total?.reset();
  }
  resetApiMetrics() {
    this.port = undefined;
    // TODO: Create a separate metric for API build errors,
    // or stop using metrics for the TUI error message.
    this.hasError = false;
    this.ponder_http_server_active_requests.reset();
    this.ponder_http_server_request_duration_ms.reset();
    this.ponder_http_server_request_size_bytes.reset();
    this.ponder_http_server_response_size_bytes.reset();
  }
}
type MetricsAggregationRequest = {
  type: typeof GET_METRICS_REQ;
  requestId: number;
};
type MetricsAggregationResponse = {
  type: typeof GET_METRICS_RES;
  requestId: number;
  error?: string;
  metrics: prometheus.MetricObjectWithValues<prometheus.MetricValue<string>>[];
};
export class AggregateMetricsService extends MetricsService {
  workers: Worker[];
  requests: Map<
    number,
    {
      responses: prometheus.MetricObjectWithValues<
        prometheus.MetricValue<string>
      >[][];
      workerIds: number[];
      pending: number;
      pwr: PromiseWithResolvers<void>;
    }
  >;
  requestId: number;
  mainThreadMetrics: MetricsService;
  constructor(mainThreadMetrics: MetricsService, workers: Worker[]) {
    super();
    this.mainThreadMetrics = mainThreadMetrics;
    this.workers = workers;
    this.requests = new Map();
    this.requestId = 0;
    for (const worker of workers) {
      worker.on("message", (message: MetricsAggregationResponse) => {
        if (message.type === GET_METRICS_RES) {
          const request = this.requests.get(message.requestId);
          if (request === undefined) return;
          if (message.error) {
            request.pwr.reject(new Error(message.error));
            return;
          }
          request.responses.push(message.metrics);
          request.workerIds.push(worker.threadId);
          request.pending--;
          if (request.pending === 0) {
            request.pwr.resolve();
          }
        }
      });
    }
  }
  override async getMetrics() {
    const requestId = this.requestId++;
    const pwr = promiseWithResolvers<void>();
    this.requests.set(requestId, {
      responses: [],
      workerIds: [],
      pending: this.workers.length,
      pwr,
    });
    for (const worker of this.workers) {
      worker.postMessage({
        type: GET_METRICS_REQ,
        requestId,
      } satisfies MetricsAggregationRequest);
    }
    await pwr.promise;
    const request = this.requests.get(requestId)!;
    this.requests.delete(requestId);
    // Sort response by worker id for consistent metrics
    const responseIndexSort = new Array(this.workers.length)
      .fill(0)
      .map((_, index) => index)
      .sort((a, b) => request.workerIds[a]! - request.workerIds[b]!);
    return prometheus.AggregatorRegistry.aggregate([
      ...responseIndexSort.map((index) => request.responses[index]!),
      await this.registry.getMetricsAsJSON(),
      await this.mainThreadMetrics.registry.getMetricsAsJSON(),
    ]).metrics();
  }
  override async getRegistry() {
    const requestId = this.requestId++;
    const pwr = promiseWithResolvers<void>();
    this.requests.set(requestId, {
      responses: [],
      workerIds: [],
      pending: this.workers.length,
      pwr,
    });
    for (const worker of this.workers) {
      worker.postMessage({
        type: GET_METRICS_REQ,
        requestId,
      } satisfies MetricsAggregationRequest);
    }
    await pwr.promise;
    const request = this.requests.get(requestId)!;
    this.requests.delete(requestId);
    // Sort response by worker id for consistent metrics
    const responseIndexSort = new Array(this.workers.length)
      .fill(0)
      .map((_, index) => index)
      .sort((a, b) => request.workerIds[a]! - request.workerIds[b]!);
    return prometheus.AggregatorRegistry.aggregate([
      ...responseIndexSort.map((index) => request.responses[index]!),
      await this.registry.getMetricsAsJSON(),
      await this.mainThreadMetrics.registry.getMetricsAsJSON(),
    ]) as prometheus.Registry;
  }
  // Note: `resetIndexingMetrics` and `resetApiMetrics` are never called with `AggregateMetricsService`.
}
export class IsolatedMetricsService extends MetricsService {
  constructor() {
    super();
    if (parentPort) {
      parentPort.on("message", (message: MetricsAggregationRequest) => {
        if (message.type === GET_METRICS_REQ) {
          this.registry
            .getMetricsAsJSON()
            .then((metrics) => {
              parentPort!.postMessage({
                type: GET_METRICS_RES,
                requestId: message.requestId,
                metrics,
              });
            })
            .catch((error) => {
              parentPort!.postMessage({
                type: GET_METRICS_RES,
                requestId: message.requestId,
                error: error.message,
              });
            });
        }
      });
    }
  }
}
const extractMetric = (
  metric: prometheus.MetricObjectWithValues<prometheus.MetricValue<"chain">>,
  chain: string,
) => {
  return metric.values.find((m) => m.labels.chain === chain)?.value;
};
export async function getSyncProgress(metrics: MetricsService): Promise<
  {
    chainName: string;
    block: number | undefined;
    status: "backfill" | "live" | "complete";
    progress: number;
    rps: number;
  }[]
> {
  const totalBlocksMetric = await metrics.ponder_historical_total_blocks.get();
  const cachedBlocksMetric =
    await metrics.ponder_historical_cached_blocks.get();
  const completedBlocksMetric =
    await metrics.ponder_historical_completed_blocks.get();
  const syncBlockMetric = await metrics.ponder_sync_block.get();
  const syncIsRealtimeMetrics = await metrics.ponder_sync_is_realtime.get();
  const syncIsCompleteMetrics = await metrics.ponder_sync_is_complete.get();
  const requestCount: { [chain: string]: number } = {};
  const rpcRequestMetrics = await metrics.ponder_rpc_request_duration.get();
  for (const m of rpcRequestMetrics.values) {
    const chain = m.labels.chain!;
    if (m.metricName === "ponder_rpc_request_duration_count") {
      if (requestCount[chain] === undefined) {
        requestCount[chain] = 0;
      }
      requestCount[m.labels.chain!]! += m.value;
    }
  }
  for (const [chainName, count] of Object.entries(requestCount)) {
    if (metrics.rps[chainName] === undefined) {
      metrics.rps[chainName] = [{ count, timestamp: Date.now() }];
    } else {
      metrics.rps[chainName]!.push({ count, timestamp: Date.now() });
    }
    if (metrics.rps[chainName]!.length > 100) {
      metrics.rps[chainName]!.shift();
    }
  }
  return totalBlocksMetric.values.map(({ value, labels }) => {
    const chain = labels.chain as string;
    const totalBlocks = value;
    const cachedBlocks = extractMetric(cachedBlocksMetric, chain) ?? 0;
    const completedBlocks = extractMetric(completedBlocksMetric, chain) ?? 0;
    const syncBlock = extractMetric(syncBlockMetric, chain);
    const isRealtime = extractMetric(syncIsRealtimeMetrics, chain);
    const isComplete = extractMetric(syncIsCompleteMetrics, chain);
    const progress =
      totalBlocks === 0 ? 1 : (completedBlocks + cachedBlocks) / totalBlocks;
    const _length = metrics.rps[labels.chain!]!.length;
    const _firstRps = metrics.rps[labels.chain!]![0]!;
    const _lastRps = metrics.rps[labels.chain!]![_length - 1]!;
    const requests = _lastRps.count - (_length > 1 ? _firstRps.count : 0);
    const seconds =
      _length === 1 ? 0.1 : (_lastRps.timestamp - _firstRps.timestamp) / 1_000;
    return {
      chainName: chain,
      block: syncBlock,
      progress,
      status: isComplete ? "complete" : isRealtime ? "live" : "backfill",
      rps: requests / seconds,
    } as const;
  });
}
export async function getIndexingProgress(metrics: MetricsService) {
  const indexingCompletedEventsMetric = (
    await metrics.ponder_indexing_completed_events.get()
  ).values;
  const indexingFunctionDurationMetric = (
    await metrics.ponder_indexing_function_duration.get()
  ).values;
  const indexingDurationSum: Record<string, number> = {};
  const indexingDurationCount: Record<string, number> = {};
  for (const m of indexingFunctionDurationMetric) {
    if (m.metricName === "ponder_indexing_function_duration_sum")
      indexingDurationSum[m.labels.event!] = m.value;
    if (m.metricName === "ponder_indexing_function_duration_count")
      indexingDurationCount[m.labels.event!] = m.value;
  }
  const events = indexingCompletedEventsMetric.map((m) => {
    const count = m.value;
    const durationSum = indexingDurationSum[m.labels.event as string] ?? 0;
    const durationCount = indexingDurationCount[m.labels.event as string] ?? 0;
    const averageDuration =
      durationCount === 0 ? 0 : durationSum / durationCount;
    const eventName = truncate(m.labels.event as string);
    return { eventName, count, averageDuration };
  });
  return {
    hasError: metrics.hasError,
    events,
  };
}
export async function getAppProgress(metrics: MetricsService): Promise<{
  mode: "backfill" | "live" | undefined;
  progress: number | undefined;
  eta: number | undefined;
}> {
  // Note: `getRegistry` must be used because this function is used with "experimental_isolated" ordering.
  const registry = await metrics.getRegistry();
  const totalSecondsMetric = await registry
    .getSingleMetric("ponder_historical_total_indexing_seconds")!
    .get();
  const cachedSecondsMetric = await registry
    .getSingleMetric("ponder_historical_cached_indexing_seconds")!
    .get();
  const completedSecondsMetric = await registry
    .getSingleMetric("ponder_historical_completed_indexing_seconds")!
    .get();
  const timestampMetric = await registry
    .getSingleMetric("ponder_indexing_timestamp")!
    .get();
  const settingsMetric = await registry
    .getSingleMetric("ponder_settings_info")!
    .get();
  const ordering: PreBuild["ordering"] | undefined = settingsMetric?.values[0]
    ?.labels.ordering as any;
  switch (ordering) {
    case undefined:
      return {
        mode: "backfill",
        progress: undefined,
        eta: undefined,
      };
    case "omnichain": {
      const totalSeconds = totalSecondsMetric?.values
        .map(({ value }) => value)
        .reduce((prev, curr) => prev + curr, 0);
      const cachedSeconds = cachedSecondsMetric?.values
        .map(({ value }) => value)
        .reduce((prev, curr) => prev + curr, 0);
      const completedSeconds = completedSecondsMetric?.values
        .map(({ value }) => value)
        .reduce((prev, curr) => prev + curr, 0);
      const timestamp = timestampMetric?.values
        .map(({ value }) => value)
        .reduce((prev, curr) => Math.max(prev, curr), 0);
      const progress =
        timestamp === 0
          ? 0
          : totalSeconds === 0
            ? 1
            : (completedSeconds + cachedSeconds) / totalSeconds;
      return {
        mode: progress === 1 ? "live" : "backfill",
        progress: progress,
        eta: calculateEta(
          metrics.progressMetadata.general!,
          totalSeconds,
          cachedSeconds,
          completedSeconds,
        ),
      };
    }
    case "multichain":
    case "experimental_isolated": {
      const perChainAppProgress: Awaited<ReturnType<typeof getAppProgress>>[] =
        [];
      for (const chainName of totalSecondsMetric?.values.map(
        ({ labels }) => labels.chain as string,
      ) ?? []) {
        const totalSeconds = extractMetric(totalSecondsMetric, chainName);
        const cachedSeconds = extractMetric(cachedSecondsMetric, chainName);
        const completedSeconds = extractMetric(
          completedSecondsMetric,
          chainName,
        );
        const timestamp = extractMetric(timestampMetric, chainName);
        if (
          totalSeconds === undefined ||
          cachedSeconds === undefined ||
          completedSeconds === undefined ||
          timestamp === undefined
        ) {
          continue;
        }
        const progress =
          timestamp === 0
            ? 0
            : totalSeconds === 0
              ? 1
              : (completedSeconds + cachedSeconds) / totalSeconds;
        if (!metrics.progressMetadata[chainName]) {
          metrics.progressMetadata[chainName] = {
            batches: [{ elapsedSeconds: 0, completedSeconds: 0 }],
            previousTimestamp: Date.now(),
            previousCompletedSeconds: 0,
            rate: 0,
          };
        }
        const eta: number | undefined = calculateEta(
          metrics.progressMetadata[chainName]!,
          totalSeconds,
          cachedSeconds,
          completedSeconds,
        );
        perChainAppProgress.push({
          mode: progress === 1 ? "live" : "backfill",
          progress,
          eta,
        });
      }
      return perChainAppProgress.reduce(
        (prev, curr) => ({
          mode: curr.mode === "backfill" ? curr.mode : prev.mode,
          progress:
            prev.progress === undefined || curr.progress === undefined
              ? undefined
              : Math.min(prev.progress, curr.progress),
          eta:
            curr.progress === 1
              ? prev.eta
              : prev.eta === undefined || curr.eta === undefined
                ? undefined
                : Math.max(prev.eta, curr.eta),
        }),
        {
          mode: "live",
          progress: 1,
          eta: 0,
        },
      );
    }
  }
}
function calculateEta(
  progressMetadata: {
    batches: {
      elapsedSeconds: number;
      completedSeconds: number;
    }[];
    previousTimestamp: number;
    previousCompletedSeconds: number;
    rate: number;
  },
  totalSeconds: number,
  cachedSeconds: number,
  completedSeconds: number,
) {
  const remainingSeconds = Math.max(
    totalSeconds - (completedSeconds + cachedSeconds),
    0,
  );
  let eta: number | undefined = undefined;
  if (completedSeconds > 0) {
    const currentTimestamp = Date.now();
    progressMetadata.batches.at(-1)!.elapsedSeconds =
      Math.max(currentTimestamp - progressMetadata.previousTimestamp, 0) /
      1_000;
    progressMetadata.batches.at(-1)!.completedSeconds = Math.max(
      completedSeconds - progressMetadata.previousCompletedSeconds,
      0,
    );
    if (
      currentTimestamp - progressMetadata.previousTimestamp > 5_000 &&
      progressMetadata.batches.at(-1)!.completedSeconds > 0
    ) {
      progressMetadata.batches.push({
        elapsedSeconds: 0,
        completedSeconds: 0,
      });
      if (progressMetadata.batches.length > 10) {
        progressMetadata.batches.shift();
      }
      progressMetadata.previousCompletedSeconds = completedSeconds;
      progressMetadata.previousTimestamp = currentTimestamp;
      const averages: number[] = [];
      let count = 0;
      // Note: Calculate ETA only after at least 3 batches were collected for stable eta.
      if (progressMetadata.batches.length >= 3) {
        for (let i = 0; i < progressMetadata.batches.length - 1; ++i) {
          const batch = progressMetadata.batches[i]!;
          if (batch.completedSeconds === 0) continue;
          const multiplier = 1 / 1.5 ** (9 - i);
          averages.push(
            (multiplier * batch.elapsedSeconds) / batch.completedSeconds,
          );
          count += multiplier;
        }
        progressMetadata.rate =
          count === 0
            ? 0
            : averages.reduce((prev, curr) => prev + curr, 0) / count;
      }
    }
    if (progressMetadata.batches.length >= 3) {
      eta = progressMetadata.rate * remainingSeconds;
    }
  }
  return eta;
}
</file>

<file path="packages/core/src/internal/options.ts">
import path from "node:path";
import v8 from "node:v8";
import type { CliOptions } from "@/bin/ponder.js";
import type { LevelWithSilent } from "pino";
import { type SemVer, parse } from "semver";
export type Options = {
  command: "dev" | "start" | "serve" | "codegen" | "list" | "prune";
  version: SemVer | null;
  configFile: string;
  schemaFile: string;
  apiDir: string;
  apiFile: string;
  rootDir: string;
  indexingDir: string;
  generatedDir: string;
  ponderDir: string;
  logDir: string;
  port: number;
  hostname?: string;
  telemetryUrl: string;
  telemetryDisabled: boolean;
  telemetryConfigDir: string | undefined;
  logLevel: LevelWithSilent;
  logFormat: "json" | "pretty";
  databaseHeartbeatInterval: number;
  databaseHeartbeatTimeout: number;
  databaseMaxQueryParameters: number;
  maxThreads: number;
  factoryAddressCountThreshold: number;
  indexingCacheMaxBytes: number;
  rpcMaxConcurrency: number;
  syncEventsQuerySize: number;
};
export const buildOptions = ({ cliOptions }: { cliOptions: CliOptions }) => {
  let rootDir: string;
  if (cliOptions.root !== undefined) {
    rootDir = path.resolve(cliOptions.root);
  } else {
    rootDir = path.resolve(".");
  }
  let logLevel: LevelWithSilent;
  if (cliOptions.logLevel) {
    logLevel = cliOptions.logLevel as LevelWithSilent;
  } else if (cliOptions.trace === true) {
    logLevel = "trace";
  } else if (cliOptions.debug === true) {
    logLevel = "debug";
  } else if (
    process.env.PONDER_LOG_LEVEL !== undefined &&
    ["silent", "fatal", "error", "warn", "info", "debug", "trace"].includes(
      process.env.PONDER_LOG_LEVEL,
    )
  ) {
    logLevel = process.env.PONDER_LOG_LEVEL as LevelWithSilent;
  } else {
    logLevel = "info";
  }
  const port =
    process.env.PORT !== undefined
      ? Number(process.env.PORT)
      : cliOptions.port !== undefined
        ? cliOptions.port
        : 42069;
  const hostname = cliOptions.hostname;
  return {
    command: cliOptions.command,
    version: parse(cliOptions.version),
    rootDir,
    configFile: path.join(rootDir, cliOptions.config),
    schemaFile: path.join(rootDir, "ponder.schema.ts"),
    apiDir: path.join(rootDir, "src", "api"),
    apiFile: path.join(rootDir, "src", "api", "index.ts"),
    indexingDir: path.join(rootDir, "src"),
    generatedDir: path.join(rootDir, "generated"),
    ponderDir: path.join(rootDir, ".ponder"),
    logDir: path.join(rootDir, ".ponder", "logs"),
    port,
    hostname,
    telemetryUrl: "https://ponder.sh/api/telemetry",
    telemetryDisabled: Boolean(process.env.PONDER_TELEMETRY_DISABLED),
    telemetryConfigDir: undefined,
    logLevel,
    logFormat: cliOptions.logFormat! as Options["logFormat"],
    databaseHeartbeatInterval: 10 * 1000,
    databaseHeartbeatTimeout: 25 * 1000,
    // Half of the max query parameters for PGlite
    databaseMaxQueryParameters: 16_000,
    maxThreads:
      process.env.PONDER_MAX_THREADS !== undefined
        ? Number(process.env.PONDER_MAX_THREADS)
        : 4,
    factoryAddressCountThreshold: 1_000,
    rpcMaxConcurrency: 256,
    // v8.getHeapStatistics().heap_size_limit / 5, rounded up to the nearest 64 MB
    indexingCacheMaxBytes:
      process.env.PONDER_CACHE_BYTES !== undefined
        ? Number(process.env.PONDER_CACHE_BYTES)
        : Math.ceil(
            v8.getHeapStatistics().heap_size_limit / 1_024 / 1_024 / 5 / 64,
          ) *
          64 *
          1_024 *
          1_024,
    syncEventsQuerySize: 12_000,
  } satisfies Options;
};
</file>

<file path="packages/core/src/internal/shutdown.ts">
export type Shutdown = {
  add: (callback: () => unknown | Promise<unknown>) => void;
  kill: () => Promise<void>;
  isKilled: boolean;
  abortController: AbortController;
};
export const createShutdown = (): Shutdown => {
  const abortController = new AbortController();
  const callbacks: (() => unknown | Promise<unknown>)[] = [];
  return {
    add: (callback) => {
      if (abortController.signal.aborted) {
        callback();
        return;
      }
      callbacks.push(callback);
    },
    kill: async () => {
      if (abortController.signal.aborted) return;
      abortController.abort();
      await Promise.all(callbacks.map((callback) => callback()));
    },
    get isKilled() {
      return abortController.signal.aborted;
    },
    abortController,
  };
};
</file>

<file path="packages/core/src/internal/telemetry.test.ts">
import { randomUUID } from "node:crypto";
import { mkdirSync } from "node:fs";
import os from "node:os";
import path from "node:path";
import { context } from "@/_test/setup.js";
import { IS_BUN_TEST, stubGlobal } from "@/_test/utils.js";
import { createTelemetry } from "@/internal/telemetry.js";
import { rimrafSync } from "rimraf";
import { afterEach, beforeEach, expect, test, vi } from "vitest";
import type { Common } from "./common.js";
import { createLogger } from "./logger.js";
import { createShutdown } from "./shutdown.js";
const fetchSpy = vi.fn();
let unstub: () => void;
beforeEach(() => {
  fetchSpy.mockReset();
  if (IS_BUN_TEST) {
    unstub = stubGlobal("fetch", fetchSpy);
  } else {
    vi.stubGlobal("fetch", fetchSpy);
  }
});
afterEach(() => {
  if (IS_BUN_TEST) unstub();
  else vi.unstubAllGlobals();
});
beforeEach(() => {
  const tempDir = path.join(os.tmpdir(), randomUUID());
  mkdirSync(tempDir, { recursive: true });
  context.common = {
    logger: createLogger({ level: "silent" }),
    options: {
      telemetryUrl: "https://ponder.sh/api/telemetry",
      telemetryDisabled: false,
      telemetryConfigDir: tempDir,
    },
  } as unknown as Common;
  return () => {
    rimrafSync(tempDir);
  };
});
test("telemetry calls fetch with event body", async () => {
  const shutdown = createShutdown();
  const telemetry = createTelemetry({
    options: context.common.options,
    logger: context.common.logger,
    shutdown,
  });
  telemetry.record({
    name: "lifecycle:heartbeat_send",
    properties: { duration_seconds: process.uptime() },
  });
  await telemetry.flush();
  await shutdown.kill();
  expect(fetchSpy).toHaveBeenCalledTimes(1);
  const fetchUrl = fetchSpy.mock.calls[0][0];
  expect(fetchUrl).toBe(context.common.options.telemetryUrl);
  const fetchBody = JSON.parse(fetchSpy.mock.calls[0][1].body);
  expect(fetchBody).toMatchObject({
    distinctId: expect.any(String),
    event: "lifecycle:heartbeat_send",
    properties: { duration_seconds: expect.any(Number) },
  });
});
test("telemetry does not submit events if telemetry is disabled", async () => {
  const shutdown = createShutdown();
  const telemetry = createTelemetry({
    options: { ...context.common.options, telemetryDisabled: true },
    logger: context.common.logger,
    shutdown,
  });
  telemetry.record({
    name: "lifecycle:session_start",
    properties: { cli_command: "test" },
  });
  telemetry.record({
    name: "lifecycle:heartbeat_send",
    properties: { duration_seconds: process.uptime() },
  });
  await telemetry.flush();
  await shutdown.kill();
  expect(fetchSpy).not.toHaveBeenCalled();
});
test("telemetry throws if event is submitted after kill", async () => {
  const shutdown = createShutdown();
  const telemetry = createTelemetry({
    options: context.common.options,
    logger: context.common.logger,
    shutdown,
  });
  for (let i = 0; i < 5; i++) {
    telemetry.record({
      name: "lifecycle:heartbeat_send",
      properties: { duration_seconds: process.uptime() },
    });
  }
  await telemetry.flush();
  await shutdown.kill();
  expect(fetchSpy).toHaveBeenCalledTimes(5);
  telemetry.record({
    name: "lifecycle:heartbeat_send",
    properties: { duration_seconds: process.uptime() },
  });
  await telemetry.flush();
  expect(fetchSpy).toHaveBeenCalledTimes(5);
});
</file>

<file path="packages/core/src/internal/telemetry.ts">
import { exec } from "node:child_process";
import { createHash, randomBytes } from "node:crypto";
import { existsSync, readFileSync } from "node:fs";
import os from "node:os";
import path from "node:path";
import { promisify } from "node:util";
import type { Options } from "@/internal/options.js";
import { createQueue } from "@/utils/queue.js";
import { startClock } from "@/utils/timer.js";
import Conf from "conf";
import { type PM, detect, getNpmVersion } from "detect-package-manager";
import { ShutdownError } from "./errors.js";
import type { Logger } from "./logger.js";
import type { Shutdown } from "./shutdown.js";
import type { IndexingBuild } from "./types.js";
import type { PreBuild, SchemaBuild } from "./types.js";
const HEARTBEAT_INTERVAL_MS = 60_000;
type TelemetryEvent =
  | {
      name: "lifecycle:session_start";
      properties: { cli_command: string };
    }
  | {
      name: "lifecycle:session_end";
      properties: { duration_seconds: number };
    }
  | {
      name: "lifecycle:heartbeat_send";
      properties: { duration_seconds: number };
    };
type CommonProperties = {
  // Identification
  project_id: string;
  session_id: string;
  is_internal: boolean;
};
type SessionProperties = {
  // Environment and package versions
  package_manager: string;
  package_manager_version: string;
  node_version: string;
  ponder_core_version: string;
  viem_version: string;
  // System and hardware
  system_platform: NodeJS.Platform;
  system_release: string;
  system_architecture: string;
  cpu_count: number;
  cpu_model: string;
  cpu_speed: number;
  total_memory_bytes: number;
};
type DeviceConf = {
  notifiedAt?: string;
  anonymousId?: string;
  salt?: string;
};
export type Telemetry = ReturnType<typeof createTelemetry>;
export function createTelemetry({
  options,
  logger,
  shutdown,
}: { options: Options; logger: Logger; shutdown: Shutdown }) {
  if (options.telemetryDisabled) {
    return {
      record: (_event: TelemetryEvent) => {},
      flush: async () => {},
    };
  }
  const conf = new Conf<DeviceConf>({
    projectName: "ponder",
    cwd: options.telemetryConfigDir,
  });
  if (conf.get("notifiedAt") === undefined) {
    conf.set("notifiedAt", Date.now().toString());
    logger.info({
      msg: "Ponder collects anonymous telemetry data to identify issues and prioritize features. See https://ponder.sh/docs/advanced/telemetry for more information.",
    });
  }
  const sessionId = randomBytes(8).toString("hex");
  let anonymousId = conf.get("anonymousId") as string;
  if (anonymousId === undefined) {
    anonymousId = randomBytes(8).toString("hex");
    conf.set("anonymousId", anonymousId);
  }
  // Before 0.4.3, the anonymous ID was 64 characters long. Truncate it to 16
  // here to align with new ID lengths.
  if (anonymousId.length > 16) anonymousId = anonymousId.slice(0, 16);
  let salt = conf.get("salt") as string;
  if (salt === undefined) {
    salt = randomBytes(8).toString("hex");
    conf.set("salt", salt);
  }
  // Prepend the value with a secret salt to ensure a credible one-way hash.
  const oneWayHash = (value: string) => {
    const hash = createHash("sha256");
    hash.update(salt);
    hash.update(value);
    return hash.digest("hex").slice(0, 16);
  };
  const buildContext = async () => {
    // Project ID is a one-way hash of the git remote URL OR the current working directory.
    const gitRemoteUrl = await getGitRemoteUrl();
    const projectIdRaw = gitRemoteUrl ?? process.cwd();
    const projectId = oneWayHash(projectIdRaw);
    const { packageManager, packageManagerVersion } = await getPackageManager();
    // Attempt to find and read the users package.json file.
    const packageJson = getPackageJson(options.rootDir);
    const ponderVersion = packageJson?.dependencies?.ponder ?? "unknown";
    const viemVersion = packageJson?.dependencies?.viem ?? "unknown";
    // Make a guess as to whether the project is internal (within the monorepo) or not.
    const isInternal = ponderVersion === "workspace:*";
    const cpus = os.cpus();
    return {
      common: {
        session_id: sessionId,
        project_id: projectId,
        is_internal: isInternal,
      } satisfies CommonProperties,
      session: {
        ponder_core_version: ponderVersion,
        viem_version: viemVersion,
        package_manager: packageManager,
        package_manager_version: packageManagerVersion,
        node_version: process.versions.node,
        system_platform: os.platform(),
        system_release: os.release(),
        system_architecture: os.arch(),
        cpu_count: cpus.length,
        cpu_model: cpus.length > 0 ? cpus[0]!.model : "unknown",
        cpu_speed: cpus.length > 0 ? cpus[0]!.speed : 0,
        total_memory_bytes: os.totalmem(),
      } satisfies SessionProperties,
    };
  };
  let context: Awaited<ReturnType<typeof buildContext>> | undefined = undefined;
  const contextPromise = buildContext();
  const queue = createQueue({
    initialStart: true,
    concurrency: 10,
    worker: async (event: TelemetryEvent) => {
      if (shutdown.isKilled) return;
      const endClock = startClock();
      try {
        if (context === undefined) context = await contextPromise;
        const properties =
          event.name === "lifecycle:session_start"
            ? { ...event.properties, ...context.common, ...context.session }
            : { ...event.properties, ...context.common };
        const body = JSON.stringify({
          distinctId: anonymousId,
          event: event.name,
          properties,
        });
        await fetch(options.telemetryUrl, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body,
        });
        logger.trace({
          msg: "Sent telemetry event",
          event: event.name,
          duration: endClock(),
        });
      } catch (error_) {
        const error = error_ as Error;
        if (shutdown.isKilled) {
          throw new ShutdownError();
        }
        logger.trace({
          msg: "Failed to send telemetry event",
          event: event.name,
          duration: endClock(),
          error,
        });
      }
    },
  });
  const record = (event: TelemetryEvent) => {
    queue.add(event);
  };
  const heartbeatInterval = setInterval(() => {
    record({
      name: "lifecycle:heartbeat_send",
      properties: { duration_seconds: process.uptime() },
    });
  }, HEARTBEAT_INTERVAL_MS);
  shutdown.add(() => {
    clearInterval(heartbeatInterval);
  });
  // Note that this method is only used for testing.
  const flush = async () => {
    await queue.onIdle();
  };
  return { record, flush };
}
async function getPackageManager() {
  let packageManager: PM = "unknown" as PM;
  let packageManagerVersion = "unknown";
  try {
    packageManager = await detect();
    packageManagerVersion = await getNpmVersion(packageManager);
  } catch (e) {}
  return { packageManager, packageManagerVersion };
}
const execa = promisify(exec);
async function getGitRemoteUrl() {
  const result = await execa("git config --local --get remote.origin.url", {
    timeout: 250,
    windowsHide: true,
  }).catch(() => undefined);
  return result?.stdout.trim();
}
type PackageJson = {
  name?: string;
  version?: string;
  dependencies?: { [key: string]: string };
  devDependencies?: { [key: string]: string };
};
function getPackageJson(rootDir: string) {
  try {
    const rootPath = path.join(rootDir, "package.json");
    const cwdPath = path.join(process.cwd(), "package.json");
    const packageJsonPath = existsSync(rootPath)
      ? rootPath
      : existsSync(cwdPath)
        ? cwdPath
        : undefined;
    if (packageJsonPath === undefined) return undefined;
    const packageJsonString = readFileSync(packageJsonPath, "utf8");
    const packageJson = JSON.parse(packageJsonString) as PackageJson;
    return packageJson;
  } catch (e) {
    return undefined;
  }
}
export function buildPayload({
  preBuild,
  schemaBuild,
  indexingBuild,
}: {
  preBuild: PreBuild;
  schemaBuild?: SchemaBuild;
  indexingBuild?: IndexingBuild;
}) {
  const table_count = schemaBuild ? Object.keys(schemaBuild.schema).length : 0;
  const indexing_function_count = indexingBuild
    ? Object.values(indexingBuild.indexingFunctions).reduce(
        (acc, f) => acc + Object.keys(f).length,
        0,
      )
    : 0;
  return {
    database_kind: preBuild?.databaseConfig.kind,
    contract_count: 0,
    network_count: indexingBuild?.chains.length ?? 0,
    table_count,
    indexing_function_count,
  };
}
</file>

<file path="packages/core/src/internal/types.ts">
import type { SqlStatements } from "@/drizzle/kit/index.js";
import type { Rpc } from "@/rpc/index.js";
import type {
  Block,
  Log,
  Trace,
  Transaction,
  TransactionReceipt,
  Transfer,
} from "@/types/eth.js";
import type { PartialExcept, Prettify } from "@/types/utils.js";
import type { Trace as DebugTrace } from "@/utils/debug.js";
import type { PGliteOptions } from "@/utils/pglite.js";
import type { PGlite } from "@electric-sql/pglite";
import type { Hono } from "hono";
import type { PoolConfig } from "pg";
import type {
  Abi,
  AbiEvent,
  AbiFunction,
  Address,
  BlockTag,
  Hex,
  LogTopic,
  RpcBlock,
  RpcTransaction,
  RpcTransactionReceipt,
  Transport,
  Chain as ViemChain,
  Log as ViemLog,
} from "viem";
import type { RetryableError } from "./errors.js";
// Database
export type DatabaseConfig =
  | { kind: "pglite"; options: PGliteOptions }
  | { kind: "pglite_test"; instance: PGlite }
  | { kind: "postgres"; poolConfig: Prettify<PoolConfig & { max: number }> };
// Indexing
/** Indexing functions as defined in `ponder.on()` */
export type IndexingFunctions = {
  /** Name of the event */
  name: string;
  /** Callback function */
  fn: (...args: any) => any;
}[];
// Filters
/** Filter definition based on the fundamental data model of the Ethereum blockchain. */
export type Filter =
  | LogFilter
  | BlockFilter
  | TransferFilter
  | TransactionFilter
  | TraceFilter;
/**
 * Filter that matches addresses.
 *
 * @dev This object is used as a unique constraint in the `ponder_sync.factories` table.
 * Any changes to the type must be backwards compatible and probably requires updating
 * `syncStore.getChildAddresses` and `syncStore.insertChildAddresses`.
 */
export type Factory = LogFactory;
export type FilterAddress<
  factory extends Factory | undefined = Factory | undefined,
> = factory extends Factory ? factory : Address | Address[] | undefined;
export type BlockFilter = {
  type: "block";
  chainId: number;
  sourceId: string;
  interval: number;
  offset: number;
  fromBlock: number | undefined;
  toBlock: number | undefined;
  hasTransactionReceipt: false;
  include: `block.${keyof Block}`[];
};
export type TransactionFilter<
  fromFactory extends Factory | undefined = Factory | undefined,
  toFactory extends Factory | undefined = Factory | undefined,
> = {
  type: "transaction";
  chainId: number;
  sourceId: string;
  fromAddress: FilterAddress<fromFactory>;
  toAddress: FilterAddress<toFactory>;
  includeReverted: boolean;
  fromBlock: number | undefined;
  toBlock: number | undefined;
  hasTransactionReceipt: true;
  include: (
    | `block.${keyof Block}`
    | `transaction.${keyof Transaction}`
    | `transactionReceipt.${keyof TransactionReceipt}`
  )[];
};
export type TraceFilter<
  fromFactory extends Factory | undefined = Factory | undefined,
  toFactory extends Factory | undefined = Factory | undefined,
> = {
  type: "trace";
  chainId: number;
  sourceId: string;
  fromAddress: FilterAddress<fromFactory>;
  toAddress: FilterAddress<toFactory>;
  functionSelector: Hex;
  callType: Trace["type"] | undefined;
  includeReverted: boolean;
  fromBlock: number | undefined;
  toBlock: number | undefined;
  hasTransactionReceipt: boolean;
  include: (
    | `block.${keyof Block}`
    | `transaction.${keyof Transaction}`
    | `transactionReceipt.${keyof TransactionReceipt}`
    | `trace.${keyof Trace}`
  )[];
};
export type LogFilter<
  factory extends Factory | undefined = Factory | undefined,
> = {
  type: "log";
  chainId: number;
  sourceId: string;
  address: FilterAddress<factory>;
  topic0: Hex;
  topic1: LogTopic;
  topic2: LogTopic;
  topic3: LogTopic;
  fromBlock: number | undefined;
  toBlock: number | undefined;
  hasTransactionReceipt: boolean;
  include: (
    | `block.${keyof Block}`
    | `transaction.${keyof Transaction}`
    | `transactionReceipt.${keyof TransactionReceipt}`
    | `log.${keyof Log}`
  )[];
};
export type TransferFilter<
  fromFactory extends Factory | undefined = Factory | undefined,
  toFactory extends Factory | undefined = Factory | undefined,
> = {
  type: "transfer";
  chainId: number;
  sourceId: string;
  fromAddress: FilterAddress<fromFactory>;
  toAddress: FilterAddress<toFactory>;
  includeReverted: boolean;
  fromBlock: number | undefined;
  toBlock: number | undefined;
  hasTransactionReceipt: boolean;
  include: (
    | `block.${keyof Block}`
    | `transaction.${keyof Transaction}`
    | `transactionReceipt.${keyof TransactionReceipt}`
    | `trace.${keyof Trace}`
  )[];
};
export type FactoryId = string;
export type LogFactory = {
  id: FactoryId;
  type: "log";
  chainId: number;
  sourceId: string;
  address: Address | Address[] | undefined;
  eventSelector: Hex;
  childAddressLocation: "topic1" | "topic2" | "topic3" | `offset${number}`;
  fromBlock: number | undefined;
  toBlock: number | undefined;
};
// Fragments
export type FragmentAddress =
  | Address
  | {
      address: Address | null;
      eventSelector: Factory["eventSelector"];
      childAddressLocation: Factory["childAddressLocation"];
    }
  | null;
export type FragmentAddressId =
  | Address
  | `${Address | null}_${Factory["eventSelector"]}_${Factory["childAddressLocation"]}`
  | null;
export type FragmentTopic = Hex | null;
export type Fragment =
  | {
      type: "block";
      chainId: number;
      interval: number;
      offset: number;
    }
  | {
      type: "transaction";
      chainId: number;
      fromAddress: FragmentAddress;
      toAddress: FragmentAddress;
    }
  | {
      type: "trace";
      chainId: number;
      fromAddress: FragmentAddress;
      toAddress: FragmentAddress;
      functionSelector: Hex;
      includeTransactionReceipts: boolean;
    }
  | {
      type: "log";
      chainId: number;
      address: FragmentAddress;
      topic0: Hex;
      topic1: FragmentTopic;
      topic2: FragmentTopic;
      topic3: FragmentTopic;
      includeTransactionReceipts: boolean;
    }
  | {
      type: "transfer";
      chainId: number;
      fromAddress: FragmentAddress;
      toAddress: FragmentAddress;
      includeTransactionReceipts: boolean;
    }
  | {
      type: "factory_log";
      chainId: number;
      address: Address | null;
      eventSelector: Factory["eventSelector"];
      childAddressLocation: Factory["childAddressLocation"];
      fromBlock: number | null;
      toBlock: number | null;
    };
/** Minimum slice of a {@link Filter} */
export type FragmentId =
  /** block_{chainId}_{interval}_{offset} */
  | `block_${number}_${number}_${number}`
  /** transaction_{chainId}_{fromAddress}_{toAddress} */
  | `transaction_${number}_${FragmentAddressId}_${FragmentAddressId}`
  /** trace_{chainId}_{fromAddress}_{toAddress}_{functionSelector}_{includeReceipts} */
  | `trace_${number}_${FragmentAddressId}_${FragmentAddressId}_${Hex | null}_${0 | 1}`
  /** log_{chainId}_{address}_{topic0}_{topic1}_{topic2}_{topic3}_{includeReceipts} */
  | `log_${number}_${FragmentAddressId}_${FragmentTopic}_${FragmentTopic}_${FragmentTopic}_${FragmentTopic}_${0 | 1}`
  /** transfer_{chainId}_{fromAddress}_{toAddress}_{includeReceipts} */
  | `transfer_${number}_${FragmentAddressId}_${FragmentAddressId}_${0 | 1}`
  /** factory_log_{chainId}_{address}_{eventSelector}_{childAddressLocation}_{fromBlock}_{toBlock} */
  | `factory_log_${number}_${Address | null}_${Factory["eventSelector"]}_${Factory["childAddressLocation"]}_${number | null}_${number | null}`;
// Contract
export type Contract = {
  abi: Abi;
  address?: Address | readonly Address[];
  startBlock?: number;
  endBlock?: number;
};
// Event Callback
export type EventCallback = {
  filter: Filter;
  name: string;
  fn: (...args: any) => any;
  chain: Chain;
} & (
  | {
      type: "contract";
      abiItem: AbiEvent | AbiFunction;
      metadata: { safeName: string; abi: Abi };
    }
  | { type: "account"; direction: "from" | "to" }
  | { type: "block" }
);
export type SetupCallback = {
  name: string;
  fn: (...args: any) => any;
  chain: Chain;
  block: number | undefined;
};
// Chain
export type Chain = {
  name: string;
  id: number;
  rpc: string | string[] | Transport;
  ws: string | undefined;
  pollingInterval: number;
  finalityBlockCount: number;
  disableCache: boolean;
  ethGetLogsBlockRange: number | undefined;
  viemChain: ViemChain | undefined;
};
// Schema
/** User-defined tables, enums, and indexes. */
export type Schema = { [name: string]: unknown };
// Build artifacts
/** Database schema name. */
export type NamespaceBuild = {
  schema: string;
  viewsSchema: string | undefined;
};
/** Consolidated CLI, env vars, and config. */
export type PreBuild = {
  /** Database type and configuration */
  databaseConfig: DatabaseConfig;
  /** Ordering of events */
  ordering: "omnichain" | "multichain" | "experimental_isolated";
};
export type SchemaBuild = {
  schema: Schema;
  /** SQL statements to create the schema */
  statements: SqlStatements;
};
export type IndexingBuild = {
  /** Ten character hex string identifier. */
  buildId: string;
  /** Chains to index. */
  chains: Chain[];
  /** RPCs for all `chains`. */
  rpcs: Rpc[];
  /** Finalized blocks for all `chains`. */
  finalizedBlocks: LightBlock[];
  /** Event callbacks for all `chains`.  */
  eventCallbacks: EventCallback[][];
  /** Setup callbacks for all `chains`. */
  setupCallbacks: SetupCallback[][];
  /** Indexing functions registered with `ponder.on()`. */
  indexingFunctions: IndexingFunctions;
  /** Contracts for all `chains`. */
  contracts: {
    [name: string]: Contract;
  }[];
};
export type ApiBuild = {
  /** Hostname for server */
  hostname?: string;
  /** Port number for server */
  port: number;
  /** Hono app exported from `ponder/api/index.ts`. */
  app: Hono;
};
// Crash recovery
/**
 * @dev It is not an invariant that `chainId` and `checkpoint.chainId` are the same.
 */
export type CrashRecoveryCheckpoint =
  | {
      chainId: number;
      checkpoint: string;
    }[]
  | undefined;
// Status
export type Status = {
  [chainName: string]: {
    id: number;
    block: { number: number; timestamp: number };
  };
};
// Indexing error handler
export type IndexingErrorHandler = {
  getRetryableError: () => RetryableError | undefined;
  setRetryableError: (error: RetryableError) => void;
  clearRetryableError: () => void;
  error: RetryableError | undefined;
};
// Seconds
export type Seconds = {
  [chain: string]: { start: number; end: number; cached: number };
};
// Blockchain data
export type SyncBlock = Prettify<RpcBlock<Exclude<BlockTag, "pending">, true>>;
export type SyncBlockHeader = Omit<SyncBlock, "transactions"> & {
  transactions: undefined;
};
export type SyncTransaction = RpcTransaction<false>;
export type SyncTransactionReceipt = RpcTransactionReceipt;
export type SyncTrace = {
  trace: DebugTrace["result"] & { index: number; subcalls: number };
  transactionHash: DebugTrace["txHash"];
};
export type SyncLog = ViemLog<Hex, Hex, false>;
export type LightBlock = Pick<
  SyncBlock,
  "hash" | "parentHash" | "number" | "timestamp"
>;
export type RequiredBlockColumns = "timestamp" | "number" | "hash";
export type RequiredTransactionColumns =
  | "transactionIndex"
  | "from"
  | "to"
  | "hash"
  | "type";
export type RequiredTransactionReceiptColumns = "status" | "from" | "to";
export type RequiredTraceColumns =
  | "from"
  | "to"
  | "input"
  | "output"
  | "value"
  | "type"
  | "error"
  | "traceIndex";
export type RequiredLogColumns = keyof Log;
export type RequiredInternalBlockColumns = RequiredBlockColumns;
export type RequiredInternalTransactionColumns =
  | RequiredTransactionColumns
  | "blockNumber";
export type RequiredInternalTransactionReceiptColumns =
  | RequiredTransactionReceiptColumns
  | "blockNumber"
  | "transactionIndex";
export type RequiredInternalTraceColumns =
  | RequiredTraceColumns
  | "blockNumber"
  | "transactionIndex";
export type RequiredInternalLogColumns =
  | RequiredLogColumns
  | "blockNumber"
  | "transactionIndex";
export type InternalBlock = PartialExcept<Block, RequiredBlockColumns>;
export type InternalTransaction = PartialExcept<
  Transaction,
  RequiredTransactionColumns
> & {
  blockNumber: number;
};
export type InternalTransactionReceipt = PartialExcept<
  TransactionReceipt,
  RequiredTransactionReceiptColumns
> & {
  blockNumber: number;
  transactionIndex: number;
};
export type InternalTrace = PartialExcept<Trace, RequiredTraceColumns> & {
  blockNumber: number;
  transactionIndex: number;
};
export type InternalLog = Log & {
  blockNumber: number;
  transactionIndex: number;
};
export type UserBlock = PartialExcept<Block, RequiredBlockColumns>;
export type UserTransaction = PartialExcept<
  Transaction,
  RequiredTransactionColumns
>;
export type UserTransactionReceipt = PartialExcept<
  TransactionReceipt,
  RequiredTransactionReceiptColumns
>;
export type UserTrace = PartialExcept<Trace, RequiredTraceColumns>;
export type UserLog = Log;
// Events
export type RawEvent = {
  checkpoint: string;
  chainId: number;
  eventCallbackIndex: number;
  log?: UserLog;
  block: UserBlock;
  transaction?: UserTransaction;
  transactionReceipt?: UserTransactionReceipt;
  trace?: UserTrace;
};
export type Event =
  | BlockEvent
  | TransactionEvent
  | TraceEvent
  | LogEvent
  | TransferEvent;
export type SetupEvent = {
  type: "setup";
  checkpoint: string;
  chain: Chain;
  setupCallback: SetupCallback;
  block: bigint;
};
export type BlockEvent = {
  type: "block";
  checkpoint: string;
  chain: Chain;
  eventCallback: EventCallback;
  event: {
    id: string;
    block: UserBlock;
  };
};
export type TransactionEvent = {
  type: "transaction";
  checkpoint: string;
  chain: Chain;
  eventCallback: EventCallback;
  event: {
    id: string;
    block: UserBlock;
    transaction: UserTransaction;
    transactionReceipt?: UserTransactionReceipt;
  };
};
export type TraceEvent = {
  type: "trace";
  checkpoint: string;
  chain: Chain;
  eventCallback: EventCallback;
  event: {
    id: string;
    args: { [key: string]: unknown } | readonly unknown[] | undefined;
    result: { [key: string]: unknown } | readonly unknown[] | undefined;
    block: UserBlock;
    transaction: UserTransaction;
    transactionReceipt?: UserTransactionReceipt;
    trace: UserTrace;
  };
};
export type LogEvent = {
  type: "log";
  checkpoint: string;
  chain: Chain;
  eventCallback: EventCallback;
  event: {
    id: string;
    args: { [key: string]: unknown } | readonly unknown[] | undefined;
    block: UserBlock;
    transaction: UserTransaction;
    transactionReceipt?: UserTransactionReceipt;
    log: UserLog;
  };
};
export type TransferEvent = {
  type: "transfer";
  checkpoint: string;
  chain: Chain;
  eventCallback: EventCallback;
  event: {
    id: string;
    transfer: Transfer;
    block: UserBlock;
    transaction: UserTransaction;
    transactionReceipt?: UserTransactionReceipt;
    trace: UserTrace;
  };
};
</file>

<file path="packages/core/src/rpc/actions.ts">
import { RpcProviderError } from "@/internal/errors.js";
import type {
  LightBlock,
  SyncBlock,
  SyncBlockHeader,
  SyncLog,
  SyncTrace,
  SyncTransaction,
  SyncTransactionReceipt,
} from "@/internal/types.js";
import type { RequestParameters, Rpc } from "@/rpc/index.js";
import { zeroLogsBloom } from "@/sync-realtime/bloom.js";
import { PG_BIGINT_MAX, PG_INTEGER_MAX } from "@/utils/pg.js";
import {
  BlockNotFoundError,
  type Hex,
  TransactionReceiptNotFoundError,
  hexToBigInt,
  hexToNumber,
  isHex,
  zeroAddress,
  zeroHash,
} from "viem";
/**
 * Helper function for "eth_getBlockByNumber" request.
 */
export const eth_getBlockByNumber = <
  params extends Extract<
    RequestParameters,
    { method: "eth_getBlockByNumber" }
  >["params"],
>(
  rpc: Rpc,
  params: params,
  context?: Parameters<Rpc["request"]>[1],
): Promise<params[1] extends true ? SyncBlock : LightBlock> =>
  rpc
    .request({ method: "eth_getBlockByNumber", params }, context)
    .then((_block) => {
      if (!_block) {
        let blockNumber: bigint;
        if (isHex(params[0])) {
          blockNumber = hexToBigInt(params[0]);
        } else {
          // @ts-ignore `BlockNotFoundError` expects a bigint, but it also just passes
          // the `blockNumber` directly to the error message, so breaking the type constraint is fine.
          blockNumber = params[0];
        }
        throw new BlockNotFoundError({ blockNumber });
      }
      return standardizeBlock(_block as SyncBlock, {
        method: "eth_getBlockByNumber",
        params,
      });
    });
/**
 * Helper function for "eth_getBlockByHash" request.
 */
export const eth_getBlockByHash = <
  params extends Extract<
    RequestParameters,
    { method: "eth_getBlockByHash" }
  >["params"],
>(
  rpc: Rpc,
  params: params,
  context?: Parameters<Rpc["request"]>[1],
): Promise<params[1] extends true ? SyncBlock : LightBlock> =>
  rpc
    .request({ method: "eth_getBlockByHash", params }, context)
    .then((_block) => {
      if (!_block) throw new BlockNotFoundError({ blockHash: params[0] });
      return standardizeBlock(_block as SyncBlock, {
        method: "eth_getBlockByHash",
        params,
      });
    });
/**
 * Helper function for "eth_getLogs" rpc request.
 * Handles different error types and retries the request if applicable.
 */
export const eth_getLogs = async (
  rpc: Rpc,
  params: Extract<RequestParameters, { method: "eth_getLogs" }>["params"],
  context?: Parameters<Rpc["request"]>[1],
): Promise<SyncLog[]> => {
  const request: Extract<RequestParameters, { method: "eth_getLogs" }> = {
    method: "eth_getLogs",
    params,
  };
  return rpc.request(request, context).then((logs) => {
    if (logs === null || logs === undefined) {
      throw new Error("Received invalid empty eth_getLogs response.");
    }
    return standardizeLogs(logs as SyncLog[], request);
  });
};
/**
 * Helper function for "eth_getTransactionReceipt" request.
 */
export const eth_getTransactionReceipt = (
  rpc: Rpc,
  params: Extract<
    RequestParameters,
    { method: "eth_getTransactionReceipt" }
  >["params"],
  context?: Parameters<Rpc["request"]>[1],
): Promise<SyncTransactionReceipt> =>
  rpc
    .request({ method: "eth_getTransactionReceipt", params }, context)
    .then((receipt) => {
      if (!receipt) {
        throw new TransactionReceiptNotFoundError({
          hash: params[0],
        });
      }
      return standardizeTransactionReceipts([receipt], {
        method: "eth_getTransactionReceipt",
        params,
      })[0]!;
    });
/**
 * Helper function for "eth_getBlockReceipts" request.
 */
export const eth_getBlockReceipts = (
  rpc: Rpc,
  params: Extract<
    RequestParameters,
    { method: "eth_getBlockReceipts" }
  >["params"],
  context?: Parameters<Rpc["request"]>[1],
): Promise<SyncTransactionReceipt[]> =>
  rpc
    .request({ method: "eth_getBlockReceipts", params }, context)
    .then((receipts) => {
      if (receipts === null || receipts === undefined) {
        throw new Error(
          "Received invalid empty eth_getBlockReceipts response.",
        );
      }
      return standardizeTransactionReceipts(receipts, {
        method: "eth_getBlockReceipts",
        params,
      });
    });
/**
 * Helper function for "debug_traceBlockByNumber" request.
 */
export const debug_traceBlockByNumber = (
  rpc: Rpc,
  params: Extract<
    RequestParameters,
    { method: "debug_traceBlockByNumber" }
  >["params"],
  context?: Parameters<Rpc["request"]>[1],
): Promise<SyncTrace[]> =>
  rpc
    .request({ method: "debug_traceBlockByNumber", params }, context)
    .then((traces) => {
      if (traces === null || traces === undefined) {
        throw new Error(
          "Received invalid empty debug_traceBlockByNumber response.",
        );
      }
      const result: SyncTrace[] = [];
      let index = 0;
      // all traces that weren't included because the trace has an error
      // or the trace's parent has an error, mapped to the error string
      const failedTraces = new Map<
        (typeof traces)[number]["result"],
        { error?: string; revertReason?: string }
      >();
      const dfs = (
        frames: (typeof traces)[number]["result"][],
        transactionHash: Hex,
        parentFrame: (typeof traces)[number]["result"] | undefined,
      ) => {
        for (const frame of frames) {
          if (frame.error !== undefined) {
            failedTraces.set(frame, {
              error: frame.error,
              revertReason: frame.revertReason,
            });
          } else if (parentFrame && failedTraces.has(parentFrame)) {
            const error = failedTraces.get(parentFrame)!;
            frame.error = error.error;
            frame.revertReason = error.revertReason;
            failedTraces.set(frame, error);
          }
          // @ts-ignore
          frame.index = index;
          // @ts-ignore
          frame.subcalls = frame.calls?.length ?? 0;
          result.push({ trace: frame as SyncTrace["trace"], transactionHash });
          index++;
          if (frame.calls) {
            dfs(frame.calls, transactionHash, frame);
          }
        }
      };
      for (const trace of traces) {
        index = 0;
        dfs([trace.result], trace.txHash, undefined);
      }
      return result.map((trace) =>
        standardizeTrace(trace, {
          method: "debug_traceBlockByNumber",
          params,
        }),
      );
    });
/**
 * Helper function for "debug_traceBlockByHash" request.
 */
export const debug_traceBlockByHash = (
  rpc: Rpc,
  params: Extract<
    RequestParameters,
    { method: "debug_traceBlockByHash" }
  >["params"],
  context?: Parameters<Rpc["request"]>[1],
): Promise<SyncTrace[]> =>
  rpc
    .request({ method: "debug_traceBlockByHash", params }, context)
    .then((traces) => {
      if (traces === null || traces === undefined) {
        throw new Error(
          "Received invalid empty debug_traceBlockByHash response.",
        );
      }
      const result: SyncTrace[] = [];
      let index = 0;
      // all traces that weren't included because the trace has an error
      // or the trace's parent has an error, mapped to the error string
      const failedTraces = new Map<
        (typeof traces)[number]["result"],
        { error?: string; revertReason?: string }
      >();
      const dfs = (
        frames: (typeof traces)[number]["result"][],
        transactionHash: Hex,
        parentFrame: (typeof traces)[number]["result"] | undefined,
      ) => {
        for (const frame of frames) {
          if (frame.error !== undefined) {
            failedTraces.set(frame, {
              error: frame.error,
              revertReason: frame.revertReason,
            });
          } else if (parentFrame && failedTraces.has(parentFrame)) {
            const error = failedTraces.get(parentFrame)!;
            frame.error = error.error;
            frame.revertReason = error.revertReason;
            failedTraces.set(frame, error);
          }
          // @ts-ignore
          frame.index = index;
          // @ts-ignore
          frame.subcalls = frame.calls?.length ?? 0;
          result.push({ trace: frame as SyncTrace["trace"], transactionHash });
          index++;
          if (frame.calls) {
            dfs(frame.calls, transactionHash, frame);
          }
        }
      };
      for (const trace of traces) {
        index = 0;
        dfs([trace.result], trace.txHash, undefined);
      }
      return result.map((trace) =>
        standardizeTrace(trace, {
          method: "debug_traceBlockByHash",
          params,
        }),
      );
    });
/**
 * Validate that the transactions are consistent with the block.
 */
export const validateTransactionsAndBlock = (
  block: SyncBlock,
  request: Extract<
    RequestParameters,
    { method: "eth_getBlockByNumber" | "eth_getBlockByHash" }
  >,
) => {
  for (const [index, transaction] of block.transactions.entries()) {
    if (block.hash !== transaction.blockHash) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The transaction at index ${index} of the 'block.transactions' array has a 'transaction.blockHash' of ${transaction.blockHash}, but the block itself has a 'block.hash' of ${block.hash}.`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (block.number !== transaction.blockNumber) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The transaction at index ${index} of the 'block.transactions' array has a 'transaction.blockNumber' of ${transaction.blockNumber} (${hexToNumber(transaction.blockNumber)}), but the block itself has a 'block.number' of ${block.number} (${hexToNumber(block.number)}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
  }
};
/**
 * Validate that the logs are consistent with the block.
 *
 * @dev Allows `log.transactionHash` to be `zeroHash`.
 * @dev Allows `block.logsBloom` to be `zeroLogsBloom`.
 */
export const validateLogsAndBlock = (
  logs: SyncLog[],
  block: SyncBlock,
  logsRequest: Extract<RequestParameters, { method: "eth_getLogs" }>,
  blockRequest: Extract<
    RequestParameters,
    { method: "eth_getBlockByNumber" | "eth_getBlockByHash" }
  >,
) => {
  if (block.logsBloom !== zeroLogsBloom && logs.length === 0) {
    const error = new RpcProviderError(
      `Inconsistent RPC response data. The logs array has length 0, but the associated block has a non-empty 'block.logsBloom'.`,
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(blockRequest),
      requestText(logsRequest),
    ];
    error.stack = undefined;
    throw error;
  }
  const transactionByIndex = new Map<Hex, SyncTransaction>(
    block.transactions.map((transaction) => [
      transaction.transactionIndex,
      transaction,
    ]),
  );
  for (const log of logs) {
    if (block.hash !== log.blockHash) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The log with 'logIndex' ${log.logIndex} (${hexToNumber(log.logIndex)}) has a 'log.blockHash' of ${log.blockHash}, but the associated block has a 'block.hash' of ${block.hash}.`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(blockRequest),
        requestText(logsRequest),
      ];
      error.stack = undefined;
      throw error;
    }
    if (block.number !== log.blockNumber) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The log with 'logIndex' ${log.logIndex} (${hexToNumber(log.logIndex)}) has a 'log.blockNumber' of ${log.blockNumber} (${hexToNumber(log.blockNumber)}), but the associated block has a 'block.number' of ${block.number} (${hexToNumber(block.number)}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(blockRequest),
        requestText(logsRequest),
      ];
      error.stack = undefined;
      throw error;
    }
    if (log.transactionHash !== zeroHash) {
      const transaction = transactionByIndex.get(log.transactionIndex);
      if (transaction === undefined) {
        const error = new RpcProviderError(
          `Inconsistent RPC response data. The log with 'logIndex' ${log.logIndex} (${hexToNumber(log.logIndex)}) has a 'log.transactionIndex' of ${log.transactionIndex} (${hexToNumber(log.transactionIndex)}), but the associated 'block.transactions' array does not contain a transaction matching that 'transactionIndex'.`,
        );
        error.meta = [
          "Please report this error to the RPC operator.",
          requestText(blockRequest),
          requestText(logsRequest),
        ];
        error.stack = undefined;
        throw error;
      } else if (transaction.hash !== log.transactionHash) {
        const error = new RpcProviderError(
          `Inconsistent RPC response data. The log with 'logIndex' ${log.logIndex} (${hexToNumber(log.logIndex)}) matches a transaction in the associated 'block.transactions' array by 'transactionIndex' ${log.transactionIndex} (${hexToNumber(log.transactionIndex)}), but the log has a 'log.transactionHash' of ${log.transactionHash} while the transaction has a 'transaction.hash' of ${transaction.hash}.`,
        );
        error.meta = [
          "Please report this error to the RPC operator.",
          requestText(blockRequest),
          requestText(logsRequest),
        ];
        error.stack = undefined;
        throw error;
      }
    }
  }
};
/**
 * Validate that the traces are consistent with the block.
 */
export const validateTracesAndBlock = (
  traces: SyncTrace[],
  block: SyncBlock,
  tracesRequest: Extract<
    RequestParameters,
    { method: "debug_traceBlockByNumber" | "debug_traceBlockByHash" }
  >,
  blockRequest: Extract<
    RequestParameters,
    { method: "eth_getBlockByNumber" | "eth_getBlockByHash" }
  >,
) => {
  const transactionHashes = new Set(block.transactions.map((t) => t.hash));
  for (const [index, trace] of traces.entries()) {
    if (transactionHashes.has(trace.transactionHash) === false) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The top-level trace at array index ${index} has a 'transactionHash' of ${trace.transactionHash}, but the associated 'block.transactions' array does not contain a transaction matching that hash.`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(blockRequest),
        requestText(tracesRequest),
      ];
      error.stack = undefined;
      throw error;
    }
  }
  // Use the fact that any transaction produces a trace to validate.
  if (block.transactions.length !== 0 && traces.length === 0) {
    const error = new RpcProviderError(
      `Inconsistent RPC response data. The traces array has length 0, but the associated 'block.transactions' array has length ${block.transactions.length}.`,
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(blockRequest),
      requestText(tracesRequest),
    ];
    error.stack = undefined;
    throw error;
  }
};
/**
 * Validate that the receipts are consistent with the block.
 */
export const validateReceiptsAndBlock = (
  receipts: SyncTransactionReceipt[],
  block: SyncBlock,
  receiptsRequest: Extract<
    RequestParameters,
    { method: "eth_getBlockReceipts" | "eth_getTransactionReceipt" }
  >,
  blockRequest: Extract<
    RequestParameters,
    { method: "eth_getBlockByNumber" | "eth_getBlockByHash" }
  >,
) => {
  const transactionByIndex = new Map<Hex, SyncTransaction>(
    block.transactions.map((transaction) => [
      transaction.transactionIndex,
      transaction,
    ]),
  );
  for (const [index, receipt] of receipts.entries()) {
    if (block.hash !== receipt.blockHash) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The receipt at array index ${index} has a 'receipt.blockHash' of ${receipt.blockHash}, but the associated block has a 'block.hash' of ${block.hash}.`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(blockRequest),
        requestText(receiptsRequest),
      ];
      error.stack = undefined;
      throw error;
    }
    if (block.number !== receipt.blockNumber) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The receipt at array index ${index} has a 'receipt.blockNumber' of ${receipt.blockNumber} (${hexToNumber(receipt.blockNumber)}), but the associated block has a 'block.number' of ${block.number} (${hexToNumber(block.number)}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(blockRequest),
        requestText(receiptsRequest),
      ];
      error.stack = undefined;
      throw error;
    }
    const transaction = transactionByIndex.get(receipt.transactionIndex);
    if (transaction === undefined) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The receipt at array index ${index} has a 'receipt.transactionIndex' of ${receipt.transactionIndex} (${hexToNumber(receipt.transactionIndex)}), but the associated 'block.transactions' array does not contain a transaction matching that 'transactionIndex'.`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(blockRequest),
        requestText(receiptsRequest),
      ];
      error.stack = undefined;
      throw error;
    } else if (transaction.hash !== receipt.transactionHash) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The receipt at array index ${index} matches a transaction in the associated 'block.transactions' array by 'transactionIndex' ${receipt.transactionIndex} (${hexToNumber(receipt.transactionIndex)}), but the receipt has a 'receipt.transactionHash' of ${receipt.transactionHash} while the transaction has a 'transaction.hash' of ${transaction.hash}.`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(blockRequest),
        requestText(receiptsRequest),
      ];
      error.stack = undefined;
      throw error;
    }
  }
  if (
    receiptsRequest.method === "eth_getBlockReceipts" &&
    block.transactions.length !== receipts.length
  ) {
    const error = new RpcProviderError(
      `Inconsistent RPC response data. The receipts array has length ${receipts.length}, but the associated 'block.transactions' array has length ${block.transactions.length}.`,
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(blockRequest),
      requestText(receiptsRequest),
    ];
    error.stack = undefined;
    throw error;
  }
};
/**
 * Validate required block properties and set non-required properties.
 *
 * Required properties:
 * - hash
 * - number
 * - timestamp
 * - logsBloom
 * - parentHash
 * - transactions
 *
 * Non-required properties:
 * - miner
 * - gasUsed
 * - gasLimit
 * - baseFeePerGas
 * - nonce
 * - mixHash
 * - stateRoot
 * - transactionsRoot
 * - sha3Uncles
 * - size
 * - difficulty
 * - totalDifficulty
 * - extraData
 */
export const standardizeBlock = <
  block extends
    | SyncBlock
    | (Omit<SyncBlock, "transactions"> & {
        transactions: string[] | undefined;
      }),
>(
  block: block,
  request:
    | Extract<
        RequestParameters,
        { method: "eth_getBlockByNumber" | "eth_getBlockByHash" }
      >
    | { method: "eth_subscribe"; params: ["newHeads"] },
): block extends SyncBlock ? SyncBlock : SyncBlockHeader => {
  // required properties
  if (block.hash === undefined) {
    const error = new RpcProviderError(
      "Invalid RPC response: 'block.hash' is a required property",
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  if (block.number === undefined) {
    const error = new RpcProviderError(
      "Invalid RPC response: 'block.number' is a required property",
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  if (block.timestamp === undefined) {
    const error = new RpcProviderError(
      "Invalid RPC response: 'block.timestamp' is a required property",
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  if (block.logsBloom === undefined) {
    const error = new RpcProviderError(
      "Invalid RPC response: 'block.logsBloom' is a required property",
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  if (block.parentHash === undefined) {
    const error = new RpcProviderError(
      "Invalid RPC response: 'block.parentHash' is a required property",
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  // non-required properties
  if (block.miner === undefined) {
    block.miner = zeroAddress;
  }
  if (block.gasUsed === undefined) {
    block.gasUsed = "0x0";
  }
  if (block.gasLimit === undefined) {
    block.gasLimit = "0x0";
  }
  if (block.baseFeePerGas === undefined) {
    block.baseFeePerGas = "0x0";
  }
  if (block.nonce === undefined) {
    block.nonce = "0x0";
  }
  if (block.mixHash === undefined) {
    block.mixHash = zeroHash;
  }
  if (block.stateRoot === undefined) {
    block.stateRoot = zeroHash;
  }
  if (block.transactionsRoot === undefined) {
    block.transactionsRoot = zeroHash;
  }
  if (block.sha3Uncles === undefined) {
    block.sha3Uncles = zeroHash;
  }
  if (block.size === undefined) {
    block.size = "0x0";
  }
  if (block.difficulty === undefined) {
    block.difficulty = "0x0";
  }
  if (block.totalDifficulty === undefined) {
    block.totalDifficulty = "0x0";
  }
  if (block.extraData === undefined) {
    block.extraData = "0x";
  }
  if (hexToBigInt(block.number) > PG_BIGINT_MAX) {
    const error = new RpcProviderError(
      `Invalid RPC response: 'block.number' (${hexToBigInt(block.number)}) is larger than the maximum allowed value (${PG_BIGINT_MAX}).`,
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  if (hexToBigInt(block.timestamp) > PG_BIGINT_MAX) {
    const error = new RpcProviderError(
      `Invalid RPC response: 'block.timestamp' (${hexToBigInt(block.timestamp)}) is larger than the maximum allowed value (${PG_BIGINT_MAX}).`,
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  // Note: block headers for some providers may contain transactions hashes,
  // but Ponder coerces the transactions property to undefined.
  if (request.method === "eth_subscribe" || request.params[1] === false) {
    block.transactions = undefined;
    return block as block extends SyncBlock ? SyncBlock : SyncBlockHeader;
  } else {
    if (block.transactions === undefined) {
      throw new Error(
        "Invalid RPC response: 'block.transactions' is a required property",
      );
    }
    block.transactions = standardizeTransactions(
      (block as SyncBlock).transactions,
      request,
    );
    return block as block extends SyncBlock ? SyncBlock : SyncBlockHeader;
  }
};
/**
 * Validate required transaction properties and set non-required properties.
 *
 * Required properties:
 * - hash
 * - transactionIndex
 * - blockNumber
 * - blockHash
 * - from
 * - to
 *
 * Non-required properties:
 * - input
 * - value
 * - nonce
 * - r
 * - s
 * - v
 * - type
 * - gas
 */
export const standardizeTransactions = (
  transactions: SyncTransaction[],
  request: Extract<
    RequestParameters,
    { method: "eth_getBlockByNumber" | "eth_getBlockByHash" }
  >,
): SyncTransaction[] => {
  const transactionIds = new Set<Hex>();
  for (const transaction of transactions) {
    if (transactionIds.has(transaction.transactionIndex)) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The 'block.transactions' array contains two objects with a 'transactionIndex' of ${transaction.transactionIndex} (${hexToNumber(transaction.transactionIndex)}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    } else {
      transactionIds.add(transaction.transactionIndex);
    }
    // required properties
    if (transaction.hash === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'transaction.hash' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (transaction.transactionIndex === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'transaction.transactionIndex' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (transaction.blockNumber === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'transaction.blockNumber' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (transaction.blockHash === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'transaction.blockHash' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (transaction.from === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'transaction.from' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    // Note: `to` is a required property but can be coerced to `null`.
    if (transaction.to === undefined) {
      transaction.to = null;
    }
    // non-required properties
    if (transaction.input === undefined) {
      transaction.input = "0x";
    }
    if (transaction.value === undefined) {
      transaction.value = "0x0";
    }
    if (transaction.nonce === undefined) {
      transaction.nonce = "0x0";
    }
    if (transaction.r === undefined) {
      transaction.r = "0x0";
    }
    if (transaction.s === undefined) {
      transaction.s = "0x0";
    }
    if (transaction.v === undefined) {
      transaction.v = "0x0";
    }
    if (transaction.type === undefined) {
      // @ts-ignore
      transaction.type = "0x0";
    }
    if (transaction.gas === undefined) {
      transaction.gas = "0x0";
    }
    if (hexToBigInt(transaction.blockNumber) > PG_BIGINT_MAX) {
      const error = new RpcProviderError(
        `Invalid RPC response: 'transaction.blockNumber' (${hexToBigInt(transaction.blockNumber)}) is larger than the maximum allowed value (${PG_BIGINT_MAX}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (hexToBigInt(transaction.transactionIndex) > BigInt(PG_INTEGER_MAX)) {
      const error = new RpcProviderError(
        `Invalid RPC response: 'transaction.transactionIndex' (${hexToBigInt(transaction.transactionIndex)}) is larger than the maximum allowed value (${PG_INTEGER_MAX}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (hexToBigInt(transaction.nonce) > BigInt(PG_INTEGER_MAX)) {
      const error = new RpcProviderError(
        `Invalid RPC response: 'transaction.nonce' (${hexToBigInt(transaction.nonce)}) is larger than the maximum allowed value (${PG_INTEGER_MAX}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
  }
  return transactions;
};
/**
 * Validate required log properties and set properties.
 *
 * Required properties:
 * - blockNumber
 * - logIndex
 * - blockHash
 * - address
 * - topics
 * - data
 * - transactionHash
 * - transactionIndex
 *
 * Non-required properties:
 * - removed
 */
export const standardizeLogs = (
  logs: SyncLog[],
  request: Extract<RequestParameters, { method: "eth_getLogs" }>,
): SyncLog[] => {
  const logIds = new Set<string>();
  for (const log of logs) {
    if (logIds.has(`${log.blockNumber}_${log.logIndex}`)) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The logs array contains two objects with 'blockNumber' ${log.blockNumber} (${hexToNumber(log.blockNumber)}) and 'logIndex' ${log.logIndex} (${hexToNumber(log.logIndex)}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      console.log(logs);
      error.stack = undefined;
      throw error;
    } else {
      logIds.add(`${log.blockNumber}_${log.logIndex}`);
    }
    // required properties
    if (log.blockNumber === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'log.blockNumber' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (log.logIndex === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'log.logIndex' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (log.blockHash === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'log.blockHash' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (log.address === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'log.address' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (log.topics === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'log.topics' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (log.data === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'log.data' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (log.transactionHash === undefined) {
      log.transactionHash = zeroHash;
    }
    if (log.transactionIndex === undefined) {
      log.transactionIndex = "0x0";
    }
    // non-required properties
    if (log.removed === undefined) {
      log.removed = false;
    }
    if (hexToBigInt(log.blockNumber) > PG_BIGINT_MAX) {
      const error = new RpcProviderError(
        `Invalid RPC response: 'log.blockNumber' (${hexToBigInt(log.blockNumber)}) is larger than the maximum allowed value (${PG_BIGINT_MAX}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (hexToBigInt(log.transactionIndex) > BigInt(PG_INTEGER_MAX)) {
      const error = new RpcProviderError(
        `Invalid RPC response: 'log.transactionIndex' (${hexToBigInt(log.transactionIndex)}) is larger than the maximum allowed value (${PG_INTEGER_MAX}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (hexToBigInt(log.logIndex) > BigInt(PG_INTEGER_MAX)) {
      const error = new RpcProviderError(
        `Invalid RPC response: 'log.logIndex' (${hexToBigInt(log.logIndex)}) is larger than the maximum allowed value (${PG_INTEGER_MAX}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
  }
  return logs;
};
/**
 * Validate required trace properties and set non-required properties.
 *
 * Required properties:
 * - transactionHash
 * - type
 * - from
 * - input
 *
 * Non-required properties:
 * - gas
 * - gasUsed
 */
export const standardizeTrace = (
  trace: SyncTrace,
  request: Extract<
    RequestParameters,
    { method: "debug_traceBlockByNumber" | "debug_traceBlockByHash" }
  >,
): SyncTrace => {
  // required properties
  if (trace.transactionHash === undefined) {
    const error = new RpcProviderError(
      "Invalid RPC response: 'trace.transactionHash' is a required property",
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  if (trace.trace.type === undefined) {
    const error = new RpcProviderError(
      "Invalid RPC response: 'trace.type' is a required property",
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  if (trace.trace.from === undefined) {
    const error = new RpcProviderError(
      "Invalid RPC response: 'trace.from' is a required property",
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  if (trace.trace.input === undefined) {
    const error = new RpcProviderError(
      "Invalid RPC response: 'trace.input' is a required property",
    );
    error.meta = [
      "Please report this error to the RPC operator.",
      requestText(request),
    ];
    error.stack = undefined;
    throw error;
  }
  // non-required properties
  if (trace.trace.gas === undefined) {
    trace.trace.gas = "0x0";
  }
  if (trace.trace.gasUsed === undefined) {
    trace.trace.gasUsed = "0x0";
  }
  // Note: All INTEGER and BIGINT `trace` columns are generated, not derived from
  // RPC responses.
  return trace;
};
/**
 * Validate required transaction receipt properties and set non-required properties.
 *
 * Required properties:
 * - blockHash
 * - blockNumber
 * - transactionHash
 * - transactionIndex
 * - from
 * - to
 * - status
 *
 * Non-required properties:
 * - logs
 * - logsBloom
 * - gasUsed
 * - cumulativeGasUsed
 * - effectiveGasPrice
 * - root
 * - type
 */
export const standardizeTransactionReceipts = (
  receipts: SyncTransactionReceipt[],
  request: Extract<
    RequestParameters,
    { method: "eth_getBlockReceipts" | "eth_getTransactionReceipt" }
  >,
): SyncTransactionReceipt[] => {
  const receiptIds = new Set<string>();
  for (const receipt of receipts) {
    if (receiptIds.has(receipt.transactionHash)) {
      const error = new RpcProviderError(
        `Inconsistent RPC response data. The receipts array contains two objects with a 'transactionHash' of ${receipt.transactionHash}.`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    } else {
      receiptIds.add(receipt.transactionHash);
    }
    // required properties
    if (receipt.blockHash === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'receipt.blockHash' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (receipt.blockNumber === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'receipt.blockNumber' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (receipt.transactionHash === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'receipt.transactionHash' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (receipt.transactionIndex === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'receipt.transactionIndex' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (receipt.from === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'receipt.from' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (receipt.status === undefined) {
      const error = new RpcProviderError(
        "Invalid RPC response: 'receipt.status' is a required property",
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    // Note: `to` is a required property but can be coerced to `null`.
    if (receipt.to === undefined) {
      receipt.to = null;
    }
    // non-required properties
    if (receipt.logs === undefined) {
      receipt.logs = [];
    }
    if (receipt.logsBloom === undefined) {
      receipt.logsBloom = zeroLogsBloom;
    }
    if (receipt.gasUsed === undefined) {
      receipt.gasUsed = "0x0";
    }
    if (receipt.cumulativeGasUsed === undefined) {
      receipt.cumulativeGasUsed = "0x0";
    }
    if (receipt.effectiveGasPrice === undefined) {
      receipt.effectiveGasPrice = "0x0";
    }
    if (receipt.root === undefined) {
      receipt.root = zeroHash;
    }
    if (receipt.type === undefined) {
      // @ts-ignore
      receipt.type = "0x0";
    }
    if (hexToBigInt(receipt.blockNumber) > PG_BIGINT_MAX) {
      const error = new RpcProviderError(
        `Invalid RPC response: 'receipt.blockNumber' (${hexToBigInt(receipt.blockNumber)}) is larger than the maximum allowed value (${PG_BIGINT_MAX}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
    if (hexToBigInt(receipt.transactionIndex) > BigInt(PG_INTEGER_MAX)) {
      const error = new RpcProviderError(
        `Invalid RPC response: 'receipt.transactionIndex' (${hexToBigInt(receipt.transactionIndex)}) is larger than the maximum allowed value (${PG_INTEGER_MAX}).`,
      );
      error.meta = [
        "Please report this error to the RPC operator.",
        requestText(request),
      ];
      error.stack = undefined;
      throw error;
    }
  }
  return receipts;
};
function requestText(request: { method: string; params: any[] }): string {
  return `Request: ${JSON.stringify(
    {
      method: request.method,
      params: request.params,
    },
    null,
    2,
  )}`;
}
</file>

<file path="packages/core/src/rpc/http.test.ts">
import http from "node:http";
import type { AddressInfo } from "node:net";
import { context, setupCommon } from "@/_test/setup.js";
import { getChain } from "@/_test/utils.js";
import { TimeoutError } from "viem";
import { beforeEach, expect, test } from "vitest";
import { getHttpRpcClient } from "./http.js";
beforeEach(setupCommon);
test("slow body returns TimeoutError", async () => {
  const responseDelayMs = 500;
  const timeoutMs = 300;
  const server = http.createServer((_req, res) => {
    res.writeHead(200, { "Content-Type": "text/plain" });
    const start = Date.now();
    const interval = setInterval(() => {
      const elapsed = Date.now() - start;
      res.write(`data:${elapsed}\n`);
      if (elapsed >= responseDelayMs) {
        clearInterval(interval);
        res.end("done\n"); // final chunk terminates the response
      }
    }, 100);
  });
  const port = await new Promise<number>((resolve, reject) => {
    // Pass 0 to listen() for a random available port
    server.listen(0, () => {
      const port = (server.address() as AddressInfo)?.port;
      if (typeof port !== "number") {
        reject(new Error("Failed to get available port"));
      }
      resolve(port);
    });
  });
  const client = getHttpRpcClient(`http://localhost:${port}`, {
    timeout: timeoutMs,
    common: context.common,
    chain: getChain(),
  });
  await expect(
    client.request({
      body: { method: "test", id: 1, jsonrpc: "2.0", params: ["test"] },
    }),
  ).rejects.toThrow(TimeoutError);
});
</file>

<file path="packages/core/src/rpc/http.ts">
import { URL } from "node:url";
import type { Common } from "@/internal/common.js";
import type { Chain } from "@/internal/types.js";
import { HttpRequestError, RpcRequestError, TimeoutError } from "viem";
import {
  type HttpRequestParameters,
  type HttpRequestReturnType,
  type HttpRpcClientOptions,
  stringify,
} from "viem/utils";
export type RpcRequest = {
  jsonrpc?: "2.0" | undefined;
  method: string;
  params?: any | undefined;
  id?: number | undefined;
};
export type HttpRpcClient = {
  request<body extends RpcRequest>(
    params: HttpRequestParameters<body>,
  ): Promise<HttpRequestReturnType<body>>;
};
export function getHttpRpcClient(
  url: string,
  options: HttpRpcClientOptions & { common: Common; chain: Chain },
): HttpRpcClient {
  const { common, chain } = options;
  const timeoutMs = options?.timeout ?? 10_000;
  let id = 1;
  return {
    async request(params) {
      // biome-ignore lint/suspicious/noAsyncPromiseExecutor: <explanation>
      return new Promise(async (resolve, reject) => {
        let isTimeoutRejected = false;
        const { body } = params;
        const fetchOptions = {
          ...(params.fetchOptions ?? {}),
        };
        const { headers, method } = fetchOptions;
        let reader: ReadableStreamDefaultReader<Uint8Array> | undefined;
        const controller = new AbortController();
        const timeoutId = setTimeout(async () => {
          isTimeoutRejected = true;
          controller.abort();
          reject(new TimeoutError({ body, url }));
          if (reader) {
            common.logger.warn({
              msg: "JSON-RPC request timed out reading response body",
              chain: chain.name,
              chain_id: chain.id,
              hostname: new URL(url).hostname,
              // @ts-ignore
              request_id: headers ? headers["X-Request-ID"] : undefined,
              method: body.method,
              request: JSON.stringify(body),
              duration: timeoutMs,
            });
            try {
              await reader.cancel("Timeout");
            } catch {}
          }
        }, timeoutMs);
        try {
          const init: RequestInit = {
            body: stringify({
              jsonrpc: "2.0",
              id: body.id ?? id++,
              ...body,
            }),
            headers: {
              "Content-Type": "application/json",
              ...headers,
            },
            method: method || "POST",
            signal: controller.signal,
          };
          const request = new Request(url, init);
          const response = await fetch(request);
          reader = response.body?.getReader()!;
          const chunks: Uint8Array[] = [];
          let totalLength = 0;
          try {
            while (true) {
              const { done, value } = await reader.read();
              if (done) break;
              chunks.push(value);
              totalLength += value.length;
            }
          } finally {
            reader.releaseLock();
            reader = undefined;
          }
          if (isTimeoutRejected) return;
          let offset = 0;
          const fullData = new Uint8Array(totalLength);
          for (const chunk of chunks) {
            fullData.set(chunk, offset);
            offset += chunk.length;
          }
          const text = new TextDecoder().decode(fullData);
          let data: any = text;
          try {
            data = JSON.parse(data || "{}");
          } catch (err) {
            if (response.ok) throw err;
            data = { error: data };
          }
          clearTimeout(timeoutId);
          if (!response.ok) {
            reject(
              new HttpRequestError({
                body,
                details: stringify(data.error) || response.statusText,
                headers: response.headers,
                status: response.status,
                url,
              }),
            );
            return;
          }
          if (data.error) {
            reject(
              new RpcRequestError({
                body,
                error: data.error,
                url: url,
              }),
            );
          } else {
            resolve(data.result);
          }
        } catch (_error) {
          const error = _error as Error;
          clearTimeout(timeoutId);
          if (isTimeoutRejected) return;
          if (error.name === "AbortError") {
            reject(new TimeoutError({ body, url }));
          }
          if (error instanceof HttpRequestError) reject(error);
          reject(new HttpRequestError({ body, cause: error, url }));
        }
      });
    },
  };
}
</file>

<file path="packages/core/src/rpc/index.test.ts">
import { context, setupAnvil, setupCommon } from "@/_test/setup.js";
import { simulateBlock } from "@/_test/simulate.js";
import { getChain } from "@/_test/utils.js";
import { wait } from "@/utils/wait.js";
import { beforeEach, expect, test, vi } from "vitest";
import { createRpc } from "./index.js";
beforeEach(setupCommon);
beforeEach(setupAnvil);
test("createRpc()", async () => {
  const chain = getChain();
  const rpc = createRpc({
    common: context.common,
    chain,
  });
  await rpc.request({ method: "eth_blockNumber" });
});
test("createRpc() handles rate limiting", async () => {
  const chain = getChain();
  const rpc = createRpc({
    common: context.common,
    chain,
  });
  vi.spyOn(globalThis, "fetch").mockResolvedValueOnce(
    new Response(JSON.stringify({ message: "Too Many Requests" }), {
      status: 429,
      statusText: "Too Many Requests",
      headers: { "Content-Type": "application/json" },
    }),
  );
  await rpc.request({ method: "eth_blockNumber" });
});
test("createRpc() retry BlockNotFoundError", async () => {
  const chain = getChain();
  const rpc = createRpc({
    common: context.common,
    chain,
  });
  await simulateBlock();
  vi.spyOn(globalThis, "fetch").mockResolvedValueOnce(
    new Response(JSON.stringify({ jsonrpc: "2.0", result: null, id: 1 })),
  );
  const block = await rpc.request(
    { method: "eth_getBlockByNumber", params: ["0x1", true] },
    {
      retryNullBlockRequest: true,
    },
  );
  expect(block).not.toBeNull();
});
test("https://github.com/ponder-sh/ponder/pull/2143", async () => {
  const chain = getChain();
  const rpc = createRpc({
    common: context.common,
    chain,
  });
  for (let i = 0; i < 10; i++) {
    for (let j = 0; j < 20; j++) {
      await rpc.request({ method: "eth_blockNumber" });
    }
    await wait(1000);
  }
  vi.spyOn(globalThis, "fetch").mockResolvedValueOnce(
    new Response(JSON.stringify({ message: "Too Many Requests" }), {
      status: 429,
      statusText: "Too Many Requests",
      headers: { "Content-Type": "application/json" },
    }),
  );
  await rpc.request({ method: "eth_blockNumber" });
}, 15_000);
</file>

<file path="packages/core/src/rpc/index.ts">
import crypto, { type UUID } from "node:crypto";
import url from "node:url";
import type { Common } from "@/internal/common.js";
import type { Logger } from "@/internal/logger.js";
import type { Chain, SyncBlock, SyncBlockHeader } from "@/internal/types.js";
import { eth_getBlockByNumber, standardizeBlock } from "@/rpc/actions.js";
import { createQueue } from "@/utils/queue.js";
import { startClock } from "@/utils/timer.js";
import { wait } from "@/utils/wait.js";
import {
  type GetLogsRetryHelperParameters,
  getLogsRetryHelper,
} from "@ponder/utils";
import {
  BlockNotFoundError,
  type EIP1193Parameters,
  type EIP1193RequestFn,
  type Hash,
  HttpRequestError,
  JsonRpcVersionUnsupportedError,
  MethodNotFoundRpcError,
  MethodNotSupportedRpcError,
  ParseRpcError,
  type PublicRpcSchema,
  type RpcError,
  type RpcTransactionReceipt,
  TimeoutError,
  custom,
  hexToNumber,
  isHex,
  webSocket,
} from "viem";
import { WebSocket } from "ws";
import type { DebugRpcSchema } from "../utils/debug.js";
import { getHttpRpcClient } from "./http.js";
export type RpcSchema = [
  ...PublicRpcSchema,
  ...DebugRpcSchema,
  /**
   * @description Returns the receipts of a block specified by hash
   *
   * @example
   * provider.request({ method: 'eth_getBlockReceipts', params: ['0x...'] })
   * // => [{ ... }, { ... }]
   */
  {
    Method: "eth_getBlockReceipts";
    Parameters: [hash: Hash];
    ReturnType: RpcTransactionReceipt[] | null;
  },
];
export type RequestParameters = EIP1193Parameters<RpcSchema>;
export type RequestReturnType<
  method extends EIP1193Parameters<RpcSchema>["method"],
> = Extract<RpcSchema[number], { Method: method }>["ReturnType"];
export type Rpc = {
  hostnames: string[];
  request: <TParameters extends RequestParameters>(
    parameters: TParameters,
    context?: { logger?: Logger; retryNullBlockRequest?: boolean },
  ) => Promise<RequestReturnType<TParameters["method"]>>;
  subscribe: (params: {
    onBlock: (block: SyncBlock | SyncBlockHeader) => Promise<boolean>;
    onError: (error: Error) => void;
  }) => void;
  unsubscribe: () => Promise<void>;
};
const RETRY_COUNT = 9;
const BASE_DURATION = 125;
const INITIAL_REACTIVATION_DELAY = 100;
const MAX_REACTIVATION_DELAY = 5_000;
const BACKOFF_FACTOR = 1.5;
const LATENCY_WINDOW_SIZE = 500;
/** Hurdle rate for switching to a faster bucket. */
const LATENCY_HURDLE_RATE = 0.1;
/** Exploration rate. */
const EPSILON = 0.1;
const INITIAL_MAX_RPS = 20;
const MIN_RPS = 3;
const MAX_RPS = 500;
const RPS_INCREASE_FACTOR = 1.05;
const RPS_DECREASE_FACTOR = 0.95;
const RPS_INCREASE_QUALIFIER = 0.9;
const SUCCESS_MULTIPLIER = 5;
type Bucket = {
  index: number;
  hostname: string;
  /** Reactivation delay in milliseconds. */
  reactivationDelay: number;
  /** Number of active connections. */
  activeConnections: number;
  /** Is the bucket available to send requests. */
  isActive: boolean;
  /** Is the bucket recently activated and yet to complete successful requests. */
  isWarmingUp: boolean;
  latencyMetadata: {
    latencies: { value: number; success: boolean }[];
    successfulLatencies: number;
    latencySum: number;
  };
  expectedLatency: number;
  rps: { count: number; timestamp: number }[];
  /** Number of consecutive successful requests. */
  consecutiveSuccessfulRequests: number;
  /** Maximum requests per second (dynamic). */
  rpsLimit: number;
  request: EIP1193RequestFn<RpcSchema>;
};
const addLatency = (bucket: Bucket, latency: number, success: boolean) => {
  bucket.latencyMetadata.latencies.push({ value: latency, success });
  bucket.latencyMetadata.latencySum += latency;
  if (success) {
    bucket.latencyMetadata.successfulLatencies++;
  }
  if (bucket.latencyMetadata.latencies.length > LATENCY_WINDOW_SIZE) {
    const record = bucket.latencyMetadata.latencies.shift()!;
    bucket.latencyMetadata.latencySum -= record.value;
    if (record.success) {
      bucket.latencyMetadata.successfulLatencies--;
    }
  }
  bucket.expectedLatency =
    bucket.latencyMetadata.latencySum /
    bucket.latencyMetadata.successfulLatencies;
};
/**
 * Return `true` if the bucket is available to send a request.
 */
const isAvailable = (bucket: Bucket) => {
  if (bucket.isActive === false) return false;
  const now = Math.floor(Date.now() / 1000);
  const currentRPS = bucket.rps.find((r) => r.timestamp === now);
  if (currentRPS && currentRPS.count + 1 > bucket.rpsLimit) {
    return false;
  }
  if (bucket.rps.length > 0 && bucket.rps[0]!.timestamp < now) {
    const elapsed = now - bucket.rps[0]!.timestamp;
    const totalCount = bucket.rps.reduce((acc, rps) => acc + rps.count, 0);
    if (totalCount > bucket.rpsLimit * (1 + elapsed)) {
      return false;
    }
  }
  if (bucket.isWarmingUp && bucket.activeConnections > 3) {
    return false;
  }
  return true;
};
export const createRpc = ({
  common,
  chain,
  concurrency = 25,
}: { common: Common; chain: Chain; concurrency?: number }): Rpc => {
  let backends: { request: EIP1193RequestFn<RpcSchema>; hostname: string }[];
  let requestId: UUID | undefined;
  if (typeof chain.rpc === "string") {
    const protocol = new url.URL(chain.rpc).protocol;
    const hostname = new url.URL(chain.rpc).hostname;
    if (protocol === "https:" || protocol === "http:") {
      const httpRpcClient = getHttpRpcClient(chain.rpc, {
        common,
        chain,
        timeout: 10_000,
      });
      backends = [
        {
          request: custom({
            request(body) {
              if (requestId) {
                return httpRpcClient.request({
                  body,
                  fetchOptions: { headers: { "X-Request-ID": requestId } },
                });
              }
              return httpRpcClient.request({ body });
            },
          })({
            chain: chain.viemChain,
            retryCount: 0,
          }).request,
          hostname,
        },
      ];
    } else if (protocol === "wss:" || protocol === "ws:") {
      backends = [
        {
          request: webSocket(chain.rpc)({
            chain: chain.viemChain,
            retryCount: 0,
            timeout: 10_000,
          }).request,
          hostname,
        },
      ];
    } else {
      throw new Error(`Unsupported RPC URL protocol: ${protocol}`);
    }
  } else if (Array.isArray(chain.rpc)) {
    backends = chain.rpc.map((rpc) => {
      const protocol = new url.URL(rpc).protocol;
      const hostname = new url.URL(chain.rpc).hostname;
      if (protocol === "https:" || protocol === "http:") {
        const httpRpcClient = getHttpRpcClient(rpc, {
          common,
          chain,
          timeout: 10_000,
        });
        return {
          request: custom({
            request(body) {
              if (requestId) {
                return httpRpcClient.request({
                  body,
                  fetchOptions: { headers: { "X-Request-ID": requestId } },
                });
              }
              return httpRpcClient.request({ body });
            },
          })({
            chain: chain.viemChain,
            retryCount: 0,
          }).request,
          hostname,
        };
      } else if (protocol === "wss:" || protocol === "ws:") {
        return {
          request: webSocket(rpc)({
            chain: chain.viemChain,
            retryCount: 0,
            timeout: 10_000,
          }).request,
          hostname,
        };
      } else {
        throw new Error(`Unsupported RPC URL protocol: ${protocol}`);
      }
    });
  } else {
    backends = [
      {
        request: chain.rpc({
          chain: chain.viemChain,
          retryCount: 0,
          timeout: 10_000,
        }).request,
        hostname: "custom_transport",
      },
    ];
  }
  if (typeof chain.ws === "string") {
    const protocol = new url.URL(chain.ws).protocol;
    if (protocol !== "wss:" && protocol !== "ws:") {
      throw new Error(
        `Inconsistent RPC URL protocol: ${protocol}. Expected wss or ws.`,
      );
    }
  }
  const buckets: Bucket[] = backends.map(
    ({ request, hostname }, index) =>
      ({
        index,
        hostname,
        reactivationDelay: INITIAL_REACTIVATION_DELAY,
        activeConnections: 0,
        isActive: true,
        isWarmingUp: false,
        latencyMetadata: {
          latencies: [],
          successfulLatencies: 0,
          latencySum: 0,
        },
        expectedLatency: 200,
        rps: [],
        consecutiveSuccessfulRequests: 0,
        rpsLimit: INITIAL_MAX_RPS,
        request,
      }) satisfies Bucket,
  );
  let noAvailableBucketsTimer: NodeJS.Timeout | undefined;
  /** Tracks all active bucket reactivation timeouts to cleanup during shutdown */
  const timeouts = new Set<NodeJS.Timeout>();
  const scheduleBucketActivation = (bucket: Bucket) => {
    const delay = bucket.reactivationDelay;
    const timeoutId = setTimeout(() => {
      bucket.isActive = true;
      bucket.isWarmingUp = true;
      timeouts.delete(timeoutId);
      common.logger.debug({
        msg: "JSON-RPC provider reactivated after rate limiting",
        chain: chain.name,
        chain_id: chain.id,
        hostname: bucket.hostname,
        retry_delay: Math.round(delay),
      });
    }, delay);
    common.logger.debug({
      msg: "JSON-RPC provider deactivated due to rate limiting",
      chain: chain.name,
      chain_id: chain.id,
      hostname: bucket.hostname,
      retry_delay: Math.round(delay),
    });
    timeouts.add(timeoutId);
  };
  const getBucket = async (): Promise<Bucket> => {
    let availableBuckets: Bucket[];
    // Note: wait for the next event loop to ensure that the bucket rps are updated
    await new Promise((resolve) => setImmediate(resolve));
    while (true) {
      // Remove old request per second data
      const timestamp = Math.floor(Date.now() / 1000);
      for (const bucket of buckets) {
        bucket.rps = bucket.rps.filter((r) => r.timestamp > timestamp - 10);
      }
      availableBuckets = buckets.filter(isAvailable);
      if (availableBuckets.length > 0) {
        break;
      }
      if (noAvailableBucketsTimer === undefined) {
        noAvailableBucketsTimer = setTimeout(() => {
          common.logger.warn({
            msg: "All JSON-RPC providers are inactive due to rate limiting",
            chain: chain.name,
            chain_id: chain.id,
            rate_limits: JSON.stringify(buckets.map((b) => b.rpsLimit)),
          });
        }, 5_000);
      }
      await wait(20);
    }
    clearTimeout(noAvailableBucketsTimer);
    noAvailableBucketsTimer = undefined;
    if (Math.random() < EPSILON) {
      const randomBucket =
        availableBuckets[Math.floor(Math.random() * availableBuckets.length)]!;
      randomBucket.activeConnections++;
      return randomBucket;
    }
    const fastestBucket = availableBuckets.reduce((fastest, current) => {
      const currentLatency = current.expectedLatency;
      const fastestLatency = fastest.expectedLatency;
      if (currentLatency < fastestLatency * (1 - LATENCY_HURDLE_RATE)) {
        return current;
      }
      if (
        currentLatency <= fastestLatency &&
        current.activeConnections < fastest.activeConnections
      ) {
        return current;
      }
      return fastest;
    }, availableBuckets[0]!);
    fastestBucket.activeConnections++;
    return fastestBucket;
  };
  const increaseMaxRPS = (bucket: Bucket) => {
    if (bucket.rps.length < 10) return;
    if (
      bucket.consecutiveSuccessfulRequests <
      bucket.rpsLimit * SUCCESS_MULTIPLIER
    ) {
      return;
    }
    for (const { count } of bucket.rps) {
      if (count < bucket.rpsLimit * RPS_INCREASE_QUALIFIER) {
        return;
      }
    }
    bucket.rpsLimit = Math.min(bucket.rpsLimit * RPS_INCREASE_FACTOR, MAX_RPS);
    bucket.consecutiveSuccessfulRequests = 0;
    common.logger.debug({
      msg: "Increased JSON-RPC provider RPS limit",
      chain: chain.name,
      chain_id: chain.id,
      hostname: bucket.hostname,
      rps_limit: Math.floor(bucket.rpsLimit),
    });
  };
  const queue = createQueue<
    Awaited<ReturnType<Rpc["request"]>>,
    {
      body: Parameters<Rpc["request"]>[0];
      context?: Parameters<Rpc["request"]>[1];
    }
  >({
    initialStart: true,
    concurrency,
    worker: async ({ body, context }) => {
      const logger = context?.logger ?? common.logger;
      for (let i = 0; i <= RETRY_COUNT; i++) {
        let endClock = startClock();
        const t = setTimeout(() => {
          logger.warn({
            msg: "Unable to find available JSON-RPC provider within expected time",
            chain: chain.name,
            chain_id: chain.id,
            rate_limit: JSON.stringify(buckets.map((b) => b.rpsLimit)),
            is_active: JSON.stringify(buckets.map((b) => b.isActive)),
            is_warming_up: JSON.stringify(buckets.map((b) => b.isWarmingUp)),
            duration: 15_000,
          });
        }, 15_000);
        const bucket = await getBucket();
        clearTimeout(t);
        const getBucketDuration = endClock();
        endClock = startClock();
        const id = crypto.randomUUID();
        const surpassTimeout = setTimeout(() => {
          logger.warn({
            msg: "JSON-RPC request unexpectedly surpassed timeout",
            chain: chain.name,
            chain_id: chain.id,
            hostname: bucket.hostname,
            request_id: id,
            method: body.method,
            duration: 15_000,
          });
        }, 15_000);
        try {
          logger.trace({
            msg: "Sent JSON-RPC request",
            chain: chain.name,
            chain_id: chain.id,
            hostname: bucket.hostname,
            request_id: id,
            method: body.method,
            duration: getBucketDuration,
          });
          // Add request per second data
          const timestamp = Math.floor(Date.now() / 1000);
          if (
            bucket.rps.length === 0 ||
            bucket.rps[bucket.rps.length - 1]!.timestamp < timestamp
          ) {
            bucket.rps.push({ count: 1, timestamp });
          } else {
            bucket.rps[bucket.rps.length - 1]!.count++;
          }
          requestId = id;
          const response = await bucket.request(body);
          if (response === undefined) {
            throw new Error("Response is undefined");
          }
          if (
            response === null &&
            (body.method === "eth_getBlockByNumber" ||
              body.method === "eth_getBlockByHash") &&
            context?.retryNullBlockRequest === true
          ) {
            // Note: Throwing this error will cause the request to be retried.
            throw new BlockNotFoundError({
              // @ts-ignore
              blockNumber: body.params[0],
            });
          }
          const duration = endClock();
          logger.trace({
            msg: "Received JSON-RPC response",
            chain: chain.name,
            chain_id: chain.id,
            hostname: bucket.hostname,
            request_id: id,
            method: body.method,
            duration,
          });
          common.metrics.ponder_rpc_request_duration.observe(
            { method: body.method, chain: chain.name },
            duration,
          );
          addLatency(bucket, duration, true);
          bucket.consecutiveSuccessfulRequests++;
          increaseMaxRPS(bucket);
          bucket.isWarmingUp = false;
          bucket.reactivationDelay = INITIAL_REACTIVATION_DELAY;
          return response as RequestReturnType<typeof body.method>;
        } catch (e) {
          const error = e as Error;
          common.metrics.ponder_rpc_request_error_total.inc(
            { method: body.method, chain: chain.name },
            1,
          );
          if (
            body.method === "eth_getLogs" &&
            isHex(body.params[0].fromBlock) &&
            isHex(body.params[0].toBlock) &&
            chain.ethGetLogsBlockRange === undefined
          ) {
            const getLogsErrorResponse = getLogsRetryHelper({
              params: body.params as GetLogsRetryHelperParameters["params"],
              error: error as RpcError,
            });
            if (getLogsErrorResponse.shouldRetry) {
              common.logger.trace({
                msg: "Caught eth_getLogs range error",
                chain: chain.name,
                chain_id: chain.id,
                hostname: bucket.hostname,
                request_id: id,
                method: body.method,
                request: JSON.stringify(body),
                retry_ranges: JSON.stringify(getLogsErrorResponse.ranges),
                error: error as Error,
              });
              throw error;
            } else {
              // @ts-ignore
              error.meta = [
                "Tip: Use the ethGetLogsBlockRange option to override the default behavior for this chain",
              ];
            }
          }
          addLatency(bucket, endClock(), false);
          if (
            // @ts-ignore
            error.code === 429 ||
            // @ts-ignore
            error.status === 429 ||
            error instanceof TimeoutError
          ) {
            if (bucket.isActive) {
              bucket.isActive = false;
              bucket.isWarmingUp = false;
              bucket.rpsLimit = Math.max(
                bucket.rpsLimit * RPS_DECREASE_FACTOR,
                MIN_RPS,
              );
              bucket.consecutiveSuccessfulRequests = 0;
              common.logger.debug({
                msg: "JSON-RPC provider rate limited",
                chain: chain.name,
                chain_id: chain.id,
                hostname: bucket.hostname,
                rps_limit: Math.floor(bucket.rpsLimit),
              });
              scheduleBucketActivation(bucket);
              if (buckets.every((b) => b.isActive === false)) {
                logger.warn({
                  msg: "All JSON-RPC providers are inactive",
                  chain: chain.name,
                  chain_id: chain.id,
                });
              }
              bucket.reactivationDelay =
                error instanceof TimeoutError
                  ? INITIAL_REACTIVATION_DELAY
                  : Math.min(
                      bucket.reactivationDelay * BACKOFF_FACTOR,
                      MAX_REACTIVATION_DELAY,
                    );
            }
          }
          if (shouldRetry(error) === false) {
            logger.warn({
              msg: "Received JSON-RPC error",
              chain: chain.name,
              chain_id: chain.id,
              hostname: bucket.hostname,
              request_id: id,
              method: body.method,
              request: JSON.stringify(body),
              duration: endClock(),
              error,
            });
            throw error;
          }
          if (i === RETRY_COUNT) {
            logger.warn({
              msg: "Received JSON-RPC error",
              chain: chain.name,
              chain_id: chain.id,
              hostname: bucket.hostname,
              request_id: id,
              method: body.method,
              request: JSON.stringify(body),
              duration: endClock(),
              retry_count: i + 1,
              error,
            });
            throw error;
          }
          const duration = BASE_DURATION * 2 ** i;
          logger.warn({
            msg: "Received JSON-RPC error",
            chain: chain.name,
            chain_id: chain.id,
            hostname: bucket.hostname,
            request_id: id,
            method: body.method,
            request: JSON.stringify(body),
            duration: endClock(),
            retry_count: i + 1,
            retry_delay: duration,
            error,
          });
          await wait(duration);
        } finally {
          bucket.activeConnections--;
          clearTimeout(surpassTimeout);
        }
      }
      throw "unreachable";
    },
  });
  let ws: WebSocket | undefined;
  let isUnsubscribed = false;
  let subscriptionId: string | undefined;
  let webSocketErrorCount = 0;
  let interval: NodeJS.Timeout | undefined;
  const rpc: Rpc = {
    hostnames: backends.map((backend) => backend.hostname),
    // @ts-ignore
    request: (parameters, context) => queue.add({ body: parameters, context }),
    subscribe({ onBlock, onError }) {
      (async () => {
        while (true) {
          if (isUnsubscribed) return;
          if (chain.ws === undefined || webSocketErrorCount >= RETRY_COUNT) {
            common.logger.debug({
              msg: "Created JSON-RPC polling subscription",
              chain: chain.name,
              chain_id: chain.id,
              polling_interval: chain.pollingInterval,
            });
            interval = setInterval(async () => {
              try {
                const block = await eth_getBlockByNumber(rpc, ["latest", true]);
                common.logger.trace({
                  msg: "Received successful JSON-RPC polling response",
                  chain: chain.name,
                  chain_id: chain.id,
                  block_number: hexToNumber(block.number),
                  block_hash: block.hash,
                });
                // Note: `onBlock` should never throw.
                await onBlock(block);
              } catch (error) {
                onError(error as Error);
              }
            }, chain.pollingInterval);
            common.shutdown.add(() => {
              clearInterval(interval);
            });
            return;
          }
          await new Promise<void>((resolve) => {
            ws = new WebSocket(chain.ws!);
            ws.on("open", () => {
              common.logger.debug({
                msg: "Created JSON-RPC WebSocket connection",
                chain: chain.name,
                chain_id: chain.id,
              });
              const subscriptionRequest = {
                jsonrpc: "2.0",
                id: 1,
                method: "eth_subscribe",
                params: ["newHeads"],
              };
              ws?.send(JSON.stringify(subscriptionRequest));
            });
            ws.on("message", (data: Buffer) => {
              try {
                const msg = JSON.parse(data.toString());
                if (
                  msg.method === "eth_subscription" &&
                  msg.params.subscription === subscriptionId
                ) {
                  common.logger.trace({
                    msg: "Received successful JSON-RPC WebSocket subscription data",
                    chain: chain.name,
                    chain_id: chain.id,
                    block_number: msg.params.result.number
                      ? hexToNumber(msg.params.result.number)
                      : undefined,
                    block_hash: msg.params.result.hash,
                  });
                  webSocketErrorCount = 0;
                  onBlock(
                    standardizeBlock(msg.params.result, {
                      method: "eth_subscribe",
                      params: ["newHeads"],
                    }),
                  );
                } else if (msg.result) {
                  common.logger.debug({
                    msg: "Created JSON-RPC WebSocket subscription",
                    chain: chain.name,
                    chain_id: chain.id,
                    request: JSON.stringify({
                      method: "eth_subscribe",
                      params: ["newHeads"],
                    }),
                    subscription: msg.result,
                  });
                  subscriptionId = msg.result;
                } else if (msg.error) {
                  common.logger.warn({
                    msg: "Failed JSON-RPC WebSocket subscription",
                    chain: chain.name,
                    chain_id: chain.id,
                    request: JSON.stringify({
                      method: "eth_subscribe",
                      params: ["newHeads"],
                    }),
                    retry_count: webSocketErrorCount + 1,
                    error: msg.error as Error,
                  });
                  if (webSocketErrorCount < RETRY_COUNT) {
                    webSocketErrorCount += 1;
                  }
                  ws?.close();
                } else {
                  common.logger.warn({
                    msg: "Received unrecognized JSON-RPC WebSocket message",
                    chain: chain.name,
                    websocket_message: msg,
                  });
                }
              } catch (error) {
                common.logger.warn({
                  msg: "Failed JSON-RPC WebSocket subscription",
                  chain: chain.name,
                  chain_id: chain.id,
                  request: JSON.stringify({
                    method: "eth_subscribe",
                    params: ["newHeads"],
                  }),
                  retry_count: webSocketErrorCount + 1,
                  error: error as Error,
                });
                if (webSocketErrorCount < RETRY_COUNT) {
                  webSocketErrorCount += 1;
                }
                ws?.close();
              }
            });
            ws.on("error", async (error) => {
              common.logger.warn({
                msg: "Failed JSON-RPC WebSocket subscription",
                chain: chain.name,
                chain_id: chain.id,
                request: JSON.stringify({
                  method: "eth_subscribe",
                  params: ["newHeads"],
                }),
                retry_count: webSocketErrorCount + 1,
                error: error as Error,
              });
              if (webSocketErrorCount < RETRY_COUNT) {
                webSocketErrorCount += 1;
              }
              if (ws && ws.readyState === ws.OPEN) {
                ws.close();
              } else {
                resolve();
              }
            });
            ws.on("close", async () => {
              common.logger.debug({
                msg: "Closed JSON-RPC WebSocket connection",
                chain: chain.name,
                chain_id: chain.id,
              });
              ws = undefined;
              if (isUnsubscribed || webSocketErrorCount >= RETRY_COUNT) {
                resolve();
              } else {
                const duration = BASE_DURATION * 2 ** webSocketErrorCount;
                common.logger.debug({
                  msg: "Retrying JSON-RPC WebSocket connection",
                  chain: chain.name,
                  retry_count: webSocketErrorCount + 1,
                  retry_delay: duration,
                });
                await wait(duration);
                resolve();
              }
            });
          });
        }
      })();
    },
    async unsubscribe() {
      clearInterval(interval);
      isUnsubscribed = true;
      if (ws) {
        if (subscriptionId) {
          const unsubscribeRequest = {
            jsonrpc: "2.0",
            id: 1,
            method: "eth_unsubscribe",
            params: [subscriptionId],
          };
          common.logger.debug({
            msg: "Ended JSON-RPC WebSocket subscription",
            chain: chain.name,
            chain_id: chain.id,
            request: JSON.stringify({
              method: "eth_unsubscribe",
              params: [subscriptionId],
            }),
          });
          ws.send(JSON.stringify(unsubscribeRequest));
        }
        ws.close();
      }
    },
  };
  common.shutdown.add(() => {
    for (const timeoutId of timeouts) {
      clearTimeout(timeoutId);
    }
    timeouts.clear();
  });
  return rpc;
};
/**
 * @link https://github.com/wevm/viem/blob/main/src/utils/buildtask.ts#L192
 */
function shouldRetry(error: Error) {
  if ("code" in error && typeof error.code === "number") {
    // Invalid JSON
    if (error.code === ParseRpcError.code) return false;
    // Method does not exist
    if (error.code === MethodNotFoundRpcError.code) return false;
    // Method is not implemented
    if (error.code === MethodNotSupportedRpcError.code) return false;
    // Version of JSON-RPC protocol is not supported
    if (error.code === JsonRpcVersionUnsupportedError.code) return false;
    // eth_call reverted
    if (error.message.includes("revert")) return false;
  }
  if (error instanceof HttpRequestError && error.status) {
    // Method Not Allowed
    if (error.status === 405) return false;
    // Not Found
    if (error.status === 404) return false;
    // Not Implemented
    if (error.status === 501) return false;
    // HTTP Version Not Supported
    if (error.status === 505) return false;
  }
  return true;
}
</file>

<file path="packages/core/src/runtime/events.test.ts">
import { ALICE, BOB } from "@/_test/constants.js";
import { erc20ABI } from "@/_test/generated.js";
import { context, setupCommon } from "@/_test/setup.js";
import {
  getAccountsIndexingBuild,
  getBlocksIndexingBuild,
  getChain,
  getErc20IndexingBuild,
} from "@/_test/utils.js";
import type {
  BlockEvent,
  Event,
  LogEvent,
  RawEvent,
  TraceEvent,
  TransferEvent,
} from "@/internal/types.js";
import { ZERO_CHECKPOINT_STRING } from "@/utils/checkpoint.js";
import {
  type Hex,
  encodeEventTopics,
  padHex,
  parseEther,
  toHex,
  zeroAddress,
} from "viem";
import { encodeFunctionData, encodeFunctionResult } from "viem/utils";
import { beforeEach, expect, test } from "vitest";
import { decodeEvents, splitEvents } from "./events.js";
beforeEach(setupCommon);
test("splitEvents()", async () => {
  const events = [
    {
      chain: { id: 1 },
      checkpoint: "0",
      event: {
        block: {
          hash: "0x1",
          timestamp: 1,
          number: 1n,
        },
      },
    },
    {
      chain: { id: 1 },
      checkpoint: "0",
      event: {
        block: {
          hash: "0x2",
          timestamp: 2,
          number: 2n,
        },
      },
    },
  ] as unknown as Event[];
  const result = splitEvents(events);
  expect(result).toMatchInlineSnapshot(`
    [
      {
        "chainId": 1,
        "checkpoint": "000000000100000000000000010000000000000001999999999999999999999999999999999",
        "events": [
          {
            "chain": {
              "id": 1,
            },
            "checkpoint": "0",
            "event": {
              "block": {
                "hash": "0x1",
                "number": 1n,
                "timestamp": 1,
              },
            },
          },
        ],
      },
      {
        "chainId": 1,
        "checkpoint": "000000000200000000000000010000000000000002999999999999999999999999999999999",
        "events": [
          {
            "chain": {
              "id": 1,
            },
            "checkpoint": "0",
            "event": {
              "block": {
                "hash": "0x2",
                "number": 2n,
                "timestamp": 2,
              },
            },
          },
        ],
      },
    ]
  `);
});
test("decodeEvents() log", async () => {
  const { common } = context;
  const { eventCallbacks } = getErc20IndexingBuild({
    address: zeroAddress,
  });
  const topics = encodeEventTopics({
    abi: erc20ABI,
    eventName: "Transfer",
    args: {
      from: zeroAddress,
      to: ALICE,
    },
  });
  const data = padHex(toHex(parseEther("1")), { size: 32 });
  const rawEvent = {
    chainId: 1,
    eventCallbackIndex: 0,
    checkpoint: ZERO_CHECKPOINT_STRING,
    block: {} as RawEvent["block"],
    transaction: {} as RawEvent["transaction"],
    log: { data, topics },
  } as RawEvent;
  const events = decodeEvents(common, getChain(), eventCallbacks, [
    rawEvent,
  ]) as [LogEvent];
  expect(events).toHaveLength(1);
  expect(events[0].event.args).toMatchObject({
    from: zeroAddress,
    to: ALICE.toLowerCase(),
    amount: parseEther("1"),
  });
});
test("decodeEvents() log error", async () => {
  const { common } = context;
  const { eventCallbacks } = getErc20IndexingBuild({
    address: zeroAddress,
  });
  const topics = encodeEventTopics({
    abi: erc20ABI,
    eventName: "Transfer",
    args: {
      from: zeroAddress,
      to: ALICE,
    },
  });
  // invalid log.data, causing an error when decoding
  const rawEvent = {
    chainId: 1,
    eventCallbackIndex: 0,
    checkpoint: ZERO_CHECKPOINT_STRING,
    block: {} as RawEvent["block"],
    transaction: {} as RawEvent["transaction"],
    log: {
      data: "0x0" as Hex,
      topics,
    },
  } as RawEvent;
  const events = decodeEvents(common, getChain(), eventCallbacks, [
    rawEvent,
  ]) as [LogEvent];
  expect(events).toHaveLength(0);
});
test("decodeEvents() block", async () => {
  const { common } = context;
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const rawEvent = {
    chainId: 1,
    eventCallbackIndex: 0,
    checkpoint: ZERO_CHECKPOINT_STRING,
    block: {
      number: 1n,
    } as RawEvent["block"],
    transaction: undefined,
    log: undefined,
  } as RawEvent;
  const events = decodeEvents(common, getChain(), eventCallbacks, [
    rawEvent,
  ]) as [BlockEvent];
  expect(events).toHaveLength(1);
  expect(events[0].event.block).toMatchObject({
    number: 1n,
  });
});
test("decodeEvents() transfer", async () => {
  const { common } = context;
  const { eventCallbacks } = getAccountsIndexingBuild({
    address: ALICE,
  });
  const rawEvent = {
    chainId: 1,
    eventCallbackIndex: 3,
    checkpoint: ZERO_CHECKPOINT_STRING,
    block: {} as RawEvent["block"],
    transaction: {} as RawEvent["transaction"],
    log: undefined,
    trace: {
      type: "CALL",
      from: ALICE,
      to: BOB,
      gas: 0n,
      gasUsed: 0n,
      input: "0x0",
      output: "0x0",
      value: parseEther("1"),
      traceIndex: 0,
      subcalls: 0,
      blockNumber: 0,
      transactionIndex: 0,
    },
  } as RawEvent;
  const events = decodeEvents(common, getChain(), eventCallbacks, [
    rawEvent,
  ]) as [TransferEvent];
  expect(events).toHaveLength(1);
  expect(events[0].event.transfer).toMatchObject({
    from: ALICE,
    to: BOB,
    value: parseEther("1"),
  });
});
test("decodeEvents() transaction", async () => {
  const { common } = context;
  const { eventCallbacks } = getAccountsIndexingBuild({
    address: ALICE,
  });
  const rawEvent = {
    chainId: 1,
    eventCallbackIndex: 0,
    checkpoint: ZERO_CHECKPOINT_STRING,
    block: {} as RawEvent["block"],
    transaction: {} as RawEvent["transaction"],
    log: undefined,
    trace: undefined,
  } as RawEvent;
  const events = decodeEvents(common, getChain(), eventCallbacks, [
    rawEvent,
  ]) as [TransferEvent];
  expect(events).toHaveLength(1);
});
test("decodeEvents() trace", async () => {
  const { common } = context;
  const { eventCallbacks } = getErc20IndexingBuild({
    address: zeroAddress,
    includeCallTraces: true,
  });
  const rawEvent = {
    chainId: 1,
    eventCallbackIndex: 0,
    checkpoint: ZERO_CHECKPOINT_STRING,
    block: {} as RawEvent["block"],
    transaction: {} as RawEvent["transaction"],
    log: undefined,
    trace: {
      type: "CALL",
      from: ALICE,
      to: BOB,
      input: encodeFunctionData({
        abi: erc20ABI,
        functionName: "transfer",
        args: [BOB, parseEther("1")],
      }),
      output: encodeFunctionResult({
        abi: erc20ABI,
        functionName: "transfer",
        result: true,
      }),
      gas: 0n,
      gasUsed: 0n,
      value: 0n,
      traceIndex: 0,
      subcalls: 0,
      blockNumber: 0,
      transactionIndex: 0,
    },
  } as RawEvent;
  const events = decodeEvents(common, getChain(), eventCallbacks, [
    rawEvent,
  ]) as [TraceEvent];
  expect(events).toHaveLength(1);
  expect(events[0].event.args).toStrictEqual([BOB, parseEther("1")]);
  expect(events[0].event.result).toBe(true);
});
test("decodeEvents() trace w/o output", async () => {
  const { common } = context;
  const { eventCallbacks } = getErc20IndexingBuild({
    address: zeroAddress,
    includeCallTraces: true,
  });
  // Remove output from the trace abi
  // @ts-ignore
  eventCallbacks[0].abiItem.outputs = [];
  const rawEvent = {
    chainId: 1,
    eventCallbackIndex: 0,
    checkpoint: ZERO_CHECKPOINT_STRING,
    block: {} as RawEvent["block"],
    transaction: {} as RawEvent["transaction"],
    log: undefined,
    trace: {
      type: "CALL",
      from: ALICE,
      to: BOB,
      input: encodeFunctionData({
        abi: erc20ABI,
        functionName: "transfer",
        args: [BOB, parseEther("1")],
      }),
      output: undefined,
      gas: 0n,
      gasUsed: 0n,
      value: 0n,
      traceIndex: 0,
      subcalls: 0,
      blockNumber: 0,
      transactionIndex: 0,
    },
  } as RawEvent;
  const events = decodeEvents(common, getChain(), eventCallbacks, [
    rawEvent,
  ]) as [TraceEvent];
  expect(events).toHaveLength(1);
  expect(events[0].event.args).toStrictEqual([BOB, parseEther("1")]);
  expect(events[0].event.result).toBe(undefined);
});
test("decodeEvents() trace error", async () => {
  const { common } = context;
  const { eventCallbacks } = getErc20IndexingBuild({
    address: zeroAddress,
    includeCallTraces: true,
  });
  const rawEvent = {
    chainId: 1,
    eventCallbackIndex: 0,
    checkpoint: ZERO_CHECKPOINT_STRING,
    block: {} as RawEvent["block"],
    transaction: {} as RawEvent["transaction"],
    log: undefined,
    trace: {
      type: "CALL",
      from: ALICE,
      to: BOB,
      input: "0x",
      output: encodeFunctionResult({
        abi: erc20ABI,
        functionName: "transfer",
        result: true,
      }),
      gas: 0n,
      gasUsed: 0n,
      value: 0n,
      traceIndex: 0,
      subcalls: 0,
      blockNumber: 0,
      transactionIndex: 0,
    },
  } as RawEvent;
  const events = decodeEvents(common, getChain(), eventCallbacks, [
    rawEvent,
  ]) as [TraceEvent];
  expect(events).toHaveLength(0);
});
</file>

<file path="packages/core/src/runtime/events.ts">
import type { Common } from "@/internal/common.js";
import type {
  BlockFilter,
  Chain,
  Event,
  EventCallback,
  FactoryId,
  InternalBlock,
  InternalLog,
  InternalTrace,
  InternalTransaction,
  InternalTransactionReceipt,
  LogFilter,
  RawEvent,
  SyncBlock,
  SyncBlockHeader,
  SyncLog,
  SyncTrace,
  SyncTransaction,
  SyncTransactionReceipt,
  TraceFilter,
  TransactionFilter,
  TransferFilter,
} from "@/internal/types.js";
import type {
  Block,
  Trace,
  Transaction,
  TransactionReceipt,
} from "@/types/eth.js";
import {
  EVENT_TYPES,
  MAX_CHECKPOINT,
  ZERO_CHECKPOINT,
  encodeCheckpoint,
} from "@/utils/checkpoint.js";
import { decodeEventLog } from "@/utils/decodeEventLog.js";
import { toLowerCase } from "@/utils/lowercase.js";
import {
  type AbiEvent,
  type AbiFunction,
  type Address,
  type Hash,
  type Hex,
  decodeFunctionData,
  decodeFunctionResult,
  hexToBigInt,
  hexToNumber,
  toEventSelector,
  toFunctionSelector,
} from "viem";
import {
  isAddressMatched,
  isBlockFilterMatched,
  isLogFilterMatched,
  isTraceFilterMatched,
  isTransactionFilterMatched,
  isTransferFilterMatched,
} from "./filter.js";
import { isAddressFactory } from "./filter.js";
/**
 * Create `RawEvent`s from raw data types
 */
export const buildEvents = ({
  eventCallbacks,
  blocks,
  logs,
  transactions,
  transactionReceipts,
  traces,
  childAddresses,
  chainId,
}: {
  eventCallbacks: EventCallback[];
  blocks: InternalBlock[];
  logs: InternalLog[];
  transactions: InternalTransaction[];
  transactionReceipts: InternalTransactionReceipt[];
  traces: InternalTrace[];
  childAddresses: Map<FactoryId, Map<Address, number>>;
  chainId: number;
}) => {
  const events: RawEvent[] = [];
  const blockEventCallbackIndexes: number[] = [];
  const transactionEventCallbackIndexes: number[] = [];
  const logEventCallbackIndexes: number[] = [];
  const traceEventCallbackIndexes: number[] = [];
  const transferEventCallbackIndexes: number[] = [];
  for (let i = 0; i < eventCallbacks.length; i++) {
    const eventCallback = eventCallbacks[i]!;
    if (chainId !== eventCallback.filter.chainId) continue;
    if (eventCallback.filter.type === "block") {
      blockEventCallbackIndexes.push(i);
    } else if (eventCallback.filter.type === "transaction") {
      transactionEventCallbackIndexes.push(i);
    } else if (eventCallback.filter.type === "log") {
      logEventCallbackIndexes.push(i);
    } else if (eventCallback.filter.type === "trace") {
      traceEventCallbackIndexes.push(i);
    } else if (eventCallback.filter.type === "transfer") {
      transferEventCallbackIndexes.push(i);
    }
  }
  let blocksIndex = 0;
  let transactionsIndex = 0;
  let transactionReceiptsIndex = 0;
  for (const block of blocks) {
    for (const blockEventCallbackIndex of blockEventCallbackIndexes) {
      const filter = eventCallbacks[blockEventCallbackIndex]!
        .filter as BlockFilter;
      if (isBlockFilterMatched({ filter, block })) {
        events.push({
          chainId: filter.chainId,
          eventCallbackIndex: blockEventCallbackIndex,
          checkpoint: encodeCheckpoint({
            blockTimestamp: block.timestamp,
            chainId: filter.chainId,
            blockNumber: block.number,
            transactionIndex: MAX_CHECKPOINT.transactionIndex,
            eventType: EVENT_TYPES.blocks,
            eventIndex: ZERO_CHECKPOINT.eventIndex,
          }),
          block,
          log: undefined,
          trace: undefined,
          transaction: undefined,
          transactionReceipt: undefined,
        });
      }
    }
  }
  for (const transaction of transactions) {
    const blockNumber = transaction.blockNumber;
    const transactionIndex = transaction.transactionIndex;
    while (
      blocksIndex < blocks.length &&
      Number(blocks[blocksIndex]!.number) < blockNumber
    ) {
      blocksIndex++;
    }
    const block = blocks[blocksIndex]!;
    if (block === undefined) {
      throw new Error(
        `Failed to build events from block data. Missing block ${blockNumber} for chain ID ${chainId}`,
      );
    }
    while (
      transactionReceiptsIndex < transactionReceipts.length &&
      (transactionReceipts[transactionReceiptsIndex]!.blockNumber <
        blockNumber ||
        (transactionReceipts[transactionReceiptsIndex]!.blockNumber ===
          blockNumber &&
          transactionReceipts[transactionReceiptsIndex]!.transactionIndex <
            transactionIndex))
    ) {
      transactionReceiptsIndex++;
    }
    const transactionReceipt = transactionReceipts[transactionReceiptsIndex]!;
    for (const transactionEventCallbackIndex of transactionEventCallbackIndexes) {
      const filter = eventCallbacks[transactionEventCallbackIndex]!
        .filter as TransactionFilter;
      if (
        isTransactionFilterMatched({ filter, transaction }) &&
        (isAddressFactory(filter.fromAddress)
          ? isAddressMatched({
              address: transaction.from,
              blockNumber,
              childAddresses: childAddresses.get(filter.fromAddress.id)!,
            })
          : true) &&
        (isAddressFactory(filter.toAddress)
          ? isAddressMatched({
              address: transaction.to ?? undefined,
              blockNumber,
              childAddresses: childAddresses.get(filter.toAddress.id)!,
            })
          : true) &&
        (filter.includeReverted
          ? true
          : transactionReceipt.status === "success")
      ) {
        if (filter.hasTransactionReceipt && transactionReceipt === undefined) {
          throw new Error(
            `Failed to build events from block data. Missing transaction receipt for block ${blockNumber} and transaction index ${transactionIndex} for chain ID ${chainId}`,
          );
        }
        events.push({
          chainId: filter.chainId,
          eventCallbackIndex: transactionEventCallbackIndex,
          checkpoint: encodeCheckpoint({
            blockTimestamp: block.timestamp,
            chainId: filter.chainId,
            blockNumber,
            transactionIndex,
            eventType: EVENT_TYPES.transactions,
            eventIndex: 0n,
          }),
          log: undefined,
          trace: undefined,
          block,
          transaction,
          transactionReceipt,
        });
      }
    }
  }
  blocksIndex = 0;
  transactionReceiptsIndex = 0;
  for (const trace of traces) {
    const blockNumber = trace.blockNumber;
    const transactionIndex = trace.transactionIndex;
    const traceIndex = trace.traceIndex;
    while (
      blocksIndex < blocks.length &&
      Number(blocks[blocksIndex]!.number) < blockNumber
    ) {
      blocksIndex++;
    }
    const block = blocks[blocksIndex]!;
    if (block === undefined) {
      throw new Error(
        `Failed to build events from block data. Missing block ${blockNumber} for chain ID ${chainId}`,
      );
    }
    while (
      transactionsIndex < transactions.length &&
      (transactions[transactionsIndex]!.blockNumber < blockNumber ||
        (transactions[transactionsIndex]!.blockNumber === blockNumber &&
          transactions[transactionsIndex]!.transactionIndex < transactionIndex))
    ) {
      transactionsIndex++;
    }
    let transaction: InternalTransaction | undefined;
    if (
      transactionsIndex < transactions.length &&
      transactions[transactionsIndex]!.blockNumber === blockNumber &&
      transactions[transactionsIndex]!.transactionIndex === transactionIndex
    ) {
      transaction = transactions[transactionsIndex]!;
    }
    if (transaction === undefined) {
      throw new Error(
        `Failed to build events from block data. Missing transaction for block ${blockNumber} and transaction index ${transactionIndex} for chain ID ${chainId}`,
      );
    }
    while (
      transactionReceiptsIndex < transactionReceipts.length &&
      (transactionReceipts[transactionReceiptsIndex]!.blockNumber <
        blockNumber ||
        (transactionReceipts[transactionReceiptsIndex]!.blockNumber ===
          blockNumber &&
          transactionReceipts[transactionReceiptsIndex]!.transactionIndex <
            transactionIndex))
    ) {
      transactionReceiptsIndex++;
    }
    let transactionReceipt: InternalTransactionReceipt | undefined;
    if (
      transactionReceiptsIndex < transactionReceipts.length &&
      transactionReceipts[transactionReceiptsIndex]!.blockNumber ===
        blockNumber &&
      transactionReceipts[transactionReceiptsIndex]!.transactionIndex ===
        transactionIndex
    ) {
      transactionReceipt = transactionReceipts[transactionReceiptsIndex]!;
    }
    for (const traceEventCallbackIndex of traceEventCallbackIndexes) {
      const filter = eventCallbacks[traceEventCallbackIndex]!
        .filter as TraceFilter;
      if (
        isTraceFilterMatched({ filter, trace, block }) &&
        (isAddressFactory(filter.fromAddress)
          ? isAddressMatched({
              address: trace.from,
              blockNumber,
              childAddresses: childAddresses.get(filter.fromAddress.id)!,
            })
          : true) &&
        (isAddressFactory(filter.toAddress)
          ? isAddressMatched({
              address: trace.to ?? undefined,
              blockNumber,
              childAddresses: childAddresses.get(filter.toAddress.id)!,
            })
          : true) &&
        (filter.callType === undefined
          ? true
          : filter.callType === trace.type) &&
        (filter.includeReverted ? true : trace.error === undefined)
      ) {
        if (filter.hasTransactionReceipt && transactionReceipt === undefined) {
          throw new Error(
            `Failed to build events from block data. Missing transaction receipt for block ${blockNumber} and transaction index ${transactionIndex} for chain ID ${chainId}`,
          );
        }
        events.push({
          chainId: filter.chainId,
          eventCallbackIndex: traceEventCallbackIndex,
          checkpoint: encodeCheckpoint({
            blockTimestamp: block.timestamp,
            chainId: filter.chainId,
            blockNumber,
            transactionIndex,
            eventType: EVENT_TYPES.traces,
            eventIndex: traceIndex,
          }),
          log: undefined,
          trace,
          block,
          transaction,
          transactionReceipt: filter.hasTransactionReceipt
            ? transactionReceipt
            : undefined,
        });
      }
    }
    for (const transferEventCallbackIndex of transferEventCallbackIndexes) {
      const filter = eventCallbacks[transferEventCallbackIndex]!
        .filter as TransferFilter;
      if (
        isTransferFilterMatched({ filter, trace, block }) &&
        (isAddressFactory(filter.fromAddress)
          ? isAddressMatched({
              address: trace.from,
              blockNumber,
              childAddresses: childAddresses.get(filter.fromAddress.id)!,
            })
          : true) &&
        (isAddressFactory(filter.toAddress)
          ? isAddressMatched({
              address: trace.to ?? undefined,
              blockNumber,
              childAddresses: childAddresses.get(filter.toAddress.id)!,
            })
          : true) &&
        (filter.includeReverted ? true : trace.error === undefined)
      ) {
        if (filter.hasTransactionReceipt && transactionReceipt === undefined) {
          throw new Error(
            `Failed to build events from block data. Missing transaction receipt for block ${blockNumber} and transaction index ${transactionIndex} for chain ID ${chainId}`,
          );
        }
        events.push({
          chainId: filter.chainId,
          eventCallbackIndex: transferEventCallbackIndex,
          checkpoint: encodeCheckpoint({
            blockTimestamp: block.timestamp,
            chainId: filter.chainId,
            blockNumber,
            transactionIndex,
            eventType: EVENT_TYPES.traces,
            eventIndex: trace.traceIndex,
          }),
          log: undefined,
          trace,
          block,
          transaction,
          transactionReceipt: filter.hasTransactionReceipt
            ? transactionReceipt
            : undefined,
        });
      }
    }
  }
  blocksIndex = 0;
  transactionsIndex = 0;
  transactionReceiptsIndex = 0;
  for (const log of logs) {
    const blockNumber = log.blockNumber;
    const transactionIndex = log.transactionIndex;
    while (
      blocksIndex < blocks.length &&
      Number(blocks[blocksIndex]!.number) < blockNumber
    ) {
      blocksIndex++;
    }
    const block = blocks[blocksIndex]!;
    if (block === undefined) {
      throw new Error(
        `Failed to build events from block data. Missing block ${blockNumber} for chain ID ${chainId}`,
      );
    }
    while (
      transactionsIndex < transactions.length &&
      (transactions[transactionsIndex]!.blockNumber < blockNumber ||
        (transactions[transactionsIndex]!.blockNumber === blockNumber &&
          transactions[transactionsIndex]!.transactionIndex < transactionIndex))
    ) {
      transactionsIndex++;
    }
    let transaction: InternalTransaction | undefined;
    if (
      transactionsIndex < transactions.length &&
      transactions[transactionsIndex]!.blockNumber === blockNumber &&
      transactions[transactionsIndex]!.transactionIndex === transactionIndex
    ) {
      transaction = transactions[transactionsIndex]!;
    }
    // Note: transaction can be undefined, this is expected behavior on
    // chains like zkSync.
    while (
      transactionReceiptsIndex < transactionReceipts.length &&
      (transactionReceipts[transactionReceiptsIndex]!.blockNumber <
        blockNumber ||
        (transactionReceipts[transactionReceiptsIndex]!.blockNumber ===
          blockNumber &&
          transactionReceipts[transactionReceiptsIndex]!.transactionIndex <
            transactionIndex))
    ) {
      transactionReceiptsIndex++;
    }
    let transactionReceipt: InternalTransactionReceipt | undefined;
    if (
      transactionReceiptsIndex < transactionReceipts.length &&
      transactionReceipts[transactionReceiptsIndex]!.blockNumber ===
        blockNumber &&
      transactionReceipts[transactionReceiptsIndex]!.transactionIndex ===
        transactionIndex
    ) {
      transactionReceipt = transactionReceipts[transactionReceiptsIndex]!;
    }
    for (const logEventCallbackIndex of logEventCallbackIndexes) {
      const filter = eventCallbacks[logEventCallbackIndex]!.filter as LogFilter;
      if (
        isLogFilterMatched({ filter, log }) &&
        (isAddressFactory(filter.address)
          ? isAddressMatched({
              address: log.address,
              blockNumber,
              childAddresses: childAddresses.get(filter.address.id)!,
            })
          : true)
      ) {
        if (filter.hasTransactionReceipt && transactionReceipt === undefined) {
          throw new Error(
            `Failed to build events from block data. Missing transaction receipt for block ${blockNumber} and transaction index ${transactionIndex} for chain ID ${chainId}`,
          );
        }
        events.push({
          chainId: filter.chainId,
          eventCallbackIndex: logEventCallbackIndex,
          checkpoint: encodeCheckpoint({
            blockTimestamp: block.timestamp,
            chainId: filter.chainId,
            blockNumber,
            transactionIndex: log.transactionIndex,
            eventType: EVENT_TYPES.logs,
            eventIndex: log.logIndex,
          }),
          log,
          block,
          transaction,
          transactionReceipt: filter.hasTransactionReceipt
            ? transactionReceipt
            : undefined,
          trace: undefined,
        });
      }
    }
  }
  return events.sort((a, b) => (a.checkpoint < b.checkpoint ? -1 : 1));
};
export const splitEvents = (
  events: Event[],
): { events: Event[]; chainId: number; checkpoint: string }[] => {
  let hash: Hash | undefined;
  const result: { events: Event[]; chainId: number; checkpoint: string }[] = [];
  for (const event of events) {
    if (hash === undefined || hash !== event.event.block.hash) {
      result.push({
        events: [],
        chainId: event.chain.id,
        checkpoint: encodeCheckpoint({
          ...MAX_CHECKPOINT,
          blockTimestamp: event.event.block.timestamp,
          chainId: BigInt(event.chain.id),
          blockNumber: event.event.block.number,
        }),
      });
      hash = event.event.block.hash;
    }
    result[result.length - 1]!.events.push(event);
  }
  return result;
};
export const decodeEvents = (
  common: Common,
  chain: Chain,
  eventCallbacks: EventCallback[],
  rawEvents: RawEvent[],
): Event[] => {
  const events: Event[] = [];
  const logDecodeFailureSelectors = new Set<Hex>();
  let logDecodeFailureCount = 0;
  let logDecodeSuccessCount = 0;
  const traceDecodeFailureSelectors = new Set<Hex>();
  let traceDecodeFailureCount = 0;
  let traceDecodeSuccessCount = 0;
  for (const event of rawEvents) {
    const eventCallback = eventCallbacks[event.eventCallbackIndex]!;
    if (
      eventCallback.type === "contract" &&
      eventCallback.filter.type === "log"
    ) {
      let args: any;
      try {
        args = decodeEventLog({
          abiItem: eventCallback.abiItem as AbiEvent,
          data: event.log!.data,
          topics: event.log!.topics,
        });
        logDecodeSuccessCount++;
      } catch (err) {
        logDecodeFailureCount++;
        const selector = toEventSelector(eventCallback.abiItem as AbiEvent);
        if (!logDecodeFailureSelectors.has(selector)) {
          logDecodeFailureSelectors.add(selector);
          common.logger.debug({
            msg: "Failed to decode matched event log using provided ABI item",
            chain: eventCallback.chain.name,
            chain_id: eventCallback.chain.id,
            event: eventCallback.name,
            block_number: event?.block?.number ?? "unknown",
            log_index: event.log?.logIndex,
            data: event.log?.data,
            topics: JSON.stringify(event.log?.topics),
          });
        }
        continue;
      }
      events.push({
        type: "log",
        checkpoint: event.checkpoint,
        chain,
        eventCallback,
        event: {
          id: event.checkpoint,
          args,
          log: event.log!,
          block: event.block as Block,
          transaction: event.transaction! as Transaction,
          transactionReceipt: event.transactionReceipt as TransactionReceipt,
        },
      });
    } else if (
      eventCallback.type === "contract" &&
      eventCallback.filter.type === "trace"
    ) {
      let decodedData: { args: readonly unknown[]; functionName: string };
      let decodedResult: readonly unknown[];
      try {
        decodedData = decodeFunctionData({
          abi: [eventCallback.abiItem as AbiFunction],
          data: event.trace!.input,
        });
        decodedResult = decodeFunctionResult({
          abi: [eventCallback.abiItem as AbiFunction],
          data: event.trace!.output ?? "0x",
          functionName: decodedData.functionName,
        });
        traceDecodeSuccessCount++;
      } catch (err) {
        traceDecodeFailureCount++;
        const selector = toFunctionSelector(
          eventCallback.abiItem as AbiFunction,
        );
        if (!traceDecodeFailureSelectors.has(selector)) {
          traceDecodeFailureSelectors.add(selector);
          common.logger.debug({
            msg: "Failed to decode matched call trace using provided ABI item",
            chain: eventCallback.chain.name,
            chain_id: eventCallback.chain.id,
            function: eventCallback.name,
            block_number: event?.block?.number ?? "unknown",
            transaction_index: event.transaction?.transactionIndex,
            trace_index: event.trace?.traceIndex,
            input: event.trace?.input,
            output: event.trace?.output,
          });
        }
        continue;
      }
      events.push({
        type: "trace",
        checkpoint: event.checkpoint,
        chain,
        eventCallback,
        event: {
          id: event.checkpoint,
          args: decodedData.args,
          result: decodedResult,
          trace: event.trace! as Trace,
          block: event.block as Block,
          transaction: event.transaction! as Transaction,
          transactionReceipt: event.transactionReceipt as TransactionReceipt,
        },
      });
    } else if (
      eventCallback.type === "account" &&
      eventCallback.filter.type === "transaction"
    ) {
      events.push({
        type: "transaction",
        checkpoint: event.checkpoint,
        chain,
        eventCallback,
        event: {
          id: event.checkpoint,
          block: event.block as Block,
          transaction: event.transaction! as Transaction,
          transactionReceipt: event.transactionReceipt as TransactionReceipt,
        },
      });
    } else if (
      eventCallback.type === "account" &&
      eventCallback.filter.type === "transfer"
    ) {
      events.push({
        type: "transfer",
        checkpoint: event.checkpoint,
        chain,
        eventCallback,
        event: {
          id: event.checkpoint,
          transfer: {
            from: event.trace!.from,
            to: event.trace!.to!,
            value: event.trace!.value!,
          },
          block: event.block as Block,
          transaction: event.transaction! as Transaction,
          transactionReceipt: event.transactionReceipt as TransactionReceipt,
          trace: event.trace! as Trace,
        },
      });
    } else if (eventCallback.type === "block") {
      events.push({
        type: "block",
        checkpoint: event.checkpoint,
        chain,
        eventCallback,
        event: {
          id: event.checkpoint,
          block: event.block as Block,
        },
      });
    }
  }
  if (logDecodeFailureCount > 0) {
    common.logger.debug({
      msg: "Event batch contained logs that could not be decoded",
      failure_count: logDecodeFailureCount,
      success_count: logDecodeSuccessCount,
    });
  }
  if (traceDecodeFailureCount > 0) {
    common.logger.debug({
      msg: "Event batch contained traces that could not be decoded",
      failure_count: traceDecodeFailureCount,
      success_count: traceDecodeSuccessCount,
    });
  }
  return events;
};
export const syncBlockToInternal = ({
  block,
}: { block: SyncBlock | SyncBlockHeader }): InternalBlock => ({
  baseFeePerGas: block.baseFeePerGas ? hexToBigInt(block.baseFeePerGas) : null,
  difficulty: hexToBigInt(block.difficulty),
  extraData: block.extraData,
  gasLimit: hexToBigInt(block.gasLimit),
  gasUsed: hexToBigInt(block.gasUsed),
  hash: block.hash,
  logsBloom: block.logsBloom,
  miner: toLowerCase(block.miner),
  mixHash: block.mixHash,
  nonce: block.nonce,
  number: hexToBigInt(block.number),
  parentHash: block.parentHash,
  receiptsRoot: block.receiptsRoot,
  sha3Uncles: block.sha3Uncles,
  size: hexToBigInt(block.size),
  stateRoot: block.stateRoot,
  timestamp: hexToBigInt(block.timestamp),
  totalDifficulty: block.totalDifficulty
    ? hexToBigInt(block.totalDifficulty)
    : null,
  transactionsRoot: block.transactionsRoot,
});
export const syncLogToInternal = ({ log }: { log: SyncLog }): InternalLog => ({
  blockNumber: hexToNumber(log.blockNumber),
  logIndex: hexToNumber(log.logIndex),
  transactionIndex: hexToNumber(log.transactionIndex),
  address: toLowerCase(log.address!),
  data: log.data,
  removed: false,
  topics: log.topics,
});
export const syncTransactionToInternal = ({
  transaction,
}: {
  transaction: SyncTransaction;
}): InternalTransaction => ({
  blockNumber: hexToNumber(transaction.blockNumber),
  transactionIndex: hexToNumber(transaction.transactionIndex),
  from: toLowerCase(transaction.from),
  gas: hexToBigInt(transaction.gas),
  hash: transaction.hash,
  input: transaction.input,
  nonce: hexToNumber(transaction.nonce),
  r: transaction.r,
  s: transaction.s,
  to: transaction.to ? toLowerCase(transaction.to) : transaction.to,
  value: hexToBigInt(transaction.value),
  v: transaction.v ? hexToBigInt(transaction.v) : null,
  ...(transaction.type === "0x0"
    ? {
        type: "legacy",
        gasPrice: hexToBigInt(transaction.gasPrice),
      }
    : transaction.type === "0x1"
      ? {
          type: "eip2930",
          gasPrice: hexToBigInt(transaction.gasPrice),
          accessList: transaction.accessList,
        }
      : transaction.type === "0x2"
        ? {
            type: "eip1559",
            maxFeePerGas: hexToBigInt(transaction.maxFeePerGas),
            maxPriorityFeePerGas: hexToBigInt(transaction.maxPriorityFeePerGas),
          }
        : // @ts-ignore
          transaction.type === "0x7e"
          ? {
              type: "deposit",
              // @ts-ignore
              maxFeePerGas: transaction.maxFeePerGas
                ? // @ts-ignore
                  hexToBigInt(transaction.maxFeePerGas)
                : undefined,
              // @ts-ignore
              maxPriorityFeePerGas: transaction.maxPriorityFeePerGas
                ? // @ts-ignore
                  hexToBigInt(transaction.maxPriorityFeePerGas)
                : undefined,
            }
          : {
              // @ts-ignore
              type: transaction.type,
            }),
});
export const syncTransactionReceiptToInternal = ({
  transactionReceipt,
}: {
  transactionReceipt: SyncTransactionReceipt;
}): InternalTransactionReceipt => ({
  blockNumber: hexToNumber(transactionReceipt.blockNumber),
  transactionIndex: hexToNumber(transactionReceipt.transactionIndex),
  contractAddress: transactionReceipt.contractAddress
    ? toLowerCase(transactionReceipt.contractAddress)
    : null,
  cumulativeGasUsed: hexToBigInt(transactionReceipt.cumulativeGasUsed),
  effectiveGasPrice: hexToBigInt(transactionReceipt.effectiveGasPrice),
  from: toLowerCase(transactionReceipt.from),
  gasUsed: hexToBigInt(transactionReceipt.gasUsed),
  logsBloom: transactionReceipt.logsBloom,
  status:
    transactionReceipt.status === "0x1"
      ? "success"
      : transactionReceipt.status === "0x0"
        ? "reverted"
        : (transactionReceipt.status as InternalTransactionReceipt["status"]),
  to: transactionReceipt.to ? toLowerCase(transactionReceipt.to) : null,
  type:
    transactionReceipt.type === "0x0"
      ? "legacy"
      : transactionReceipt.type === "0x1"
        ? "eip2930"
        : transactionReceipt.type === "0x2"
          ? "eip1559"
          : transactionReceipt.type === "0x7e"
            ? "deposit"
            : transactionReceipt.type,
});
export const syncTraceToInternal = ({
  trace,
  block,
  transaction,
}: {
  trace: SyncTrace;
  block: Pick<SyncBlock, "number">;
  transaction: Pick<SyncTransaction, "transactionIndex">;
}): InternalTrace => ({
  blockNumber: hexToNumber(block.number),
  traceIndex: trace.trace.index,
  transactionIndex: hexToNumber(transaction.transactionIndex),
  type: trace.trace.type,
  from: toLowerCase(trace.trace.from),
  to: trace.trace.to ? toLowerCase(trace.trace.to) : null,
  gas: hexToBigInt(trace.trace.gas),
  gasUsed: hexToBigInt(trace.trace.gasUsed),
  input: trace.trace.input,
  output: trace.trace.output,
  error: trace.trace.error,
  revertReason: trace.trace.revertReason,
  value: trace.trace.value ? hexToBigInt(trace.trace.value) : null,
  subcalls: trace.trace.subcalls,
});
</file>

<file path="packages/core/src/runtime/filter.test.ts">
import { ALICE, BOB } from "@/_test/constants.js";
import { context, setupAnvil, setupCommon } from "@/_test/setup.js";
import {
  createPair,
  deployErc20,
  deployFactory,
  mintErc20,
  transferErc20,
  transferEth,
} from "@/_test/simulate.js";
import {
  getAccountsIndexingBuild,
  getBlocksIndexingBuild,
  getChain,
  getErc20IndexingBuild,
  getPairWithFactoryIndexingBuild,
} from "@/_test/utils.js";
import type {
  BlockFilter,
  Factory,
  LogFactory,
  LogFilter,
  SyncLog,
  TraceFilter,
  TransactionFilter,
  TransferFilter,
} from "@/internal/types.js";
import { eth_getBlockByNumber } from "@/rpc/actions.js";
import { createRpc } from "@/rpc/index.js";
import { type Address, parseEther, zeroAddress, zeroHash } from "viem";
import { beforeEach, expect, test } from "vitest";
import {
  getChildAddress,
  isBlockFilterMatched,
  isLogFactoryMatched,
  isLogFilterMatched,
  isTraceFilterMatched,
  isTransactionFilterMatched,
  isTransferFilterMatched,
} from "./filter.js";
beforeEach(setupCommon);
beforeEach(setupAnvil);
test("getChildAddress() topics", () => {
  const factory = {
    type: "log",
    childAddressLocation: "topic1",
  } as unknown as LogFactory;
  const log = {
    topics: [
      null,
      "0x000000000000000000000000a21a16ec22a940990922220e4ab5bf4c2310f556",
    ],
  } as unknown as SyncLog;
  expect(getChildAddress({ log, factory })).toBe(
    "0xa21a16ec22a940990922220e4ab5bf4c2310f556",
  );
});
test("getChildAddress() offset", () => {
  const factory = {
    type: "log",
    childAddressLocation: "offset32",
  } as unknown as LogFactory;
  const log = {
    data: "0x0000000000000000000000000000000000000000000000000000000017d435c9000000000000000000000000a21a16ec22a940990922220e4ab5bf4c2310f556",
  } as unknown as SyncLog;
  expect(getChildAddress({ log, factory })).toBe(
    "0xa21a16ec22a940990922220e4ab5bf4c2310f556",
  );
});
test("isLogFactoryMatched()", async () => {
  const { address } = await deployFactory({ sender: ALICE });
  const blockData = await createPair({
    factory: address,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({
    address,
  });
  const filter = eventCallbacks[0]!.filter as LogFilter<Factory>;
  let isMatched = isLogFactoryMatched({
    factory: filter.address,
    log: blockData.log,
  });
  expect(isMatched).toBe(true);
  filter.address.address = [filter.address.address as Address];
  isMatched = isLogFactoryMatched({
    factory: filter.address,
    log: blockData.log,
  });
  expect(isMatched).toBe(true);
  filter.address.address = undefined;
  isMatched = isLogFactoryMatched({
    factory: filter.address,
    log: blockData.log,
  });
  expect(isMatched).toBe(true);
  blockData.log.topics[0] = zeroHash;
  isMatched = isLogFactoryMatched({
    factory: filter.address,
    log: blockData.log,
  });
  expect(isMatched).toBe(false);
});
test("isLogFilterMatched()", async () => {
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getErc20IndexingBuild({
    address,
  });
  const filter = eventCallbacks[0]!.filter as LogFilter;
  let isMatched = isLogFilterMatched({ filter, log: blockData.log });
  expect(isMatched).toBe(true);
  blockData.log.address = zeroAddress;
  isMatched = isLogFilterMatched({ filter, log: blockData.log });
  expect(isMatched).toBe(false);
});
test("isBlockFilterMatched", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const filter = eventCallbacks[0]!.filter as BlockFilter;
  const rpcBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  let isMatched = isBlockFilterMatched({
    filter,
    block: rpcBlock,
  });
  expect(isMatched).toBe(true);
  filter.interval = 2;
  filter.offset = 1;
  isMatched = isBlockFilterMatched({
    filter,
    block: rpcBlock,
  });
  expect(isMatched).toBe(false);
});
test("isTransactionFilterMatched()", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  await transferEth({
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getAccountsIndexingBuild({
    address: ALICE,
  });
  // transaction:from
  const filter = eventCallbacks[1]!.filter as TransactionFilter<
    undefined,
    undefined
  >;
  const rpcBlock = await eth_getBlockByNumber(rpc, ["0x1", true]);
  let isMatched = isTransactionFilterMatched({
    filter,
    transaction: rpcBlock.transactions[0]!,
  });
  expect(isMatched).toBe(true);
  rpcBlock.transactions[0]!.from = zeroAddress;
  isMatched = isTransactionFilterMatched({
    filter,
    transaction: rpcBlock.transactions[0]!,
  });
  expect(isMatched).toBe(false);
});
test("isTransactionFilterMatched() with null transaction.to", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  await transferEth({
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getAccountsIndexingBuild({
    address: ALICE,
  });
  // transaction:to
  const filter = eventCallbacks[1]!.filter as TransactionFilter<
    undefined,
    undefined
  >;
  filter.toAddress = BOB.toLowerCase() as Address;
  const rpcBlock = await eth_getBlockByNumber(rpc, ["0x1", true]);
  let isMatched = isTransactionFilterMatched({
    filter,
    transaction: rpcBlock.transactions[0]!,
  });
  expect(isMatched).toBe(true);
  rpcBlock.transactions[0]!.to = null;
  isMatched = isTransactionFilterMatched({
    filter,
    transaction: rpcBlock.transactions[0]!,
  });
  expect(isMatched).toBe(false);
});
test("isTransferFilterMatched()", async () => {
  const blockData = await transferEth({
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getAccountsIndexingBuild({
    address: ALICE,
  });
  // transfer:from
  const filter = eventCallbacks[3]!.filter as TransferFilter;
  let isMatched = isTransferFilterMatched({
    filter,
    block: blockData.block,
    trace: blockData.trace.trace,
  });
  expect(isMatched).toBe(true);
  blockData.trace.trace.value = "0x0";
  isMatched = isTransferFilterMatched({
    filter,
    block: blockData.block,
    trace: blockData.trace.trace,
  });
  expect(isMatched).toBe(false);
});
test("isTraceFilterMatched()", async () => {
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const blockData = await transferErc20({
    erc20: address,
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getErc20IndexingBuild({
    address,
    includeCallTraces: true,
  });
  const filter = eventCallbacks[0]!.filter as TraceFilter;
  let isMatched = isTraceFilterMatched({
    filter,
    block: blockData.block,
    trace: blockData.trace.trace,
  });
  expect(isMatched).toBe(true);
  blockData.trace.trace.to = zeroAddress;
  isMatched = isTraceFilterMatched({
    filter,
    block: blockData.block,
    trace: blockData.trace.trace,
  });
  expect(isMatched).toBe(false);
});
</file>

<file path="packages/core/src/runtime/filter.ts">
import type {
  BlockFilter,
  Factory,
  Filter,
  InternalBlock,
  InternalLog,
  InternalTrace,
  InternalTransaction,
  LogFactory,
  LogFilter,
  RequiredBlockColumns,
  RequiredLogColumns,
  RequiredTraceColumns,
  RequiredTransactionColumns,
  RequiredTransactionReceiptColumns,
  SyncBlock,
  SyncBlockHeader,
  SyncLog,
  SyncTrace,
  SyncTransaction,
  TraceFilter,
  TransactionFilter,
  TransferFilter,
} from "@/internal/types.js";
import type {
  Block,
  Log,
  Trace,
  Transaction,
  TransactionReceipt,
} from "@/types/eth.js";
import { toLowerCase } from "@/utils/lowercase.js";
import { type Address, hexToNumber } from "viem";
/** Returns true if `address` is an address filter. */
export const isAddressFactory = (
  address: Address | Address[] | Factory | undefined | null,
): address is LogFactory => {
  if (
    address === undefined ||
    address === null ||
    typeof address === "string"
  ) {
    return false;
  }
  return Array.isArray(address) ? isAddressFactory(address[0]) : true;
};
export const getChildAddress = ({
  log,
  factory,
}: { log: SyncLog; factory: Factory }): Address => {
  if (factory.childAddressLocation.startsWith("offset")) {
    const childAddressOffset = Number(
      factory.childAddressLocation.substring(6),
    );
    const start = 2 + 12 * 2 + childAddressOffset * 2;
    const length = 20 * 2;
    return `0x${log.data.substring(start, start + length)}`;
  } else {
    const start = 2 + 12 * 2;
    const length = 20 * 2;
    const topicIndex =
      factory.childAddressLocation === "topic1"
        ? 1
        : factory.childAddressLocation === "topic2"
          ? 2
          : 3;
    return `0x${log.topics[topicIndex]!.substring(start, start + length)}`;
  }
};
export const isAddressMatched = ({
  address,
  blockNumber,
  childAddresses,
}: {
  address: Address | undefined;
  blockNumber: number;
  childAddresses: Map<Address, number>;
}) => {
  if (address === undefined) return false;
  if (
    childAddresses.has(toLowerCase(address)) &&
    childAddresses.get(toLowerCase(address))! <= blockNumber
  ) {
    return true;
  }
  return false;
};
const isValueMatched = <T extends string>(
  filterValue: T | T[] | null | undefined,
  eventValue: T | undefined,
): boolean => {
  // match all
  if (filterValue === null || filterValue === undefined) return true;
  // missing value
  if (eventValue === undefined) return false;
  // array
  if (
    Array.isArray(filterValue) &&
    filterValue.some((v) => v === toLowerCase(eventValue))
  ) {
    return true;
  }
  // single
  if (filterValue === toLowerCase(eventValue)) return true;
  return false;
};
/**
 * Returns `true` if `log` matches `factory`
 */
export const isLogFactoryMatched = ({
  factory,
  log,
}: { factory: LogFactory; log: InternalLog | SyncLog }): boolean => {
  if (factory.address !== undefined) {
    const addresses = Array.isArray(factory.address)
      ? factory.address
      : [factory.address];
    if (addresses.every((address) => address !== toLowerCase(log.address))) {
      return false;
    }
  }
  if (log.topics.length === 0) return false;
  if (factory.eventSelector !== toLowerCase(log.topics[0]!)) return false;
  if (
    factory.fromBlock !== undefined &&
    (typeof log.blockNumber === "number"
      ? factory.fromBlock > log.blockNumber
      : factory.fromBlock > hexToNumber(log.blockNumber))
  )
    return false;
  if (
    factory.toBlock !== undefined &&
    (typeof log.blockNumber === "number"
      ? factory.toBlock < log.blockNumber
      : factory.toBlock < hexToNumber(log.blockNumber))
  )
    return false;
  return true;
};
/**
 * Returns `true` if `log` matches `filter`
 */
export const isLogFilterMatched = ({
  filter,
  log,
}: {
  filter: LogFilter;
  log: InternalLog | SyncLog;
}): boolean => {
  // Return `false` for out of range blocks
  if (
    Number(log.blockNumber) < (filter.fromBlock ?? 0) ||
    Number(log.blockNumber) > (filter.toBlock ?? Number.POSITIVE_INFINITY)
  ) {
    return false;
  }
  if (isValueMatched(filter.topic0, log.topics[0]) === false) return false;
  if (isValueMatched(filter.topic1, log.topics[1]) === false) return false;
  if (isValueMatched(filter.topic2, log.topics[2]) === false) return false;
  if (isValueMatched(filter.topic3, log.topics[3]) === false) return false;
  if (
    isAddressFactory(filter.address) === false &&
    isValueMatched(
      filter.address as Address | Address[] | undefined,
      log.address,
    ) === false
  ) {
    return false;
  }
  return true;
};
/**
 * Returns `true` if `transaction` matches `filter`
 */
export const isTransactionFilterMatched = ({
  filter,
  transaction,
}: {
  filter: TransactionFilter;
  transaction: InternalTransaction | SyncTransaction;
}): boolean => {
  // Return `false` for out of range blocks
  if (
    Number(transaction.blockNumber) < (filter.fromBlock ?? 0) ||
    Number(transaction.blockNumber) >
      (filter.toBlock ?? Number.POSITIVE_INFINITY)
  ) {
    return false;
  }
  if (
    isAddressFactory(filter.fromAddress) === false &&
    isValueMatched(
      filter.fromAddress as Address | Address[] | undefined,
      transaction.from,
    ) === false
  ) {
    return false;
  }
  if (
    isAddressFactory(filter.toAddress) === false &&
    isValueMatched(
      filter.toAddress as Address | Address[] | undefined,
      transaction.to ?? undefined,
    ) === false
  ) {
    return false;
  }
  // NOTE: `filter.includeReverted` is intentionally ignored
  return true;
};
/**
 * Returns `true` if `trace` matches `filter`
 */
export const isTraceFilterMatched = ({
  filter,
  trace,
  block,
}: {
  filter: TraceFilter;
  trace: InternalTrace | SyncTrace["trace"];
  block: Pick<InternalBlock | SyncBlock, "number">;
}): boolean => {
  // Return `false` for out of range blocks
  if (
    Number(block.number) < (filter.fromBlock ?? 0) ||
    Number(block.number) > (filter.toBlock ?? Number.POSITIVE_INFINITY)
  ) {
    return false;
  }
  if (
    isAddressFactory(filter.fromAddress) === false &&
    isValueMatched(
      filter.fromAddress as Address | Address[] | undefined,
      trace.from,
    ) === false
  ) {
    return false;
  }
  if (
    isAddressFactory(filter.toAddress) === false &&
    isValueMatched(
      filter.toAddress as Address | Address[] | undefined,
      trace.to ?? undefined,
    ) === false
  ) {
    return false;
  }
  if (
    isValueMatched(filter.functionSelector, trace.input.slice(0, 10)) === false
  ) {
    return false;
  }
  // NOTE: `filter.callType` and `filter.includeReverted` is intentionally ignored
  return true;
};
/**
 * Returns `true` if `trace` matches `filter`
 */
export const isTransferFilterMatched = ({
  filter,
  trace,
  block,
}: {
  filter: TransferFilter;
  trace: InternalTrace | SyncTrace["trace"];
  block: Pick<InternalBlock | SyncBlock, "number">;
}): boolean => {
  // Return `false` for out of range blocks
  if (
    Number(block.number) < (filter.fromBlock ?? 0) ||
    Number(block.number) > (filter.toBlock ?? Number.POSITIVE_INFINITY)
  ) {
    return false;
  }
  if (
    trace.value === undefined ||
    trace.value === null ||
    BigInt(trace.value) === 0n
  ) {
    return false;
  }
  if (
    isAddressFactory(filter.fromAddress) === false &&
    isValueMatched(
      filter.fromAddress as Address | Address[] | undefined,
      trace.from,
    ) === false
  ) {
    return false;
  }
  if (
    isAddressFactory(filter.toAddress) === false &&
    isValueMatched(
      filter.toAddress as Address | Address[] | undefined,
      trace.to ?? undefined,
    ) === false
  ) {
    return false;
  }
  // NOTE: `filter.includeReverted` is intentionally ignored
  return true;
};
/**
 * Returns `true` if `block` matches `filter`
 */
export const isBlockFilterMatched = ({
  filter,
  block,
}: {
  filter: BlockFilter;
  block: Pick<InternalBlock | SyncBlock | SyncBlockHeader, "number">;
}): boolean => {
  // Return `false` for out of range blocks
  if (
    Number(block.number) < (filter.fromBlock ?? 0) ||
    Number(block.number) > (filter.toBlock ?? Number.POSITIVE_INFINITY)
  ) {
    return false;
  }
  return (Number(block.number) - filter.offset) % filter.interval === 0;
};
export const getFilterFactories = (filter: Filter): Factory[] => {
  const factories: Factory[] = [];
  switch (filter.type) {
    case "log":
      if (isAddressFactory(filter.address)) {
        factories.push(filter.address);
      }
      break;
    case "trace":
    case "transfer":
    case "transaction": {
      if (isAddressFactory(filter.fromAddress)) {
        factories.push(filter.fromAddress);
      }
      if (isAddressFactory(filter.toAddress)) {
        factories.push(filter.toAddress);
      }
      break;
    }
  }
  return factories;
};
export const getFilterFromBlock = (filter: Filter): number => {
  const blocks: number[] = [filter.fromBlock ?? 0];
  switch (filter.type) {
    case "log":
      if (isAddressFactory(filter.address)) {
        blocks.push(filter.address.fromBlock ?? 0);
      }
      break;
    case "transaction":
    case "trace":
    case "transfer":
      if (isAddressFactory(filter.fromAddress)) {
        blocks.push(filter.fromAddress.fromBlock ?? 0);
      }
      if (isAddressFactory(filter.toAddress)) {
        blocks.push(filter.toAddress.fromBlock ?? 0);
      }
  }
  return Math.min(...blocks);
};
export const getFilterToBlock = (filter: Filter): number => {
  const blocks: number[] = [filter.toBlock ?? Number.POSITIVE_INFINITY];
  // Note: factories cannot have toBlock > `filter.toBlock`
  switch (filter.type) {
    case "log":
      if (isAddressFactory(filter.address)) {
        blocks.push(filter.address.toBlock ?? Number.POSITIVE_INFINITY);
      }
      break;
    case "transaction":
    case "trace":
    case "transfer":
      if (isAddressFactory(filter.fromAddress)) {
        blocks.push(filter.fromAddress.toBlock ?? Number.POSITIVE_INFINITY);
      }
      if (isAddressFactory(filter.toAddress)) {
        blocks.push(filter.toAddress.toBlock ?? Number.POSITIVE_INFINITY);
      }
  }
  return Math.max(...blocks);
};
export const isBlockInFilter = (filter: Filter, blockNumber: number) => {
  // Return `false` for out of range blocks
  if (
    blockNumber < (filter.fromBlock ?? 0) ||
    blockNumber > (filter.toBlock ?? Number.POSITIVE_INFINITY)
  ) {
    return false;
  }
  return true;
};
export const defaultBlockInclude: (keyof Block)[] = [
  "baseFeePerGas",
  "difficulty",
  "extraData",
  "gasLimit",
  "gasUsed",
  "hash",
  "logsBloom",
  "miner",
  "mixHash",
  "totalDifficulty",
  "nonce",
  "number",
  "parentHash",
  "receiptsRoot",
  "sha3Uncles",
  "size",
  "stateRoot",
  "timestamp",
  "transactionsRoot",
];
export const requiredBlockInclude: RequiredBlockColumns[] = [
  "timestamp",
  "number",
  "hash",
];
export const defaultTransactionInclude: (keyof Transaction)[] = [
  "from",
  "gas",
  "hash",
  "input",
  "nonce",
  "r",
  "s",
  "to",
  "transactionIndex",
  "v",
  "value",
  "type",
  "gasPrice",
  "accessList",
  "maxFeePerGas",
  "maxPriorityFeePerGas",
];
export const requiredTransactionInclude: RequiredTransactionColumns[] = [
  "transactionIndex",
  "from",
  "to",
  "hash",
  "type",
];
export const defaultTransactionReceiptInclude: (keyof TransactionReceipt)[] = [
  "contractAddress",
  "cumulativeGasUsed",
  "effectiveGasPrice",
  "from",
  "gasUsed",
  "logsBloom",
  "status",
  "to",
  "type",
];
export const requiredTransactionReceiptInclude: RequiredTransactionReceiptColumns[] =
  ["status", "from", "to"];
export const defaultTraceInclude: (keyof Trace)[] = [
  "traceIndex",
  "type",
  "from",
  "to",
  "gas",
  "gasUsed",
  "input",
  "output",
  "error",
  "revertReason",
  "value",
  "subcalls",
];
export const requiredTraceInclude: RequiredTraceColumns[] = [
  "traceIndex",
  "type",
  "from",
  "to",
  "input",
  "output",
  "error",
  "value",
];
export const defaultLogInclude: (keyof Log)[] = [
  "address",
  "data",
  "logIndex",
  "removed",
  "topics",
];
export const requiredLogInclude: RequiredLogColumns[] = defaultLogInclude;
export const defaultBlockFilterInclude: BlockFilter["include"] =
  defaultBlockInclude.map((value) => `block.${value}` as const);
export const requiredBlockFilterInclude: BlockFilter["include"] =
  requiredBlockInclude.map((value) => `block.${value}` as const);
export const defaultLogFilterInclude: LogFilter["include"] = [
  ...defaultLogInclude.map((value) => `log.${value}` as const),
  ...defaultTransactionInclude.map((value) => `transaction.${value}` as const),
  ...defaultBlockInclude.map((value) => `block.${value}` as const),
];
export const requiredLogFilterInclude: LogFilter["include"] = [
  ...requiredLogInclude.map((value) => `log.${value}` as const),
  ...requiredTransactionInclude.map((value) => `transaction.${value}` as const),
  ...requiredBlockInclude.map((value) => `block.${value}` as const),
];
export const defaultTransactionFilterInclude: TransactionFilter["include"] = [
  ...defaultTransactionInclude.map((value) => `transaction.${value}` as const),
  ...defaultTransactionReceiptInclude.map(
    (value) => `transactionReceipt.${value}` as const,
  ),
  ...defaultBlockInclude.map((value) => `block.${value}` as const),
];
export const requiredTransactionFilterInclude: TransactionFilter["include"] = [
  ...requiredTransactionInclude.map((value) => `transaction.${value}` as const),
  ...requiredTransactionReceiptInclude.map(
    (value) => `transactionReceipt.${value}` as const,
  ),
  ...requiredBlockInclude.map((value) => `block.${value}` as const),
];
export const defaultTraceFilterInclude: TraceFilter["include"] = [
  ...defaultBlockInclude.map((value) => `block.${value}` as const),
  ...defaultTransactionInclude.map((value) => `transaction.${value}` as const),
  ...defaultTraceInclude.map((value) => `trace.${value}` as const),
];
export const requiredTraceFilterInclude: TraceFilter["include"] = [
  ...requiredBlockInclude.map((value) => `block.${value}` as const),
  ...requiredTransactionInclude.map((value) => `transaction.${value}` as const),
  ...requiredTraceInclude.map((value) => `trace.${value}` as const),
];
export const defaultTransferFilterInclude: TransferFilter["include"] = [
  ...defaultBlockInclude.map((value) => `block.${value}` as const),
  ...defaultTransactionInclude.map((value) => `transaction.${value}` as const),
  ...defaultTraceInclude.map((value) => `trace.${value}` as const),
];
export const requiredTransferFilterInclude: TransferFilter["include"] = [
  ...requiredBlockInclude.map((value) => `block.${value}` as const),
  ...requiredTransactionInclude.map((value) => `transaction.${value}` as const),
  ...requiredTraceInclude.map((value) => `trace.${value}` as const),
];
export const unionFilterIncludeBlock = (filters: Filter[]): (keyof Block)[] => {
  const includeBlock = new Set<keyof Block>();
  for (const filter of filters) {
    for (const include of filter.include) {
      const [data, column] = include.split(".") as [string, keyof Block];
      if (data === "block") {
        includeBlock.add(column);
      }
    }
  }
  return Array.from(includeBlock);
};
export const unionFilterIncludeTransaction = (
  filters: Filter[],
): (keyof Transaction)[] => {
  const includeTransaction = new Set<keyof Transaction>();
  for (const filter of filters) {
    for (const include of filter.include) {
      const [data, column] = include.split(".") as [string, keyof Transaction];
      if (data === "transaction") {
        includeTransaction.add(column);
      }
    }
  }
  return Array.from(includeTransaction);
};
export const unionFilterIncludeTransactionReceipt = (
  filters: Filter[],
): (keyof TransactionReceipt)[] => {
  const includeTransactionReceipt = new Set<keyof TransactionReceipt>();
  for (const filter of filters) {
    for (const include of filter.include) {
      const [data, column] = include.split(".") as [
        string,
        keyof TransactionReceipt,
      ];
      if (data === "transactionReceipt") {
        includeTransactionReceipt.add(column);
      }
    }
  }
  return Array.from(includeTransactionReceipt);
};
export const unionFilterIncludeTrace = (filters: Filter[]): (keyof Trace)[] => {
  const includeTrace = new Set<keyof Trace>();
  for (const filter of filters) {
    for (const include of filter.include) {
      const [data, column] = include.split(".") as [string, keyof Trace];
      if (data === "trace") {
        includeTrace.add(column);
      }
    }
  }
  return Array.from(includeTrace);
};
</file>

<file path="packages/core/src/runtime/fragments.test.ts">
import {
  EMPTY_BLOCK_FILTER,
  EMPTY_LOG_FILTER,
  EMPTY_TRACE_FILTER,
  EMPTY_TRANSACTION_FILTER,
  EMPTY_TRANSFER_FILTER,
} from "@/_test/constants.js";
import type { Filter } from "@/internal/types.js";
import { zeroHash } from "viem";
import { expect, test } from "vitest";
import {
  decodeFragment,
  encodeFragment,
  getFragments,
  recoverFilter,
} from "./fragments.js";
test("getFragments() block filter", () => {
  const fragments = getFragments({
    ...EMPTY_BLOCK_FILTER,
    interval: 100,
    offset: 5,
  });
  expect(fragments).toMatchInlineSnapshot(`
    [
      {
        "adjacentIds": [
          "block_1_100_5",
        ],
        "fragment": {
          "chainId": 1,
          "interval": 100,
          "offset": 5,
          "type": "block",
        },
      },
    ]
  `);
});
test("getFragments() transaction filter", () => {
  const fragments = getFragments({
    ...EMPTY_TRANSACTION_FILTER,
    fromAddress: "0xa",
    toAddress: "0xb",
    includeReverted: false,
  });
  expect(fragments).toMatchInlineSnapshot(`
    [
      {
        "adjacentIds": [
          "transaction_1_0xa_0xb",
          "transaction_1_0xa_null",
          "transaction_1_null_0xb",
          "transaction_1_null_null",
        ],
        "fragment": {
          "chainId": 1,
          "fromAddress": "0xa",
          "toAddress": "0xb",
          "type": "transaction",
        },
      },
    ]
  `);
});
test("getFragments() log filter", () => {
  const fragments = getFragments({
    ...EMPTY_LOG_FILTER,
    address: ["0xa", "0xb"],
    topic0: zeroHash,
    topic1: null,
    topic2: "0xe",
    topic3: null,
  });
  expect(fragments).toMatchInlineSnapshot(`
    [
      {
        "adjacentIds": [
          "log_1_0xa_0x0000000000000000000000000000000000000000000000000000000000000000_null_0xe_null_0",
          "log_1_0xa_0x0000000000000000000000000000000000000000000000000000000000000000_null_0xe_null_1",
          "log_1_0xa_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_0",
          "log_1_0xa_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_1",
          "log_1_null_0x0000000000000000000000000000000000000000000000000000000000000000_null_0xe_null_0",
          "log_1_null_0x0000000000000000000000000000000000000000000000000000000000000000_null_0xe_null_1",
          "log_1_null_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_0",
          "log_1_null_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_1",
        ],
        "fragment": {
          "address": "0xa",
          "chainId": 1,
          "includeTransactionReceipts": false,
          "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
          "topic1": null,
          "topic2": "0xe",
          "topic3": null,
          "type": "log",
        },
      },
      {
        "adjacentIds": [
          "log_1_0xb_0x0000000000000000000000000000000000000000000000000000000000000000_null_0xe_null_0",
          "log_1_0xb_0x0000000000000000000000000000000000000000000000000000000000000000_null_0xe_null_1",
          "log_1_0xb_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_0",
          "log_1_0xb_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_1",
          "log_1_null_0x0000000000000000000000000000000000000000000000000000000000000000_null_0xe_null_0",
          "log_1_null_0x0000000000000000000000000000000000000000000000000000000000000000_null_0xe_null_1",
          "log_1_null_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_0",
          "log_1_null_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_1",
        ],
        "fragment": {
          "address": "0xb",
          "chainId": 1,
          "includeTransactionReceipts": false,
          "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
          "topic1": null,
          "topic2": "0xe",
          "topic3": null,
          "type": "log",
        },
      },
    ]
  `);
});
test("getFragments() log filter with transaction receipts", () => {
  const fragments = getFragments({
    ...EMPTY_LOG_FILTER,
    hasTransactionReceipt: true,
  });
  expect(fragments).toMatchInlineSnapshot(`
    [
      {
        "adjacentIds": [
          "log_1_null_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_1",
        ],
        "fragment": {
          "address": null,
          "chainId": 1,
          "includeTransactionReceipts": true,
          "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
          "topic1": null,
          "topic2": null,
          "topic3": null,
          "type": "log",
        },
      },
    ]
  `);
});
test("getFragments() trace filter", () => {
  const fragments = getFragments({
    ...EMPTY_TRACE_FILTER,
    fromAddress: "0xa",
    toAddress: undefined,
    includeReverted: false,
    functionSelector: "0xb",
    callType: "CALL",
  });
  expect(fragments).toMatchInlineSnapshot(`
    [
      {
        "adjacentIds": [
          "trace_1_0xa_null_0xb_0",
          "trace_1_0xa_null_0xb_1",
          "trace_1_null_null_0xb_0",
          "trace_1_null_null_0xb_1",
        ],
        "fragment": {
          "chainId": 1,
          "fromAddress": "0xa",
          "functionSelector": "0xb",
          "includeTransactionReceipts": false,
          "toAddress": null,
          "type": "trace",
        },
      },
    ]
  `);
});
test("getFragments() transfer filter", () => {
  const fragments = getFragments({
    ...EMPTY_TRANSFER_FILTER,
    fromAddress: "0xa",
    toAddress: undefined,
    includeReverted: false,
  });
  expect(fragments).toMatchInlineSnapshot(`
    [
      {
        "adjacentIds": [
          "transfer_1_0xa_null_0",
          "transfer_1_0xa_null_1",
          "transfer_1_null_null_0",
          "transfer_1_null_null_1",
        ],
        "fragment": {
          "chainId": 1,
          "fromAddress": "0xa",
          "includeTransactionReceipts": false,
          "toAddress": null,
          "type": "transfer",
        },
      },
    ]
  `);
});
test("getFragments() factory with topic", () => {
  const fragments = getFragments({
    ...EMPTY_LOG_FILTER,
    address: {
      id: `log_${"0xa"}_${1}_topic${1}_${"0xb"}_${"undefined"}_${"undefined"}`,
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: "0xa",
      eventSelector: "0xb",
      childAddressLocation: "topic1",
      fromBlock: undefined,
      toBlock: undefined,
    },
  });
  expect(fragments).toMatchInlineSnapshot(`
    [
      {
        "adjacentIds": [
          "log_1_0xa_0xb_topic1_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_0",
          "log_1_0xa_0xb_topic1_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_1",
        ],
        "fragment": {
          "address": {
            "address": "0xa",
            "childAddressLocation": "topic1",
            "eventSelector": "0xb",
          },
          "chainId": 1,
          "includeTransactionReceipts": false,
          "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
          "topic1": null,
          "topic2": null,
          "topic3": null,
          "type": "log",
        },
      },
    ]
  `);
});
test("getFragments() factory with offset", () => {
  const fragments = getFragments({
    ...EMPTY_LOG_FILTER,
    address: {
      id: `log_${"0xa"}_${1}_offset${64}_${"0xb"}_${"undefined"}_${"undefined"}`,
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: "0xa",
      eventSelector: "0xb",
      childAddressLocation: "offset64",
      fromBlock: undefined,
      toBlock: undefined,
    },
  });
  expect(fragments).toMatchInlineSnapshot(`
    [
      {
        "adjacentIds": [
          "log_1_0xa_0xb_offset64_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_0",
          "log_1_0xa_0xb_offset64_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_1",
        ],
        "fragment": {
          "address": {
            "address": "0xa",
            "childAddressLocation": "offset64",
            "eventSelector": "0xb",
          },
          "chainId": 1,
          "includeTransactionReceipts": false,
          "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
          "topic1": null,
          "topic2": null,
          "topic3": null,
          "type": "log",
        },
      },
    ]
  `);
});
test("getFragments() multiple factories", () => {
  const fragments = getFragments({
    ...EMPTY_LOG_FILTER,
    address: {
      id: `log_${["0xa", "0xb"].join("_")}_${1}_topic${1}_${"0xb"}_${"undefined"}_${"undefined"}`,
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: ["0xa", "0xb"],
      eventSelector: "0xc",
      childAddressLocation: "topic1",
      fromBlock: undefined,
      toBlock: undefined,
    },
  });
  expect(fragments).toMatchInlineSnapshot(`
    [
      {
        "adjacentIds": [
          "log_1_0xa_0xc_topic1_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_0",
          "log_1_0xa_0xc_topic1_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_1",
        ],
        "fragment": {
          "address": {
            "address": "0xa",
            "childAddressLocation": "topic1",
            "eventSelector": "0xc",
          },
          "chainId": 1,
          "includeTransactionReceipts": false,
          "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
          "topic1": null,
          "topic2": null,
          "topic3": null,
          "type": "log",
        },
      },
      {
        "adjacentIds": [
          "log_1_0xb_0xc_topic1_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_0",
          "log_1_0xb_0xc_topic1_0x0000000000000000000000000000000000000000000000000000000000000000_null_null_null_1",
        ],
        "fragment": {
          "address": {
            "address": "0xb",
            "childAddressLocation": "topic1",
            "eventSelector": "0xc",
          },
          "chainId": 1,
          "includeTransactionReceipts": false,
          "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
          "topic1": null,
          "topic2": null,
          "topic3": null,
          "type": "log",
        },
      },
    ]
  `);
});
test("decodeFragment()", () => {
  const [blockFragment] = getFragments({
    ...EMPTY_BLOCK_FILTER,
    interval: 100,
    offset: 5,
  });
  expect(decodeFragment(encodeFragment(blockFragment!.fragment))).toStrictEqual(
    blockFragment!.fragment,
  );
  const [logFragment] = getFragments({
    ...EMPTY_LOG_FILTER,
    chainId: 1,
    address: ["0xa", "0xb"],
    topic0: zeroHash,
    topic1: null,
    topic2: "0xe",
    topic3: null,
  });
  expect(decodeFragment(encodeFragment(logFragment!.fragment))).toStrictEqual(
    logFragment!.fragment,
  );
  const [traceFragment] = getFragments({
    ...EMPTY_TRACE_FILTER,
    fromAddress: {
      id: `log_${"0xa"}_${1}_topic${1}_${"0xc"}_${"undefined"}_${"undefined"}`,
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: "0xa",
      eventSelector: "0xc",
      childAddressLocation: "topic1",
      fromBlock: undefined,
      toBlock: undefined,
    },
    toAddress: "0xb",
    includeReverted: false,
    functionSelector: "0xd",
    callType: "CALL",
  });
  expect(decodeFragment(encodeFragment(traceFragment!.fragment))).toStrictEqual(
    traceFragment!.fragment,
  );
  const [transferFragment] = getFragments({
    ...EMPTY_TRANSFER_FILTER,
    fromAddress: "0xa",
    toAddress: undefined,
    includeReverted: false,
  });
  expect(
    decodeFragment(encodeFragment(transferFragment!.fragment)),
  ).toStrictEqual(transferFragment!.fragment);
});
test("recoverFilter() block filter", () => {
  const filter = { ...EMPTY_BLOCK_FILTER, interval: 100, offset: 5 };
  const fragments = getFragments(filter);
  const recovered = recoverFilter(
    filter,
    fragments.map((f) => f.fragment),
  );
  expect(recovered).toStrictEqual(filter);
});
test("recoverFilter() transaction filter", () => {
  const filter = {
    ...EMPTY_TRANSACTION_FILTER,
    fromAddress: "0xa",
    toAddress: "0xb",
  } satisfies Filter;
  const fragments = getFragments(filter);
  const recovered = recoverFilter(
    filter,
    fragments.map((f) => f.fragment),
  );
  expect(recovered).toStrictEqual(filter);
});
test("recoverFilter() log filter", () => {
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: ["0xa", "0xb"],
    topic0: zeroHash,
    topic1: null,
    topic2: "0xe",
    topic3: null,
  } satisfies Filter;
  const fragments = getFragments(filter);
  const recovered = recoverFilter(
    filter,
    fragments.map((f) => f.fragment),
  );
  expect(recovered).toStrictEqual(filter);
});
test("recoverFilter() trace filter", () => {
  const filter = {
    ...EMPTY_TRACE_FILTER,
    callType: "CALL",
    functionSelector: "0xb",
    fromAddress: "0xa",
    toAddress: undefined,
  } satisfies Filter;
  const fragments = getFragments(filter);
  const recovered = recoverFilter(
    filter,
    fragments.map((f) => f.fragment),
  );
  expect(recovered).toStrictEqual(filter);
});
test("recoverFilter() transfer filter", () => {
  const filter = {
    ...EMPTY_TRANSFER_FILTER,
    fromAddress: "0xa",
    toAddress: undefined,
  } satisfies Filter;
  const fragments = getFragments(filter);
  const recovered = recoverFilter(
    filter,
    fragments.map((f) => f.fragment),
  );
  expect(recovered).toStrictEqual(filter);
});
test("recoverFilter() factory", () => {
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: {
      id: `log_${"0xa"}_${1}_topic${1}_${"0xb"}_${"undefined"}_${"undefined"}`,
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: "0xa",
      eventSelector: "0xb",
      childAddressLocation: "topic1",
      fromBlock: undefined,
      toBlock: undefined,
    },
  } satisfies Filter;
  const fragments = getFragments(filter);
  const recovered = recoverFilter(
    filter,
    fragments.map((f) => f.fragment),
  );
  expect(recovered).toStrictEqual(filter);
});
test("recoverFilter() multiple factories", () => {
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: {
      id: `log_${["0xa", "0xb"].join("_")}_${1}_topic${1}_${"0xc"}_${"undefined"}_${"undefined"}`,
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: ["0xa", "0xb"],
      eventSelector: "0xc",
      childAddressLocation: "topic1",
      fromBlock: undefined,
      toBlock: undefined,
    },
  } satisfies Filter;
  const fragments = getFragments(filter);
  const recovered = recoverFilter(
    filter,
    fragments.map((f) => f.fragment),
  );
  expect(recovered).toStrictEqual(filter);
});
</file>

<file path="packages/core/src/runtime/fragments.ts">
import type {
  BlockFilter,
  Factory,
  Filter,
  FilterAddress,
  Fragment,
  FragmentAddress,
  FragmentAddressId,
  FragmentId,
  LogFilter,
  TraceFilter,
  TransactionFilter,
  TransferFilter,
} from "@/internal/types.js";
import { dedupe } from "@/utils/dedupe.js";
import type { Address, Hex } from "viem";
import { isAddressFactory } from "./filter.js";
export const isFragmentAddressFactory = (
  fragmentAddress: FragmentAddress,
): boolean => {
  if (fragmentAddress === null) return false;
  if (typeof fragmentAddress === "string") return false;
  return true;
};
type FragmentReturnType = {
  fragment: Fragment;
  adjacentIds: FragmentId[];
}[];
export const getFragments = (filter: Filter): FragmentReturnType => {
  switch (filter.type) {
    case "block":
      return getBlockFilterFragment(filter as BlockFilter);
    case "transaction":
      return getTransactionFilterFragments(filter as TransactionFilter);
    case "trace":
      return getTraceFilterFragments(filter as TraceFilter);
    case "log":
      return getLogFilterFragments(filter as LogFilter);
    case "transfer":
      return getTransferFilterFragments(filter as TransferFilter);
  }
};
export const getFactoryFragments = (factory: Factory): Fragment[] => {
  const fragments: Fragment[] = [];
  for (const fragmentAddress of Array.isArray(factory.address)
    ? factory.address
    : [factory.address]) {
    const fragment = {
      type: "factory_log",
      chainId: factory.chainId,
      address: fragmentAddress ?? null,
      eventSelector: factory.eventSelector,
      childAddressLocation: factory.childAddressLocation,
      fromBlock: factory.fromBlock ?? null,
      toBlock: factory.toBlock ?? null,
    } satisfies Fragment;
    fragments.push(fragment);
  }
  return fragments;
};
export const getAddressFragments = (
  address: Address | Address[] | Factory | undefined,
) => {
  const fragments: {
    fragment: FragmentAddress;
    adjacentIds: FragmentAddressId[];
  }[] = [];
  if (isAddressFactory(address)) {
    for (const fragmentAddress of Array.isArray(address.address)
      ? address.address
      : [address.address]) {
      const fragment = {
        address: fragmentAddress ?? null,
        eventSelector: address.eventSelector,
        childAddressLocation: address.childAddressLocation,
      } satisfies FragmentAddress;
      fragments.push({
        fragment,
        adjacentIds: [
          `${fragmentAddress ?? null}_${address.eventSelector}_${address.childAddressLocation}` as const,
        ],
      });
    }
  } else {
    for (const fragmentAddress of Array.isArray(address)
      ? address
      : [address ?? null]) {
      fragments.push({
        fragment: fragmentAddress,
        adjacentIds: fragmentAddress
          ? [fragmentAddress, null]
          : [fragmentAddress],
      });
    }
  }
  return fragments;
};
export const getBlockFilterFragment = ({
  chainId,
  interval,
  offset,
}: Omit<BlockFilter, "fromBlock" | "toBlock">): FragmentReturnType => {
  return [
    {
      fragment: {
        type: "block",
        chainId,
        interval,
        offset,
      },
      adjacentIds: [`block_${chainId}_${interval}_${offset}`],
    },
  ];
};
export const getTransactionFilterFragments = ({
  chainId,
  fromAddress,
  toAddress,
}: Omit<TransactionFilter, "fromBlock" | "toBlock"> & {
  chainId: number;
}): FragmentReturnType => {
  const fragments: FragmentReturnType = [];
  const fromAddressFragments = getAddressFragments(fromAddress);
  const toAddressFragments = getAddressFragments(toAddress);
  for (const fromAddressFragment of fromAddressFragments) {
    for (const toAddressFragment of toAddressFragments) {
      const fragment = {
        type: "transaction",
        chainId,
        fromAddress: fromAddressFragment.fragment,
        toAddress: toAddressFragment.fragment,
      } satisfies Fragment;
      const adjacentIds: FragmentId[] = [];
      for (const fromAddressAdjacentId of fromAddressFragment.adjacentIds) {
        for (const toAddressAdjacentId of toAddressFragment.adjacentIds) {
          adjacentIds.push(
            `transaction_${chainId}_${fromAddressAdjacentId}_${toAddressAdjacentId}`,
          );
        }
      }
      fragments.push({ fragment, adjacentIds });
    }
  }
  return fragments;
};
export const getTraceFilterFragments = ({
  chainId,
  fromAddress,
  toAddress,
  callType,
  functionSelector,
  ...filter
}: Omit<TraceFilter, "fromBlock" | "toBlock"> & {
  chainId: number;
}): FragmentReturnType => {
  const fragments: FragmentReturnType = [];
  const fromAddressFragments = getAddressFragments(fromAddress);
  const toAddressFragments = getAddressFragments(toAddress);
  for (const fromAddressFragment of fromAddressFragments) {
    for (const toAddressFragment of toAddressFragments) {
      const fragment = {
        type: "trace",
        chainId,
        fromAddress: fromAddressFragment.fragment,
        toAddress: toAddressFragment.fragment,
        functionSelector,
        includeTransactionReceipts: filter.hasTransactionReceipt,
      } satisfies Fragment;
      const adjacentIds: FragmentId[] = [];
      for (const fromAddressAdjacentId of fromAddressFragment.adjacentIds) {
        for (const toAddressAdjacentId of toAddressFragment.adjacentIds) {
          for (const adjacentTxr of filter.hasTransactionReceipt
            ? [1]
            : [0, 1]) {
            adjacentIds.push(
              `trace_${chainId}_${fromAddressAdjacentId}_${toAddressAdjacentId}_${functionSelector}_${adjacentTxr as 0 | 1}`,
            );
          }
        }
      }
      fragments.push({ fragment, adjacentIds });
    }
  }
  return fragments;
};
export const getLogFilterFragments = ({
  chainId,
  address,
  topic0,
  topic1,
  topic2,
  topic3,
  ...filter
}: Omit<LogFilter, "fromBlock" | "toBlock">): FragmentReturnType => {
  const fragments: FragmentReturnType = [];
  const addressFragments = getAddressFragments(address);
  const eventSelector = topic0;
  for (const addressFragment of addressFragments) {
    for (const fragmentTopic1 of Array.isArray(topic1) ? topic1 : [topic1]) {
      for (const fragmentTopic2 of Array.isArray(topic2) ? topic2 : [topic2]) {
        for (const fragmentTopic3 of Array.isArray(topic3)
          ? topic3
          : [topic3]) {
          const fragment = {
            type: "log",
            chainId,
            address: addressFragment.fragment,
            topic0: eventSelector,
            topic1: fragmentTopic1 ?? null,
            topic2: fragmentTopic2 ?? null,
            topic3: fragmentTopic3 ?? null,
            includeTransactionReceipts: filter.hasTransactionReceipt,
          } satisfies Fragment;
          const adjacentIds: FragmentId[] = [];
          for (const addressAdjacentId of addressFragment.adjacentIds) {
            for (const adjacentTopic1 of fragmentTopic1
              ? [fragmentTopic1, null]
              : [null]) {
              for (const adjacentTopic2 of fragmentTopic2
                ? [fragmentTopic2, null]
                : [null]) {
                for (const adjacentTopic3 of fragmentTopic3
                  ? [fragmentTopic3, null]
                  : [null]) {
                  for (const adjacentTxr of filter.hasTransactionReceipt
                    ? [1]
                    : [0, 1]) {
                    adjacentIds.push(
                      `log_${chainId}_${addressAdjacentId}_${eventSelector}_${adjacentTopic1}_${adjacentTopic2}_${adjacentTopic3}_${adjacentTxr as 0 | 1}`,
                    );
                  }
                }
              }
            }
          }
          fragments.push({ fragment, adjacentIds });
        }
      }
    }
  }
  return fragments;
};
export const getTransferFilterFragments = ({
  chainId,
  fromAddress,
  toAddress,
  ...filter
}: Omit<TransferFilter, "fromBlock" | "toBlock"> & {
  chainId: number;
}): FragmentReturnType => {
  const fragments: FragmentReturnType = [];
  const fromAddressFragments = getAddressFragments(fromAddress);
  const toAddressFragments = getAddressFragments(toAddress);
  for (const fromAddressFragment of fromAddressFragments) {
    for (const toAddressFragment of toAddressFragments) {
      const fragment = {
        type: "transfer",
        chainId,
        fromAddress: fromAddressFragment.fragment,
        toAddress: toAddressFragment.fragment,
        includeTransactionReceipts: filter.hasTransactionReceipt,
      } satisfies Fragment;
      const adjacentIds: FragmentId[] = [];
      for (const fromAddressAdjacentId of fromAddressFragment.adjacentIds) {
        for (const toAddressAdjacentId of toAddressFragment.adjacentIds) {
          for (const adjacentTxr of filter.hasTransactionReceipt
            ? [1]
            : [0, 1]) {
            adjacentIds.push(
              `transfer_${chainId}_${fromAddressAdjacentId}_${toAddressAdjacentId}_${adjacentTxr as 0 | 1}`,
            );
          }
        }
      }
      fragments.push({ fragment, adjacentIds });
    }
  }
  return fragments;
};
export const fragmentAddressToId = (
  fragmentAddress: FragmentAddress,
): FragmentAddressId => {
  if (fragmentAddress === null) return null;
  if (typeof fragmentAddress === "string") return fragmentAddress;
  return `${fragmentAddress.address}_${fragmentAddress.eventSelector}_${fragmentAddress.childAddressLocation}`;
};
export const encodeFragment = (fragment: Fragment): FragmentId => {
  switch (fragment.type) {
    case "block":
      return `block_${fragment.chainId}_${fragment.interval}_${fragment.offset}`;
    case "transaction":
      return `transaction_${fragment.chainId}_${fragmentAddressToId(fragment.fromAddress)}_${fragmentAddressToId(fragment.toAddress)}`;
    case "trace":
      return `trace_${fragment.chainId}_${fragmentAddressToId(fragment.fromAddress)}_${fragmentAddressToId(fragment.toAddress)}_${fragment.functionSelector}_${fragment.includeTransactionReceipts ? 1 : 0}`;
    case "log":
      return `log_${fragment.chainId}_${fragmentAddressToId(fragment.address)}_${fragment.topic0}_${fragment.topic1}_${fragment.topic2}_${fragment.topic3}_${fragment.includeTransactionReceipts ? 1 : 0}`;
    case "transfer":
      return `transfer_${fragment.chainId}_${fragmentAddressToId(fragment.fromAddress)}_${fragmentAddressToId(fragment.toAddress)}_${fragment.includeTransactionReceipts ? 1 : 0}`;
    case "factory_log":
      return `factory_log_${fragment.chainId}_${fragment.address}_${fragment.eventSelector}_${fragment.childAddressLocation}_${fragment.fromBlock}_${fragment.toBlock}`;
  }
};
export const decodeFragment = (fragmentId: FragmentId): Fragment => {
  const [type, chainId] = fragmentId.split("_");
  const decodeFragmentAddress = (offset: number): FragmentAddress => {
    const fragmentAddressId = fragmentId.split("_").slice(offset);
    if (fragmentAddressId[0] === "null") {
      return null;
    }
    if (fragmentAddressId.length === 1) {
      return fragmentAddressId[0] as Address;
    }
    if (
      fragmentAddressId.length >= 3 &&
      (fragmentAddressId[2]!.startsWith("topic") ||
        fragmentAddressId[2]!.startsWith("offset"))
    ) {
      return {
        address: fragmentAddressId[0] as Address,
        eventSelector: fragmentAddressId[1] as Hex,
        childAddressLocation:
          fragmentAddressId[2] as Factory["childAddressLocation"],
      } satisfies FragmentAddress;
    }
    return fragmentAddressId[0] as Address;
  };
  switch (type! as Fragment["type"]) {
    case "block": {
      const [, chainId, interval, offset] = fragmentId.split("_");
      return {
        type: "block",
        chainId: Number(chainId),
        interval: Number(interval),
        offset: Number(offset),
      };
    }
    case "transaction": {
      const fragmentFromAddress = decodeFragmentAddress(2);
      if (isFragmentAddressFactory(fragmentFromAddress)) {
        const fragmentToAddress = decodeFragmentAddress(5);
        return {
          type: "transaction",
          chainId: Number(chainId),
          fromAddress: fragmentFromAddress,
          toAddress: fragmentToAddress,
        };
      }
      const fragmentToAddress = decodeFragmentAddress(3);
      return {
        type: "transaction",
        chainId: Number(chainId),
        fromAddress: fragmentFromAddress,
        toAddress: fragmentToAddress,
      };
    }
    case "trace": {
      const fragmentFromAddress = decodeFragmentAddress(2);
      if (isFragmentAddressFactory(fragmentFromAddress)) {
        const fragmentToAddress = decodeFragmentAddress(5);
        if (isFragmentAddressFactory(fragmentToAddress)) {
          const [, , , , , , , , functionSelector, includeTxr] =
            fragmentId.split("_");
          return {
            type: "trace",
            chainId: Number(chainId),
            fromAddress: fragmentFromAddress,
            toAddress: fragmentToAddress,
            functionSelector: functionSelector as Hex,
            includeTransactionReceipts: includeTxr === "1",
          };
        }
        const [, , , , , , functionSelector, includeTxr] =
          fragmentId.split("_");
        return {
          type: "trace",
          chainId: Number(chainId),
          fromAddress: fragmentFromAddress,
          toAddress: fragmentToAddress,
          functionSelector: functionSelector as Hex,
          includeTransactionReceipts: includeTxr === "1",
        };
      }
      const fragmentToAddress = decodeFragmentAddress(3);
      if (isFragmentAddressFactory(fragmentToAddress)) {
        const [, , , , , , functionSelector, includeTxr] =
          fragmentId.split("_");
        return {
          type: "trace",
          chainId: Number(chainId),
          fromAddress: fragmentFromAddress,
          toAddress: fragmentToAddress,
          functionSelector: functionSelector as Hex,
          includeTransactionReceipts: includeTxr === "1",
        };
      }
      const [, , , , functionSelector, includeTxr] = fragmentId.split("_");
      return {
        type: "trace",
        chainId: Number(chainId),
        fromAddress: fragmentFromAddress,
        toAddress: fragmentToAddress,
        functionSelector: functionSelector as Hex,
        includeTransactionReceipts: includeTxr === "1",
      };
    }
    case "log": {
      const fragmentAddress = decodeFragmentAddress(2);
      if (isFragmentAddressFactory(fragmentAddress)) {
        const [, , , , , topic0, topic1, topic2, topic3, includeTxr] =
          fragmentId.split("_");
        return {
          type: "log",
          chainId: Number(chainId),
          address: fragmentAddress,
          topic0: topic0 as Hex,
          topic1: topic1! === "null" ? null : (topic1 as Hex),
          topic2: topic2! === "null" ? null : (topic2 as Hex),
          topic3: topic3! === "null" ? null : (topic3 as Hex),
          includeTransactionReceipts: includeTxr === "1",
        };
      }
      const [, , , topic0, topic1, topic2, topic3, includeTxr] =
        fragmentId.split("_");
      return {
        type: "log",
        chainId: Number(chainId),
        address: fragmentAddress,
        topic0: topic0 as Hex,
        topic1: topic1! === "null" ? null : (topic1 as Hex),
        topic2: topic2! === "null" ? null : (topic2 as Hex),
        topic3: topic3! === "null" ? null : (topic3 as Hex),
        includeTransactionReceipts: includeTxr === "1",
      };
    }
    case "transfer": {
      const fragmentFromAddress = decodeFragmentAddress(2);
      if (isFragmentAddressFactory(fragmentFromAddress)) {
        const fragmentToAddress = decodeFragmentAddress(5);
        if (isFragmentAddressFactory(fragmentToAddress)) {
          const [, , , , , , , , includeTxr] = fragmentId.split("_");
          return {
            type: "transfer",
            chainId: Number(chainId),
            fromAddress: fragmentFromAddress,
            toAddress: fragmentToAddress,
            includeTransactionReceipts: includeTxr === "1",
          };
        }
        const [, , , , , , includeTxr] = fragmentId.split("_");
        return {
          type: "transfer",
          chainId: Number(chainId),
          fromAddress: fragmentFromAddress,
          toAddress: fragmentToAddress,
          includeTransactionReceipts: includeTxr === "1",
        };
      }
      const fragmentToAddress = decodeFragmentAddress(3);
      if (isFragmentAddressFactory(fragmentToAddress)) {
        const [, , , , , , includeTxr] = fragmentId.split("_");
        return {
          type: "transfer",
          chainId: Number(chainId),
          fromAddress: fragmentFromAddress,
          toAddress: fragmentToAddress,
          includeTransactionReceipts: includeTxr === "1",
        };
      }
      const [, , , , includeTxr] = fragmentId.split("_");
      return {
        type: "transfer",
        chainId: Number(chainId),
        fromAddress: fragmentFromAddress,
        toAddress: fragmentToAddress,
        includeTransactionReceipts: includeTxr === "1",
      };
    }
    case "factory_log": {
      const [
        ,
        chainId,
        address,
        eventSelector,
        childAddressLocation,
        fromBlock,
        toBlock,
      ] = fragmentId.split("_");
      return {
        type: "factory_log",
        chainId: Number(chainId),
        address: address as Address,
        eventSelector: eventSelector as Factory["eventSelector"],
        childAddressLocation:
          childAddressLocation as Factory["childAddressLocation"],
        fromBlock: fromBlock ? Number(fromBlock) : null,
        toBlock: toBlock ? Number(toBlock) : null,
      };
    }
  }
};
const recoverAddress = <filterAddress extends FilterAddress>(
  baseAddress: filterAddress,
  fragmentAddresses: FragmentAddress[],
): filterAddress => {
  if (baseAddress === undefined) return undefined as filterAddress;
  if (typeof baseAddress === "string") return baseAddress as filterAddress;
  if (Array.isArray(baseAddress)) {
    return dedupe(fragmentAddresses) as filterAddress;
  }
  // Note: At this point, `baseAddress` is a factory. We explicitly don't try to recover the factory
  // address from the fragments because we want a `insertChildAddresses` and `getChildAddresses` to
  // use the factory as a stable key.
  return baseAddress;
};
const recoverTopic = (
  base: Hex | Hex[] | null,
  fragments: (Hex | null)[],
): Hex | Hex[] | null => {
  if (base === null) return null;
  if (typeof base === "string") return base;
  return dedupe(fragments) as Hex[];
};
export const recoverFilter = (
  baseFilter: Filter,
  fragments: Fragment[],
): Filter => {
  switch (baseFilter.type) {
    case "block": {
      return baseFilter;
    }
    case "transaction": {
      return {
        ...baseFilter,
        fromAddress: recoverAddress(
          baseFilter.fromAddress,
          (fragments as Extract<Fragment, { type: "transaction" }>[]).map(
            (fragment) => fragment.fromAddress,
          ),
        ),
        toAddress: recoverAddress(
          baseFilter.toAddress,
          (fragments as Extract<Fragment, { type: "transaction" }>[]).map(
            (fragment) => fragment.toAddress,
          ),
        ),
      };
    }
    case "trace": {
      return {
        ...baseFilter,
        fromAddress: recoverAddress(
          baseFilter.fromAddress,
          (fragments as Extract<Fragment, { type: "transaction" }>[]).map(
            (fragment) => fragment.fromAddress,
          ),
        ),
        toAddress: recoverAddress(
          baseFilter.toAddress,
          (fragments as Extract<Fragment, { type: "transaction" }>[]).map(
            (fragment) => fragment.toAddress,
          ),
        ),
        functionSelector: baseFilter.functionSelector,
      };
    }
    case "log": {
      return {
        ...baseFilter,
        address: recoverAddress(
          baseFilter.address,
          (fragments as Extract<Fragment, { type: "log" }>[]).map(
            (fragment) => fragment.address,
          ),
        ),
        topic0: baseFilter.topic0,
        topic1: recoverTopic(
          baseFilter.topic1,
          fragments.map(
            (fragment) =>
              (fragment as Extract<Fragment, { type: "log" }>).topic1,
          ),
        ),
        topic2: recoverTopic(
          baseFilter.topic2,
          fragments.map(
            (fragment) =>
              (fragment as Extract<Fragment, { type: "log" }>).topic2,
          ),
        ),
        topic3: recoverTopic(
          baseFilter.topic3,
          fragments.map(
            (fragment) =>
              (fragment as Extract<Fragment, { type: "log" }>).topic3,
          ),
        ),
      };
    }
    case "transfer": {
      return {
        ...baseFilter,
        fromAddress: recoverAddress(
          baseFilter.fromAddress,
          (fragments as Extract<Fragment, { type: "transaction" }>[]).map(
            (fragment) => fragment.fromAddress,
          ),
        ),
        toAddress: recoverAddress(
          baseFilter.toAddress,
          (fragments as Extract<Fragment, { type: "transaction" }>[]).map(
            (fragment) => fragment.toAddress,
          ),
        ),
      };
    }
  }
};
</file>

<file path="packages/core/src/runtime/historical.test.ts">
import {
  context,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { setupAnvil } from "@/_test/setup.js";
import { getBlocksIndexingBuild, getChain, testClient } from "@/_test/utils.js";
import type { Chain } from "@/internal/types.js";
import { eth_getBlockByNumber } from "@/rpc/actions.js";
import { createRpc } from "@/rpc/index.js";
import * as ponderSyncSchema from "@/sync-store/schema.js";
import { MAX_CHECKPOINT_STRING } from "@/utils/checkpoint.js";
import { drainAsyncGenerator } from "@/utils/generators.js";
import { beforeEach, expect, test, vi } from "vitest";
import {
  getHistoricalEventsMultichain,
  getLocalEventGenerator,
  getLocalSyncGenerator,
} from "./historical.js";
import {
  type CachedIntervals,
  type ChildAddresses,
  type SyncProgress,
  getCachedIntervals,
  getChildAddresses,
  getLocalSyncProgress,
} from "./index.js";
beforeEach(setupCommon);
beforeEach(setupAnvil);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
test("getLocalEventGenerator()", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await testClient.mine({ blocks: 1 });
  const cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  const syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x1", true]),
    cachedIntervals,
  });
  const eventGenerator = getLocalEventGenerator({
    common: context.common,
    chain,
    rpc,
    eventCallbacks,
    childAddresses: new Map(),
    syncProgress,
    cachedIntervals,
    database,
    from: syncProgress.getCheckpoint({ tag: "start" })!,
    to: syncProgress.getCheckpoint({ tag: "finalized" })!,
    limit: 100,
    isCatchup: false,
  });
  const events = await drainAsyncGenerator(eventGenerator);
  expect(events).toHaveLength(1);
});
test("getLocalEventGenerator() pagination", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await testClient.mine({ blocks: 2 });
  const cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  const syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x2", true]),
    cachedIntervals,
  });
  const eventGenerator = getLocalEventGenerator({
    common: context.common,
    chain,
    rpc,
    database,
    eventCallbacks,
    childAddresses: new Map(),
    syncProgress,
    cachedIntervals,
    from: syncProgress.getCheckpoint({ tag: "start" })!,
    to: syncProgress.getCheckpoint({ tag: "finalized" })!,
    limit: 1,
    isCatchup: false,
  });
  const events = await drainAsyncGenerator(eventGenerator);
  expect(events.length).toBeGreaterThan(1);
});
test("getLocalEventGenerator() pagination with zero interval", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await testClient.mine({ blocks: 2 });
  const cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  const syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x0", true]),
    cachedIntervals,
  });
  const eventGenerator = getLocalEventGenerator({
    common: context.common,
    chain,
    rpc,
    database,
    eventCallbacks,
    childAddresses: new Map(),
    syncProgress,
    cachedIntervals,
    from: syncProgress.getCheckpoint({ tag: "start" })!,
    to: syncProgress.getCheckpoint({ tag: "finalized" })!,
    limit: 1,
    isCatchup: false,
  });
  const events = await drainAsyncGenerator(eventGenerator);
  expect(events.length).toBe(1);
});
test("getLocalSyncGenerator()", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await testClient.mine({ blocks: 1 });
  const cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  const syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x1", true]),
    cachedIntervals,
  });
  const syncGenerator = getLocalSyncGenerator({
    common: context.common,
    chain,
    rpc,
    eventCallbacks,
    childAddresses: new Map(),
    cachedIntervals,
    database,
    syncProgress,
    isCatchup: false,
  });
  await drainAsyncGenerator(syncGenerator);
  const intervals = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.intervals).execute(),
  );
  expect(intervals).toHaveLength(1);
  expect(intervals[0]!.blocks).toBe("{[0,2]}");
});
test("getLocalSyncGenerator() with partial cache", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await testClient.mine({ blocks: 1 });
  let cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  let syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x1", true]),
    cachedIntervals,
  });
  let syncGenerator = getLocalSyncGenerator({
    common: context.common,
    chain,
    rpc,
    eventCallbacks,
    childAddresses: new Map(),
    syncProgress,
    cachedIntervals,
    database,
    isCatchup: false,
  });
  await drainAsyncGenerator(syncGenerator);
  await testClient.mine({ blocks: 1 });
  cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x2", true]),
    cachedIntervals,
  });
  syncGenerator = getLocalSyncGenerator({
    common: context.common,
    chain,
    rpc,
    eventCallbacks,
    childAddresses: new Map(),
    syncProgress,
    cachedIntervals,
    database,
    isCatchup: false,
  });
  await drainAsyncGenerator(syncGenerator);
  const intervals = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.intervals).execute(),
  );
  expect(intervals).toHaveLength(1);
  expect(intervals[0]!.blocks).toBe("{[0,3]}");
});
test("getLocalSyncGenerator() with full cache", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await testClient.mine({ blocks: 1 });
  // finalized block: 1
  chain.finalityBlockCount = 0;
  let cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  let syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x1", true]),
    cachedIntervals,
  });
  let syncGenerator = getLocalSyncGenerator({
    common: context.common,
    chain,
    rpc,
    eventCallbacks,
    childAddresses: new Map(),
    syncProgress,
    cachedIntervals,
    database,
    isCatchup: false,
  });
  await drainAsyncGenerator(syncGenerator);
  cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x1", true]),
    cachedIntervals,
  });
  syncGenerator = getLocalSyncGenerator({
    common: context.common,
    chain,
    rpc,
    eventCallbacks,
    childAddresses: new Map(),
    syncProgress,
    cachedIntervals,
    database,
    isCatchup: false,
  });
  const insertSpy = vi.spyOn(syncStore, "insertIntervals");
  const requestSpy = vi.spyOn(rpc, "request");
  const checkpoints = await drainAsyncGenerator(syncGenerator);
  expect(checkpoints).toHaveLength(1);
  expect(insertSpy).toHaveBeenCalledTimes(0);
  expect(requestSpy).toHaveBeenCalledTimes(0);
});
test("getHistoricalEventsMultichain()", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await testClient.mine({ blocks: 1 });
  const perChainSync = new Map<
    Chain,
    {
      syncProgress: SyncProgress;
      childAddresses: ChildAddresses;
      cachedIntervals: CachedIntervals;
    }
  >();
  const cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  const syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x1", true]),
    cachedIntervals,
  });
  const childAddresses = await getChildAddresses({
    filters: eventCallbacks.map(({ filter }) => filter),
    syncStore,
  });
  perChainSync.set(chain, { syncProgress, childAddresses, cachedIntervals });
  const events = await drainAsyncGenerator(
    getHistoricalEventsMultichain({
      common: context.common,
      indexingBuild: {
        eventCallbacks: [eventCallbacks],
        chains: [chain],
        rpcs: [rpc],
        finalizedBlocks: [await eth_getBlockByNumber(rpc, ["0x1", true])],
      },
      crashRecoveryCheckpoint: undefined,
      perChainSync,
      database,
    }),
  );
  expect(events).toHaveLength(1);
  expect(events.flatMap(({ events }) => events)).toHaveLength(2);
});
test("getHistoricalEvents() omnichain", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await testClient.mine({ blocks: 1 });
  const perChainSync = new Map<
    Chain,
    {
      syncProgress: SyncProgress;
      childAddresses: ChildAddresses;
      cachedIntervals: CachedIntervals;
    }
  >();
  const cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  const syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x1", true]),
    cachedIntervals,
  });
  const childAddresses = await getChildAddresses({
    filters: eventCallbacks.map(({ filter }) => filter),
    syncStore,
  });
  perChainSync.set(chain, { syncProgress, childAddresses, cachedIntervals });
  const events = await drainAsyncGenerator(
    getHistoricalEventsMultichain({
      common: context.common,
      indexingBuild: {
        eventCallbacks: [eventCallbacks],
        chains: [chain],
        rpcs: [rpc],
        finalizedBlocks: [await eth_getBlockByNumber(rpc, ["0x1", true])],
      },
      crashRecoveryCheckpoint: undefined,
      perChainSync,
      database,
    }),
  );
  expect(events).toHaveLength(1);
  expect(events.flatMap(({ events }) => events)).toHaveLength(2);
});
test("getHistoricalEvents() with crash recovery checkpoint", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await testClient.mine({ blocks: 2 });
  const perChainSync = new Map<
    Chain,
    {
      syncProgress: SyncProgress;
      childAddresses: ChildAddresses;
      cachedIntervals: CachedIntervals;
    }
  >();
  const cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  const syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x2", true]),
    cachedIntervals,
  });
  const childAddresses = await getChildAddresses({
    filters: eventCallbacks.map(({ filter }) => filter),
    syncStore,
  });
  perChainSync.set(chain, { syncProgress, childAddresses, cachedIntervals });
  const events = await drainAsyncGenerator(
    getHistoricalEventsMultichain({
      common: context.common,
      indexingBuild: {
        eventCallbacks: [eventCallbacks],
        chains: [chain],
        rpcs: [rpc],
        finalizedBlocks: [await eth_getBlockByNumber(rpc, ["0x2", true])],
      },
      crashRecoveryCheckpoint: [
        { chainId: 1, checkpoint: MAX_CHECKPOINT_STRING },
      ],
      perChainSync,
      database,
    }),
  );
  expect(events.flatMap(({ events }) => events)).toHaveLength(0);
});
</file>

<file path="packages/core/src/runtime/historical.ts">
import type { Database } from "@/database/index.js";
import type { Common } from "@/internal/common.js";
import { ShutdownError } from "@/internal/errors.js";
import type {
  Chain,
  CrashRecoveryCheckpoint,
  Event,
  EventCallback,
  IndexingBuild,
  RawEvent,
  SyncBlock,
} from "@/internal/types.js";
import { eth_getBlockByNumber } from "@/rpc/actions.js";
import type { Rpc } from "@/rpc/index.js";
import { buildEvents, decodeEvents } from "@/runtime/events.js";
import { createHistoricalSync } from "@/sync-historical/index.js";
import { type SyncStore, createSyncStore } from "@/sync-store/index.js";
import {
  MAX_CHECKPOINT,
  ZERO_CHECKPOINT,
  decodeCheckpoint,
  encodeCheckpoint,
  min,
} from "@/utils/checkpoint.js";
import { estimate } from "@/utils/estimate.js";
import { formatPercentage } from "@/utils/format.js";
import {
  bufferAsyncGenerator,
  createCallbackGenerator,
  mergeAsyncGenerators,
} from "@/utils/generators.js";
import { type Interval, intervalSum } from "@/utils/interval.js";
import { partition } from "@/utils/partition.js";
import { promiseWithResolvers } from "@/utils/promiseWithResolvers.js";
import { startClock } from "@/utils/timer.js";
import { hexToNumber, numberToHex } from "viem";
import {
  type CachedIntervals,
  type ChildAddresses,
  type SyncProgress,
  getRequiredIntervals,
  getRequiredIntervalsWithFilters,
} from "./index.js";
import { initEventGenerator, initRefetchEvents } from "./init.js";
import { getOmnichainCheckpoint } from "./omnichain.js";
export async function* getHistoricalEventsOmnichain(params: {
  common: Common;
  indexingBuild: Pick<
    IndexingBuild,
    "eventCallbacks" | "chains" | "rpcs" | "finalizedBlocks"
  >;
  crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  perChainSync: Map<
    Chain,
    {
      syncProgress: SyncProgress;
      childAddresses: ChildAddresses;
      cachedIntervals: CachedIntervals;
    }
  >;
  database: Database;
}): AsyncGenerator<
  | {
      type: "events";
      result: {
        chainId: number;
        events: Event[];
        checkpoint: string;
        blockRange: [number, number];
      }[];
    }
  | { type: "pending"; result: Event[] }
> {
  let pendingEvents: Event[] = [];
  let isCatchup = false;
  const perChainCursor = new Map<Chain, string>();
  while (true) {
    const eventGenerators = Array.from(params.perChainSync.entries()).map(
      async function* ([
        chain,
        { syncProgress, childAddresses, cachedIntervals },
      ]) {
        const rpc =
          params.indexingBuild.rpcs[
            params.indexingBuild.chains.findIndex((c) => c.id === chain.id)
          ]!;
        const eventCallbacks =
          params.indexingBuild.eventCallbacks[
            params.indexingBuild.chains.findIndex((c) => c.id === chain.id)
          ]!;
        const crashRecoveryCheckpoint = params.crashRecoveryCheckpoint?.find(
          ({ chainId }) => chainId === chain.id,
        )?.checkpoint;
        const to = min(
          syncProgress.getCheckpoint({ tag: "finalized" }),
          syncProgress.getCheckpoint({ tag: "end" }),
        );
        const omnichainTo = min(
          getOmnichainCheckpoint({
            perChainSync: params.perChainSync,
            tag: "finalized",
          }),
          getOmnichainCheckpoint({
            perChainSync: params.perChainSync,
            tag: "end",
          }),
        );
        let from: string;
        if (isCatchup === false) {
          // In order to speed up the "extract" phase when there is a crash recovery,
          // the beginning cursor is moved forwards. This only works when `crashRecoveryCheckpoint`
          // is defined.
          if (crashRecoveryCheckpoint === undefined) {
            from = syncProgress.getCheckpoint({ tag: "start" });
          } else if (
            Number(decodeCheckpoint(crashRecoveryCheckpoint).chainId) ===
            chain.id
          ) {
            from = crashRecoveryCheckpoint;
          } else {
            const fromBlock = await createSyncStore({
              common: params.common,
              qb: params.database.syncQB,
            }).getSafeCrashRecoveryBlock({
              chainId: chain.id,
              timestamp: Number(
                decodeCheckpoint(crashRecoveryCheckpoint).blockTimestamp,
              ),
            });
            if (fromBlock === undefined) {
              from = syncProgress.getCheckpoint({ tag: "start" });
            } else {
              from = encodeCheckpoint({
                ...ZERO_CHECKPOINT,
                blockNumber: fromBlock.number,
                blockTimestamp: fromBlock.timestamp,
                chainId: BigInt(chain.id),
              });
            }
          }
        } else {
          // Previous iterations `to` value
          const cursor = perChainCursor.get(chain)!;
          // Yield pending events from previous iterations. Note that it is possible for
          // previous pending events to still be pending after the catchup.
          const events = pendingEvents.filter(
            (event) =>
              event.chain.id === chain.id && event.checkpoint <= omnichainTo,
          );
          pendingEvents = pendingEvents.filter(
            (event) =>
              (event.chain.id === chain.id &&
                event.checkpoint <= omnichainTo) === false,
          );
          if (events.length > 0) {
            if (omnichainTo >= cursor) {
              const blockRange = [
                Number(decodeCheckpoint(events[0]!.checkpoint).blockNumber),
                Number(decodeCheckpoint(cursor).blockNumber),
              ] satisfies [number, number];
              yield { events, checkpoint: cursor, blockRange };
            } else {
              const checkpoint = events[events.length - 1]!.checkpoint;
              const blockRange = [
                Number(decodeCheckpoint(events[0]!.checkpoint).blockNumber),
                Number(decodeCheckpoint(checkpoint).blockNumber),
              ] satisfies [number, number];
              yield { events, checkpoint, blockRange };
            }
          }
          from = encodeCheckpoint({
            ...ZERO_CHECKPOINT,
            blockTimestamp: decodeCheckpoint(cursor).blockTimestamp,
            chainId: decodeCheckpoint(cursor).chainId,
            blockNumber: decodeCheckpoint(cursor).blockNumber + 1n,
          });
          if (from > to) return;
        }
        params.common.logger.info({
          msg: "Started backfill indexing",
          chain: chain.name,
          chain_id: chain.id,
          block_range: JSON.stringify([
            Number(decodeCheckpoint(from).blockNumber),
            Number(decodeCheckpoint(to).blockNumber),
          ]),
        });
        const eventGenerator = await initEventGenerator({
          common: params.common,
          indexingBuild: params.indexingBuild,
          chain,
          rpc,
          eventCallbacks,
          childAddresses,
          syncProgress,
          cachedIntervals,
          from,
          to,
          limit:
            Math.round(
              params.common.options.syncEventsQuerySize /
                (params.indexingBuild.chains.length + 1),
            ) + 6,
          database: params.database,
          isCatchup,
        });
        for await (let {
          events: rawEvents,
          checkpoint,
          blockRange,
        } of eventGenerator) {
          const endClock = startClock();
          let events = decodeEvents(
            params.common,
            chain,
            eventCallbacks,
            rawEvents,
          );
          params.common.logger.trace({
            msg: "Decoded events",
            chain: chain.name,
            chain_id: chain.id,
            event_count: events.length,
            duration: endClock(),
          });
          params.common.metrics.ponder_historical_extract_duration.inc(
            { step: "decode" },
            endClock(),
          );
          // Removes events that have a checkpoint earlier than (or equal to)
          // the crash recovery checkpoint.
          if (crashRecoveryCheckpoint) {
            const [left, right] = partition(
              events,
              (event) => event.checkpoint <= crashRecoveryCheckpoint,
            );
            events = right;
            if (left.length > 0) {
              params.common.logger.trace({
                msg: "Filtered events before crash recovery checkpoint",
                chain: chain.name,
                chain_id: chain.id,
                event_count: left.length,
                checkpoint: crashRecoveryCheckpoint,
              });
            }
          }
          // Sort out any events between the omnichain finalized checkpoint and the single-chain
          // finalized checkpoint and add them to pendingEvents. These events are synced during
          // the historical phase, but must be indexed in the realtime phase because events
          // synced in realtime on other chains might be ordered before them.
          if (checkpoint > omnichainTo) {
            const [left, right] = partition(
              events,
              (event) => event.checkpoint <= omnichainTo,
            );
            events = left;
            pendingEvents = pendingEvents.concat(right);
            params.common.logger.trace({
              msg: "Filtered pending events",
              chain: chain.name,
              chain_id: chain.id,
              event_count: right.length,
              checkpoint: omnichainTo,
            });
            if (left.length > 0) {
              checkpoint = left[left.length - 1]!.checkpoint;
              blockRange[1] = Number(
                decodeCheckpoint(left[left.length - 1]!.checkpoint).blockNumber,
              );
              yield { events, checkpoint, blockRange };
            }
          } else {
            yield { events, checkpoint, blockRange };
          }
        }
        perChainCursor.set(chain, to);
      },
    );
    const eventGenerator = mergeAsyncGeneratorsWithEventOrder(eventGenerators);
    for await (const mergeResults of eventGenerator) {
      yield { type: "events", result: mergeResults };
    }
    const context = {
      logger: params.common.logger.child({ action: "refetch_finalized_block" }),
      retryNullBlockRequest: true,
    };
    const endClock = startClock();
    const finalizedBlocks = await Promise.all(
      params.indexingBuild.chains.map((chain, i) => {
        const rpc = params.indexingBuild.rpcs[i]!;
        return eth_getBlockByNumber(rpc, ["latest", false], context)
          .then((latest) =>
            eth_getBlockByNumber(
              rpc,
              [
                numberToHex(
                  Math.max(
                    hexToNumber(latest.number) - chain.finalityBlockCount,
                    0,
                  ),
                ),
                false,
              ],
              context,
            ),
          )
          .then((finalizedBlock) => {
            const finalizedBlockNumber = hexToNumber(finalizedBlock.number);
            params.common.logger.debug({
              msg: "Refetched finalized block for backfill cutover",
              chain: chain.name,
              chain_id: chain.id,
              finalized_block: finalizedBlockNumber,
              duration: endClock(),
            });
            return finalizedBlock;
          });
      }),
    );
    let shouldCatchup = false;
    for (let i = 0; i < params.indexingBuild.chains.length; i++) {
      const chain = params.indexingBuild.chains[i]!;
      const oldFinalizedBlock =
        params.perChainSync.get(chain)!.syncProgress.finalized;
      const newFinalizedBlock = finalizedBlocks[i]!;
      if (
        hexToNumber(newFinalizedBlock.number) -
          hexToNumber(oldFinalizedBlock.number) >
        chain.finalityBlockCount
      ) {
        shouldCatchup = true;
        break;
      }
    }
    if (shouldCatchup === false) break;
    for (let i = 0; i < params.indexingBuild.chains.length; i++) {
      const chain = params.indexingBuild.chains[i]!;
      const finalizedBlock = finalizedBlocks[i]!;
      params.perChainSync.get(chain)!.syncProgress.finalized = finalizedBlock;
    }
    isCatchup = true;
  }
  yield { type: "pending", result: pendingEvents };
}
export async function* getHistoricalEventsMultichain(params: {
  common: Common;
  indexingBuild: Pick<
    IndexingBuild,
    "eventCallbacks" | "chains" | "rpcs" | "finalizedBlocks"
  >;
  crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  perChainSync: Map<
    Chain,
    {
      syncProgress: SyncProgress;
      childAddresses: ChildAddresses;
      cachedIntervals: CachedIntervals;
    }
  >;
  database: Database;
}) {
  let isCatchup = false;
  let lastUnfinalizedRefetch = Date.now();
  const perChainCursor = new Map<Chain, string>();
  while (true) {
    const eventGenerators = Array.from(params.perChainSync.entries()).map(
      async function* ([
        chain,
        { syncProgress, childAddresses, cachedIntervals },
      ]) {
        const rpc =
          params.indexingBuild.rpcs[
            params.indexingBuild.chains.findIndex((c) => c.id === chain.id)
          ]!;
        const eventCallbacks =
          params.indexingBuild.eventCallbacks[
            params.indexingBuild.chains.findIndex((c) => c.id === chain.id)
          ]!;
        const crashRecoveryCheckpoint = params.crashRecoveryCheckpoint?.find(
          ({ chainId }) => chainId === chain.id,
        )?.checkpoint;
        const to = min(
          syncProgress.getCheckpoint({ tag: "finalized" }),
          syncProgress.getCheckpoint({ tag: "end" }),
        );
        let from: string;
        if (isCatchup === false) {
          // In order to speed up the "extract" phase when there is a crash recovery,
          // the beginning cursor is moved forwards. This only works when `crashRecoveryCheckpoint`
          // is defined.
          if (crashRecoveryCheckpoint === undefined) {
            from = syncProgress.getCheckpoint({ tag: "start" });
          } else if (
            Number(decodeCheckpoint(crashRecoveryCheckpoint).chainId) ===
            chain.id
          ) {
            from = crashRecoveryCheckpoint;
          } else {
            const fromBlock = await createSyncStore({
              common: params.common,
              qb: params.database.syncQB,
            }).getSafeCrashRecoveryBlock({
              chainId: chain.id,
              timestamp: Number(
                decodeCheckpoint(crashRecoveryCheckpoint).blockTimestamp,
              ),
            });
            if (fromBlock === undefined) {
              from = syncProgress.getCheckpoint({ tag: "start" });
            } else {
              from = encodeCheckpoint({
                ...ZERO_CHECKPOINT,
                blockNumber: fromBlock.number,
                blockTimestamp: fromBlock.timestamp,
                chainId: BigInt(chain.id),
              });
            }
          }
        } else {
          const cursor = perChainCursor.get(chain)!;
          from = encodeCheckpoint({
            ...ZERO_CHECKPOINT,
            blockTimestamp: decodeCheckpoint(cursor).blockTimestamp,
            chainId: decodeCheckpoint(cursor).chainId,
            blockNumber: decodeCheckpoint(cursor).blockNumber + 1n,
          });
          if (from > to) return;
        }
        params.common.logger.info({
          msg: "Started backfill indexing",
          chain: chain.name,
          chain_id: chain.id,
          block_range: JSON.stringify([
            Number(decodeCheckpoint(from).blockNumber),
            Number(decodeCheckpoint(to).blockNumber),
          ]),
        });
        const eventGenerator = await initEventGenerator({
          common: params.common,
          indexingBuild: params.indexingBuild,
          chain,
          rpc,
          eventCallbacks,
          childAddresses,
          syncProgress,
          cachedIntervals,
          from,
          to,
          limit:
            Math.round(
              params.common.options.syncEventsQuerySize /
                (params.indexingBuild.chains.length + 1),
            ) + 6,
          database: params.database,
          isCatchup,
        });
        for await (const {
          events: rawEvents,
          checkpoint,
          blockRange,
        } of eventGenerator) {
          const endClock = startClock();
          let events = decodeEvents(
            params.common,
            chain,
            eventCallbacks,
            rawEvents,
          );
          params.common.logger.trace({
            msg: "Decoded events",
            chain: chain.name,
            chain_id: chain.id,
            event_count: events.length,
            duration: endClock(),
          });
          params.common.metrics.ponder_historical_extract_duration.inc(
            { step: "decode" },
            endClock(),
          );
          // Removes events that have a checkpoint earlier than (or equal to)
          // the crash recovery checkpoint.
          if (crashRecoveryCheckpoint) {
            const [left, right] = partition(
              events,
              (event) => event.checkpoint <= crashRecoveryCheckpoint,
            );
            events = right;
            if (left.length > 0) {
              params.common.logger.trace({
                msg: "Filtered events before crash recovery checkpoint",
                chain: chain.name,
                chain_id: chain.id,
                event_count: left.length,
                checkpoint: crashRecoveryCheckpoint,
              });
            }
          }
          yield { chainId: chain.id, events, checkpoint, blockRange };
        }
        perChainCursor.set(chain, to);
      },
    );
    yield* mergeAsyncGenerators(eventGenerators);
    if (Date.now() - lastUnfinalizedRefetch < 30_000) {
      break;
    }
    lastUnfinalizedRefetch = Date.now();
    const context = {
      logger: params.common.logger.child({ action: "refetch_finalized_block" }),
      retryNullBlockRequest: true,
    };
    const endClock = startClock();
    const finalizedBlocks = await Promise.all(
      params.indexingBuild.chains.map((chain, i) => {
        const rpc = params.indexingBuild.rpcs[i]!;
        return eth_getBlockByNumber(rpc, ["latest", false], context)
          .then((latest) =>
            eth_getBlockByNumber(
              rpc,
              [
                numberToHex(
                  Math.max(
                    hexToNumber(latest.number) - chain.finalityBlockCount,
                    0,
                  ),
                ),
                false,
              ],
              context,
            ),
          )
          .then((finalizedBlock) => {
            const finalizedBlockNumber = hexToNumber(finalizedBlock.number);
            params.common.logger.debug({
              msg: "Refetched finalized block for backfill cutover",
              chain: chain.name,
              chain_id: chain.id,
              finalized_block: finalizedBlockNumber,
              duration: endClock(),
            });
            return finalizedBlock;
          });
      }),
    );
    let shouldCatchup = false;
    for (let i = 0; i < params.indexingBuild.chains.length; i++) {
      const chain = params.indexingBuild.chains[i]!;
      const oldFinalizedBlock =
        params.perChainSync.get(chain)!.syncProgress.finalized;
      const newFinalizedBlock = finalizedBlocks[i]!;
      if (
        hexToNumber(newFinalizedBlock.number) -
          hexToNumber(oldFinalizedBlock.number) >
        chain.finalityBlockCount
      ) {
        shouldCatchup = true;
        break;
      }
    }
    if (shouldCatchup === false) break;
    for (let i = 0; i < params.indexingBuild.chains.length; i++) {
      const chain = params.indexingBuild.chains[i]!;
      const finalizedBlock = finalizedBlocks[i]!;
      params.perChainSync.get(chain)!.syncProgress.finalized = finalizedBlock;
    }
    isCatchup = true;
  }
}
export async function* getHistoricalEventsIsolated(params: {
  common: Common;
  indexingBuild: Pick<
    IndexingBuild,
    "eventCallbacks" | "chains" | "rpcs" | "finalizedBlocks"
  >;
  crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  chain: Chain;
  syncProgress: SyncProgress;
  childAddresses: ChildAddresses;
  cachedIntervals: CachedIntervals;
  database: Database;
}) {
  let isCatchup = false;
  let lastUnfinalizedRefetch = Date.now();
  let cursor: string | undefined;
  while (true) {
    const rpc =
      params.indexingBuild.rpcs[
        params.indexingBuild.chains.findIndex((c) => c.id === params.chain.id)
      ]!;
    const eventCallbacks =
      params.indexingBuild.eventCallbacks[
        params.indexingBuild.chains.findIndex((c) => c.id === params.chain.id)
      ]!;
    const crashRecoveryCheckpoint = params.crashRecoveryCheckpoint?.find(
      ({ chainId }) => chainId === params.chain.id,
    )?.checkpoint;
    const to = min(
      params.syncProgress.getCheckpoint({ tag: "finalized" }),
      params.syncProgress.getCheckpoint({ tag: "end" }),
    );
    let from: string;
    if (isCatchup === false) {
      // In order to speed up the "extract" phase when there is a crash recovery,
      // the beginning cursor is moved forwards. This only works when `crashRecoveryCheckpoint`
      // is defined.
      if (crashRecoveryCheckpoint === undefined) {
        from = params.syncProgress.getCheckpoint({ tag: "start" });
      } else if (
        Number(decodeCheckpoint(crashRecoveryCheckpoint).chainId) ===
        params.chain.id
      ) {
        from = crashRecoveryCheckpoint;
      } else {
        from = params.syncProgress.getCheckpoint({ tag: "start" });
      }
    } else {
      from = encodeCheckpoint({
        ...ZERO_CHECKPOINT,
        blockTimestamp: decodeCheckpoint(cursor!).blockTimestamp,
        chainId: decodeCheckpoint(cursor!).chainId,
        blockNumber: decodeCheckpoint(cursor!).blockNumber + 1n,
      });
      if (from > to) return;
    }
    params.common.logger.info({
      msg: "Started backfill indexing",
      chain: params.chain.name,
      chain_id: params.chain.id,
      block_range: JSON.stringify([
        Number(decodeCheckpoint(from).blockNumber),
        Number(decodeCheckpoint(to).blockNumber),
      ]),
    });
    const eventGenerator = await initEventGenerator({
      common: params.common,
      indexingBuild: params.indexingBuild,
      chain: params.chain,
      rpc,
      eventCallbacks,
      childAddresses: params.childAddresses,
      syncProgress: params.syncProgress,
      cachedIntervals: params.cachedIntervals,
      from,
      to,
      limit:
        Math.round(
          params.common.options.syncEventsQuerySize /
            (params.indexingBuild.chains.length + 1),
        ) + 6,
      database: params.database,
      isCatchup,
    });
    for await (const {
      events: rawEvents,
      checkpoint,
      blockRange,
    } of eventGenerator) {
      const endClock = startClock();
      let events = decodeEvents(
        params.common,
        params.chain,
        eventCallbacks,
        rawEvents,
      );
      params.common.logger.trace({
        msg: "Decoded events",
        chain: params.chain.name,
        chain_id: params.chain.id,
        event_count: events.length,
        duration: endClock(),
      });
      params.common.metrics.ponder_historical_extract_duration.inc(
        { step: "decode" },
        endClock(),
      );
      // Removes events that have a checkpoint earlier than (or equal to)
      // the crash recovery checkpoint.
      if (crashRecoveryCheckpoint) {
        const [left, right] = partition(
          events,
          (event) => event.checkpoint <= crashRecoveryCheckpoint,
        );
        events = right;
        if (left.length > 0) {
          params.common.logger.trace({
            msg: "Filtered events before crash recovery checkpoint",
            chain: params.chain.name,
            chain_id: params.chain.id,
            event_count: left.length,
            checkpoint: crashRecoveryCheckpoint,
          });
        }
      }
      yield { chainId: params.chain.id, events, checkpoint, blockRange };
    }
    cursor = to;
    if (Date.now() - lastUnfinalizedRefetch < 30_000) {
      break;
    }
    lastUnfinalizedRefetch = Date.now();
    const context = {
      logger: params.common.logger.child({ action: "refetch_finalized_block" }),
    };
    const endClock = startClock();
    const finalizedBlock = await eth_getBlockByNumber(
      rpc,
      ["latest", false],
      context,
    ).then((latest) =>
      eth_getBlockByNumber(
        rpc,
        [
          numberToHex(
            Math.max(
              hexToNumber(latest.number) - params.chain.finalityBlockCount,
              0,
            ),
          ),
          false,
        ],
        context,
      ),
    );
    const finalizedBlockNumber = hexToNumber(finalizedBlock.number);
    params.common.logger.debug({
      msg: "Refetched finalized block for backfill cutover",
      chain: params.chain.name,
      chain_id: params.chain.id,
      finalized_block: finalizedBlockNumber,
      duration: endClock(),
    });
    if (
      hexToNumber(finalizedBlock.number) -
        hexToNumber(params.syncProgress.finalized.number) <=
      params.chain.finalityBlockCount
    ) {
      break;
    }
    params.syncProgress.finalized = finalizedBlock;
    isCatchup = true;
  }
}
export async function refetchHistoricalEvents(params: {
  common: Common;
  indexingBuild: Pick<IndexingBuild, "eventCallbacks" | "chains">;
  perChainSync: Map<Chain, { childAddresses: ChildAddresses }>;
  events: Event[];
  syncStore: SyncStore;
}): Promise<Event[]> {
  const events: Event[] = new Array(params.events.length);
  for (const chain of params.indexingBuild.chains) {
    const { childAddresses } = params.perChainSync.get(chain)!;
    // Note: All filters are refetched, no matter if they are resolved or not.
    const eventCallbacks =
      params.indexingBuild.eventCallbacks[
        params.indexingBuild.chains.findIndex((c) => c.id === chain.id)
      ]!;
    const chainEvents = params.events.filter(
      (event) => event.chain.id === chain.id,
    );
    if (chainEvents.length === 0) continue;
    const rawEvents = await initRefetchEvents({
      common: params.common,
      chain,
      childAddresses,
      eventCallbacks,
      events: chainEvents,
      syncStore: params.syncStore,
    });
    const endClock = startClock();
    const refetchedEvents = decodeEvents(
      params.common,
      chain,
      eventCallbacks,
      rawEvents,
    );
    params.common.logger.trace({
      msg: "Decoded events",
      chain: chain.name,
      chain_id: chain.id,
      event_count: events.length,
      duration: endClock(),
    });
    params.common.metrics.ponder_historical_extract_duration.inc(
      { step: "decode" },
      endClock(),
    );
    let i = 0;
    let j = 0;
    while (i < params.events.length && j < refetchedEvents.length) {
      if (params.events[i]?.chain.id === chain.id) {
        events[i] = refetchedEvents[j]!;
        i++;
        j++;
      } else {
        i++;
      }
    }
  }
  return events;
}
export async function refetchLocalEvents(params: {
  common: Common;
  chain: Chain;
  childAddresses: ChildAddresses;
  eventCallbacks: EventCallback[];
  events: Event[];
  syncStore: SyncStore;
}): Promise<RawEvent[]> {
  const from = params.events[0]!.checkpoint;
  const to = params.events[params.events.length - 1]!.checkpoint;
  const fromBlock = Number(decodeCheckpoint(from).blockNumber);
  const toBlock = Number(decodeCheckpoint(to).blockNumber);
  let cursor = fromBlock;
  let events: RawEvent[] | undefined;
  while (cursor <= toBlock) {
    const queryEndClock = startClock();
    const {
      blocks,
      logs,
      transactions,
      transactionReceipts,
      traces,
      cursor: queryCursor,
    } = await params.syncStore.getEventData({
      filters: params.eventCallbacks.map(({ filter }) => filter),
      fromBlock: cursor,
      toBlock,
      chainId: params.chain.id,
      limit: params.events.length,
    });
    const endClock = startClock();
    const rawEvents = buildEvents({
      eventCallbacks: params.eventCallbacks,
      blocks,
      logs,
      transactions,
      transactionReceipts,
      traces,
      childAddresses: params.childAddresses,
      chainId: params.chain.id,
    });
    params.common.logger.trace({
      msg: "Constructed events from block data",
      chain: params.chain.name,
      chain_id: params.chain.id,
      block_range: JSON.stringify([cursor, queryCursor]),
      event_count: rawEvents.length,
      duration: endClock(),
    });
    params.common.metrics.ponder_historical_extract_duration.inc(
      { step: "build" },
      endClock(),
    );
    params.common.logger.debug({
      msg: "Queried backfill JSON-RPC data from database",
      chain: params.chain.name,
      chain_id: params.chain.id,
      block_range: JSON.stringify([cursor, queryCursor]),
      event_count: rawEvents.length,
      duration: queryEndClock(),
    });
    await new Promise(setImmediate);
    if (events === undefined) {
      events = rawEvents;
    } else {
      events.push(...rawEvents);
    }
    cursor = queryCursor + 1;
  }
  return events!;
}
export async function* getLocalEventGenerator(params: {
  common: Common;
  chain: Chain;
  rpc: Rpc;
  eventCallbacks: EventCallback[];
  childAddresses: ChildAddresses;
  syncProgress: SyncProgress;
  cachedIntervals: CachedIntervals;
  from: string;
  to: string;
  limit: number;
  database: Database;
  isCatchup: boolean;
}) {
  const syncStore = createSyncStore({
    common: params.common,
    qb: params.database.syncQB,
  });
  const fromBlock = Number(decodeCheckpoint(params.from).blockNumber);
  const toBlock = Number(decodeCheckpoint(params.to).blockNumber);
  let cursor = fromBlock;
  const localSyncGenerator = getLocalSyncGenerator(params);
  for await (const syncCursor of bufferAsyncGenerator(
    localSyncGenerator,
    Number.POSITIVE_INFINITY,
  )) {
    while (cursor <= Math.min(syncCursor, toBlock)) {
      const queryEndClock = startClock();
      const {
        blocks,
        logs,
        transactions,
        transactionReceipts,
        traces,
        cursor: queryCursor,
      } = await syncStore.getEventData({
        filters: params.eventCallbacks.map(({ filter }) => filter),
        fromBlock: cursor,
        toBlock: Math.min(syncCursor, toBlock),
        chainId: params.chain.id,
        limit: params.limit,
      });
      const endClock = startClock();
      const events = buildEvents({
        eventCallbacks: params.eventCallbacks,
        blocks,
        logs,
        transactions,
        transactionReceipts,
        traces,
        childAddresses: params.childAddresses,
        chainId: params.chain.id,
      });
      params.common.logger.trace({
        msg: "Constructed events from block data",
        chain: params.chain.name,
        chain_id: params.chain.id,
        block_range: JSON.stringify([cursor, queryCursor]),
        event_count: events.length,
        duration: endClock(),
      });
      params.common.metrics.ponder_historical_extract_duration.inc(
        { step: "build" },
        endClock(),
      );
      params.common.logger.debug({
        msg: "Queried backfill JSON-RPC data from database",
        chain: params.chain.name,
        chain_id: params.chain.id,
        block_range: JSON.stringify([cursor, queryCursor]),
        event_count: events.length,
        duration: queryEndClock(),
      });
      await new Promise(setImmediate);
      const blockRange = [cursor, queryCursor] satisfies [number, number];
      cursor = queryCursor + 1;
      if (cursor >= toBlock) {
        yield { events, checkpoint: params.to, blockRange };
      } else if (blocks.length > 0) {
        const checkpoint = encodeCheckpoint({
          ...MAX_CHECKPOINT,
          blockTimestamp: blocks[blocks.length - 1]!.timestamp,
          chainId: BigInt(params.chain.id),
          blockNumber: blocks[blocks.length - 1]!.number,
        });
        yield { events, checkpoint, blockRange };
      }
    }
  }
}
export async function* getLocalSyncGenerator(params: {
  common: Common;
  chain: Chain;
  rpc: Rpc;
  eventCallbacks: EventCallback[];
  syncProgress: SyncProgress;
  childAddresses: ChildAddresses;
  cachedIntervals: CachedIntervals;
  database: Database;
  isCatchup: boolean;
}) {
  const backfillEndClock = startClock();
  const label = { chain: params.chain.name };
  let first = hexToNumber(params.syncProgress.start.number);
  const last =
    params.syncProgress.end === undefined
      ? params.syncProgress.finalized
      : hexToNumber(params.syncProgress.end.number) >
          hexToNumber(params.syncProgress.finalized.number)
        ? params.syncProgress.finalized
        : params.syncProgress.end;
  // Estimate optimal range (blocks) to sync at a time, eventually to be used to
  // determine `interval` passed to `historicalSync.sync()`.
  let estimateRange = 25;
  // Handle two special cases:
  // 1. `syncProgress.start` > `syncProgress.finalized`
  // 2. `cached` is defined
  if (
    hexToNumber(params.syncProgress.start.number) >
    hexToNumber(params.syncProgress.finalized.number)
  ) {
    params.syncProgress.current = params.syncProgress.finalized;
    params.common.logger.info({
      msg: "Skipped fetching backfill JSON-RPC data (chain only requires live indexing)",
      chain: params.chain.name,
      chain_id: params.chain.id,
      finalized_block: hexToNumber(params.syncProgress.finalized.number),
      start_block: hexToNumber(params.syncProgress.start.number),
    });
    params.common.metrics.ponder_sync_block.set(
      label,
      hexToNumber(params.syncProgress.current.number),
    );
    params.common.metrics.ponder_sync_block_timestamp.set(
      label,
      hexToNumber(params.syncProgress.current.timestamp),
    );
    params.common.metrics.ponder_historical_total_blocks.set(label, 0);
    params.common.metrics.ponder_historical_cached_blocks.set(label, 0);
    return;
  }
  const totalInterval = [
    hexToNumber(params.syncProgress.start.number),
    hexToNumber(last!.number),
  ] satisfies Interval;
  const requiredIntervals = getRequiredIntervals({
    filters: params.eventCallbacks.map(({ filter }) => filter),
    interval: totalInterval,
    cachedIntervals: params.cachedIntervals,
  });
  const required = intervalSum(requiredIntervals);
  const total = totalInterval[1] - totalInterval[0] + 1;
  params.common.metrics.ponder_historical_total_blocks.set(label, total);
  params.common.metrics.ponder_historical_cached_blocks.set(
    label,
    total - required,
  );
  // Handle cache hit
  if (params.syncProgress.current !== undefined) {
    params.common.metrics.ponder_sync_block.set(
      label,
      hexToNumber(params.syncProgress.current.number),
    );
    params.common.metrics.ponder_sync_block_timestamp.set(
      label,
      hexToNumber(params.syncProgress.current.timestamp),
    );
    // `getEvents` can make progress without calling `sync`, so immediately "yield"
    yield hexToNumber(params.syncProgress.current.number);
    if (
      hexToNumber(params.syncProgress.current.number) ===
      hexToNumber(last!.number)
    ) {
      if (params.isCatchup === false) {
        params.common.logger.info({
          msg: "Skipped fetching backfill JSON-RPC data (cache contains all required data)",
          chain: params.chain.name,
          chain_id: params.chain.id,
          cached_block: hexToNumber(params.syncProgress.current.number),
          cache_rate: "100%",
        });
      }
      return;
    } else if (params.isCatchup === false) {
      params.common.logger.info({
        msg: "Started fetching backfill JSON-RPC data",
        chain: params.chain.name,
        chain_id: params.chain.id,
        cached_block: hexToNumber(params.syncProgress.current.number),
        cache_rate: formatPercentage((total - required) / total),
      });
    }
    first = hexToNumber(params.syncProgress.current.number) + 1;
  } else {
    params.common.logger.info({
      msg: "Started fetching backfill JSON-RPC data",
      chain: params.chain.name,
      chain_id: params.chain.id,
      cache_rate: "0%",
    });
  }
  const historicalSync = createHistoricalSync(params);
  const { callback: intervalCallback, generator: intervalGenerator } =
    createCallbackGenerator<{
      interval: Interval;
      promise: Promise<void>;
    }>();
  intervalCallback({
    interval: [
      first,
      Math.min(first + estimateRange, hexToNumber(last.number)),
    ],
    promise: Promise.resolve(),
  });
  /**
   * @returns `true` if any data was inserted into the database.
   */
  async function syncInterval({
    interval,
    promise,
  }: { interval: Interval; promise: Promise<void> }): Promise<boolean> {
    const endClock = startClock();
    const isSyncComplete = interval[1] === hexToNumber(last.number);
    const {
      intervals: requiredIntervals,
      factoryIntervals: requiredFactoryIntervals,
    } = getRequiredIntervalsWithFilters({
      interval,
      filters: params.eventCallbacks.map(({ filter }) => filter),
      cachedIntervals: params.cachedIntervals,
    });
    let closestToTipBlock: SyncBlock | undefined;
    if (requiredIntervals.length > 0 || requiredFactoryIntervals.length > 0) {
      const pwr = promiseWithResolvers<void>();
      const durationTimer = setTimeout(
        () => {
          params.common.logger.warn({
            msg: "Fetching backfill JSON-RPC data is taking longer than expected",
            chain: params.chain.name,
            chain_id: params.chain.id,
            block_range: JSON.stringify(interval),
            duration: endClock(),
          });
        },
        params.common.options.command === "dev" ? 10_000 : 50_000,
      );
      closestToTipBlock = await params.database.syncQB
        .transaction(async (tx) => {
          const syncStore = createSyncStore({ common: params.common, qb: tx });
          const logs = await historicalSync.syncBlockRangeData({
            interval,
            requiredIntervals,
            requiredFactoryIntervals,
            syncStore,
          });
          // Wait for the previous interval to complete `syncBlockData`.
          await promise;
          if (isSyncComplete === false) {
            // Queue the next interval
            intervalCallback({
              interval: [
                Math.min(interval[1] + 1, hexToNumber(last.number)),
                Math.min(
                  interval[1] + 1 + estimateRange,
                  hexToNumber(last.number),
                ),
              ],
              promise: pwr.promise,
            });
          }
          const closestToTipBlock = await historicalSync.syncBlockData({
            interval,
            requiredIntervals,
            logs,
            syncStore,
          });
          if (params.chain.disableCache === false) {
            await syncStore.insertIntervals({
              intervals: requiredIntervals,
              factoryIntervals: requiredFactoryIntervals,
              chainId: params.chain.id,
            });
          }
          return closestToTipBlock;
        })
        .catch((error) => {
          if (error instanceof ShutdownError) {
            throw error;
          }
          params.common.logger.warn({
            msg: "Failed to fetch backfill JSON-RPC data",
            chain: params.chain.name,
            chain_id: params.chain.id,
            block_range: JSON.stringify(interval),
            duration: endClock(),
            error,
          });
          throw error;
        });
      clearTimeout(durationTimer);
      const duration = endClock();
      // Use the duration and interval of the last call to `sync` to update estimate
      estimateRange = estimate({
        from: interval[0],
        to: interval[1],
        target: params.common.options.command === "dev" ? 2_000 : 10_000,
        result: duration,
        min: 25,
        max: 100_000,
        prev: estimateRange,
        maxIncrease: 1.5,
      });
      params.common.logger.trace({
        msg: "Updated block range estimate for fetching backfill JSON-RPC data",
        chain: params.chain.name,
        chain_id: params.chain.id,
        range: estimateRange,
      });
      // Resolve promise so the next interval can continue.
      pwr.resolve();
    } else {
      // Wait for the previous interval to complete `syncBlockData`.
      await promise;
      if (isSyncComplete === false) {
        // Queue the next interval
        intervalCallback({
          interval: [
            Math.min(interval[1] + 1, hexToNumber(last.number)),
            Math.min(interval[1] + 1 + estimateRange, hexToNumber(last.number)),
          ],
          promise: Promise.resolve(),
        });
      }
    }
    if (interval[1] === hexToNumber(last.number)) {
      params.syncProgress.current = last;
    }
    if (closestToTipBlock) {
      params.common.metrics.ponder_sync_block.set(
        label,
        hexToNumber(closestToTipBlock.number),
      );
      params.common.metrics.ponder_sync_block_timestamp.set(
        label,
        hexToNumber(closestToTipBlock.timestamp),
      );
    } else {
      params.common.metrics.ponder_sync_block.set(label, interval[1]);
    }
    params.common.metrics.ponder_historical_completed_blocks.inc(
      label,
      interval[1] - interval[0] + 1,
    );
    return requiredIntervals.length > 0;
  }
  const { callback, generator } =
    createCallbackGenerator<IteratorResult<number>>();
  (async () => {
    for await (const { interval, promise } of intervalGenerator) {
      // Note: this relies on the invariant that `syncInterval`
      // will always resolve promises in the order it was called.
      syncInterval({ interval, promise }).then((didInsertData) => {
        const isDone = interval[1] === hexToNumber(last.number);
        if (didInsertData || isDone) {
          callback({ value: interval[1], done: false });
        }
        if (isDone) {
          callback({ value: undefined, done: true });
        }
      });
    }
  })();
  for await (const result of generator) {
    if (result.done) break;
    yield result.value;
  }
  params.common.logger.info({
    msg: "Finished fetching backfill JSON-RPC data",
    chain: params.chain.name,
    chain_id: params.chain.id,
    duration: backfillEndClock(),
  });
}
/**
 * Merges multiple event generators into a single generator while preserving
 * the order of events.
 *
 * @param generators - Generators to merge.
 * @returns A single generator that yields events from all generators.
 */
export async function* mergeAsyncGeneratorsWithEventOrder(
  generators: AsyncGenerator<{
    events: Event[];
    checkpoint: string;
    blockRange: [number, number];
  }>[],
): AsyncGenerator<
  {
    chainId: number;
    events: Event[];
    checkpoint: string;
    blockRange: [number, number];
  }[]
> {
  const results = await Promise.all(generators.map((gen) => gen.next()));
  while (results.some((res) => res.done !== true)) {
    const supremum = min(
      ...results.map((res) => (res.done ? undefined : res.value.checkpoint)),
    );
    const mergedResults: {
      chainId: number;
      events: Event[];
      checkpoint: string;
      blockRange: [number, number];
    }[] = [];
    for (const result of results) {
      if (result.done === false) {
        const [left, right] = partition(
          result.value.events,
          (event) => event.checkpoint <= supremum,
        );
        const event = left[left.length - 1];
        if (event) {
          const blockRange = [
            result.value.blockRange[0],
            right.length > 0
              ? Number(decodeCheckpoint(event.checkpoint).blockNumber)
              : result.value.blockRange[1],
          ] satisfies [number, number];
          mergedResults.push({
            events: left,
            chainId: event.chain.id,
            checkpoint:
              right.length > 0 ? event.checkpoint : result.value.checkpoint,
            blockRange,
          });
        }
        result.value.events = right;
      }
    }
    const index = results.findIndex(
      (res) => res.done === false && res.value.checkpoint === supremum,
    );
    const resultPromise = generators[index]!.next();
    if (mergedResults.length > 0) {
      yield mergedResults;
    }
    results[index] = await resultPromise;
  }
}
</file>

<file path="packages/core/src/runtime/index.test.ts">
import {
  ALICE,
  EMPTY_BLOCK_FILTER,
  EMPTY_LOG_FILTER,
} from "@/_test/constants.js";
import {
  context,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { setupAnvil } from "@/_test/setup.js";
import { deployErc20, mintErc20 } from "@/_test/simulate.js";
import {
  getBlocksIndexingBuild,
  getChain,
  getErc20IndexingBuild,
} from "@/_test/utils.js";
import type {
  BlockFilter,
  Event,
  Factory,
  Filter,
  Fragment,
  LogFilter,
} from "@/internal/types.js";
import { eth_getBlockByNumber } from "@/rpc/actions.js";
import { createRpc } from "@/rpc/index.js";
import { encodeCheckpoint } from "@/utils/checkpoint.js";
import { drainAsyncGenerator } from "@/utils/generators.js";
import type { Interval } from "@/utils/interval.js";
import { promiseWithResolvers } from "@/utils/promiseWithResolvers.js";
import { parseEther, zeroAddress } from "viem";
import { beforeEach, expect, test } from "vitest";
import {
  syncBlockToInternal,
  syncLogToInternal,
  syncTransactionToInternal,
} from "./events.js";
import { getFactoryFragments, getFragments } from "./fragments.js";
import { mergeAsyncGeneratorsWithEventOrder } from "./historical.js";
import {
  type CachedIntervals,
  getCachedBlock,
  getLocalSyncProgress,
  getRequiredIntervals,
  getRequiredIntervalsWithFilters,
} from "./index.js";
beforeEach(setupCommon);
beforeEach(setupAnvil);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
test("getLocalSyncProgress()", async () => {
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >();
  for (const eventCallback of eventCallbacks) {
    for (const { fragment } of getFragments(eventCallback.filter)) {
      cachedIntervals.set(eventCallback.filter, [{ fragment, intervals: [] }]);
    }
  }
  const syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x0", true]),
    cachedIntervals,
  });
  expect(syncProgress.finalized.number).toBe("0x0");
  expect(syncProgress.start.number).toBe("0x0");
  expect(syncProgress.end).toBe(undefined);
  expect(syncProgress.current).toBe(undefined);
});
test("getLocalSyncProgress() future end block", async () => {
  const chain = getChain();
  const rpc = createRpc({ chain, common: context.common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  eventCallbacks[0]!.filter.toBlock = 12;
  const cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >();
  for (const eventCallback of eventCallbacks) {
    for (const { fragment } of getFragments(eventCallback.filter)) {
      cachedIntervals.set(eventCallback.filter, [{ fragment, intervals: [] }]);
    }
  }
  const syncProgress = await getLocalSyncProgress({
    common: context.common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc,
    finalizedBlock: await eth_getBlockByNumber(rpc, ["0x0", true]),
    cachedIntervals,
  });
  expect(syncProgress.finalized.number).toBe("0x0");
  expect(syncProgress.start.number).toBe("0x0");
  expect(syncProgress.end).toMatchInlineSnapshot(`
    {
      "hash": "0x",
      "number": "0xc",
      "parentHash": "0x",
      "timestamp": "0x2540be3ff",
    }
  `);
  expect(syncProgress.current).toBe(undefined);
});
test("getCachedBlock() no cached intervals", async () => {
  const filter = {
    ...EMPTY_BLOCK_FILTER,
    fromBlock: 0,
    toBlock: 100,
  } satisfies BlockFilter;
  const cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([[filter, []]]);
  const cachedBlock = getCachedBlock({
    filters: [filter],
    cachedIntervals,
  });
  expect(cachedBlock).toBe(undefined);
});
test("getCachedBlock() with cache", async () => {
  const filter = {
    ...EMPTY_BLOCK_FILTER,
    fromBlock: 0,
    toBlock: 100,
  } satisfies BlockFilter;
  let cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([[filter, [{ fragment: {} as Fragment, intervals: [[0, 24]] }]]]);
  let cachedBlock = getCachedBlock({
    filters: [filter],
    cachedIntervals,
  });
  expect(cachedBlock).toBe(24);
  cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([
    [
      filter,
      [
        {
          fragment: {} as Fragment,
          intervals: [
            [0, 50],
            [50, 102],
          ],
        },
      ],
    ],
  ]);
  cachedBlock = getCachedBlock({
    filters: [filter],
    cachedIntervals,
  });
  expect(cachedBlock).toBe(100);
});
test("getCachedBlock() with incomplete cache", async () => {
  const filter = {
    ...EMPTY_BLOCK_FILTER,
    fromBlock: 0,
    toBlock: 100,
  } satisfies BlockFilter;
  const cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([[filter, [{ fragment: {} as Fragment, intervals: [[1, 24]] }]]]);
  const cachedBlock = getCachedBlock({
    filters: [filter],
    cachedIntervals,
  });
  expect(cachedBlock).toBeUndefined();
});
test("getCachedBlock() with multiple filters", async () => {
  const filters = [
    {
      ...EMPTY_BLOCK_FILTER,
      fromBlock: 0,
      toBlock: 100,
    },
    {
      ...EMPTY_BLOCK_FILTER,
      offset: 1,
      fromBlock: 50,
      toBlock: 150,
    },
  ] satisfies BlockFilter[];
  let cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([
    [filters[0]!, [{ fragment: {} as Fragment, intervals: [[0, 24]] }]],
    [filters[1]!, []],
  ]);
  let cachedBlock = getCachedBlock({
    filters,
    cachedIntervals,
  });
  expect(cachedBlock).toBe(24);
  cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([
    [filters[0]!, [{ fragment: {} as Fragment, intervals: [[0, 24]] }]],
    [filters[1]!, [{ fragment: {} as Fragment, intervals: [[50, 102]] }]],
  ]);
  cachedBlock = getCachedBlock({
    filters,
    cachedIntervals,
  });
  expect(cachedBlock).toBe(24);
  cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([
    [filters[0]!, [{ fragment: {} as Fragment, intervals: [[0, 60]] }]],
    [filters[1]!, []],
  ]);
  cachedBlock = getCachedBlock({
    filters,
    cachedIntervals,
  });
  expect(cachedBlock).toBe(49);
});
test("getCachedBlock() with factory", async () => {
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: {
      id: "id",
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
      eventSelector:
        "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
      childAddressLocation: "topic1",
      fromBlock: 2,
      toBlock: 5,
    },
    fromBlock: 10,
    toBlock: 20,
  } satisfies LogFilter;
  // @ts-ignore
  let cachedIntervals: CachedIntervals = new Map([
    [filter, [{ fragment: {} as Fragment, intervals: [[10, 20]] }]],
    [filter.address, []],
  ]);
  let cachedBlock = getCachedBlock({
    filters: [filter],
    cachedIntervals,
  });
  expect(cachedBlock).toBe(1);
  // @ts-ignore
  cachedIntervals = new Map([
    [
      filter,
      [
        {
          fragment: {} as Fragment,
          intervals: [[10, 18]],
        },
      ],
    ],
    [
      filter.address,
      [
        {
          fragment: {} as Fragment,
          intervals: [[2, 5]],
        },
      ],
    ],
  ]);
  cachedBlock = getCachedBlock({ filters: [filter], cachedIntervals });
  expect(cachedBlock).toBe(18);
});
test("getRequiredIntervals()", async () => {
  const filters = [
    {
      ...EMPTY_BLOCK_FILTER,
      fromBlock: 0,
      toBlock: 100,
    },
    {
      ...EMPTY_BLOCK_FILTER,
      offset: 1,
      fromBlock: 50,
      toBlock: 150,
    },
  ] satisfies BlockFilter[];
  let cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([
    [filters[0]!, [{ fragment: {} as Fragment, intervals: [[0, 24]] }]],
    [filters[1]!, []],
  ]);
  let requiredIntervals = getRequiredIntervals({
    filters,
    interval: [0, 150],
    cachedIntervals,
  });
  expect(requiredIntervals).toMatchInlineSnapshot(`
    [
      [
        25,
        150,
      ],
    ]
  `);
  cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([
    [filters[0]!, [{ fragment: {} as Fragment, intervals: [[0, 24]] }]],
    [filters[1]!, [{ fragment: {} as Fragment, intervals: [[50, 102]] }]],
  ]);
  requiredIntervals = getRequiredIntervals({
    filters,
    interval: [0, 150],
    cachedIntervals,
  });
  expect(requiredIntervals).toMatchInlineSnapshot(`
    [
      [
        25,
        100,
      ],
      [
        103,
        150,
      ],
    ]
  `);
  cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([
    [filters[0]!, [{ fragment: {} as Fragment, intervals: [[0, 60]] }]],
    [filters[1]!, []],
  ]);
  requiredIntervals = getRequiredIntervals({
    filters,
    interval: [0, 150],
    cachedIntervals,
  });
  expect(requiredIntervals).toMatchInlineSnapshot(`
    [
      [
        50,
        150,
      ],
    ]
  `);
});
test("getRequiredIntervalsWithFilters()", async () => {
  const filter: LogFilter = {
    ...EMPTY_LOG_FILTER,
    fromBlock: 0,
    toBlock: 100,
    address: zeroAddress,
  };
  let fragments = getFragments(filter);
  let cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([[filter, [{ fragment: fragments[0]!.fragment, intervals: [[0, 24]] }]]]);
  let requiredIntervals = getRequiredIntervalsWithFilters({
    filters: [filter],
    interval: [0, 100],
    cachedIntervals,
  });
  expect(requiredIntervals).toMatchInlineSnapshot(`
    {
      "factoryIntervals": [],
      "intervals": [
        {
          "filter": {
            "address": "0x0000000000000000000000000000000000000000",
            "chainId": 1,
            "fromBlock": 0,
            "hasTransactionReceipt": false,
            "include": [],
            "sourceId": "test",
            "toBlock": 100,
            "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "topic1": null,
            "topic2": null,
            "topic3": null,
            "type": "log",
          },
          "interval": [
            25,
            100,
          ],
        },
      ],
    }
  `);
  filter.address = [zeroAddress, ALICE];
  fragments = getFragments(filter);
  cachedIntervals = new Map<
    Filter,
    { fragment: Fragment; intervals: Interval[] }[]
  >([
    [
      filter,
      [
        { fragment: fragments[0]!.fragment, intervals: [[0, 50]] },
        { fragment: fragments[1]!.fragment, intervals: [[0, 24]] },
      ],
    ],
  ]);
  requiredIntervals = getRequiredIntervalsWithFilters({
    filters: [filter],
    interval: [25, 50],
    cachedIntervals,
  });
  expect(requiredIntervals).toMatchInlineSnapshot(`
    {
      "factoryIntervals": [],
      "intervals": [
        {
          "filter": {
            "address": [
              "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
            ],
            "chainId": 1,
            "fromBlock": 0,
            "hasTransactionReceipt": false,
            "include": [],
            "sourceId": "test",
            "toBlock": 100,
            "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "topic1": null,
            "topic2": null,
            "topic3": null,
            "type": "log",
          },
          "interval": [
            25,
            50,
          ],
        },
      ],
    }
  `);
});
test("getRequiredIntervalsWithFilters() with factory", async () => {
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: {
      id: "id",
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
      eventSelector:
        "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
      childAddressLocation: "topic1",
      fromBlock: 2,
      toBlock: 5,
    },
    fromBlock: 10,
    toBlock: 20,
  } satisfies LogFilter;
  const fragments = getFragments(filter);
  const factoryFragments = getFactoryFragments(filter.address);
  // @ts-ignore
  const cachedIntervals: CachedIntervals = new Map([
    [filter, [{ fragment: fragments[0]!.fragment, intervals: [[10, 24]] }]],
    [filter.address, [{ fragment: factoryFragments[0]!, intervals: [[3, 5]] }]],
  ]);
  const requiredIntervals = getRequiredIntervalsWithFilters({
    filters: [filter],
    interval: [0, 100],
    cachedIntervals,
  });
  expect(requiredIntervals).toMatchInlineSnapshot(`
    {
      "factoryIntervals": [
        {
          "factory": {
            "address": "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
            "chainId": 1,
            "childAddressLocation": "topic1",
            "eventSelector": "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
            "fromBlock": 2,
            "id": "id",
            "sourceId": "factory",
            "toBlock": 5,
            "type": "log",
          },
          "interval": [
            2,
            2,
          ],
        },
      ],
      "intervals": [
        {
          "filter": {
            "address": {
              "address": "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
              "chainId": 1,
              "childAddressLocation": "topic1",
              "eventSelector": "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
              "fromBlock": 2,
              "id": "id",
              "sourceId": "factory",
              "toBlock": 5,
              "type": "log",
            },
            "chainId": 1,
            "fromBlock": 10,
            "hasTransactionReceipt": false,
            "include": [],
            "sourceId": "test",
            "toBlock": 20,
            "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "topic1": null,
            "topic2": null,
            "topic3": null,
            "type": "log",
          },
          "interval": [
            10,
            20,
          ],
        },
      ],
    }
  `);
});
test("getRequiredIntervals() with factory", async () => {
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: {
      id: "id",
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
      eventSelector:
        "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
      childAddressLocation: "topic1",
      fromBlock: 2,
      toBlock: 5,
    } satisfies Factory,
    fromBlock: 10,
    toBlock: 20,
  } satisfies LogFilter;
  // @ts-ignore
  let cachedIntervals: CachedIntervals = new Map([
    [filter, [{ fragment: {} as Fragment, intervals: [[10, 18]] }]],
    [
      filter.address as Factory,
      [{ fragment: {} as Fragment, intervals: [[2, 5]] }],
    ],
  ]);
  let requiredIntervals = getRequiredIntervals({
    filters: [filter],
    interval: [2, 20],
    cachedIntervals,
  });
  expect(requiredIntervals).toMatchInlineSnapshot(`
    [
      [
        19,
        20,
      ],
    ]
  `);
  // @ts-ignore
  cachedIntervals = new Map([
    [filter, [{ fragment: {} as Fragment, intervals: [[10, 18]] }]],
    [filter.address, [{ fragment: {} as Fragment, intervals: [[2, 4]] }]],
  ]);
  requiredIntervals = getRequiredIntervals({
    filters: [filter],
    interval: [2, 20],
    cachedIntervals,
  });
  expect(requiredIntervals).toMatchInlineSnapshot(`
    [
      [
        5,
        5,
      ],
      [
        10,
        20,
      ],
    ]
  `);
});
test("mergeAsyncGeneratorsWithEventOrder()", async () => {
  const p1 = promiseWithResolvers<{
    events: Event[];
    checkpoint: string;
    blockRange: [number, number];
  }>();
  const p2 = promiseWithResolvers<{
    events: Event[];
    checkpoint: string;
    blockRange: [number, number];
  }>();
  const p3 = promiseWithResolvers<{
    events: Event[];
    checkpoint: string;
    blockRange: [number, number];
  }>();
  const p4 = promiseWithResolvers<{
    events: Event[];
    checkpoint: string;
    blockRange: [number, number];
  }>();
  async function* generator1() {
    yield await p1.promise;
    yield await p2.promise;
  }
  async function* generator2() {
    yield await p3.promise;
    yield await p4.promise;
  }
  const createCheckpoint = (i: number) =>
    encodeCheckpoint({
      blockTimestamp: BigInt(i),
      chainId: 0n,
      blockNumber: BigInt(i),
      transactionIndex: 0n,
      eventType: 0,
      eventIndex: 0n,
    });
  const generator = mergeAsyncGeneratorsWithEventOrder([
    generator1(),
    generator2(),
  ]);
  p1.resolve({
    events: [
      { checkpoint: createCheckpoint(1), chain: { id: 1 } },
      { checkpoint: createCheckpoint(7), chain: { id: 1 } },
    ] as Event[],
    checkpoint: createCheckpoint(10),
    blockRange: [1, 7],
  });
  p3.resolve({
    events: [
      { checkpoint: createCheckpoint(2), chain: { id: 2 } },
      { checkpoint: createCheckpoint(5), chain: { id: 2 } },
    ] as Event[],
    checkpoint: createCheckpoint(6),
    blockRange: [2, 5],
  });
  await new Promise((res) => setTimeout(res));
  p4.resolve({
    events: [
      { checkpoint: createCheckpoint(8), chain: { id: 2 } },
      { checkpoint: createCheckpoint(11), chain: { id: 2 } },
    ] as Event[],
    checkpoint: createCheckpoint(20),
    blockRange: [8, 11],
  });
  p2.resolve({
    events: [
      { checkpoint: createCheckpoint(8), chain: { id: 1 } },
      { checkpoint: createCheckpoint(13), chain: { id: 1 } },
    ] as Event[],
    checkpoint: createCheckpoint(20),
    blockRange: [8, 13],
  });
  await new Promise((res) => setTimeout(res));
  const results = await drainAsyncGenerator(generator);
  expect(results).toMatchInlineSnapshot(`
    [
      [
        {
          "blockRange": [
            1,
            1,
          ],
          "chainId": 1,
          "checkpoint": "000000000100000000000000000000000000000001000000000000000000000000000000000",
          "events": [
            {
              "chain": {
                "id": 1,
              },
              "checkpoint": "000000000100000000000000000000000000000001000000000000000000000000000000000",
            },
          ],
        },
        {
          "blockRange": [
            2,
            5,
          ],
          "chainId": 2,
          "checkpoint": "000000000600000000000000000000000000000006000000000000000000000000000000000",
          "events": [
            {
              "chain": {
                "id": 2,
              },
              "checkpoint": "000000000200000000000000000000000000000002000000000000000000000000000000000",
            },
            {
              "chain": {
                "id": 2,
              },
              "checkpoint": "000000000500000000000000000000000000000005000000000000000000000000000000000",
            },
          ],
        },
      ],
      [
        {
          "blockRange": [
            1,
            7,
          ],
          "chainId": 1,
          "checkpoint": "000000001000000000000000000000000000000010000000000000000000000000000000000",
          "events": [
            {
              "chain": {
                "id": 1,
              },
              "checkpoint": "000000000700000000000000000000000000000007000000000000000000000000000000000",
            },
          ],
        },
        {
          "blockRange": [
            8,
            8,
          ],
          "chainId": 2,
          "checkpoint": "000000000800000000000000000000000000000008000000000000000000000000000000000",
          "events": [
            {
              "chain": {
                "id": 2,
              },
              "checkpoint": "000000000800000000000000000000000000000008000000000000000000000000000000000",
            },
          ],
        },
      ],
      [
        {
          "blockRange": [
            8,
            13,
          ],
          "chainId": 1,
          "checkpoint": "000000002000000000000000000000000000000020000000000000000000000000000000000",
          "events": [
            {
              "chain": {
                "id": 1,
              },
              "checkpoint": "000000000800000000000000000000000000000008000000000000000000000000000000000",
            },
            {
              "chain": {
                "id": 1,
              },
              "checkpoint": "000000001300000000000000000000000000000013000000000000000000000000000000000",
            },
          ],
        },
        {
          "blockRange": [
            8,
            11,
          ],
          "chainId": 2,
          "checkpoint": "000000002000000000000000000000000000000020000000000000000000000000000000000",
          "events": [
            {
              "chain": {
                "id": 2,
              },
              "checkpoint": "000000001100000000000000000000000000000011000000000000000000000000000000000",
            },
          ],
        },
      ],
    ]
  `);
});
test("historical events match realtime events", async () => {
  const { syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getErc20IndexingBuild({
    address,
    includeTransactionReceipts: true,
  });
  await syncStore.insertBlocks({ blocks: [blockData.block], chainId: 1 });
  await syncStore.insertTransactions({
    transactions: [blockData.transaction],
    chainId: 1,
  });
  await syncStore.insertLogs({
    logs: [blockData.log],
    chainId: 1,
  });
  const { logs: historicalLogs } = await syncStore.getEventData({
    filters: [eventCallbacks[0]!.filter],
    fromBlock: 0,
    toBlock: 10,
    chainId: 1,
    limit: 3,
  });
  const realtimeBlockData = [
    {
      block: syncBlockToInternal({ block: blockData.block }),
      logs: [syncLogToInternal({ log: blockData.log })],
      transactions: syncTransactionToInternal({
        transaction: blockData.transaction,
      }),
      transactionReceipts: [],
      traces: [],
    },
  ];
  // Note: blocks and transactions are not asserted because they are non deterministic
  expect(historicalLogs).toMatchInlineSnapshot(`
    [
      {
        "address": "0x5fbdb2315678afecb367f032d93f642f64180aa3",
        "blockNumber": 2,
        "data": "0x0000000000000000000000000000000000000000000000000de0b6b3a7640000",
        "logIndex": 0,
        "removed": false,
        "topic0": undefined,
        "topic1": undefined,
        "topic2": undefined,
        "topic3": undefined,
        "topics": [
          "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
          "0x0000000000000000000000000000000000000000000000000000000000000000",
          "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
          null,
        ],
        "transactionIndex": 0,
      },
    ]
  `);
  expect(realtimeBlockData[0]!.logs).toMatchInlineSnapshot(`
    [
      {
        "address": "0x5fbdb2315678afecb367f032d93f642f64180aa3",
        "blockNumber": 2,
        "data": "0x0000000000000000000000000000000000000000000000000de0b6b3a7640000",
        "logIndex": 0,
        "removed": false,
        "topics": [
          "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
          "0x0000000000000000000000000000000000000000000000000000000000000000",
          "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
        ],
        "transactionIndex": 0,
      },
    ]
  `);
});
</file>

<file path="packages/core/src/runtime/index.ts">
import type { Common } from "@/internal/common.js";
import type {
  Chain,
  Factory,
  FactoryId,
  Filter,
  Fragment,
  LightBlock,
} from "@/internal/types.js";
import type { SyncBlock } from "@/internal/types.js";
import { eth_getBlockByNumber } from "@/rpc/actions.js";
import type { Rpc } from "@/rpc/index.js";
import {
  getFilterFactories,
  getFilterFromBlock,
  getFilterToBlock,
  isAddressFactory,
} from "@/runtime/filter.js";
import {
  getFactoryFragments,
  getFragments,
  recoverFilter,
} from "@/runtime/fragments.js";
import type { SyncStore } from "@/sync-store/index.js";
import {
  MAX_CHECKPOINT,
  blockToCheckpoint,
  encodeCheckpoint,
} from "@/utils/checkpoint.js";
import {
  type Interval,
  intervalBounds,
  intervalDifference,
  intervalIntersection,
  intervalIntersectionMany,
  intervalUnion,
  sortIntervals,
} from "@/utils/interval.js";
import { type Address, hexToNumber, numberToHex, toHex } from "viem";
export type SyncProgress = {
  start: SyncBlock | LightBlock;
  end: SyncBlock | LightBlock | undefined;
  current: SyncBlock | LightBlock | undefined;
  finalized: SyncBlock | LightBlock;
  isEnd: () => boolean;
  isFinalized: () => boolean;
  getCheckpoint: <tag extends "start" | "end" | "current" | "finalized">({
    tag,
  }: { tag: tag }) => tag extends "end" ? string | undefined : string;
};
export type ChildAddresses = Map<FactoryId, Map<Address, number>>;
export type CachedIntervals = Map<
  Filter | Factory,
  { fragment: Fragment; intervals: Interval[] }[]
>;
export type IntervalWithFilter = {
  interval: Interval;
  filter: Filter;
};
export type IntervalWithFactory = {
  interval: Interval;
  factory: Factory;
};
export async function getLocalSyncProgress(params: {
  common: Common;
  chain: Chain;
  rpc: Rpc;
  filters: Filter[];
  finalizedBlock: LightBlock;
  cachedIntervals: CachedIntervals;
}): Promise<SyncProgress> {
  const syncProgress = {
    isEnd: () => {
      if (
        syncProgress.end === undefined ||
        syncProgress.current === undefined
      ) {
        return false;
      }
      return (
        hexToNumber(syncProgress.current.number) >=
        hexToNumber(syncProgress.end.number)
      );
    },
    isFinalized: () => {
      if (syncProgress.current === undefined) {
        return false;
      }
      return (
        hexToNumber(syncProgress.current.number) >=
        hexToNumber(syncProgress.finalized.number)
      );
    },
    getCheckpoint: ({ tag }) => {
      if (tag === "end" && syncProgress.end === undefined) {
        return undefined;
      }
      // Note: `current` is guaranteed to be defined because it is only used once the historical
      // backfill is complete.
      const block = syncProgress[tag]!;
      return encodeCheckpoint(
        blockToCheckpoint(
          block,
          params.chain.id,
          // The checkpoint returned by this function is meant to be used in
          // a closed interval (includes endpoints), so "start" should be inclusive.
          tag === "start" ? "down" : "up",
        ),
      );
    },
  } as SyncProgress;
  // Earliest `fromBlock` among all `filters`
  const start = Math.min(...params.filters.map(getFilterFromBlock));
  const cached = getCachedBlock({
    filters: params.filters,
    cachedIntervals: params.cachedIntervals,
  });
  const diagnostics = await Promise.all(
    cached === undefined
      ? [
          eth_getBlockByNumber(params.rpc, [numberToHex(start), false], {
            retryNullBlockRequest: true,
          }),
        ]
      : [
          eth_getBlockByNumber(params.rpc, [numberToHex(start), false], {
            retryNullBlockRequest: true,
          }),
          eth_getBlockByNumber(params.rpc, [numberToHex(cached), false], {
            retryNullBlockRequest: true,
          }),
        ],
  );
  syncProgress.finalized = params.finalizedBlock;
  syncProgress.start = diagnostics[0];
  if (diagnostics.length === 2) {
    syncProgress.current = diagnostics[1];
  }
  if (params.filters.some((filter) => filter.toBlock === undefined)) {
    return syncProgress;
  }
  // Latest `toBlock` among all `filters`
  const end = Math.max(...params.filters.map((filter) => filter.toBlock!));
  if (end > hexToNumber(params.finalizedBlock.number)) {
    syncProgress.end = {
      number: toHex(end),
      hash: "0x",
      parentHash: "0x",
      timestamp: toHex(MAX_CHECKPOINT.blockTimestamp),
    } satisfies LightBlock;
  } else {
    syncProgress.end = await eth_getBlockByNumber(
      params.rpc,
      [numberToHex(end), false],
      { retryNullBlockRequest: true },
    );
  }
  return syncProgress;
}
export async function getChildAddresses(params: {
  filters: Filter[];
  syncStore: SyncStore;
}): Promise<ChildAddresses> {
  const childAddresses: ChildAddresses = new Map();
  for (const filter of params.filters) {
    switch (filter.type) {
      case "log":
        if (isAddressFactory(filter.address)) {
          const _childAddresses = await params.syncStore.getChildAddresses({
            factory: filter.address,
          });
          childAddresses.set(filter.address.id, _childAddresses);
        }
        break;
      case "transaction":
      case "transfer":
      case "trace":
        if (isAddressFactory(filter.fromAddress)) {
          const _childAddresses = await params.syncStore.getChildAddresses({
            factory: filter.fromAddress,
          });
          childAddresses.set(filter.fromAddress.id, _childAddresses);
        }
        if (isAddressFactory(filter.toAddress)) {
          const _childAddresses = await params.syncStore.getChildAddresses({
            factory: filter.toAddress,
          });
          childAddresses.set(filter.toAddress.id, _childAddresses);
        }
        break;
    }
  }
  return childAddresses;
}
export async function getCachedIntervals(params: {
  chain: Chain;
  filters: Filter[];
  syncStore: SyncStore;
}): Promise<CachedIntervals> {
  /**
   * Intervals that have been completed for all filters in `args.filters`.
   *
   * Note: `intervalsCache` is not updated after a new interval is synced.
   */
  let cachedIntervals: CachedIntervals;
  if (params.chain.disableCache) {
    cachedIntervals = new Map();
    for (const filter of params.filters) {
      cachedIntervals.set(filter, []);
      for (const { fragment } of getFragments(filter)) {
        cachedIntervals.get(filter)!.push({ fragment, intervals: [] });
      }
      for (const factory of getFilterFactories(filter)) {
        cachedIntervals.set(factory, []);
        for (const fragment of getFactoryFragments(factory)) {
          cachedIntervals.get(factory)!.push({ fragment, intervals: [] });
        }
      }
    }
  } else {
    cachedIntervals = await params.syncStore.getIntervals({
      filters: params.filters,
    });
  }
  return cachedIntervals;
}
/**
 * Returns the intervals that need to be synced to complete the `interval`
 * for all `filters`.
 *
 * @param params.filters - The filters to sync.
 * @param params.interval - The interval to sync.
 * @param params.cachedIntervals - The cached intervals for the filters.
 * @returns The intervals that need to be synced.
 */
export const getRequiredIntervals = (params: {
  filters: Filter[];
  interval: Interval;
  cachedIntervals: CachedIntervals;
}): Interval[] => {
  const requiredIntervals: Interval[] = [];
  for (const filter of params.filters) {
    const filterTotalIntervals = intervalIntersection(
      [params.interval],
      [[filter.fromBlock ?? 0, filter.toBlock ?? Number.POSITIVE_INFINITY]],
    );
    let filterCachedIntervals = params.cachedIntervals.get(filter)!;
    const factories = getFilterFactories(filter);
    const missingFactoryIntervals: Interval[] = [];
    for (const factory of factories) {
      const factoryTotalIntervals = intervalIntersection(
        [params.interval],
        [[factory.fromBlock ?? 0, factory.toBlock ?? Number.POSITIVE_INFINITY]],
      );
      missingFactoryIntervals.push(
        ...intervalDifference(
          factoryTotalIntervals,
          intervalIntersectionMany(
            params.cachedIntervals
              .get(factory)!
              .map(({ intervals }) => intervals),
          ),
        ),
      );
    }
    if (missingFactoryIntervals.length > 0) {
      const firstMissingFactoryBlock = sortIntervals(
        missingFactoryIntervals,
      )[0]![0];
      // Note: When a filter with a factory is missing blocks,
      // all blocks after the first missing block are also missing.
      filterCachedIntervals = filterCachedIntervals.map(
        ({ fragment, intervals }) => {
          return {
            fragment,
            intervals: intervalDifference(intervals, [
              [firstMissingFactoryBlock, params.interval[1]],
            ]),
          };
        },
      );
    }
    const missingIntervals = intervalDifference(
      filterTotalIntervals,
      intervalIntersectionMany(
        filterCachedIntervals.map(({ intervals }) => intervals),
      ),
    );
    requiredIntervals.push(...missingIntervals, ...missingFactoryIntervals);
  }
  return intervalUnion(requiredIntervals);
};
/**
 * Returns the intervals that need to be synced to complete the `interval`
 * for all `filters`.
 *
 * Note: This function dynamically builds filters using `recoverFilter`.
 * Fragments are used to create a minimal filter, to avoid refetching data
 * even if a filter is only partially synced.
 *
 * @param params.filters - The filters to sync.
 * @param params.interval - The interval to sync.
 * @param params.cachedIntervals - The cached intervals for the filters.
 * @returns The intervals that need to be synced.
 */
export const getRequiredIntervalsWithFilters = (params: {
  filters: Filter[];
  interval: Interval;
  cachedIntervals: CachedIntervals;
}): {
  intervals: IntervalWithFilter[];
  factoryIntervals: IntervalWithFactory[];
} => {
  const requiredIntervals: IntervalWithFilter[] = [];
  const requiredFactoryIntervals: IntervalWithFactory[] = [];
  // Determine the requests that need to be made, and which intervals need to be inserted.
  // Fragments are used to create a minimal filter, to avoid refetching data even if a filter
  // is only partially synced.
  for (const filter of params.filters) {
    const filterTotalIntervals = intervalIntersection(
      [params.interval],
      [[filter.fromBlock ?? 0, filter.toBlock ?? Number.POSITIVE_INFINITY]],
    );
    let filterCachedIntervals = params.cachedIntervals.get(filter)!;
    const factories = getFilterFactories(filter);
    const missingFactoryIntervals: Interval[] = [];
    for (const factory of factories) {
      const factoryTotalIntervals = intervalIntersection(
        [params.interval],
        [[factory.fromBlock ?? 0, factory.toBlock ?? Number.POSITIVE_INFINITY]],
      );
      missingFactoryIntervals.push(
        ...intervalDifference(
          factoryTotalIntervals,
          intervalIntersectionMany(
            params.cachedIntervals
              .get(factory)!
              .map(({ intervals }) => intervals),
          ),
        ),
      );
    }
    if (missingFactoryIntervals.length > 0) {
      const firstMissingFactoryBlock = sortIntervals(
        missingFactoryIntervals,
      )[0]![0];
      // Note: When a filter with a factory is missing blocks,
      // all blocks after the first missing block are also missing.
      filterCachedIntervals = filterCachedIntervals.map(
        ({ fragment, intervals }) => {
          return {
            fragment,
            intervals: intervalDifference(intervals, [
              [firstMissingFactoryBlock, params.interval[1]],
            ]),
          };
        },
      );
    }
    const requiredFragmentIntervals: {
      fragment: Fragment;
      intervals: Interval[];
    }[] = [];
    for (const {
      fragment,
      intervals: fragmentIntervals,
    } of filterCachedIntervals) {
      const missingFragmentIntervals = intervalDifference(
        filterTotalIntervals,
        fragmentIntervals,
      );
      if (missingFragmentIntervals.length > 0) {
        requiredFragmentIntervals.push({
          fragment,
          intervals: missingFragmentIntervals,
        });
      }
    }
    if (requiredFragmentIntervals.length > 0) {
      const requiredInterval = intervalBounds(
        requiredFragmentIntervals.flatMap(({ intervals }) => intervals),
      );
      const requiredFilter = recoverFilter(
        filter,
        requiredFragmentIntervals.map(({ fragment }) => fragment),
      );
      requiredIntervals.push({
        filter: requiredFilter,
        interval: requiredInterval,
      });
    }
    for (const factory of factories) {
      const factoryTotalIntervals = intervalIntersection(
        [params.interval],
        [[factory.fromBlock ?? 0, factory.toBlock ?? Number.POSITIVE_INFINITY]],
      );
      const requiredFactoryFragmentIntervals: {
        fragment: Fragment;
        intervals: Interval[];
      }[] = [];
      for (const {
        fragment,
        intervals: fragmentIntervals,
      } of params.cachedIntervals.get(factory)!) {
        const missingFragmentIntervals = intervalDifference(
          factoryTotalIntervals,
          fragmentIntervals,
        );
        if (missingFragmentIntervals.length > 0) {
          requiredFactoryFragmentIntervals.push({
            fragment,
            intervals: missingFragmentIntervals,
          });
        }
      }
      if (requiredFactoryFragmentIntervals.length > 0) {
        const requiredInterval = intervalBounds(
          requiredFactoryFragmentIntervals.flatMap(
            ({ intervals }) => intervals,
          ),
        );
        requiredFactoryIntervals.push({
          factory,
          interval: requiredInterval,
        });
      }
    }
  }
  return {
    intervals: requiredIntervals,
    factoryIntervals: requiredFactoryIntervals,
  };
};
/** Returns the closest-to-tip block that has been synced for all `filters`. */
export const getCachedBlock = ({
  filters,
  cachedIntervals,
}: {
  filters: Filter[];
  cachedIntervals: CachedIntervals;
}): number | undefined => {
  const latestCompletedBlocks = filters.map((filter) => {
    const filterTotalInterval = [
      filter.fromBlock ?? 0,
      filter.toBlock ?? Number.POSITIVE_INFINITY,
    ] satisfies Interval;
    let filterCachedIntervals = cachedIntervals.get(filter)!;
    const factories = getFilterFactories(filter);
    const missingFactoryIntervals: Interval[] = [];
    for (const factory of factories) {
      const factoryTotalInterval = [
        factory.fromBlock ?? 0,
        factory.toBlock ?? Number.POSITIVE_INFINITY,
      ] satisfies Interval;
      missingFactoryIntervals.push(
        ...intervalDifference(
          [factoryTotalInterval],
          intervalIntersectionMany(
            cachedIntervals.get(factory)!.map(({ intervals }) => intervals),
          ),
        ),
      );
    }
    if (missingFactoryIntervals.length > 0) {
      const firstMissingFactoryBlock = sortIntervals(
        missingFactoryIntervals,
      )[0]![0];
      // Note: When a filter with a factory is missing blocks,
      // all blocks after the first missing block are also missing.
      filterCachedIntervals = filterCachedIntervals.map(
        ({ fragment, intervals }) => {
          return {
            fragment,
            intervals: intervalDifference(intervals, [
              [firstMissingFactoryBlock, filterTotalInterval[1]],
            ]),
          };
        },
      );
    }
    let missingIntervals = intervalDifference(
      [filterTotalInterval],
      intervalIntersectionMany(
        filterCachedIntervals.map(({ intervals }) => intervals),
      ),
    );
    if (missingIntervals.length === 0 && missingFactoryIntervals.length === 0) {
      return getFilterToBlock(filter);
    }
    missingIntervals = sortIntervals([
      ...missingIntervals,
      ...missingFactoryIntervals,
    ]);
    if (missingIntervals[0]![0] === 0) return undefined;
    // First missing block - 1 is the last completed block
    return missingIntervals[0]![0] - 1;
  });
  if (latestCompletedBlocks.every((block) => block !== undefined)) {
    return Math.min(...(latestCompletedBlocks as number[]));
  }
  return undefined;
};
</file>

<file path="packages/core/src/runtime/init.ts">
import type { Database } from "@/database/index.js";
import type { Common } from "@/internal/common.js";
import type {
  Chain,
  EventCallback,
  IndexingBuild,
  RawEvent,
} from "@/internal/types.js";
import type { Rpc } from "@/rpc/index.js";
import { getLocalEventGenerator, refetchLocalEvents } from "./historical.js";
import {
  type CachedIntervals,
  type ChildAddresses,
  type SyncProgress,
  getLocalSyncProgress,
} from "./index.js";
export async function initEventGenerator(params: {
  common: Common;
  indexingBuild: Pick<
    IndexingBuild,
    "eventCallbacks" | "chains" | "rpcs" | "finalizedBlocks"
  >;
  chain: Chain;
  rpc: Rpc;
  eventCallbacks: EventCallback[];
  childAddresses: ChildAddresses;
  syncProgress: SyncProgress;
  cachedIntervals: CachedIntervals;
  from: string;
  to: string;
  limit: number;
  database: Database;
  isCatchup: boolean;
}) {
  return getLocalEventGenerator(params);
}
export async function initRefetchEvents(
  params: Parameters<typeof refetchLocalEvents>[0],
): Promise<RawEvent[]> {
  return refetchLocalEvents(params);
}
export async function initSyncProgress(
  params: Parameters<typeof getLocalSyncProgress>[0],
): Promise<SyncProgress> {
  return getLocalSyncProgress(params);
}
</file>

<file path="packages/core/src/runtime/isolated.ts">
import {
  commitBlock,
  createLiveQueryTriggers,
  createTriggers,
  dropLiveQueryTriggers,
  dropTriggers,
  finalizeIsolated,
  revertIsolated,
} from "@/database/actions.js";
import { type Database, getPonderCheckpointTable } from "@/database/index.js";
import { getLiveQueryTempTableName } from "@/drizzle/onchain.js";
import { createIndexingCache } from "@/indexing-store/cache.js";
import { createIndexingStore } from "@/indexing-store/index.js";
import { createCachedViemClient } from "@/indexing/client.js";
import {
  createColumnAccessPattern,
  createIndexing,
  getEventCount,
} from "@/indexing/index.js";
import type { Common } from "@/internal/common.js";
import {
  InvalidEventAccessError,
  NonRetryableUserError,
  type RetryableError,
} from "@/internal/errors.js";
import type {
  CrashRecoveryCheckpoint,
  IndexingBuild,
  IndexingErrorHandler,
  NamespaceBuild,
  PreBuild,
  SchemaBuild,
  Seconds,
} from "@/internal/types.js";
import { splitEvents } from "@/runtime/events.js";
import type { RealtimeSyncEvent } from "@/sync-realtime/index.js";
import { createSyncStore } from "@/sync-store/index.js";
import {
  ZERO_CHECKPOINT_STRING,
  decodeCheckpoint,
  min,
} from "@/utils/checkpoint.js";
import {
  bufferAsyncGenerator,
  recordAsyncGenerator,
} from "@/utils/generators.js";
import { never } from "@/utils/never.js";
import { startClock } from "@/utils/timer.js";
import { eq, getTableName, isTable, sql } from "drizzle-orm";
import {
  getHistoricalEventsIsolated,
  refetchHistoricalEvents,
} from "./historical.js";
import { getCachedIntervals, getChildAddresses } from "./index.js";
import { initSyncProgress } from "./init.js";
import { getRealtimeEventsIsolated } from "./realtime.js";
export async function runIsolated({
  common,
  preBuild,
  namespaceBuild,
  schemaBuild,
  indexingBuild,
  crashRecoveryCheckpoint,
  database,
  onReady,
}: {
  common: Common;
  preBuild: PreBuild;
  namespaceBuild: NamespaceBuild;
  schemaBuild: SchemaBuild;
  indexingBuild: IndexingBuild;
  crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  database: Database;
  onReady: () => void;
}) {
  const chain = indexingBuild.chains[0]!;
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild,
  });
  const syncStore = createSyncStore({ common, qb: database.syncQB });
  const PONDER_CHECKPOINT = getPonderCheckpointTable(namespaceBuild.schema);
  const eventCount = getEventCount(indexingBuild.indexingFunctions);
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild,
    syncStore,
    eventCount,
  });
  const indexingErrorHandler: IndexingErrorHandler = {
    getRetryableError: () => {
      return indexingErrorHandler.error;
    },
    setRetryableError: (error: RetryableError) => {
      indexingErrorHandler.error = error;
    },
    clearRetryableError: () => {
      indexingErrorHandler.error = undefined;
    },
    error: undefined as RetryableError | undefined,
  };
  const indexingCache = createIndexingCache({
    common,
    schemaBuild,
    crashRecoveryCheckpoint,
    eventCount,
    chainId: chain.id,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild,
    indexingCache,
    indexingErrorHandler,
    chainId: chain.id,
  });
  const indexing = createIndexing({
    common,
    indexingBuild,
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const seconds: Seconds = {};
  const eventCallbacks =
    indexingBuild.eventCallbacks[indexingBuild.chains.indexOf(chain)]!;
  const cachedIntervals = await getCachedIntervals({
    chain,
    filters: eventCallbacks.map(({ filter }) => filter),
    syncStore,
  });
  const syncProgress = await initSyncProgress({
    common,
    filters: eventCallbacks.map(({ filter }) => filter),
    chain,
    rpc: indexingBuild.rpcs[0]!,
    finalizedBlock: indexingBuild.finalizedBlocks[0]!,
    cachedIntervals,
  });
  const childAddresses = await getChildAddresses({
    filters: eventCallbacks.map(({ filter }) => filter),
    syncStore,
  });
  const unfinalizedBlocks: Omit<
    Extract<RealtimeSyncEvent, { type: "block" }>,
    "type"
  >[] = [];
  const start = Number(
    decodeCheckpoint(syncProgress.getCheckpoint({ tag: "start" }))
      .blockTimestamp,
  );
  const end = Number(
    decodeCheckpoint(
      min(
        syncProgress.getCheckpoint({ tag: "end" }),
        syncProgress.getCheckpoint({ tag: "finalized" }),
      ),
    ).blockTimestamp,
  );
  const _crashRecoveryCheckpoint = crashRecoveryCheckpoint?.find(
    ({ chainId }) => chainId === chain.id,
  )?.checkpoint;
  const cached = Math.min(
    Number(
      decodeCheckpoint(_crashRecoveryCheckpoint ?? ZERO_CHECKPOINT_STRING)
        .blockTimestamp,
    ),
    end,
  );
  seconds[chain.name] = { start, end, cached };
  const label = { chain: chain.name };
  common.metrics.ponder_historical_total_indexing_seconds.set(
    label,
    Math.max(seconds[chain.name]!.end - seconds[chain.name]!.start, 0),
  );
  common.metrics.ponder_historical_cached_indexing_seconds.set(
    label,
    Math.max(seconds[chain.name]!.cached - seconds[chain.name]!.start, 0),
  );
  common.metrics.ponder_historical_completed_indexing_seconds.set(label, 0);
  common.metrics.ponder_indexing_timestamp.set(
    label,
    Math.max(seconds[chain.name]!.cached, seconds[chain.name]!.start),
  );
  const startTimestamp = Math.round(Date.now() / 1000);
  common.metrics.ponder_historical_start_timestamp_seconds.set(
    label,
    startTimestamp,
  );
  // Reset the start timestamp so the eta estimate doesn't include
  // the startup time.
  common.metrics.start_timestamp = Date.now();
  // If the initial checkpoint is zero, we need to run setup events.
  if (crashRecoveryCheckpoint === undefined) {
    await database.userQB.transaction(async (tx) => {
      indexingStore.qb = tx;
      indexingStore.isProcessingEvents = true;
      indexingCache.qb = tx;
      await indexing.processSetupEvents();
      indexingStore.isProcessingEvents = false;
      await indexingCache.flush();
      const initialCheckpoint = min(
        syncProgress.getCheckpoint({ tag: "start" }),
        syncProgress.getCheckpoint({ tag: "finalized" }),
      );
      await tx.wrap({ label: "update_checkpoints" }, (tx) =>
        tx
          .insert(PONDER_CHECKPOINT)
          .values({
            chainName: chain.name,
            chainId: chain.id,
            latestCheckpoint: initialCheckpoint,
            safeCheckpoint: initialCheckpoint,
            finalizedCheckpoint: initialCheckpoint,
          })
          .onConflictDoUpdate({
            target: PONDER_CHECKPOINT.chainName,
            set: {
              finalizedCheckpoint: sql`excluded.finalized_checkpoint`,
              safeCheckpoint: sql`excluded.safe_checkpoint`,
              latestCheckpoint: sql`excluded.latest_checkpoint`,
            },
          }),
      );
    });
  }
  const backfillEndClock = startClock();
  // Run historical indexing until complete.
  for await (let {
    events,
    chainId,
    checkpoint,
    blockRange,
  } of recordAsyncGenerator(
    bufferAsyncGenerator(
      getHistoricalEventsIsolated({
        common,
        chain,
        indexingBuild,
        crashRecoveryCheckpoint,
        syncProgress,
        childAddresses,
        cachedIntervals,
        database,
      }),
      1,
    ),
    (params) => {
      common.metrics.ponder_historical_concurrency_group_duration.inc(
        { group: "extract" },
        params.await,
      );
      common.metrics.ponder_historical_concurrency_group_duration.inc(
        { group: "transform" },
        params.yield,
      );
    },
  )) {
    const context = {
      logger: common.logger.child({ action: "index_block_range" }),
    };
    const indexStartClock = startClock();
    indexingCache.qb = database.userQB;
    await Promise.all([
      indexingCache.prefetch({ events }),
      cachedViemClient.prefetch({ events }),
    ]);
    common.metrics.ponder_historical_transform_duration.inc(
      { step: "prefetch" },
      indexStartClock(),
    );
    let endClock = startClock();
    await database.userQB.transaction(
      async (tx) => {
        const initialCompletedEvents = structuredClone(
          await common.metrics.ponder_indexing_completed_events.get(),
        );
        try {
          indexingStore.qb = tx;
          indexingStore.isProcessingEvents = true;
          indexingCache.qb = tx;
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "begin" },
            endClock(),
          );
          endClock = startClock();
          await indexing.processHistoricalEvents({
            events,
            updateIndexingSeconds(event, chain) {
              const checkpoint = decodeCheckpoint(event!.checkpoint);
              common.metrics.ponder_historical_completed_indexing_seconds.set(
                { chain: chain.name },
                Math.max(
                  Number(checkpoint.blockTimestamp) -
                    Math.max(
                      seconds[chain.name]!.cached,
                      seconds[chain.name]!.start,
                    ),
                  0,
                ),
              );
              common.metrics.ponder_indexing_timestamp.set(
                { chain: chain.name },
                Number(checkpoint.blockTimestamp),
              );
            },
          });
          indexingStore.isProcessingEvents = false;
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "index" },
            endClock(),
          );
          endClock = startClock();
          // Note: at this point, the next events can be preloaded, as long as the are not indexed until
          // the "flush" + "finalize" is complete.
          await indexingCache.flush();
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "load" },
            endClock(),
          );
          endClock = startClock();
          await tx.wrap(
            { label: "update_checkpoints" },
            (tx) =>
              tx
                .insert(PONDER_CHECKPOINT)
                .values({
                  chainName: chain.name,
                  chainId,
                  latestCheckpoint: checkpoint,
                  finalizedCheckpoint: checkpoint,
                  safeCheckpoint: checkpoint,
                })
                .onConflictDoUpdate({
                  target: PONDER_CHECKPOINT.chainName,
                  set: {
                    safeCheckpoint: sql`excluded.safe_checkpoint`,
                    finalizedCheckpoint: sql`excluded.finalized_checkpoint`,
                    latestCheckpoint: sql`excluded.latest_checkpoint`,
                  },
                }),
            context,
          );
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "finalize" },
            endClock(),
          );
          endClock = startClock();
        } catch (error) {
          // Note: This can cause a bug with "dev" command, because there are multiple instances
          // updating the same metric.
          for (const value of initialCompletedEvents.values) {
            common.metrics.ponder_indexing_completed_events.set(
              value.labels,
              value.value,
            );
          }
          indexingCache.invalidate();
          indexingCache.clear();
          if (error instanceof InvalidEventAccessError) {
            common.logger.debug({
              msg: "Failed to index block range",
              chain: chain.name,
              chain_id: chain.id,
              block_range: JSON.stringify(blockRange),
              duration: indexStartClock(),
              error,
            });
            events = await refetchHistoricalEvents({
              common,
              indexingBuild,
              perChainSync: new Map([[chain, { childAddresses }]]),
              syncStore,
              events,
            });
          } else if (error instanceof NonRetryableUserError === false) {
            common.logger.warn({
              msg: "Failed to index block range",
              chain: chain.name,
              chain_id: chain.id,
              block_range: JSON.stringify(blockRange),
              duration: indexStartClock(),
              error: error as Error,
            });
          }
          throw error;
        }
      },
      undefined,
      context,
    );
    cachedViemClient.clear();
    common.metrics.ponder_historical_transform_duration.inc(
      { step: "commit" },
      endClock(),
    );
    await new Promise(setImmediate);
    common.logger.info({
      msg: "Indexed block range",
      chain: chain.name,
      chain_id: chain.id,
      event_count: events.length,
      block_range: JSON.stringify(blockRange),
      duration: indexStartClock(),
    });
  }
  indexingCache.clear();
  // Note: Invalidating the cache means that only predicted rows will be in memory after this point.
  indexingCache.invalidate();
  // Manually update metrics to fix a UI bug that occurs when the end
  // checkpoint is between the last processed event and the finalized
  // checkpoint.
  common.metrics.ponder_historical_completed_indexing_seconds.set(
    label,
    Math.max(
      seconds[chain.name]!.end -
        Math.max(seconds[chain.name]!.cached, seconds[chain.name]!.start),
      0,
    ),
  );
  common.metrics.ponder_indexing_timestamp.set(
    { chain: chain.name },
    seconds[chain.name]!.end,
  );
  const endTimestamp = Math.round(Date.now() / 1000);
  common.metrics.ponder_historical_end_timestamp_seconds.set(
    { chain: chain.name },
    endTimestamp,
  );
  common.logger.info({
    msg: "Completed backfill indexing",
    chain: chain.name,
    chain_id: chain.id,
    duration: backfillEndClock(),
  });
  const tables = Object.values(schemaBuild.schema).filter(isTable);
  const endClock = startClock();
  await createTriggers(database.adminQB, { tables, chainId: chain.id });
  await createLiveQueryTriggers(database.adminQB, {
    namespaceBuild,
    tables,
    chainId: chain.id,
  });
  common.logger.debug({
    msg: "Created database triggers",
    chain: chain.name,
    chain_id: chain.id,
    count: tables.length,
    duration: endClock(),
  });
  onReady();
  const bufferCallback = (bufferSize: number) => {
    // Note: Only log when the buffer size is greater than 1 because
    // a buffer size of 1 is not backpressure.
    if (bufferSize === 1) return;
    common.logger.trace({
      msg: "Detected live indexing backpressure",
      buffer_size: bufferSize,
      indexing_step: "index block",
    });
  };
  for await (const event of bufferAsyncGenerator(
    getRealtimeEventsIsolated({
      common,
      indexingBuild,
      chain,
      syncProgress,
      childAddresses,
      unfinalizedBlocks,
      database,
    }),
    100,
    bufferCallback,
  )) {
    switch (event.type) {
      case "block": {
        const context = {
          logger: common.logger.child({ action: "index_block" }),
        };
        const endClock = startClock();
        indexingCache.qb = database.userQB;
        await Promise.all([
          indexingCache.prefetch({ events: event.events }),
          cachedViemClient.prefetch({ events: event.events }),
        ]);
        await database.userQB.transaction(
          async (tx) => {
            if (database.userQB.$dialect === "postgres") {
              await tx.wrap(
                (tx) =>
                  tx.execute(
                    `CREATE TEMP TABLE ${getLiveQueryTempTableName()} (table_name TEXT PRIMARY KEY) ON COMMIT DROP`,
                  ),
                context,
              );
            } else {
              await tx.wrap(
                (tx) =>
                  tx.execute(
                    `CREATE TEMP TABLE IF NOT EXISTS ${getLiveQueryTempTableName()} (table_name TEXT PRIMARY KEY)`,
                  ),
                context,
              );
            }
            // Events must be run block-by-block, so that `database.commitBlock` can accurately
            // update the temporary `checkpoint` value set in the trigger.
            for (const { checkpoint, events } of splitEvents(event.events)) {
              try {
                indexingStore.qb = tx;
                indexingStore.isProcessingEvents = true;
                indexingCache.qb = tx;
                common.logger.trace({
                  msg: "Processing block events",
                  chain: chain.name,
                  chain_id: chain.id,
                  number: Number(decodeCheckpoint(checkpoint).blockNumber),
                  event_count: events.length,
                });
                await indexing.processRealtimeEvents({ events });
                common.logger.trace({
                  msg: "Processed block events",
                  chain: chain.name,
                  chain_id: chain.id,
                  number: Number(decodeCheckpoint(checkpoint).blockNumber),
                  event_count: events.length,
                });
                indexingStore.isProcessingEvents = false;
                await indexingCache.flush();
                await Promise.all(
                  tables.map((table) =>
                    commitBlock(tx, { table, checkpoint, preBuild }, context),
                  ),
                );
                cachedViemClient.clear();
                common.logger.trace({
                  msg: "Committed reorg data for block",
                  chain: chain.name,
                  chain_id: chain.id,
                  number: Number(decodeCheckpoint(checkpoint).blockNumber),
                  event_count: events.length,
                  checkpoint,
                });
                common.metrics.ponder_indexing_timestamp.set(
                  { chain: chain.name },
                  Number(decodeCheckpoint(checkpoint).blockTimestamp),
                );
              } catch (error) {
                indexingCache.clear();
                if (error instanceof NonRetryableUserError === false) {
                  common.logger.warn({
                    msg: "Failed to index block",
                    chain: chain.name,
                    chain_id: chain.id,
                    number: Number(decodeCheckpoint(checkpoint).blockNumber),
                    error: error,
                  });
                }
                throw error;
              }
            }
            await tx.wrap(
              { label: "update_checkpoints" },
              (db) =>
                db
                  .update(PONDER_CHECKPOINT)
                  .set({ latestCheckpoint: event.checkpoint })
                  .where(eq(PONDER_CHECKPOINT.chainName, event.chain.name)),
              context,
            );
            if (
              event.events.length > 0 &&
              database.userQB.$dialect === "pglite"
            ) {
              await tx.wrap(
                (tx) =>
                  tx.execute(`TRUNCATE TABLE ${getLiveQueryTempTableName()}`),
                context,
              );
            }
          },
          undefined,
          context,
        );
        event.blockCallback?.(true);
        common.logger.info({
          msg: "Indexed block",
          chain: event.chain.name,
          chain_id: event.chain.id,
          number: Number(decodeCheckpoint(event.checkpoint).blockNumber),
          event_count: event.events.length,
          duration: endClock(),
        });
        break;
      }
      case "reorg": {
        const context = {
          logger: common.logger.child({ action: "reorg_block" }),
        };
        const endClock = startClock();
        // Note: `_ponder_checkpoint` is not called here, instead it is called
        // in the `block` case.
        await database.userQB.transaction(
          async (tx) => {
            await dropTriggers(tx, { tables, chainId: chain.id }, context);
            await dropLiveQueryTriggers(
              tx,
              { namespaceBuild, tables, chainId: chain.id },
              context,
            );
            const counts = await revertIsolated(
              tx,
              {
                checkpoint: event.checkpoint,
                tables,
              },
              context,
            );
            for (const [index, table] of tables.entries()) {
              common.logger.debug({
                msg: "Reverted reorged database rows",
                chain: chain.name,
                chain_id: chain.id,
                table: getTableName(table),
                row_count: counts[index],
              });
            }
            await createTriggers(tx, { tables, chainId: chain.id }, context);
            await createLiveQueryTriggers(
              tx,
              { namespaceBuild, tables, chainId: chain.id },
              context,
            );
          },
          undefined,
          context,
        );
        indexingCache.clear();
        common.logger.info({
          msg: "Reorged block",
          chain: event.chain.name,
          chain_id: event.chain.id,
          number: Number(decodeCheckpoint(event.checkpoint).blockNumber),
          duration: endClock(),
        });
        break;
      }
      case "finalize": {
        const context = {
          logger: common.logger.child({ action: "finalize_block" }),
        };
        const endClock = startClock();
        await finalizeIsolated(
          database.userQB,
          {
            checkpoint: event.checkpoint,
            tables,
            namespaceBuild,
          },
          context,
        );
        common.logger.info({
          msg: "Finalized block",
          chain: event.chain.name,
          chain_id: event.chain.id,
          number: Number(decodeCheckpoint(event.checkpoint).blockNumber),
          duration: endClock(),
        });
        break;
      }
      default:
        never(event);
    }
  }
  common.logger.info({
    msg: "Completed indexing",
    chain: chain.name,
    chain_id: chain.id,
    duration: backfillEndClock(),
  });
}
</file>

<file path="packages/core/src/runtime/multichain.ts">
import {
  commitBlock,
  createIndexes,
  createLiveQueryTriggers,
  createTriggers,
  createViews,
  dropLiveQueryTriggers,
  dropTriggers,
  finalizeMultichain,
  revertMultichain,
} from "@/database/actions.js";
import {
  type Database,
  getPonderCheckpointTable,
  getPonderMetaTable,
} from "@/database/index.js";
import { getLiveQueryTempTableName } from "@/drizzle/onchain.js";
import { createIndexingCache } from "@/indexing-store/cache.js";
import { createIndexingStore } from "@/indexing-store/index.js";
import { createCachedViemClient } from "@/indexing/client.js";
import {
  createColumnAccessPattern,
  createIndexing,
  getEventCount,
} from "@/indexing/index.js";
import type { Common } from "@/internal/common.js";
import {
  InvalidEventAccessError,
  NonRetryableUserError,
  type RetryableError,
} from "@/internal/errors.js";
import { getAppProgress } from "@/internal/metrics.js";
import type {
  Chain,
  CrashRecoveryCheckpoint,
  IndexingBuild,
  IndexingErrorHandler,
  NamespaceBuild,
  PreBuild,
  SchemaBuild,
  Seconds,
} from "@/internal/types.js";
import { splitEvents } from "@/runtime/events.js";
import type { RealtimeSyncEvent } from "@/sync-realtime/index.js";
import { createSyncStore } from "@/sync-store/index.js";
import {
  ZERO_CHECKPOINT_STRING,
  decodeCheckpoint,
  min,
} from "@/utils/checkpoint.js";
import { formatEta, formatPercentage } from "@/utils/format.js";
import {
  bufferAsyncGenerator,
  recordAsyncGenerator,
} from "@/utils/generators.js";
import { never } from "@/utils/never.js";
import { startClock } from "@/utils/timer.js";
import { eq, getTableName, isTable, isView, sql } from "drizzle-orm";
import {
  getHistoricalEventsMultichain,
  refetchHistoricalEvents,
} from "./historical.js";
import {
  type CachedIntervals,
  type ChildAddresses,
  type SyncProgress,
  getCachedIntervals,
  getChildAddresses,
} from "./index.js";
import { initSyncProgress } from "./init.js";
import { getRealtimeEventsMultichain } from "./realtime.js";
export async function runMultichain({
  common,
  preBuild,
  namespaceBuild,
  schemaBuild,
  indexingBuild,
  crashRecoveryCheckpoint,
  database,
}: {
  common: Common;
  preBuild: PreBuild;
  namespaceBuild: NamespaceBuild;
  schemaBuild: SchemaBuild;
  indexingBuild: IndexingBuild;
  crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  database: Database;
}) {
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild,
  });
  const syncStore = createSyncStore({ common, qb: database.syncQB });
  const PONDER_CHECKPOINT = getPonderCheckpointTable(namespaceBuild.schema);
  const PONDER_META = getPonderMetaTable(namespaceBuild.schema);
  const eventCount = getEventCount(indexingBuild.indexingFunctions);
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild,
    syncStore,
    eventCount,
  });
  const indexingErrorHandler: IndexingErrorHandler = {
    getRetryableError: () => {
      return indexingErrorHandler.error;
    },
    setRetryableError: (error: RetryableError) => {
      indexingErrorHandler.error = error;
    },
    clearRetryableError: () => {
      indexingErrorHandler.error = undefined;
    },
    error: undefined as RetryableError | undefined,
  };
  const indexingCache = createIndexingCache({
    common,
    schemaBuild,
    crashRecoveryCheckpoint,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild,
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild,
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const perChainSync = new Map<
    Chain,
    {
      syncProgress: SyncProgress;
      childAddresses: ChildAddresses;
      cachedIntervals: CachedIntervals;
      unfinalizedBlocks: Omit<
        Extract<RealtimeSyncEvent, { type: "block" }>,
        "type"
      >[];
    }
  >();
  const seconds: Seconds = {};
  await Promise.all(
    indexingBuild.chains.map(async (chain) => {
      const eventCallbacks =
        indexingBuild.eventCallbacks[indexingBuild.chains.indexOf(chain)]!;
      const cachedIntervals = await getCachedIntervals({
        chain,
        filters: eventCallbacks.map(({ filter }) => filter),
        syncStore,
      });
      const syncProgress = await initSyncProgress({
        common,
        filters: eventCallbacks.map(({ filter }) => filter),
        chain,
        rpc: indexingBuild.rpcs[indexingBuild.chains.indexOf(chain)]!,
        finalizedBlock:
          indexingBuild.finalizedBlocks[indexingBuild.chains.indexOf(chain)]!,
        cachedIntervals,
      });
      const childAddresses = await getChildAddresses({
        filters: eventCallbacks.map(({ filter }) => filter),
        syncStore,
      });
      const unfinalizedBlocks: Omit<
        Extract<RealtimeSyncEvent, { type: "block" }>,
        "type"
      >[] = [];
      perChainSync.set(chain, {
        syncProgress,
        childAddresses,
        cachedIntervals,
        unfinalizedBlocks,
      });
      const _crashRecoveryCheckpoint = crashRecoveryCheckpoint?.find(
        ({ chainId }) => chainId === chain.id,
      )?.checkpoint;
      const start = Number(
        decodeCheckpoint(syncProgress.getCheckpoint({ tag: "start" }))
          .blockTimestamp,
      );
      const end = Number(
        decodeCheckpoint(
          min(
            syncProgress.getCheckpoint({ tag: "end" }),
            syncProgress.getCheckpoint({ tag: "finalized" }),
          ),
        ).blockTimestamp,
      );
      const cached = Math.min(
        Number(
          decodeCheckpoint(_crashRecoveryCheckpoint ?? ZERO_CHECKPOINT_STRING)
            .blockTimestamp,
        ),
        end,
      );
      seconds[chain.name] = { start, end, cached };
      const label = { chain: chain.name };
      common.metrics.ponder_historical_total_indexing_seconds.set(
        label,
        Math.max(seconds[chain.name]!.end - seconds[chain.name]!.start, 0),
      );
      common.metrics.ponder_historical_cached_indexing_seconds.set(
        label,
        Math.max(seconds[chain.name]!.cached - seconds[chain.name]!.start, 0),
      );
      common.metrics.ponder_historical_completed_indexing_seconds.set(label, 0);
      common.metrics.ponder_indexing_timestamp.set(
        label,
        Math.max(seconds[chain.name]!.cached, seconds[chain.name]!.start),
      );
    }),
  );
  const startTimestamp = Math.round(Date.now() / 1000);
  for (const chain of indexingBuild.chains) {
    common.metrics.ponder_historical_start_timestamp_seconds.set(
      { chain: chain.name },
      startTimestamp,
    );
  }
  // Reset the start timestamp so the eta estimate doesn't include
  // the startup time.
  common.metrics.start_timestamp = Date.now();
  // If the initial checkpoint is zero, we need to run setup events.
  if (crashRecoveryCheckpoint === undefined) {
    await database.userQB.transaction(async (tx) => {
      indexingStore.qb = tx;
      indexingStore.isProcessingEvents = true;
      indexingCache.qb = tx;
      await indexing.processSetupEvents();
      indexingStore.isProcessingEvents = false;
      await indexingCache.flush();
      await tx.wrap({ label: "update_checkpoints" }, (tx) =>
        tx
          .insert(PONDER_CHECKPOINT)
          .values(
            indexingBuild.chains.map((chain) => {
              const initialCheckpoint = min(
                perChainSync
                  .get(chain)!
                  .syncProgress.getCheckpoint({ tag: "start" }),
                perChainSync
                  .get(chain)!
                  .syncProgress.getCheckpoint({ tag: "finalized" }),
              );
              return {
                chainName: chain.name,
                chainId: chain.id,
                latestCheckpoint: initialCheckpoint,
                safeCheckpoint: initialCheckpoint,
                finalizedCheckpoint: initialCheckpoint,
              };
            }),
          )
          .onConflictDoUpdate({
            target: PONDER_CHECKPOINT.chainName,
            set: {
              finalizedCheckpoint: sql`excluded.finalized_checkpoint`,
              safeCheckpoint: sql`excluded.safe_checkpoint`,
              latestCheckpoint: sql`excluded.latest_checkpoint`,
            },
          }),
      );
    });
  }
  const etaInterval = setInterval(async () => {
    // underlying metrics collection is actually synchronous
    // https://github.com/siimon/prom-client/blob/master/lib/histogram.js#L102-L125
    const { eta, progress } = await getAppProgress(common.metrics);
    if (eta === undefined && progress === undefined) {
      return;
    }
    common.logger.info({
      msg: "Updated backfill indexing progress",
      progress: progress === undefined ? undefined : formatPercentage(progress),
      estimate: eta === undefined ? undefined : formatEta(eta * 1_000),
    });
  }, 5_000);
  common.shutdown.add(() => {
    clearInterval(etaInterval);
  });
  const backfillEndClock = startClock();
  // Run historical indexing until complete.
  for await (let {
    events,
    chainId,
    checkpoint,
    blockRange,
  } of recordAsyncGenerator(
    getHistoricalEventsMultichain({
      common,
      indexingBuild,
      crashRecoveryCheckpoint,
      perChainSync,
      database,
    }),
    (params) => {
      common.metrics.ponder_historical_concurrency_group_duration.inc(
        { group: "extract" },
        params.await,
      );
      common.metrics.ponder_historical_concurrency_group_duration.inc(
        { group: "transform" },
        params.yield,
      );
    },
  )) {
    const context = {
      logger: common.logger.child({ action: "index_block_range" }),
    };
    const indexStartClock = startClock();
    const chain = indexingBuild.chains.find((chain) => chain.id === chainId)!;
    indexingCache.qb = database.userQB;
    await Promise.all([
      indexingCache.prefetch({ events }),
      cachedViemClient.prefetch({ events }),
    ]);
    common.metrics.ponder_historical_transform_duration.inc(
      { step: "prefetch" },
      indexStartClock(),
    );
    let endClock = startClock();
    await database.userQB.transaction(
      async (tx) => {
        const initialCompletedEvents = structuredClone(
          await common.metrics.ponder_indexing_completed_events.get(),
        );
        try {
          indexingStore.qb = tx;
          indexingStore.isProcessingEvents = true;
          indexingCache.qb = tx;
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "begin" },
            endClock(),
          );
          endClock = startClock();
          await indexing.processHistoricalEvents({
            events,
            updateIndexingSeconds(event, chain) {
              const checkpoint = decodeCheckpoint(event!.checkpoint);
              common.metrics.ponder_historical_completed_indexing_seconds.set(
                { chain: chain.name },
                Math.max(
                  Number(checkpoint.blockTimestamp) -
                    Math.max(
                      seconds[chain.name]!.cached,
                      seconds[chain.name]!.start,
                    ),
                  0,
                ),
              );
              common.metrics.ponder_indexing_timestamp.set(
                { chain: chain.name },
                Number(checkpoint.blockTimestamp),
              );
            },
          });
          indexingStore.isProcessingEvents = false;
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "index" },
            endClock(),
          );
          endClock = startClock();
          // Note: at this point, the next events can be preloaded, as long as the are not indexed until
          // the "flush" + "finalize" is complete.
          await indexingCache.flush();
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "load" },
            endClock(),
          );
          endClock = startClock();
          await tx.wrap(
            { label: "update_checkpoints" },
            (tx) =>
              tx
                .insert(PONDER_CHECKPOINT)
                .values({
                  chainName: chain.name,
                  chainId,
                  latestCheckpoint: checkpoint,
                  finalizedCheckpoint: checkpoint,
                  safeCheckpoint: checkpoint,
                })
                .onConflictDoUpdate({
                  target: PONDER_CHECKPOINT.chainName,
                  set: {
                    safeCheckpoint: sql`excluded.safe_checkpoint`,
                    finalizedCheckpoint: sql`excluded.finalized_checkpoint`,
                    latestCheckpoint: sql`excluded.latest_checkpoint`,
                  },
                }),
            context,
          );
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "finalize" },
            endClock(),
          );
          endClock = startClock();
        } catch (error) {
          for (const value of initialCompletedEvents.values) {
            common.metrics.ponder_indexing_completed_events.set(
              value.labels,
              value.value,
            );
          }
          indexingCache.invalidate();
          indexingCache.clear();
          if (error instanceof InvalidEventAccessError) {
            common.logger.debug({
              msg: "Failed to index block range",
              chain: chain.name,
              chain_id: chain.id,
              block_range: JSON.stringify(blockRange),
              duration: indexStartClock(),
              error,
            });
            events = await refetchHistoricalEvents({
              common,
              indexingBuild,
              perChainSync,
              syncStore,
              events,
            });
          } else if (error instanceof NonRetryableUserError === false) {
            common.logger.warn({
              msg: "Failed to index block range",
              chain: chain.name,
              chain_id: chain.id,
              block_range: JSON.stringify(blockRange),
              duration: indexStartClock(),
              error: error as Error,
            });
          }
          throw error;
        }
      },
      undefined,
      context,
    );
    cachedViemClient.clear();
    common.metrics.ponder_historical_transform_duration.inc(
      { step: "commit" },
      endClock(),
    );
    await new Promise(setImmediate);
    common.logger.info({
      msg: "Indexed block range",
      chain: chain.name,
      chain_id: chain.id,
      event_count: events.length,
      block_range: JSON.stringify(blockRange),
      duration: indexStartClock(),
    });
  }
  indexingCache.clear();
  // Note: Invalidating the cache means that only predicted rows will be in memory after this point.
  indexingCache.invalidate();
  // Manually update metrics to fix a UI bug that occurs when the end
  // checkpoint is between the last processed event and the finalized
  // checkpoint.
  for (const chain of indexingBuild.chains) {
    const label = { chain: chain.name };
    common.metrics.ponder_historical_completed_indexing_seconds.set(
      label,
      Math.max(
        seconds[chain.name]!.end -
          Math.max(seconds[chain.name]!.cached, seconds[chain.name]!.start),
        0,
      ),
    );
    common.metrics.ponder_indexing_timestamp.set(
      label,
      seconds[chain.name]!.end,
    );
  }
  const endTimestamp = Math.round(Date.now() / 1000);
  for (const chain of indexingBuild.chains) {
    common.metrics.ponder_historical_end_timestamp_seconds.set(
      { chain: chain.name },
      endTimestamp,
    );
  }
  common.logger.info({
    msg: "Completed backfill indexing across all chains",
    duration: backfillEndClock(),
  });
  clearInterval(etaInterval);
  const tables = Object.values(schemaBuild.schema).filter(isTable);
  const views = Object.values(schemaBuild.schema).filter(isView);
  let endClock = startClock();
  await createIndexes(database.adminQB, { statements: schemaBuild.statements });
  if (schemaBuild.statements.indexes.sql.length > 0) {
    common.logger.info({
      msg: "Created database indexes",
      count: schemaBuild.statements.indexes.sql.length,
      duration: endClock(),
    });
  }
  endClock = startClock();
  await createTriggers(database.adminQB, { tables });
  await createLiveQueryTriggers(database.adminQB, { namespaceBuild, tables });
  common.logger.debug({
    msg: "Created database triggers",
    count: tables.length,
    duration: endClock(),
  });
  if (namespaceBuild.viewsSchema !== undefined) {
    const endClock = startClock();
    await createViews(database.adminQB, { tables, views, namespaceBuild });
    common.logger.info({
      msg: "Created database views",
      schema: namespaceBuild.viewsSchema,
      count: tables.length,
      duration: endClock(),
    });
  }
  endClock = startClock();
  await database.adminQB.wrap({ label: "update_ready" }, (db) =>
    db
      .update(PONDER_META)
      .set({ value: sql`jsonb_set(value, '{is_ready}', to_jsonb(1))` }),
  );
  common.logger.info({
    msg: "Started returning 200 responses",
    endpoint: "/ready",
  });
  const bufferCallback = (bufferSize: number) => {
    // Note: Only log when the buffer size is greater than 1 because
    // a buffer size of 1 is not backpressure.
    if (bufferSize === 1) return;
    common.logger.trace({
      msg: "Detected live indexing backpressure",
      buffer_size: bufferSize,
      indexing_step: "index block",
    });
  };
  for await (const event of bufferAsyncGenerator(
    getRealtimeEventsMultichain({
      common,
      indexingBuild,
      perChainSync,
      database,
    }),
    100,
    bufferCallback,
  )) {
    switch (event.type) {
      case "block": {
        const context = {
          logger: common.logger.child({ action: "index_block" }),
        };
        const endClock = startClock();
        indexingCache.qb = database.userQB;
        await Promise.all([
          indexingCache.prefetch({ events: event.events }),
          cachedViemClient.prefetch({ events: event.events }),
        ]);
        await database.userQB.transaction(
          async (tx) => {
            if (database.userQB.$dialect === "postgres") {
              await tx.wrap(
                (tx) =>
                  tx.execute(
                    `CREATE TEMP TABLE ${getLiveQueryTempTableName()} (table_name TEXT PRIMARY KEY) ON COMMIT DROP`,
                  ),
                context,
              );
            } else {
              await tx.wrap(
                (tx) =>
                  tx.execute(
                    `CREATE TEMP TABLE IF NOT EXISTS ${getLiveQueryTempTableName()} (table_name TEXT PRIMARY KEY)`,
                  ),
                context,
              );
            }
            // Events must be run block-by-block, so that `database.commitBlock` can accurately
            // update the temporary `checkpoint` value set in the trigger.
            for (const { checkpoint, events } of splitEvents(event.events)) {
              const chain = indexingBuild.chains.find(
                (chain) =>
                  chain.id === Number(decodeCheckpoint(checkpoint).chainId),
              )!;
              try {
                indexingStore.qb = tx;
                indexingStore.isProcessingEvents = true;
                indexingCache.qb = tx;
                common.logger.trace({
                  msg: "Processing block events",
                  chain: chain.name,
                  chain_id: chain.id,
                  number: Number(decodeCheckpoint(checkpoint).blockNumber),
                  event_count: events.length,
                });
                await indexing.processRealtimeEvents({ events });
                common.logger.trace({
                  msg: "Processed block events",
                  chain: chain.name,
                  chain_id: chain.id,
                  number: Number(decodeCheckpoint(checkpoint).blockNumber),
                  event_count: events.length,
                });
                indexingStore.isProcessingEvents = false;
                await indexingCache.flush();
                await Promise.all(
                  tables.map((table) =>
                    commitBlock(tx, { table, checkpoint, preBuild }, context),
                  ),
                );
                cachedViemClient.clear();
                common.logger.trace({
                  msg: "Committed reorg data for block",
                  chain: chain.name,
                  chain_id: chain.id,
                  number: Number(decodeCheckpoint(checkpoint).blockNumber),
                  event_count: events.length,
                  checkpoint,
                });
              } catch (error) {
                indexingCache.clear();
                if (error instanceof NonRetryableUserError === false) {
                  common.logger.warn({
                    msg: "Failed to index block",
                    chain: chain.name,
                    chain_id: chain.id,
                    number: Number(decodeCheckpoint(checkpoint).blockNumber),
                    error: error,
                  });
                }
                throw error;
              }
              common.metrics.ponder_indexing_timestamp.set(
                { chain: chain.name },
                Number(decodeCheckpoint(checkpoint).blockTimestamp),
              );
            }
            await tx.wrap(
              { label: "update_checkpoints" },
              (db) =>
                db
                  .update(PONDER_CHECKPOINT)
                  .set({ latestCheckpoint: event.checkpoint })
                  .where(eq(PONDER_CHECKPOINT.chainName, event.chain.name)),
              context,
            );
            if (database.userQB.$dialect === "pglite") {
              await tx.wrap(
                (tx) =>
                  tx.execute(`TRUNCATE TABLE ${getLiveQueryTempTableName()}`),
                context,
              );
            }
          },
          undefined,
          context,
        );
        event.blockCallback?.(true);
        common.logger.info({
          msg: "Indexed block",
          chain: event.chain.name,
          chain_id: event.chain.id,
          number: Number(decodeCheckpoint(event.checkpoint).blockNumber),
          event_count: event.events.length,
          duration: endClock(),
        });
        break;
      }
      case "reorg": {
        const context = {
          logger: common.logger.child({ action: "reorg_block" }),
        };
        const endClock = startClock();
        // Note: `_ponder_checkpoint` is not called here, instead it is called
        // in the `block` case.
        await database.userQB.transaction(
          async (tx) => {
            await dropTriggers(tx, { tables }, context);
            await dropLiveQueryTriggers(
              tx,
              { namespaceBuild, tables },
              context,
            );
            const counts = await revertMultichain(
              tx,
              {
                checkpoint: event.checkpoint,
                tables,
              },
              context,
            );
            for (const [index, table] of tables.entries()) {
              common.logger.debug({
                msg: "Reverted reorged database rows",
                table: getTableName(table),
                row_count: counts[index],
              });
            }
            await createTriggers(tx, { tables }, context);
            await createLiveQueryTriggers(
              tx,
              { namespaceBuild, tables },
              context,
            );
          },
          undefined,
          context,
        );
        indexingCache.clear();
        common.logger.info({
          msg: "Reorged block",
          chain: event.chain.name,
          chain_id: event.chain.id,
          number: Number(decodeCheckpoint(event.checkpoint).blockNumber),
          duration: endClock(),
        });
        break;
      }
      case "finalize": {
        const context = {
          logger: common.logger.child({ action: "finalize_block" }),
        };
        const endClock = startClock();
        await finalizeMultichain(
          database.userQB,
          {
            checkpoint: event.checkpoint,
            tables,
            namespaceBuild,
          },
          context,
        );
        common.logger.info({
          msg: "Finalized block",
          chain: event.chain.name,
          chain_id: event.chain.id,
          number: Number(decodeCheckpoint(event.checkpoint).blockNumber),
          duration: endClock(),
        });
        break;
      }
      default:
        never(event);
    }
  }
  common.logger.info({
    msg: "Completed indexing across all chains",
    duration: backfillEndClock(),
  });
}
</file>

<file path="packages/core/src/runtime/omnichain.ts">
import {
  commitBlock,
  createIndexes,
  createLiveQueryTriggers,
  createTriggers,
  createViews,
  dropLiveQueryTriggers,
  dropTriggers,
  finalizeOmnichain,
  revertOmnichain,
} from "@/database/actions.js";
import {
  type Database,
  getPonderCheckpointTable,
  getPonderMetaTable,
} from "@/database/index.js";
import { getLiveQueryTempTableName } from "@/drizzle/onchain.js";
import { createIndexingCache } from "@/indexing-store/cache.js";
import { createIndexingStore } from "@/indexing-store/index.js";
import { createCachedViemClient } from "@/indexing/client.js";
import {
  createColumnAccessPattern,
  createIndexing,
  getEventCount,
} from "@/indexing/index.js";
import type { Common } from "@/internal/common.js";
import {
  InvalidEventAccessError,
  NonRetryableUserError,
  type RetryableError,
} from "@/internal/errors.js";
import { getAppProgress } from "@/internal/metrics.js";
import type {
  Chain,
  CrashRecoveryCheckpoint,
  Event,
  IndexingBuild,
  IndexingErrorHandler,
  NamespaceBuild,
  PreBuild,
  SchemaBuild,
  Seconds,
} from "@/internal/types.js";
import { splitEvents } from "@/runtime/events.js";
import type { RealtimeSyncEvent } from "@/sync-realtime/index.js";
import { createSyncStore } from "@/sync-store/index.js";
import {
  ZERO_CHECKPOINT_STRING,
  decodeCheckpoint,
  max,
  min,
} from "@/utils/checkpoint.js";
import { formatEta, formatPercentage } from "@/utils/format.js";
import {
  bufferAsyncGenerator,
  recordAsyncGenerator,
} from "@/utils/generators.js";
import { never } from "@/utils/never.js";
import { startClock } from "@/utils/timer.js";
import { zipperMany } from "@/utils/zipper.js";
import { eq, getTableName, isTable, isView, sql } from "drizzle-orm";
import {
  getHistoricalEventsOmnichain,
  refetchHistoricalEvents,
} from "./historical.js";
import {
  type CachedIntervals,
  type ChildAddresses,
  type SyncProgress,
  getCachedIntervals,
  getChildAddresses,
} from "./index.js";
import { initSyncProgress } from "./init.js";
import { getRealtimeEventsOmnichain } from "./realtime.js";
export async function runOmnichain({
  common,
  preBuild,
  namespaceBuild,
  schemaBuild,
  indexingBuild,
  crashRecoveryCheckpoint,
  database,
}: {
  common: Common;
  preBuild: PreBuild;
  namespaceBuild: NamespaceBuild;
  schemaBuild: SchemaBuild;
  indexingBuild: IndexingBuild;
  crashRecoveryCheckpoint: CrashRecoveryCheckpoint;
  database: Database;
}) {
  const columnAccessPattern = createColumnAccessPattern({
    indexingBuild,
  });
  const syncStore = createSyncStore({ common, qb: database.syncQB });
  const PONDER_CHECKPOINT = getPonderCheckpointTable(namespaceBuild.schema);
  const PONDER_META = getPonderMetaTable(namespaceBuild.schema);
  const eventCount = getEventCount(indexingBuild.indexingFunctions);
  const cachedViemClient = createCachedViemClient({
    common,
    indexingBuild,
    syncStore,
    eventCount,
  });
  const indexingErrorHandler: IndexingErrorHandler = {
    getRetryableError: () => {
      return indexingErrorHandler.error;
    },
    setRetryableError: (error: RetryableError) => {
      indexingErrorHandler.error = error;
    },
    clearRetryableError: () => {
      indexingErrorHandler.error = undefined;
    },
    error: undefined as RetryableError | undefined,
  };
  const indexingCache = createIndexingCache({
    common,
    schemaBuild,
    crashRecoveryCheckpoint,
    eventCount,
  });
  const indexingStore = createIndexingStore({
    common,
    schemaBuild,
    indexingCache,
    indexingErrorHandler,
  });
  const indexing = createIndexing({
    common,
    indexingBuild,
    indexingStore,
    indexingCache,
    client: cachedViemClient,
    indexingErrorHandler,
    columnAccessPattern,
    eventCount,
  });
  const perChainSync = new Map<
    Chain,
    {
      syncProgress: SyncProgress;
      childAddresses: ChildAddresses;
      cachedIntervals: CachedIntervals;
      unfinalizedBlocks: Omit<
        Extract<RealtimeSyncEvent, { type: "block" }>,
        "type"
      >[];
    }
  >();
  const seconds: Seconds = {};
  await Promise.all(
    indexingBuild.chains.map(async (chain) => {
      const eventCallbacks =
        indexingBuild.eventCallbacks[indexingBuild.chains.indexOf(chain)]!;
      const cachedIntervals = await getCachedIntervals({
        chain,
        filters: eventCallbacks.map(({ filter }) => filter),
        syncStore,
      });
      const syncProgress = await initSyncProgress({
        common,
        filters: eventCallbacks.map(({ filter }) => filter),
        chain,
        rpc: indexingBuild.rpcs[indexingBuild.chains.indexOf(chain)]!,
        finalizedBlock:
          indexingBuild.finalizedBlocks[indexingBuild.chains.indexOf(chain)]!,
        cachedIntervals,
      });
      const childAddresses = await getChildAddresses({
        filters: eventCallbacks.map(({ filter }) => filter),
        syncStore,
      });
      const unfinalizedBlocks: Omit<
        Extract<RealtimeSyncEvent, { type: "block" }>,
        "type"
      >[] = [];
      perChainSync.set(chain, {
        syncProgress,
        childAddresses,
        cachedIntervals,
        unfinalizedBlocks,
      });
    }),
  );
  const start = Number(
    decodeCheckpoint(getOmnichainCheckpoint({ perChainSync, tag: "start" }))
      .blockTimestamp,
  );
  const end = Number(
    decodeCheckpoint(
      min(
        getOmnichainCheckpoint({ perChainSync, tag: "end" }),
        getOmnichainCheckpoint({ perChainSync, tag: "finalized" }),
      ),
    ).blockTimestamp,
  );
  for (const chain of indexingBuild.chains) {
    const _crashRecoveryCheckpoint = crashRecoveryCheckpoint?.find(
      ({ chainId }) => chainId === chain.id,
    )?.checkpoint;
    const cached = Math.min(
      Number(
        decodeCheckpoint(_crashRecoveryCheckpoint ?? ZERO_CHECKPOINT_STRING)
          .blockTimestamp,
      ),
      end,
    );
    seconds[chain.name] = { start, end, cached };
    const label = { chain: chain.name };
    common.metrics.ponder_historical_total_indexing_seconds.set(
      label,
      Math.max(seconds[chain.name]!.end - seconds[chain.name]!.start, 0),
    );
    common.metrics.ponder_historical_cached_indexing_seconds.set(
      label,
      Math.max(seconds[chain.name]!.cached - seconds[chain.name]!.start, 0),
    );
    common.metrics.ponder_historical_completed_indexing_seconds.set(label, 0);
    common.metrics.ponder_indexing_timestamp.set(
      label,
      Math.max(seconds[chain.name]!.cached, seconds[chain.name]!.start),
    );
  }
  const startTimestamp = Math.round(Date.now() / 1000);
  for (const chain of indexingBuild.chains) {
    common.metrics.ponder_historical_start_timestamp_seconds.set(
      { chain: chain.name },
      startTimestamp,
    );
  }
  // Reset the start timestamp so the eta estimate doesn't include
  // the startup time.
  common.metrics.start_timestamp = Date.now();
  // If the initial checkpoint is zero, we need to run setup events.
  if (crashRecoveryCheckpoint === undefined) {
    await database.userQB.transaction(async (tx) => {
      indexingStore.qb = tx;
      indexingStore.isProcessingEvents = true;
      indexingCache.qb = tx;
      await indexing.processSetupEvents();
      indexingStore.isProcessingEvents = false;
      await indexingCache.flush();
      await tx.wrap({ label: "update_checkpoints" }, (tx) =>
        tx
          .insert(PONDER_CHECKPOINT)
          .values(
            indexingBuild.chains.map((chain) => {
              const initialCheckpoint = min(
                perChainSync
                  .get(chain)!
                  .syncProgress.getCheckpoint({ tag: "start" }),
                perChainSync
                  .get(chain)!
                  .syncProgress.getCheckpoint({ tag: "finalized" }),
              );
              return {
                chainName: chain.name,
                chainId: chain.id,
                latestCheckpoint: initialCheckpoint,
                safeCheckpoint: initialCheckpoint,
                finalizedCheckpoint: initialCheckpoint,
              };
            }),
          )
          .onConflictDoUpdate({
            target: PONDER_CHECKPOINT.chainName,
            set: {
              finalizedCheckpoint: sql`excluded.finalized_checkpoint`,
              safeCheckpoint: sql`excluded.safe_checkpoint`,
              latestCheckpoint: sql`excluded.latest_checkpoint`,
            },
          }),
      );
    });
  }
  const etaInterval = setInterval(async () => {
    // underlying metrics collection is actually synchronous
    // https://github.com/siimon/prom-client/blob/master/lib/histogram.js#L102-L125
    const { eta, progress } = await getAppProgress(common.metrics);
    if (eta === undefined && progress === undefined) {
      return;
    }
    common.logger.info({
      msg: "Updated backfill indexing progress",
      progress: progress === undefined ? undefined : formatPercentage(progress),
      estimate: eta === undefined ? undefined : formatEta(eta * 1_000),
    });
  }, 5_000);
  common.shutdown.add(() => {
    clearInterval(etaInterval);
  });
  const backfillEndClock = startClock();
  let pendingEvents: Event[] = [];
  // Run historical indexing until complete.
  for await (const result of recordAsyncGenerator(
    getHistoricalEventsOmnichain({
      common,
      indexingBuild,
      crashRecoveryCheckpoint,
      perChainSync,
      database,
    }),
    (params) => {
      common.metrics.ponder_historical_concurrency_group_duration.inc(
        { group: "extract" },
        params.await,
      );
      common.metrics.ponder_historical_concurrency_group_duration.inc(
        { group: "transform" },
        params.yield,
      );
    },
  )) {
    if (result.type === "pending") {
      pendingEvents = result.result;
      continue;
    }
    const context = {
      logger: common.logger.child({ action: "index_block_range" }),
    };
    const indexStartClock = startClock();
    let events = zipperMany(
      result.result.map(({ events }) => events),
      (a, b) => (a.checkpoint < b.checkpoint ? -1 : 1),
    );
    indexingCache.qb = database.userQB;
    await Promise.all([
      indexingCache.prefetch({ events }),
      cachedViemClient.prefetch({ events }),
    ]);
    common.metrics.ponder_historical_transform_duration.inc(
      { step: "prefetch" },
      indexStartClock(),
    );
    let endClock = startClock();
    await database.userQB.transaction(
      async (tx) => {
        const initialCompletedEvents = structuredClone(
          await common.metrics.ponder_indexing_completed_events.get(),
        );
        try {
          indexingStore.qb = tx;
          indexingStore.isProcessingEvents = true;
          indexingCache.qb = tx;
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "begin" },
            endClock(),
          );
          endClock = startClock();
          await indexing.processHistoricalEvents({
            events,
            updateIndexingSeconds(event) {
              const checkpoint = decodeCheckpoint(event.checkpoint);
              for (const chain of indexingBuild.chains) {
                common.metrics.ponder_historical_completed_indexing_seconds.set(
                  { chain: chain.name },
                  Math.min(
                    Math.max(
                      Number(checkpoint.blockTimestamp) -
                        Math.max(
                          seconds[chain.name]!.cached,
                          seconds[chain.name]!.start,
                        ),
                      0,
                    ),
                    Math.max(
                      seconds[chain.name]!.end - seconds[chain.name]!.start,
                      0,
                    ),
                  ),
                );
                common.metrics.ponder_indexing_timestamp.set(
                  { chain: chain.name },
                  Math.max(
                    Number(checkpoint.blockTimestamp),
                    seconds[chain.name]!.end,
                  ),
                );
              }
            },
          });
          indexingStore.isProcessingEvents = false;
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "index" },
            endClock(),
          );
          endClock = startClock();
          // Note: at this point, the next events can be preloaded, as long as the are not indexed until
          // the "flush" + "finalize" is complete.
          await indexingCache.flush();
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "load" },
            endClock(),
          );
          endClock = startClock();
          // Note: It is an invariant that result.result.length > 0
          await tx.wrap({ label: "update_checkpoints" }, (tx) =>
            tx
              .insert(PONDER_CHECKPOINT)
              .values(
                result.result.map(({ chainId, checkpoint }) => ({
                  chainName: indexingBuild.chains.find(
                    (chain) => chain.id === chainId,
                  )!.name,
                  chainId,
                  latestCheckpoint: checkpoint,
                  safeCheckpoint: checkpoint,
                  finalizedCheckpoint: checkpoint,
                })),
              )
              .onConflictDoUpdate({
                target: PONDER_CHECKPOINT.chainName,
                set: {
                  safeCheckpoint: sql`excluded.safe_checkpoint`,
                  latestCheckpoint: sql`excluded.latest_checkpoint`,
                  finalizedCheckpoint: sql`excluded.finalized_checkpoint`,
                },
              }),
          );
          common.metrics.ponder_historical_transform_duration.inc(
            { step: "finalize" },
            endClock(),
          );
          endClock = startClock();
        } catch (error) {
          for (const value of initialCompletedEvents.values) {
            common.metrics.ponder_indexing_completed_events.set(
              value.labels,
              value.value,
            );
          }
          indexingCache.invalidate();
          indexingCache.clear();
          if (error instanceof InvalidEventAccessError) {
            common.logger.debug({
              msg: "Failed to index block range",
              duration: indexStartClock(),
              error,
            });
            events = await refetchHistoricalEvents({
              common,
              indexingBuild,
              perChainSync,
              syncStore,
              events,
            });
          } else if (error instanceof NonRetryableUserError === false) {
            common.logger.warn({
              msg: "Failed to index block range",
              duration: indexStartClock(),
              error: error as Error,
            });
          }
          throw error;
        }
      },
      undefined,
      context,
    );
    cachedViemClient.clear();
    common.metrics.ponder_historical_transform_duration.inc(
      { step: "commit" },
      endClock(),
    );
    await new Promise(setImmediate);
    for (const { chainId, events, blockRange } of result.result) {
      common.logger.info({
        msg: "Indexed block range",
        chain: indexingBuild.chains.find((chain) => chain.id === chainId)!.name,
        chain_id: chainId,
        event_count: events.length,
        block_range: JSON.stringify(blockRange),
        duration: indexStartClock(),
      });
    }
  }
  indexingCache.clear();
  // Note: Invalidating the cache means that only predicted rows will be in memory after this point.
  indexingCache.invalidate();
  // Manually update metrics to fix a UI bug that occurs when the end
  // checkpoint is between the last processed event and the finalized
  // checkpoint.
  for (const chain of indexingBuild.chains) {
    const label = { chain: chain.name };
    common.metrics.ponder_historical_completed_indexing_seconds.set(
      label,
      Math.max(
        seconds[chain.name]!.end -
          Math.max(seconds[chain.name]!.cached, seconds[chain.name]!.start),
        0,
      ),
    );
    common.metrics.ponder_indexing_timestamp.set(
      label,
      seconds[chain.name]!.end,
    );
  }
  const endTimestamp = Math.round(Date.now() / 1000);
  for (const chain of indexingBuild.chains) {
    common.metrics.ponder_historical_end_timestamp_seconds.set(
      { chain: chain.name },
      endTimestamp,
    );
  }
  common.logger.info({
    msg: "Completed backfill indexing across all chains",
    duration: backfillEndClock(),
  });
  clearInterval(etaInterval);
  const tables = Object.values(schemaBuild.schema).filter(isTable);
  const views = Object.values(schemaBuild.schema).filter(isView);
  let endClock = startClock();
  await createIndexes(database.adminQB, { statements: schemaBuild.statements });
  if (schemaBuild.statements.indexes.sql.length > 0) {
    common.logger.info({
      msg: "Created database indexes",
      count: schemaBuild.statements.indexes.sql.length,
      duration: endClock(),
    });
  }
  endClock = startClock();
  await createTriggers(database.adminQB, { tables });
  await createLiveQueryTriggers(database.adminQB, { namespaceBuild, tables });
  common.logger.debug({
    msg: "Created database triggers",
    count: tables.length,
    duration: endClock(),
  });
  if (namespaceBuild.viewsSchema !== undefined) {
    const endClock = startClock();
    await createViews(database.adminQB, { tables, views, namespaceBuild });
    common.logger.info({
      msg: "Created database views",
      schema: namespaceBuild.viewsSchema,
      count: tables.length,
      duration: endClock(),
    });
  }
  endClock = startClock();
  await database.adminQB.wrap({ label: "update_ready" }, (db) =>
    db
      .update(PONDER_META)
      .set({ value: sql`jsonb_set(value, '{is_ready}', to_jsonb(1))` }),
  );
  common.logger.info({
    msg: "Started returning 200 responses",
    endpoint: "/ready",
  });
  const bufferCallback = (bufferSize: number) => {
    // Note: Only log when the buffer size is greater than 1 because
    // a buffer size of 1 is not backpressure.
    if (bufferSize === 1) return;
    common.logger.trace({
      msg: "Detected live indexing backpressure",
      buffer_size: bufferSize,
      indexing_step: "index block",
    });
  };
  for await (const event of bufferAsyncGenerator(
    getRealtimeEventsOmnichain({
      common,
      indexingBuild,
      perChainSync,
      database,
      pendingEvents,
    }),
    100,
    bufferCallback,
  )) {
    switch (event.type) {
      case "block": {
        const context = {
          logger: common.logger.child({ action: "index_block" }),
        };
        const endClock = startClock();
        indexingCache.qb = database.userQB;
        await Promise.all([
          indexingCache.prefetch({ events: event.events }),
          cachedViemClient.prefetch({ events: event.events }),
        ]);
        await database.userQB.transaction(
          async (tx) => {
            if (database.userQB.$dialect === "postgres") {
              await tx.wrap(
                (tx) =>
                  tx.execute(
                    `CREATE TEMP TABLE ${getLiveQueryTempTableName()} (table_name TEXT PRIMARY KEY) ON COMMIT DROP`,
                  ),
                context,
              );
            } else {
              await tx.wrap(
                (tx) =>
                  tx.execute(
                    `CREATE TEMP TABLE IF NOT EXISTS ${getLiveQueryTempTableName()} (table_name TEXT PRIMARY KEY)`,
                  ),
                context,
              );
            }
            // Events must be run block-by-block, so that `database.commitBlock` can accurately
            // update the temporary `checkpoint` value set in the trigger.
            for (const { checkpoint, events } of splitEvents(event.events)) {
              const chain = indexingBuild.chains.find(
                (chain) =>
                  chain.id === Number(decodeCheckpoint(checkpoint).chainId),
              )!;
              try {
                indexingStore.qb = tx;
                indexingStore.isProcessingEvents = true;
                indexingCache.qb = tx;
                common.logger.trace({
                  msg: "Processing block events",
                  chain: chain.name,
                  chain_id: chain.id,
                  number: Number(decodeCheckpoint(checkpoint).blockNumber),
                  event_count: events.length,
                });
                await indexing.processRealtimeEvents({ events });
                common.logger.trace({
                  msg: "Processed block events",
                  chain: chain.name,
                  chain_id: chain.id,
                  number: Number(decodeCheckpoint(checkpoint).blockNumber),
                  event_count: events.length,
                });
                indexingStore.isProcessingEvents = false;
                await indexingCache.flush();
                await Promise.all(
                  tables.map(
                    (table) =>
                      commitBlock(tx, { table, checkpoint, preBuild }, context),
                    context,
                  ),
                );
                cachedViemClient.clear();
                common.logger.trace({
                  msg: "Committed reorg data for block",
                  chain: chain.name,
                  chain_id: chain.id,
                  number: Number(decodeCheckpoint(checkpoint).blockNumber),
                  event_count: events.length,
                  checkpoint,
                });
              } catch (error) {
                indexingCache.clear();
                if (error instanceof NonRetryableUserError === false) {
                  common.logger.warn({
                    msg: "Failed to index block",
                    chain: chain.name,
                    chain_id: chain.id,
                    number: Number(decodeCheckpoint(checkpoint).blockNumber),
                    error: error,
                  });
                }
                throw error;
              }
              for (const chain of indexingBuild.chains) {
                common.metrics.ponder_indexing_timestamp.set(
                  { chain: chain.name },
                  Number(decodeCheckpoint(checkpoint).blockTimestamp),
                );
              }
            }
            await tx.wrap(
              { label: "update_checkpoints" },
              (db) =>
                db
                  .update(PONDER_CHECKPOINT)
                  .set({ latestCheckpoint: event.checkpoint })
                  .where(eq(PONDER_CHECKPOINT.chainName, event.chain.name)),
              context,
            );
            if (database.userQB.$dialect === "pglite") {
              await tx.wrap(
                (tx) =>
                  tx.execute(`TRUNCATE TABLE ${getLiveQueryTempTableName()}`),
                context,
              );
            }
          },
          undefined,
          context,
        );
        event.blockCallback?.(true);
        common.logger.info({
          msg: "Indexed block",
          chain: event.chain.name,
          chain_id: event.chain.id,
          number: Number(decodeCheckpoint(event.checkpoint).blockNumber),
          event_count: event.events.length,
          duration: endClock(),
        });
        break;
      }
      case "reorg": {
        const context = {
          logger: common.logger.child({ action: "reorg_block" }),
        };
        const endClock = startClock();
        // Note: `_ponder_checkpoint` is not called here, instead it is called
        // in the `block` case.
        await database.userQB.transaction(async (tx) => {
          await dropTriggers(tx, { tables }, context);
          await dropLiveQueryTriggers(tx, { namespaceBuild, tables }, context);
          const counts = await revertOmnichain(
            tx,
            {
              tables,
              checkpoint: event.checkpoint,
            },
            context,
          );
          for (const [index, table] of tables.entries()) {
            common.logger.debug({
              msg: "Reverted reorged database rows",
              table: getTableName(table),
              row_count: counts[index],
            });
          }
          await createTriggers(tx, { tables }, context);
          await createLiveQueryTriggers(
            tx,
            { namespaceBuild, tables },
            context,
          );
        });
        indexingCache.clear();
        common.logger.info({
          msg: "Reorged block",
          chain: event.chain.name,
          chain_id: event.chain.id,
          number: Number(decodeCheckpoint(event.checkpoint).blockNumber),
          duration: endClock(),
        });
        break;
      }
      case "finalize": {
        const context = {
          logger: common.logger.child({ action: "finalize_block" }),
        };
        const endClock = startClock();
        await finalizeOmnichain(
          database.userQB,
          {
            checkpoint: event.checkpoint,
            tables,
            namespaceBuild,
          },
          context,
        );
        common.logger.info({
          msg: "Finalized block",
          chain: event.chain.name,
          chain_id: event.chain.id,
          number: Number(decodeCheckpoint(event.checkpoint).blockNumber),
          duration: endClock(),
        });
        break;
      }
      default:
        never(event);
    }
  }
  common.logger.info({
    msg: "Completed indexing across all chains",
    duration: backfillEndClock(),
  });
}
/**
 * Compute the checkpoint across all chains.
 */
export const getOmnichainCheckpoint = <
  tag extends "start" | "end" | "current" | "finalized",
>({
  perChainSync,
  tag,
}: {
  perChainSync: Map<Chain, { syncProgress: SyncProgress }>;
  tag: tag;
}): tag extends "end" ? string | undefined : string => {
  const checkpoints = Array.from(perChainSync.values()).map(
    ({ syncProgress }) => syncProgress.getCheckpoint({ tag }),
  );
  if (tag === "end") {
    if (checkpoints.some((c) => c === undefined)) {
      return undefined as tag extends "end" ? string | undefined : string;
    }
    // Note: `max` is used here because `end` is an upper bound.
    return max(...checkpoints) as tag extends "end"
      ? string | undefined
      : string;
  }
  // Note: extra logic is needed for `current` because completed chains
  // shouldn't be included in the minimum checkpoint. However, when all
  // chains are completed, the maximum checkpoint should be computed across
  // all chains.
  if (tag === "current") {
    const isComplete = Array.from(perChainSync.values()).map(
      ({ syncProgress }) => syncProgress.isEnd(),
    );
    if (isComplete.every((c) => c)) {
      return max(...checkpoints) as tag extends "end"
        ? string | undefined
        : string;
    }
    return min(
      ...checkpoints.filter((_, i) => isComplete[i] === false),
    ) as tag extends "end" ? string | undefined : string;
  }
  return min(...checkpoints) as tag extends "end" ? string | undefined : string;
};
</file>

<file path="packages/core/src/runtime/realtime.ts">
import type { Database } from "@/database/index.js";
import type { Common } from "@/internal/common.js";
import type {
  Chain,
  Event,
  EventCallback,
  Factory,
  Filter,
  IndexingBuild,
  SyncBlock,
  SyncBlockHeader,
} from "@/internal/types.js";
import type { Rpc } from "@/rpc/index.js";
import {
  buildEvents,
  decodeEvents,
  syncBlockToInternal,
  syncLogToInternal,
  syncTraceToInternal,
  syncTransactionReceiptToInternal,
  syncTransactionToInternal,
} from "@/runtime/events.js";
import {
  type RealtimeSyncEvent,
  createRealtimeSync,
} from "@/sync-realtime/index.js";
import { createSyncStore } from "@/sync-store/index.js";
import {
  ZERO_CHECKPOINT_STRING,
  blockToCheckpoint,
  encodeCheckpoint,
  min,
} from "@/utils/checkpoint.js";
import {
  bufferAsyncGenerator,
  createCallbackGenerator,
  mergeAsyncGenerators,
} from "@/utils/generators.js";
import { type Interval, intervalIntersection } from "@/utils/interval.js";
import { promiseAllSettledWithThrow } from "@/utils/promiseAllSettledWithThrow.js";
import { promiseWithResolvers } from "@/utils/promiseWithResolvers.js";
import { startClock } from "@/utils/timer.js";
import { type Address, hexToNumber } from "viem";
import { getFilterFactories } from "./filter.js";
import type { ChildAddresses, SyncProgress } from "./index.js";
import { getOmnichainCheckpoint } from "./omnichain.js";
export type RealtimeEvent =
  | {
      type: "block";
      events: Event[];
      chain: Chain;
      checkpoint: string;
      blockCallback?: (isAccepted: boolean) => void;
    }
  | { type: "reorg"; chain: Chain; checkpoint: string }
  | { type: "finalize"; chain: Chain; checkpoint: string };
export async function* getRealtimeEventsOmnichain(params: {
  common: Common;
  indexingBuild: Pick<
    IndexingBuild,
    "eventCallbacks" | "chains" | "rpcs" | "finalizedBlocks"
  >;
  perChainSync: Map<
    Chain,
    {
      syncProgress: SyncProgress;
      childAddresses: ChildAddresses;
      unfinalizedBlocks: Omit<
        Extract<RealtimeSyncEvent, { type: "block" }>,
        "type"
      >[];
    }
  >;
  database: Database;
  pendingEvents: Event[];
}): AsyncGenerator<RealtimeEvent> {
  const eventGenerators = Array.from(params.perChainSync.entries())
    .map(([chain, { syncProgress, childAddresses }]) => {
      if (syncProgress.isEnd()) {
        params.common.logger.info({
          msg: "Skipped live indexing (chain only requires backfill indexing)",
          chain: chain.name,
          chain_id: chain.id,
          end_block: hexToNumber(syncProgress.end!.number),
        });
        params.common.metrics.ponder_sync_is_complete.set(
          { chain: chain.name },
          1,
        );
        return;
      }
      const rpc =
        params.indexingBuild.rpcs[params.indexingBuild.chains.indexOf(chain)]!;
      const eventCallbacks =
        params.indexingBuild.eventCallbacks[
          params.indexingBuild.chains.findIndex((c) => c.id === chain.id)
        ]!;
      params.common.metrics.ponder_sync_is_realtime.set(
        { chain: chain.name },
        1,
      );
      const bufferCallback = (bufferSize: number) => {
        // Note: Only log when the buffer size is greater than 1 because
        // a buffer size of 1 is not backpressure.
        if (bufferSize === 1) return;
        params.common.logger.trace({
          msg: "Detected live indexing backpressure",
          chain: chain.name,
          chain_id: chain.id,
          buffer_size: bufferSize,
          indexing_step: "order block events",
        });
      };
      return bufferAsyncGenerator(
        getRealtimeEventGenerator({
          common: params.common,
          chain,
          rpc,
          eventCallbacks,
          syncProgress,
          childAddresses,
          database: params.database,
        }),
        100,
        bufferCallback,
      );
    })
    .filter(
      (
        generator,
      ): generator is AsyncGenerator<{
        chain: Chain;
        event: RealtimeSyncEvent;
      }> => generator !== undefined,
    );
  /** Events that have been executed but not finalized. */
  let executedEvents: Event[] = [];
  /** Events that have not been executed. */
  let pendingEvents: Event[] = params.pendingEvents;
  /** Closest-to-tip finalized checkpoint across all chains. */
  let finalizedCheckpoint = ZERO_CHECKPOINT_STRING;
  for await (const { chain, event } of mergeAsyncGeneratorsWithRealtimeOrder(
    eventGenerators,
  )) {
    const { syncProgress, childAddresses, unfinalizedBlocks } =
      params.perChainSync.get(chain)!;
    const eventCallbacks =
      params.indexingBuild.eventCallbacks[
        params.indexingBuild.chains.findIndex((c) => c.id === chain.id)
      ]!;
    await handleRealtimeSyncEvent(event, {
      common: params.common,
      chain,
      eventCallbacks,
      syncProgress,
      unfinalizedBlocks,
      database: params.database,
    });
    switch (event.type) {
      case "block": {
        const events = buildEvents({
          eventCallbacks,
          chainId: chain.id,
          blocks: [syncBlockToInternal({ block: event.block })],
          logs: event.logs.map((log) => syncLogToInternal({ log })),
          transactions: event.transactions.map((transaction) =>
            syncTransactionToInternal({ transaction }),
          ),
          transactionReceipts: event.transactionReceipts.map(
            (transactionReceipt) =>
              syncTransactionReceiptToInternal({ transactionReceipt }),
          ),
          traces: event.traces.map((trace) =>
            syncTraceToInternal({
              trace,
              block: event.block,
              transaction: event.transactions.find(
                (t) => t.hash === trace.transactionHash,
              )!,
            }),
          ),
          childAddresses,
        });
        params.common.logger.trace({
          msg: "Constructed events from block",
          chain: chain.name,
          chain_id: chain.id,
          number: hexToNumber(event.block.number),
          hash: event.block.hash,
          event_count: events.length,
        });
        const decodedEvents = decodeEvents(
          params.common,
          chain,
          eventCallbacks,
          events,
        );
        params.common.logger.trace({
          msg: "Decoded block events",
          chain: chain.name,
          chain_id: chain.id,
          number: hexToNumber(event.block.number),
          hash: event.block.hash,
          event_count: decodedEvents.length,
        });
        const checkpoint = encodeCheckpoint(
          blockToCheckpoint(event.block, chain.id, "up"),
        );
        const readyEvents = pendingEvents
          .concat(decodedEvents)
          .filter((e) => e.checkpoint < checkpoint)
          .sort((a, b) => (a.checkpoint < b.checkpoint ? -1 : 1));
        pendingEvents = pendingEvents
          .concat(decodedEvents)
          .filter((e) => e.checkpoint > checkpoint);
        executedEvents = executedEvents.concat(readyEvents);
        yield {
          type: "block",
          events: readyEvents,
          chain,
          checkpoint,
          blockCallback: event.blockCallback,
        };
        break;
      }
      case "finalize": {
        const from = finalizedCheckpoint;
        finalizedCheckpoint = getOmnichainCheckpoint({
          perChainSync: params.perChainSync,
          tag: "finalized",
        });
        const to = getOmnichainCheckpoint({
          perChainSync: params.perChainSync,
          tag: "finalized",
        });
        if (to <= from) continue;
        // index of the first unfinalized event
        let finalizeIndex: number | undefined = undefined;
        for (const [index, event] of executedEvents.entries()) {
          if (event.checkpoint > to) {
            finalizeIndex = index;
            break;
          }
        }
        let finalizedEvents: Event[];
        if (finalizeIndex === undefined) {
          finalizedEvents = executedEvents;
          executedEvents = [];
        } else {
          finalizedEvents = executedEvents.slice(0, finalizeIndex);
          executedEvents = executedEvents.slice(finalizeIndex);
        }
        params.common.logger.trace({
          msg: "Removed finalized events",
          event_count: finalizedEvents.length,
        });
        yield { type: "finalize", chain, checkpoint: to };
        break;
      }
      case "reorg": {
        const isReorgedEvent = (_event: Event) => {
          if (
            _event.chain.id === chain.id &&
            Number(_event.event.block.number) > hexToNumber(event.block.number)
          ) {
            return true;
          }
          return false;
        };
        const checkpoint = getOmnichainCheckpoint({
          perChainSync: params.perChainSync,
          tag: "current",
        });
        // Move events from executed to pending
        const reorgedEvents = executedEvents.filter(
          (e) => e.checkpoint > checkpoint,
        );
        executedEvents = executedEvents.filter(
          (e) => e.checkpoint < checkpoint,
        );
        pendingEvents = pendingEvents.concat(reorgedEvents);
        params.common.logger.trace({
          msg: "Removed and rescheduled reorged events",
          event_count: reorgedEvents.length,
        });
        pendingEvents = pendingEvents.filter(
          (e) => isReorgedEvent(e) === false,
        );
        yield { type: "reorg", chain, checkpoint };
        break;
      }
    }
  }
}
export async function* getRealtimeEventsMultichain(params: {
  common: Common;
  indexingBuild: Pick<
    IndexingBuild,
    "eventCallbacks" | "chains" | "rpcs" | "finalizedBlocks"
  >;
  perChainSync: Map<
    Chain,
    {
      syncProgress: SyncProgress;
      childAddresses: ChildAddresses;
      unfinalizedBlocks: Omit<
        Extract<RealtimeSyncEvent, { type: "block" }>,
        "type"
      >[];
    }
  >;
  database: Database;
}): AsyncGenerator<RealtimeEvent> {
  const eventGenerators = Array.from(params.perChainSync.entries())
    .map(([chain, { syncProgress, childAddresses }]) => {
      if (syncProgress.isEnd()) {
        params.common.logger.info({
          msg: "Skipped live indexing (chain only requires backfill indexing)",
          chain: chain.name,
          chain_id: chain.id,
          end_block: hexToNumber(syncProgress.end!.number),
        });
        params.common.metrics.ponder_sync_is_complete.set(
          { chain: chain.name },
          1,
        );
        return;
      }
      const rpc =
        params.indexingBuild.rpcs[params.indexingBuild.chains.indexOf(chain)]!;
      const eventCallbacks =
        params.indexingBuild.eventCallbacks[
          params.indexingBuild.chains.findIndex((c) => c.id === chain.id)
        ]!;
      params.common.metrics.ponder_sync_is_realtime.set(
        { chain: chain.name },
        1,
      );
      const bufferCallback = (bufferSize: number) => {
        // Note: Only log when the buffer size is greater than 1 because
        // a buffer size of 1 is not backpressure.
        if (bufferSize === 1) return;
        params.common.logger.trace({
          msg: "Detected live indexing backpressure",
          chain: chain.name,
          chain_id: chain.id,
          buffer_size: bufferSize,
          indexing_step: "order block events",
        });
      };
      return bufferAsyncGenerator(
        getRealtimeEventGenerator({
          common: params.common,
          chain,
          rpc,
          eventCallbacks,
          syncProgress,
          childAddresses,
          database: params.database,
        }),
        100,
        bufferCallback,
      );
    })
    .filter(
      (
        generator,
      ): generator is AsyncGenerator<{
        chain: Chain;
        event: RealtimeSyncEvent;
      }> => generator !== undefined,
    );
  /** Events that have been executed but not finalized. */
  let executedEvents: Event[] = [];
  /** Events that have not been executed. */
  let pendingEvents: Event[] = [];
  for await (const { chain, event } of mergeAsyncGenerators(eventGenerators)) {
    const { syncProgress, childAddresses, unfinalizedBlocks } =
      params.perChainSync.get(chain)!;
    const eventCallbacks =
      params.indexingBuild.eventCallbacks[
        params.indexingBuild.chains.findIndex((c) => c.id === chain.id)
      ]!;
    await handleRealtimeSyncEvent(event, {
      common: params.common,
      chain,
      eventCallbacks,
      syncProgress,
      unfinalizedBlocks,
      database: params.database,
    });
    switch (event.type) {
      case "block": {
        const events = buildEvents({
          eventCallbacks,
          chainId: chain.id,
          blocks: [syncBlockToInternal({ block: event.block })],
          logs: event.logs.map((log) => syncLogToInternal({ log })),
          transactions: event.transactions.map((transaction) =>
            syncTransactionToInternal({ transaction }),
          ),
          transactionReceipts: event.transactionReceipts.map(
            (transactionReceipt) =>
              syncTransactionReceiptToInternal({ transactionReceipt }),
          ),
          traces: event.traces.map((trace) =>
            syncTraceToInternal({
              trace,
              block: event.block,
              transaction: event.transactions.find(
                (t) => t.hash === trace.transactionHash,
              )!,
            }),
          ),
          childAddresses,
        });
        params.common.logger.trace({
          msg: "Constructed events from block",
          chain: chain.name,
          chain_id: chain.id,
          number: hexToNumber(event.block.number),
          hash: event.block.hash,
          event_count: events.length,
        });
        const decodedEvents = decodeEvents(
          params.common,
          chain,
          eventCallbacks,
          events,
        );
        params.common.logger.trace({
          msg: "Decoded block events",
          chain: chain.name,
          chain_id: chain.id,
          number: hexToNumber(event.block.number),
          hash: event.block.hash,
          event_count: decodedEvents.length,
        });
        const checkpoint = syncProgress.getCheckpoint({ tag: "current" });
        if (pendingEvents.length > 0) {
          params.common.logger.trace({
            msg: "Included pending events",
            chain: chain.name,
            chain_id: chain.id,
            event_count: pendingEvents.length,
          });
        }
        const readyEvents = decodedEvents
          .concat(pendingEvents)
          .sort((a, b) => (a.checkpoint < b.checkpoint ? -1 : 1));
        pendingEvents = [];
        executedEvents = executedEvents.concat(readyEvents);
        yield {
          type: "block",
          events: readyEvents,
          chain,
          checkpoint,
          blockCallback: event.blockCallback,
        };
        break;
      }
      case "finalize": {
        const checkpoint = syncProgress.getCheckpoint({ tag: "finalized" });
        // index of the first unfinalized event
        let finalizeIndex: number | undefined = undefined;
        for (const [index, event] of executedEvents.entries()) {
          const _chain = params.indexingBuild.chains.find(
            (c) => c.id === event.chain.id,
          )!;
          const _checkpoint = params.perChainSync
            .get(_chain)!
            .syncProgress.getCheckpoint({ tag: "finalized" });
          if (event.checkpoint > _checkpoint) {
            finalizeIndex = index;
            break;
          }
        }
        let finalizedEvents: Event[];
        if (finalizeIndex === undefined) {
          finalizedEvents = executedEvents;
          executedEvents = [];
        } else {
          finalizedEvents = executedEvents.slice(0, finalizeIndex);
          executedEvents = executedEvents.slice(finalizeIndex);
        }
        params.common.logger.trace({
          msg: "Removed finalized events",
          event_count: finalizedEvents.length,
        });
        yield { type: "finalize", chain, checkpoint };
        break;
      }
      case "reorg": {
        const isReorgedEvent = (_event: Event) => {
          if (
            _event.chain.id === chain.id &&
            Number(_event.event.block.number) > hexToNumber(event.block.number)
          ) {
            return true;
          }
          return false;
        };
        const checkpoint = syncProgress.getCheckpoint({ tag: "current" });
        // index of the first reorged event
        let reorgIndex: number | undefined = undefined;
        for (const [index, event] of executedEvents.entries()) {
          if (event.chain.id === chain.id && event.checkpoint > checkpoint) {
            reorgIndex = index;
            break;
          }
        }
        // Move events from executed to pending
        if (reorgIndex !== undefined) {
          const reorgedEvents = executedEvents.slice(reorgIndex);
          executedEvents = executedEvents.slice(0, reorgIndex);
          pendingEvents = pendingEvents.concat(reorgedEvents);
          params.common.logger.trace({
            msg: "Removed and rescheduled reorged events",
            event_count: reorgedEvents.length,
          });
        }
        pendingEvents = pendingEvents.filter(
          (e) => isReorgedEvent(e) === false,
        );
        yield { type: "reorg", chain, checkpoint };
        break;
      }
    }
  }
}
export async function* getRealtimeEventsIsolated(params: {
  common: Common;
  indexingBuild: Pick<
    IndexingBuild,
    "eventCallbacks" | "chains" | "rpcs" | "finalizedBlocks"
  >;
  chain: Chain;
  syncProgress: SyncProgress;
  childAddresses: ChildAddresses;
  unfinalizedBlocks: Omit<
    Extract<RealtimeSyncEvent, { type: "block" }>,
    "type"
  >[];
  database: Database;
}): AsyncGenerator<RealtimeEvent> {
  if (params.syncProgress.isEnd()) {
    params.common.logger.info({
      msg: "Skipped live indexing (chain only requires backfill indexing)",
      chain: params.chain.name,
      chain_id: params.chain.id,
      end_block: hexToNumber(params.syncProgress.end!.number),
    });
    params.common.metrics.ponder_sync_is_complete.set(
      { chain: params.chain.name },
      1,
    );
    return;
  }
  const rpc =
    params.indexingBuild.rpcs[
      params.indexingBuild.chains.indexOf(params.chain)
    ]!;
  const eventCallbacks =
    params.indexingBuild.eventCallbacks[
      params.indexingBuild.chains.indexOf(params.chain)
    ]!;
  params.common.metrics.ponder_sync_is_realtime.set(
    { chain: params.chain.name },
    1,
  );
  const bufferCallback = (bufferSize: number) => {
    // Note: Only log when the buffer size is greater than 1 because
    // a buffer size of 1 is not backpressure.
    if (bufferSize === 1) return;
    params.common.logger.trace({
      msg: "Detected live indexing backpressure",
      chain: params.chain.name,
      chain_id: params.chain.id,
      buffer_size: bufferSize,
      indexing_step: "order block events",
    });
  };
  const eventGenerator = bufferAsyncGenerator(
    getRealtimeEventGenerator({
      common: params.common,
      chain: params.chain,
      rpc,
      eventCallbacks,
      syncProgress: params.syncProgress,
      childAddresses: params.childAddresses,
      database: params.database,
    }),
    100,
    bufferCallback,
  );
  for await (const { chain, event } of eventGenerator) {
    await handleRealtimeSyncEvent(event, {
      common: params.common,
      chain,
      eventCallbacks,
      syncProgress: params.syncProgress,
      unfinalizedBlocks: params.unfinalizedBlocks,
      database: params.database,
    });
    switch (event.type) {
      case "block": {
        const rawEvents = buildEvents({
          eventCallbacks,
          chainId: chain.id,
          blocks: [syncBlockToInternal({ block: event.block })],
          logs: event.logs.map((log) => syncLogToInternal({ log })),
          transactions: event.transactions.map((transaction) =>
            syncTransactionToInternal({ transaction }),
          ),
          transactionReceipts: event.transactionReceipts.map(
            (transactionReceipt) =>
              syncTransactionReceiptToInternal({ transactionReceipt }),
          ),
          traces: event.traces.map((trace) =>
            syncTraceToInternal({
              trace,
              block: event.block,
              transaction: event.transactions.find(
                (t) => t.hash === trace.transactionHash,
              )!,
            }),
          ),
          childAddresses: params.childAddresses,
        });
        params.common.logger.trace({
          msg: "Constructed events from block",
          chain: chain.name,
          chain_id: chain.id,
          number: hexToNumber(event.block.number),
          hash: event.block.hash,
          event_count: rawEvents.length,
        });
        const events = decodeEvents(
          params.common,
          chain,
          eventCallbacks,
          rawEvents,
        );
        params.common.logger.trace({
          msg: "Decoded block events",
          chain: chain.name,
          chain_id: chain.id,
          number: hexToNumber(event.block.number),
          hash: event.block.hash,
          event_count: events.length,
        });
        const checkpoint = params.syncProgress.getCheckpoint({
          tag: "current",
        });
        yield {
          type: "block",
          events,
          chain,
          checkpoint,
          blockCallback: event.blockCallback,
        };
        break;
      }
      case "finalize": {
        const checkpoint = params.syncProgress.getCheckpoint({
          tag: "finalized",
        });
        yield { type: "finalize", chain, checkpoint };
        break;
      }
      case "reorg": {
        const checkpoint = params.syncProgress.getCheckpoint({
          tag: "current",
        });
        yield { type: "reorg", chain, checkpoint };
        break;
      }
    }
  }
}
export async function* getRealtimeEventGenerator(params: {
  common: Common;
  chain: Chain;
  rpc: Rpc;
  eventCallbacks: EventCallback[];
  syncProgress: SyncProgress;
  childAddresses: ChildAddresses;
  database: Database;
}) {
  const realtimeSync = createRealtimeSync(params);
  let childCount = 0;
  for (const [, factoryChildAddresses] of params.childAddresses) {
    childCount += factoryChildAddresses.size;
  }
  params.common.logger.info({
    msg: "Started live indexing",
    chain: params.chain.name,
    chain_id: params.chain.id,
    finalized_block: hexToNumber(params.syncProgress.finalized.number),
    factory_address_count: childCount,
  });
  const bufferCallback = (bufferSize: number) => {
    // Note: Only log when the buffer size is greater than 1 because
    // a buffer size of 1 is not backpressure.
    if (bufferSize === 1) return;
    params.common.logger.trace({
      msg: "Detected live indexing backpressure",
      chain: params.chain.name,
      chain_id: params.chain.id,
      buffer_size: bufferSize,
      indexing_step: "fetch block data",
    });
  };
  const noNewBlockWarning = () => {
    params.common.logger.warn({
      msg: "No new block received within expected time",
      chain: params.chain.name,
      chain_id: params.chain.id,
    });
  };
  let noNewBlockTimer = setTimeout(noNewBlockWarning, 30_000);
  // Hash of the most recent block received from the RPC.
  let mostRecentHash = params.syncProgress.finalized.hash;
  const { callback, generator } = createCallbackGenerator<{
    block: SyncBlock | SyncBlockHeader;
    blockCallback: (isAccepted: boolean) => void;
    endClock: () => number;
  }>(bufferCallback);
  params.rpc.subscribe({
    onBlock: (block) => {
      if (block.hash !== mostRecentHash) {
        mostRecentHash = block.hash;
        clearTimeout(noNewBlockTimer);
        noNewBlockTimer = setTimeout(noNewBlockWarning, 30_000);
      }
      const pwr = promiseWithResolvers<boolean>();
      const endClock = startClock();
      callback({ block, blockCallback: pwr.resolve, endClock });
      return pwr.promise;
    },
    onError: realtimeSync.onError,
  });
  for await (const { block, blockCallback, endClock } of generator) {
    const arrivalMs = Date.now();
    // Note: No log here because `realtimeSync.sync` logs the block
    const syncGenerator = realtimeSync.sync(block, (isAccepted) => {
      params.common.logger.trace({
        msg: `Block ${isAccepted ? "accepted into" : "rejected from"} live indexing`,
        chain: params.chain.name,
        chain_id: params.chain.id,
        number: hexToNumber(block.number),
        hash: block.hash,
        duration: endClock(),
      });
      if (isAccepted) {
        params.common.metrics.ponder_realtime_block_arrival_latency.observe(
          { chain: params.chain.name },
          arrivalMs - hexToNumber(block.timestamp) * 1_000,
        );
        params.common.metrics.ponder_realtime_latency.observe(
          { chain: params.chain.name },
          endClock(),
        );
      }
      blockCallback(isAccepted);
    });
    for await (const event of syncGenerator) {
      yield { chain: params.chain, event };
    }
    if (block.number === params.syncProgress.end?.number) {
      // The realtime service can be killed if `endBlock` is
      // defined has become finalized.
      params.common.metrics.ponder_sync_is_realtime.set(
        { chain: params.chain.name },
        0,
      );
      params.common.metrics.ponder_sync_is_complete.set(
        { chain: params.chain.name },
        1,
      );
      params.common.logger.info({
        msg: "Completed live indexing (chain end block has been indexed)",
        chain: params.chain.name,
        chain_id: params.chain.id,
        end_block: hexToNumber(params.syncProgress.end!.number),
      });
      await params.rpc.unsubscribe();
      return;
    }
  }
}
export async function handleRealtimeSyncEvent(
  event: RealtimeSyncEvent,
  params: {
    common: Common;
    chain: Chain;
    eventCallbacks: EventCallback[];
    syncProgress: SyncProgress;
    unfinalizedBlocks: Omit<
      Extract<RealtimeSyncEvent, { type: "block" }>,
      "type"
    >[];
    database: Database;
  },
) {
  switch (event.type) {
    case "block": {
      params.syncProgress.current = event.block;
      params.common.metrics.ponder_sync_block.set(
        { chain: params.chain.name },
        hexToNumber(params.syncProgress.current!.number),
      );
      params.common.metrics.ponder_sync_block_timestamp.set(
        { chain: params.chain.name },
        hexToNumber(params.syncProgress.current!.timestamp),
      );
      params.unfinalizedBlocks.push(event);
      break;
    }
    case "finalize": {
      const finalizedInterval = [
        hexToNumber(params.syncProgress.finalized.number),
        hexToNumber(event.block.number),
      ] satisfies Interval;
      params.syncProgress.finalized = event.block;
      // Remove all finalized data
      const finalizedBlocks: typeof params.unfinalizedBlocks = [];
      while (params.unfinalizedBlocks.length > 0) {
        const block = params.unfinalizedBlocks[0]!;
        if (
          hexToNumber(block.block.number) <= hexToNumber(event.block.number)
        ) {
          finalizedBlocks.push(block);
          params.unfinalizedBlocks.shift();
        } else break;
      }
      if (params.chain.disableCache) break;
      // Add finalized blocks, logs, transactions, receipts, and traces to the sync-store.
      const childAddresses = new Map<Factory, Map<Address, number>>();
      for (const block of finalizedBlocks) {
        for (const [factory, addresses] of block.childAddresses) {
          if (childAddresses.has(factory) === false) {
            childAddresses.set(factory, new Map());
          }
          for (const address of addresses) {
            if (childAddresses.get(factory)!.has(address) === false) {
              childAddresses
                .get(factory)!
                .set(address, hexToNumber(block.block.number));
            }
          }
        }
      }
      const context = {
        logger: params.common.logger.child({ action: "finalize_block_range" }),
      };
      await params.database.syncQB.transaction(
        async (tx) => {
          const syncStore = createSyncStore({ common: params.common, qb: tx });
          await promiseAllSettledWithThrow([
            syncStore.insertBlocks({
              blocks: finalizedBlocks
                .filter(({ hasMatchedFilter }) => hasMatchedFilter)
                .map(({ block }) => block),
              chainId: params.chain.id,
            }),
            syncStore.insertTransactions({
              transactions: finalizedBlocks.flatMap(
                ({ transactions }) => transactions,
              ),
              chainId: params.chain.id,
            }),
            syncStore.insertTransactionReceipts({
              transactionReceipts: finalizedBlocks.flatMap(
                ({ transactionReceipts }) => transactionReceipts,
              ),
              chainId: params.chain.id,
            }),
            syncStore.insertLogs({
              logs: finalizedBlocks.flatMap(({ logs }) => logs),
              chainId: params.chain.id,
            }),
            syncStore.insertTraces({
              traces: finalizedBlocks.flatMap(
                ({ traces, block, transactions }) =>
                  traces.map((trace) => ({
                    trace,
                    block: block as SyncBlock, // SyncBlock is expected for traces.length !== 0
                    transaction: transactions.find(
                      (t) => t.hash === trace.transactionHash,
                    )!,
                  })),
              ),
              chainId: params.chain.id,
            }),
            ...Array.from(childAddresses.entries()).map(
              ([factory, childAddresses]) =>
                syncStore.insertChildAddresses({
                  factory,
                  childAddresses,
                  chainId: params.chain.id,
                }),
            ),
          ]);
          const intervals: {
            interval: Interval;
            filter: Filter;
          }[] = [];
          const factoryIntervals: {
            interval: Interval;
            factory: Factory;
          }[] = [];
          for (const { filter } of params.eventCallbacks) {
            const completedIntervals = intervalIntersection(
              [finalizedInterval],
              [
                [
                  filter.fromBlock ?? 0,
                  filter.toBlock ?? Number.POSITIVE_INFINITY,
                ],
              ],
            );
            for (const interval of completedIntervals) {
              intervals.push({ interval, filter });
            }
            for (const factory of getFilterFactories(filter)) {
              const completedIntervals = intervalIntersection(
                [finalizedInterval],
                [
                  [
                    factory.fromBlock ?? 0,
                    factory.toBlock ?? Number.POSITIVE_INFINITY,
                  ],
                ],
              );
              for (const interval of completedIntervals) {
                factoryIntervals.push({ interval, factory });
              }
            }
          }
          await syncStore.insertIntervals(
            {
              intervals,
              factoryIntervals,
              chainId: params.chain.id,
            },
            context,
          );
        },
        undefined,
        context,
      );
      break;
    }
    case "reorg": {
      params.syncProgress.current = event.block;
      params.common.metrics.ponder_sync_block.set(
        { chain: params.chain.name },
        hexToNumber(params.syncProgress.current!.number),
      );
      params.common.metrics.ponder_sync_block_timestamp.set(
        { chain: params.chain.name },
        hexToNumber(params.syncProgress.current!.timestamp),
      );
      // Remove all reorged data
      while (params.unfinalizedBlocks.length > 0) {
        const block =
          params.unfinalizedBlocks[params.unfinalizedBlocks.length - 1]!;
        if (hexToNumber(block.block.number) > hexToNumber(event.block.number)) {
          params.unfinalizedBlocks.pop();
        } else break;
      }
      await createSyncStore({
        common: params.common,
        qb: params.database.syncQB,
      }).pruneRpcRequestResults(
        {
          chainId: params.chain.id,
          blocks: event.reorgedBlocks,
        },
        { logger: params.common.logger.child({ action: "reconcile_reorg" }) },
      );
      break;
    }
  }
}
/**
 * Merges multiple async generators into a single async generator while preserving
 * the order of "block" events.
 *
 * @dev "reorg" and "finalize" events are not ordered between chains.
 */
export async function* mergeAsyncGeneratorsWithRealtimeOrder(
  generators: AsyncGenerator<{ chain: Chain; event: RealtimeSyncEvent }>[],
): AsyncGenerator<{ chain: Chain; event: RealtimeSyncEvent }> {
  const results = await Promise.all(generators.map((gen) => gen.next()));
  while (results.some((res) => res.done !== true)) {
    let index: number;
    if (
      results.some(
        (result) =>
          result.done === false &&
          (result.value.event.type === "reorg" ||
            result.value.event.type === "finalize"),
      )
    ) {
      index = results.findIndex(
        (result) =>
          result.done === false &&
          (result.value.event.type === "reorg" ||
            result.value.event.type === "finalize"),
      );
    } else {
      const blockCheckpoints = results.map((result) =>
        result.done
          ? undefined
          : encodeCheckpoint(
              blockToCheckpoint(
                result.value.event.block,
                result.value.chain.id,
                "up",
              ),
            ),
      );
      const supremum = min(...blockCheckpoints);
      index = blockCheckpoints.findIndex(
        (checkpoint) => checkpoint === supremum,
      );
    }
    const resultPromise = generators[index]!.next();
    yield {
      chain: results[index]!.value.chain,
      event: results[index]!.value.event,
    };
    results[index] = await resultPromise;
  }
}
</file>

<file path="packages/core/src/server/error.ts">
import { addStackTrace } from "@/indexing/addStackTrace.js";
import type { Common } from "@/internal/common.js";
import type { BaseError } from "@/internal/errors.js";
import { prettyPrint } from "@/utils/print.js";
import type { Context, HonoRequest } from "hono";
import { html } from "hono/html";
export const onError = async (_error: Error, c: Context, common: Common) => {
  const error = _error as BaseError;
  // Find the filename where the error occurred
  const regex = /(\S+\.(?:js|ts|mjs|cjs)):\d+:\d+/;
  const matches = error.stack?.match(regex);
  const errorFile = (() => {
    if (!matches?.[0]) return undefined;
    const path = matches[0].trim();
    if (path.startsWith("(")) {
      return path.slice(1);
    } else if (path.startsWith("file://")) {
      return path.slice(7);
    }
    return path;
  })();
  addStackTrace(error, common.options);
  error.meta = Array.isArray(error.meta) ? error.meta : [];
  error.meta.push(
    `Request:\n${prettyPrint({
      path: c.req.path,
      method: c.req.method,
      body: await tryExtractRequestBody(c.req),
    })}`,
  );
  common.logger.warn({
    msg: "Failed to handle HTTP request",
    method: c.req.method,
    path: c.req.path,
    error,
  });
  // 500: Internal Server Error
  return c.text(
    `${error.name}: ${error.message} occurred in '${errorFile}' while handling a '${c.req.method}' request to the route '${c.req.path}'`,
    500,
  );
};
export const onNotFound = (c: Context) => {
  return c.html(
    html`<!doctype html>
      <h1>Bad news!</h1>
      <p>The route "<code>${c.req.path}</code>" does not exist</p>`,
  );
};
const tryExtractRequestBody = async (request: HonoRequest) => {
  try {
    return await request.json();
  } catch {
    try {
      const text = await request.text();
      if (text !== "") return text;
    } catch {}
  }
  return undefined;
};
</file>

<file path="packages/core/src/server/index.test.ts">
import {
  context,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import { getPonderMetaTable } from "@/database/index.js";
import { sql } from "drizzle-orm";
import { Hono } from "hono";
import { beforeEach, expect, test, vi } from "vitest";
import { createServer } from "./index.js";
beforeEach(setupCommon);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
test("listens on ipv4", async () => {
  const { database } = await setupDatabaseServices();
  await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  const response = await fetch(
    `http://localhost:${context.common.options.port}/health`,
  );
  expect(response.status).toBe(200);
});
test("listens on ipv6", async () => {
  const { database } = await setupDatabaseServices();
  await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  const response = await fetch(
    `http://[::1]:${context.common.options.port}/health`,
  );
  expect(response.status).toBe(200);
});
test("not ready", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  const response = await server.hono.request("/ready");
  expect(response.status).toBe(503);
});
test("ready", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  await database.adminQB.wrap((db) =>
    db.update(getPonderMetaTable()).set({
      value: sql`jsonb_set(value, '{is_ready}', to_jsonb(1))`,
    }),
  );
  const response = await server.hono.request("/ready");
  expect(response.status).toBe(200);
});
test("health", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  const response = await server.hono.request("/health");
  expect(response.status).toBe(200);
});
test("healthy PUT", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  const response = await server.hono.request("/health", {
    method: "PUT",
  });
  expect(response.status).toBe(404);
});
test("metrics", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  const response = await server.hono.request("/metrics");
  expect(response.status).toBe(200);
});
test("metrics error", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  const metricsSpy = vi.spyOn(context.common.metrics, "getMetrics");
  metricsSpy.mockRejectedValueOnce(new Error());
  const response = await server.hono.request("/metrics");
  expect(response.status).toBe(500);
});
test("metrics PUT", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  const response = await server.hono.request("/metrics", {
    method: "PUT",
  });
  expect(response.status).toBe(404);
});
test("metrics unmatched route", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  await server.hono.request("/unmatched");
  const response = await server.hono.request("/metrics");
  expect(response.status).toBe(200);
  const text = await response.text();
  expect(text).not.toContain('path="/unmatched"');
});
test("missing route", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  const response = await server.hono.request("/kevin");
  expect(response.status).toBe(404);
});
test("custom api route", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono().get("/hi", (c) => c.text("hi")),
      port: context.common.options.port,
    },
    database,
  });
  const response = await server.hono.request("/hi");
  expect(response.status).toBe(200);
  expect(await response.text()).toBe("hi");
});
test("custom hono route", async () => {
  const { database } = await setupDatabaseServices();
  const app = new Hono().get("/hi", (c) => c.text("hi"));
  const server = await createServer({
    common: context.common,
    apiBuild: { app, port: context.common.options.port },
    database,
  });
  const response = await server.hono.request("/hi");
  expect(response.status).toBe(200);
  expect(await response.text()).toBe("hi");
});
// Note that this test doesn't work because the `hono.request` method doesn't actually
// create a socket connection, it just calls the request handler function directly.
test.skip("kill", async () => {
  const { database } = await setupDatabaseServices();
  const server = await createServer({
    common: context.common,
    apiBuild: {
      app: new Hono(),
      port: context.common.options.port,
    },
    database,
  });
  expect(() => server.hono.request("/health")).rejects.toThrow();
});
</file>

<file path="packages/core/src/server/index.ts">
import http from "node:http";
import {
  type Database,
  getPonderCheckpointTable,
  getPonderMetaTable,
} from "@/database/index.js";
import type { Common } from "@/internal/common.js";
import type { ApiBuild, Status } from "@/internal/types.js";
import { decodeCheckpoint } from "@/utils/checkpoint.js";
import { startClock } from "@/utils/timer.js";
import { serve } from "@hono/node-server";
import { Hono } from "hono";
import { cors } from "hono/cors";
import { createMiddleware } from "hono/factory";
import { createHttpTerminator } from "http-terminator";
import { onError } from "./error.js";
export type Server = {
  hono: Hono;
};
export async function createServer({
  common,
  database,
  apiBuild,
}: {
  common: Common;
  database: Database;
  apiBuild: ApiBuild;
}): Promise<Server> {
  const metricsMiddleware = createMiddleware(async (c, next) => {
    const matchedPathLabels = c.req.matchedRoutes
      // Filter out global middlewares
      .filter((r) => r.path !== "/*")
      .map((r) => ({ method: c.req.method, path: r.path }));
    for (const labels of matchedPathLabels) {
      common.metrics.ponder_http_server_active_requests.inc(labels);
    }
    const endClock = startClock();
    try {
      await next();
    } finally {
      const requestSize = Number(c.req.header("Content-Length") ?? 0);
      const responseSize = Number(c.res.headers.get("Content-Length") ?? 0);
      const responseDuration = endClock();
      const status =
        c.res.status >= 200 && c.res.status < 300
          ? "2XX"
          : c.res.status >= 300 && c.res.status < 400
            ? "3XX"
            : c.res.status >= 400 && c.res.status < 500
              ? "4XX"
              : "5XX";
      for (const labels of matchedPathLabels) {
        common.metrics.ponder_http_server_active_requests.dec(labels);
        common.metrics.ponder_http_server_request_size_bytes.observe(
          { ...labels, status },
          requestSize,
        );
        common.metrics.ponder_http_server_response_size_bytes.observe(
          { ...labels, status },
          responseSize,
        );
        common.metrics.ponder_http_server_request_duration_ms.observe(
          { ...labels, status },
          responseDuration,
        );
      }
    }
  });
  const hono = new Hono()
    .use(metricsMiddleware)
    .use(cors({ origin: "*", maxAge: 86400 }))
    .get("/metrics", async (c) => {
      try {
        const metrics = await common.metrics.getMetrics();
        return c.text(metrics);
      } catch (error) {
        return c.json(error as Error, 500);
      }
    })
    .get("/health", (c) => {
      return c.text("", 200);
    })
    .get("/ready", async (c) => {
      const isReady = await database.readonlyQB.wrap(
        { label: "select_ready" },
        (db) =>
          db
            .select()
            .from(getPonderMetaTable())
            .then((result) => result[0]!.value.is_ready === 1),
      );
      if (isReady) {
        return c.text("", 200);
      }
      return c.text("Historical indexing is not complete.", 503);
    })
    .get("/status", async (c) => {
      const checkpoints = await database.readonlyQB.wrap(
        { label: "select_checkpoints" },
        (db) => db.select().from(getPonderCheckpointTable()),
      );
      const status: Status = {};
      for (const { chainName, chainId, latestCheckpoint } of checkpoints.sort(
        (a, b) => (a.chainId > b.chainId ? 1 : -1),
      )) {
        status[chainName] = {
          id: chainId,
          block: {
            number: Number(decodeCheckpoint(latestCheckpoint).blockNumber),
            timestamp: Number(
              decodeCheckpoint(latestCheckpoint).blockTimestamp,
            ),
          },
        };
      }
      return c.json(status);
    })
    .route("/", apiBuild.app)
    .onError((error, c) => onError(error, c, common));
  const endClock = startClock();
  // Create nodejs server
  const httpServer = await new Promise<http.Server>((resolve, reject) => {
    const timeout = setTimeout(() => {
      reject(new Error("HTTP server failed to start within 5 seconds."));
    }, 5_000);
    const httpServer = serve(
      {
        fetch: hono.fetch,
        createServer: http.createServer,
        port: apiBuild.port,
        // Note that common.options.hostname can be undefined if the user did not specify one.
        // In this case, Node.js uses `::` if IPv6 is available and `0.0.0.0` otherwise.
        // https://nodejs.org/api/net.html#serverlistenport-host-backlog-callback
        hostname: apiBuild.hostname,
      },
      () => {
        clearTimeout(timeout);
        common.metrics.port = apiBuild.port;
        common.logger.info({
          msg: "Created HTTP server",
          port: apiBuild.port,
          hostname: apiBuild.hostname,
          duration: endClock(),
        });
        common.logger.info({
          msg: "Started returning 200 responses",
          endpoint: "/health",
        });
        resolve(httpServer as http.Server);
      },
    );
  });
  const terminator = createHttpTerminator({
    server: httpServer,
    gracefulTerminationTimeout: 1000,
  });
  common.apiShutdown.add(() => terminator.terminate());
  return { hono };
}
</file>

<file path="packages/core/src/sync-historical/index.test.ts">
import { ALICE, BOB } from "@/_test/constants.js";
import {
  context,
  setupAnvil,
  setupCachedIntervals,
  setupChildAddresses,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import {
  createPair,
  deployErc20,
  deployFactory,
  mintErc20,
  simulateBlock,
  swapPair,
  transferErc20,
  transferEth,
} from "@/_test/simulate.js";
import {
  getAccountsIndexingBuild,
  getBlocksIndexingBuild,
  getChain,
  getErc20IndexingBuild,
  getPairWithFactoryIndexingBuild,
} from "@/_test/utils.js";
import { createRpc } from "@/rpc/index.js";
import {
  getCachedIntervals,
  getRequiredIntervalsWithFilters,
} from "@/runtime/index.js";
import * as ponderSyncSchema from "@/sync-store/schema.js";
import { zeroAddress } from "viem";
import { parseEther } from "viem/utils";
import { beforeEach, expect, test, vi } from "vitest";
import { createHistoricalSync } from "./index.js";
beforeEach(setupCommon);
beforeEach(setupAnvil);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
test("createHistoricalSync()", async () => {
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  expect(historicalSync).toBeDefined();
});
test("sync() with log filter", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getErc20IndexingBuild({
    address,
  });
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  const requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 2],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  const logs = await historicalSync.syncBlockRangeData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  const dbLogs = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.logs).execute(),
  );
  expect(dbLogs).toHaveLength(1);
  const dbIntervals = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.intervals).execute(),
  );
  expect(dbIntervals).toHaveLength(1);
});
test.skipIf(process.env.USER === "jaymiller")(
  "sync() with log filter and transaction receipts",
  async () => {
    const { syncStore, database } = await setupDatabaseServices();
    const chain = getChain();
    const rpc = createRpc({
      chain,
      common: context.common,
    });
    const { address } = await deployErc20({ sender: ALICE });
    await mintErc20({
      erc20: address,
      to: ALICE,
      amount: parseEther("1"),
      sender: ALICE,
    });
    const { eventCallbacks } = getErc20IndexingBuild({
      address,
      includeTransactionReceipts: true,
    });
    const historicalSync = createHistoricalSync({
      common: context.common,
      chain,
      rpc,
      childAddresses: setupChildAddresses(eventCallbacks),
    });
    const requiredIntervals = getRequiredIntervalsWithFilters({
      interval: [1, 2],
      filters: eventCallbacks.map(({ filter }) => filter),
      cachedIntervals: setupCachedIntervals(eventCallbacks),
    });
    const logs = await historicalSync.syncBlockRangeData({
      interval: [1, 2],
      requiredIntervals: requiredIntervals.intervals,
      requiredFactoryIntervals: requiredIntervals.factoryIntervals,
      syncStore,
    });
    await historicalSync.syncBlockData({
      interval: [1, 2],
      requiredIntervals: requiredIntervals.intervals,
      logs,
      syncStore,
    });
    await syncStore.insertIntervals({
      intervals: requiredIntervals.intervals,
      factoryIntervals: requiredIntervals.factoryIntervals,
      chainId: chain.id,
    });
    const transactionReceipts = await database.syncQB.wrap((db) =>
      db.select().from(ponderSyncSchema.transactionReceipts).execute(),
    );
    expect(transactionReceipts).toHaveLength(1);
    const intervals = await database.syncQB.wrap((db) =>
      db.select().from(ponderSyncSchema.intervals).execute(),
    );
    expect(intervals).toHaveLength(1);
  },
);
test("sync() with block filter", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await simulateBlock();
  await simulateBlock();
  await simulateBlock();
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  const requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 3],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  const logs = await historicalSync.syncBlockRangeData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  const blocks = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.blocks).execute(),
  );
  expect(blocks).toHaveLength(3);
  const intervals = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.intervals).execute(),
  );
  expect(intervals).toHaveLength(1);
});
test("sync() with log factory", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployFactory({ sender: ALICE });
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  await swapPair({
    pair,
    amount0Out: 1n,
    amount1Out: 1n,
    to: ALICE,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({
    address,
  });
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  const requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 3],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  const logs = await historicalSync.syncBlockRangeData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  const dbLogs = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.logs).execute(),
  );
  const factories = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.factories).execute(),
  );
  expect(dbLogs).toHaveLength(1);
  expect(factories).toHaveLength(1);
  const intervals = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.intervals).execute(),
  );
  expect(intervals).toHaveLength(2);
});
test("sync() with log factory and no address", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployFactory({ sender: ALICE });
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  await swapPair({
    pair,
    amount0Out: 1n,
    amount1Out: 1n,
    to: ALICE,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({
    address,
  });
  // @ts-ignore
  eventCallbacks[0].filter.address.address = undefined;
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  const requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 3],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  const logs = await historicalSync.syncBlockRangeData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  const dbLogs = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.logs).execute(),
  );
  const factories = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.factories).execute(),
  );
  expect(dbLogs).toHaveLength(1);
  expect(factories).toHaveLength(1);
  const intervals = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.intervals).execute(),
  );
  expect(intervals).toHaveLength(2);
});
test("sync() with log factory error", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployFactory({ sender: ALICE });
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  await swapPair({
    pair,
    amount0Out: 1n,
    amount1Out: 1n,
    to: ALICE,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({
    address,
  });
  // @ts-ignore
  eventCallbacks[0].filter.address.address = undefined;
  // @ts-ignore
  // Invalid child address location causes extracting child address to throw an error
  eventCallbacks[0].filter.address.childAddressLocation = "topic3";
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  const requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 3],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  await historicalSync.syncBlockRangeData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  const factories = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.factories).execute(),
  );
  expect(factories).toHaveLength(0);
});
test("sync() with trace filter", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const blockData = await transferErc20({
    erc20: address,
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getErc20IndexingBuild({
    address,
    includeCallTraces: true,
  });
  const request = async (request: any) => {
    if (request.method === "debug_traceBlockByNumber") {
      if (request.params[0] === "0x1") return Promise.resolve([]);
      if (request.params[0] === "0x2") return Promise.resolve([]);
      if (request.params[0] === "0x3") {
        return Promise.resolve([
          {
            txHash: blockData.trace.transactionHash,
            result: blockData.trace.trace,
          },
        ]);
      }
    }
    return rpc.request(request);
  };
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc: {
      ...rpc,
      // @ts-ignore
      request,
    },
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  const requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 3],
    filters: eventCallbacks
      .filter(({ filter }) => filter.type === "trace")
      .map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  const logs = await historicalSync.syncBlockRangeData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  const traces = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.traces).execute(),
  );
  expect(traces).toHaveLength(1);
  const intervals = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.intervals).execute(),
  );
  expect(intervals).toHaveLength(1);
});
test("sync() with transaction filter", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  await transferEth({
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getAccountsIndexingBuild({
    address: ALICE,
  });
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  const requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 1],
    filters: eventCallbacks
      .filter(({ filter }) => filter.type === "transaction")
      .map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  const logs = await historicalSync.syncBlockRangeData({
    interval: [1, 1],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 1],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  const transactions = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.transactions).execute(),
  );
  expect(transactions).toHaveLength(1);
  const transactionReceipts = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.transactionReceipts).execute(),
  );
  expect(transactionReceipts).toHaveLength(1);
  const intervals = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.intervals).execute(),
  );
  // transaction:from and transaction:to
  expect(intervals).toHaveLength(2);
});
test("sync() with transfer filter", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const blockData = await transferEth({
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getAccountsIndexingBuild({
    address: ALICE,
  });
  const request = async (request: any) => {
    if (request.method === "debug_traceBlockByNumber") {
      if (request.params[0] === "0x1") {
        return Promise.resolve([
          {
            txHash: blockData.trace.transactionHash,
            result: blockData.trace.trace,
          },
        ]);
      }
    }
    return rpc.request(request);
  };
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc: {
      ...rpc,
      // @ts-ignore
      request,
    },
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  const requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 1],
    filters: eventCallbacks
      .filter(({ filter }) => filter.type === "transfer")
      .map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  const logs = await historicalSync.syncBlockRangeData({
    interval: [1, 1],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 1],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  const transactions = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.transactions).execute(),
  );
  expect(transactions).toHaveLength(1);
  const intervals = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.intervals).execute(),
  );
  // transfer:from and transfer:to
  expect(intervals).toHaveLength(2);
});
test("sync() with many filters", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks: erc20EventCallbacks } = getErc20IndexingBuild({
    address,
  });
  const { eventCallbacks: blocksEventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses([
      ...erc20EventCallbacks,
      ...blocksEventCallbacks,
    ]),
  });
  const requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 2],
    filters: [...erc20EventCallbacks, ...blocksEventCallbacks].map(
      ({ filter }) => filter,
    ),
    cachedIntervals: setupCachedIntervals([
      ...erc20EventCallbacks,
      ...blocksEventCallbacks,
    ]),
  });
  const logs = await historicalSync.syncBlockRangeData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  const dbLogs = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.logs).execute(),
  );
  expect(dbLogs).toHaveLength(1);
  const blocks = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.blocks).execute(),
  );
  expect(blocks).toHaveLength(2);
  const intervals = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.intervals).execute(),
  );
  expect(intervals).toHaveLength(2);
});
test("sync() with cache", async () => {
  const { syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getErc20IndexingBuild({
    address,
  });
  let historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  let requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 2],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  let logs = await historicalSync.syncBlockRangeData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  // re-instantiate `historicalSync` to reset the cached intervals
  const spy = vi.spyOn(rpc, "request");
  const cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 2],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals,
  });
  logs = await historicalSync.syncBlockRangeData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  expect(spy).toHaveBeenCalledTimes(0);
});
test("sync() with partial cache", async () => {
  const { syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getErc20IndexingBuild({
    address,
  });
  let historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  let requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 2],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  let logs = await historicalSync.syncBlockRangeData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  // re-instantiate `historicalSync` to reset the cached intervals
  let spy = vi.spyOn(rpc, "request");
  // @ts-ignore
  eventCallbacks[0]!.filter.address = [
    // @ts-ignore
    eventCallbacks[0]!.filter.address,
    zeroAddress,
  ];
  let cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 2],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals,
  });
  logs = await historicalSync.syncBlockRangeData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 2],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  // `eth_getBlockByNumber` is skipped
  expect(spy).toHaveBeenCalledTimes(1);
  expect(spy).toHaveBeenCalledWith(
    {
      method: "eth_getLogs",
      params: [
        {
          address: [zeroAddress],
          fromBlock: "0x1",
          toBlock: "0x2",
          topics: [
            "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
          ],
        },
      ],
    },
    expect.any(Object),
  );
  // re-instantiate `historicalSync` to reset the cached intervals
  spy = vi.spyOn(rpc, "request");
  spy.mockClear();
  cachedIntervals = await getCachedIntervals({
    chain,
    syncStore,
    filters: eventCallbacks.map(({ filter }) => filter),
  });
  historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  await simulateBlock();
  requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 3],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals,
  });
  logs = await historicalSync.syncBlockRangeData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 3],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  // `eth_getBlockByNumber` is skipped
  expect(spy).toHaveBeenCalledTimes(1);
  expect(spy).toHaveBeenCalledWith(
    {
      method: "eth_getLogs",
      params: [
        {
          address: [address, zeroAddress],
          fromBlock: "0x3",
          toBlock: "0x3",
          topics: [
            "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
          ],
        },
      ],
    },
    expect.any(Object),
  );
});
test("syncAddress() handles many addresses", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({
    chain,
    common: context.common,
  });
  context.common.options.factoryAddressCountThreshold = 10;
  const { address } = await deployFactory({ sender: ALICE });
  for (let i = 0; i < 10; i++) {
    await createPair({ factory: address, sender: ALICE });
  }
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  await swapPair({
    pair,
    amount0Out: 1n,
    amount1Out: 1n,
    to: ALICE,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({
    address,
  });
  const historicalSync = createHistoricalSync({
    common: context.common,
    chain,
    rpc,
    childAddresses: setupChildAddresses(eventCallbacks),
  });
  const requiredIntervals = getRequiredIntervalsWithFilters({
    interval: [1, 13],
    filters: eventCallbacks.map(({ filter }) => filter),
    cachedIntervals: setupCachedIntervals(eventCallbacks),
  });
  const logs = await historicalSync.syncBlockRangeData({
    interval: [1, 13],
    requiredIntervals: requiredIntervals.intervals,
    requiredFactoryIntervals: requiredIntervals.factoryIntervals,
    syncStore,
  });
  await historicalSync.syncBlockData({
    interval: [1, 13],
    requiredIntervals: requiredIntervals.intervals,
    logs,
    syncStore,
  });
  await syncStore.insertIntervals({
    intervals: requiredIntervals.intervals,
    factoryIntervals: requiredIntervals.factoryIntervals,
    chainId: chain.id,
  });
  const dbLogs = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.logs).execute(),
  );
  const factories = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.factoryAddresses).execute(),
  );
  expect(dbLogs).toHaveLength(1);
  expect(factories).toHaveLength(11);
});
</file>

<file path="packages/core/src/sync-historical/index.ts">
import type { Common } from "@/internal/common.js";
import type {
  BlockFilter,
  Chain,
  Factory,
  FactoryId,
  LogFilter,
  SyncBlock,
  SyncLog,
  SyncTrace,
  SyncTransaction,
  SyncTransactionReceipt,
  TraceFilter,
  TransactionFilter,
  TransferFilter,
} from "@/internal/types.js";
import {
  debug_traceBlockByNumber,
  eth_getBlockByNumber,
  eth_getBlockReceipts,
  eth_getLogs,
  eth_getTransactionReceipt,
  validateLogsAndBlock,
  validateReceiptsAndBlock,
  validateTracesAndBlock,
  validateTransactionsAndBlock,
} from "@/rpc/actions.js";
import type { Rpc } from "@/rpc/index.js";
import {
  getChildAddress,
  isAddressFactory,
  isAddressMatched,
  isBlockFilterMatched,
  isBlockInFilter,
  isLogFactoryMatched,
  isLogFilterMatched,
  isTraceFilterMatched,
  isTransactionFilterMatched,
  isTransferFilterMatched,
} from "@/runtime/filter.js";
import type {
  ChildAddresses,
  IntervalWithFactory,
  IntervalWithFilter,
} from "@/runtime/index.js";
import type { SyncStore } from "@/sync-store/index.js";
import { dedupe } from "@/utils/dedupe.js";
import {
  type Interval,
  getChunks,
  intervalBounds,
  intervalRange,
} from "@/utils/interval.js";
import { promiseAllSettledWithThrow } from "@/utils/promiseAllSettledWithThrow.js";
import { createQueue } from "@/utils/queue.js";
import { startClock } from "@/utils/timer.js";
import { getLogsRetryHelper } from "@ponder/utils";
import {
  type Address,
  type Hash,
  type Hex,
  type LogTopic,
  type RpcError,
  hexToNumber,
  numberToHex,
  toHex,
  zeroHash,
} from "viem";
export type HistoricalSync = {
  /**
   * Sync block data that can be queried for a range of blocks (logs).
   */
  syncBlockRangeData(params: {
    interval: Interval;
    requiredIntervals: IntervalWithFilter[];
    requiredFactoryIntervals: IntervalWithFactory[];
    syncStore: SyncStore;
  }): Promise<SyncLog[]>;
  /**
   * Sync block data that must be queried for a single block (block, transactions, receipts, traces).
   */
  syncBlockData(params: {
    interval: Interval;
    requiredIntervals: IntervalWithFilter[];
    logs: SyncLog[];
    syncStore: SyncStore;
  }): Promise<SyncBlock | undefined>;
};
type CreateHistoricalSyncParameters = {
  common: Common;
  chain: Chain;
  rpc: Rpc;
  childAddresses: Map<FactoryId, Map<Address, number>>;
};
export const createHistoricalSync = (
  args: CreateHistoricalSyncParameters,
): HistoricalSync => {
  /**
   * Flag to fetch transaction receipts through eth_getBlockReceipts (true) or eth_getTransactionReceipt (false)
   */
  let isBlockReceipts = true;
  /**
   * Data about the range passed to "eth_getLogs" share among all log
   * filters and log factories.
   */
  let logsRequestMetadata: {
    /** Estimate optimal range to use for "eth_getLogs" requests */
    estimatedRange: number;
    /** Range suggested by an error message */
    confirmedRange?: number;
  } = {
    estimatedRange: 500,
  };
  ////////
  // Helper functions for sync tasks
  ////////
  type EthGetLogsParams = {
    address: Address | Address[] | undefined;
    topic0?: LogTopic;
    topic1?: LogTopic;
    topic2?: LogTopic;
    topic3?: LogTopic;
    interval: Interval;
  };
  /**
   * Split "eth_getLogs" requests into ranges inferred from errors
   * and batch requests.
   */
  const syncLogsDynamic = async (
    { address, topic0, topic1, topic2, topic3, interval }: EthGetLogsParams,
    context?: Parameters<Rpc["request"]>[1],
  ): Promise<SyncLog[]> => {
    const intervals = getChunks({
      interval,
      maxChunkSize:
        args.chain.ethGetLogsBlockRange ??
        logsRequestMetadata.confirmedRange ??
        logsRequestMetadata.estimatedRange,
    });
    const topics = [
      topic0 ?? null,
      topic1 ?? null,
      topic2 ?? null,
      topic3 ?? null,
    ];
    // Note: the `topics` field is very fragile for many rpc providers, and
    // cannot handle extra "null" topics
    if (topics[3] === null) {
      topics.pop();
      if (topics[2] === null) {
        topics.pop();
        if (topics[1] === null) {
          topics.pop();
          if (topics[0] === null) {
            topics.pop();
          }
        }
      }
    }
    // Batch large arrays of addresses, handling arrays that are empty
    let addressBatches: (Address | Address[] | undefined)[];
    if (address === undefined) {
      // no address (match all)
      addressBatches = [undefined];
    } else if (typeof address === "string") {
      // single address
      addressBatches = [address];
    } else if (address.length === 0) {
      // no address (factory with no children)
      return [];
    } else {
      // many addresses
      // Note: it is assumed that `address` is deduplicated
      addressBatches = [];
      for (let i = 0; i < address.length; i += 50) {
        addressBatches.push(address.slice(i, i + 50));
      }
    }
    const logs = await Promise.all(
      intervals.flatMap((interval) =>
        addressBatches.map((address) =>
          eth_getLogs(
            args.rpc,
            [
              {
                address,
                topics,
                fromBlock: numberToHex(interval[0]),
                toBlock: numberToHex(interval[1]),
              },
            ],
            context,
          ).catch((error) => {
            // Note: skip eth_getLogs range retry logic if the chain
            // has a custom block range.
            if (args.chain.ethGetLogsBlockRange !== undefined) {
              throw error;
            }
            const getLogsErrorResponse = getLogsRetryHelper({
              params: [
                {
                  address,
                  topics,
                  fromBlock: toHex(interval[0]),
                  toBlock: toHex(interval[1]),
                },
              ],
              error: error as RpcError,
            });
            if (getLogsErrorResponse.shouldRetry === false) throw error;
            const range =
              hexToNumber(getLogsErrorResponse.ranges[0]!.toBlock) -
              hexToNumber(getLogsErrorResponse.ranges[0]!.fromBlock);
            args.common.logger.debug({
              msg: "Updated eth_getLogs range",
              chain: args.chain.name,
              chain_id: args.chain.id,
              range,
            });
            logsRequestMetadata = {
              estimatedRange: range,
              confirmedRange: getLogsErrorResponse.isSuggestedRange
                ? range
                : undefined,
            };
            return syncLogsDynamic(
              { address, topic0, topic1, topic2, topic3, interval },
              context,
            );
          }),
        ),
      ),
    ).then((logs) => logs.flat());
    /**
     * Dynamically increase the range used in "eth_getLogs" if an
     * error has been received but the error didn't suggest a range.
     */
    if (logsRequestMetadata.confirmedRange === undefined) {
      logsRequestMetadata.estimatedRange = Math.round(
        logsRequestMetadata.estimatedRange * 1.05,
      );
    }
    return logs;
  };
  const syncTransactionReceipts = async (
    block: SyncBlock,
    transactionHashes: Set<Hash>,
    context?: Parameters<Rpc["request"]>[1],
  ): Promise<SyncTransactionReceipt[]> => {
    if (transactionHashes.size === 0) {
      return [];
    }
    if (isBlockReceipts === false) {
      const transactionReceipts = await Promise.all(
        Array.from(transactionHashes).map(async (hash) => {
          const receipt = await eth_getTransactionReceipt(
            args.rpc,
            [hash],
            context,
          );
          validateReceiptsAndBlock(
            [receipt],
            block,
            {
              method: "eth_getTransactionReceipt",
              params: [hash],
            },
            {
              method: "eth_getBlockByNumber",
              params: [block.number, true],
            },
          );
          return receipt;
        }),
      );
      return transactionReceipts;
    }
    let blockReceipts: SyncTransactionReceipt[];
    try {
      blockReceipts = await eth_getBlockReceipts(
        args.rpc,
        [block.hash],
        context,
      );
    } catch (_error) {
      const error = _error as Error;
      args.common.logger.warn({
        msg: "Caught eth_getBlockReceipts error, switching to eth_getTransactionReceipt method",
        action: "fetch_block_data",
        chain: args.chain.name,
        chain_id: args.chain.id,
        error,
      });
      isBlockReceipts = false;
      return syncTransactionReceipts(block, transactionHashes, context);
    }
    validateReceiptsAndBlock(
      blockReceipts,
      block,
      {
        method: "eth_getBlockReceipts",
        params: [block.hash],
      },
      {
        method: "eth_getBlockByNumber",
        params: [block.number, true],
      },
    );
    const transactionReceipts = blockReceipts.filter((receipt) =>
      transactionHashes.has(receipt.transactionHash),
    );
    return transactionReceipts;
  };
  /**
   * Fetch child addresses for `factory` within `interval`
   *
   * @dev Newly fetched child addresses are added into `args.childAddresses`
   */
  const syncAddressFactory = async (
    factory: Factory,
    interval: Interval,
    context?: Parameters<Rpc["request"]>[1],
  ): Promise<Map<Address, number>> => {
    const logs = await syncLogsDynamic(
      {
        address: factory.address,
        topic0: factory.eventSelector,
        interval,
      },
      context,
    );
    const childAddresses = new Map<Address, number>();
    const childAddressesRecord = args.childAddresses.get(factory.id)!;
    const childAddressDecodeFailureIds = new Set<string>();
    let childAddressDecodeFailureCount = 0;
    let childAddressDecodeSuccessCount = 0;
    for (const log of logs) {
      if (isLogFactoryMatched({ factory, log })) {
        let address: Address;
        try {
          address = getChildAddress({ log, factory });
          childAddressDecodeSuccessCount++;
        } catch (error) {
          if (factory.address === undefined) {
            childAddressDecodeFailureCount++;
            if (childAddressDecodeFailureIds.has(factory.id) === false) {
              childAddressDecodeFailureIds.add(factory.id);
              args.common.logger.debug({
                msg: "Failed to extract child address from log matched by factory using the provided ABI item",
                chain: args.chain.name,
                chain_id: args.chain.id,
                factory: factory.sourceId,
                block_number: hexToNumber(log.blockNumber),
                log_index: hexToNumber(log.logIndex),
                data: log.data,
                topics: JSON.stringify(log.topics),
              });
            }
            continue;
          } else {
            throw error;
          }
        }
        const existingBlockNumber = childAddressesRecord.get(address);
        const newBlockNumber = hexToNumber(log.blockNumber);
        if (
          existingBlockNumber === undefined ||
          existingBlockNumber > newBlockNumber
        ) {
          childAddresses.set(address, newBlockNumber);
          childAddressesRecord.set(address, newBlockNumber);
        }
      }
    }
    if (childAddressDecodeFailureCount > 0) {
      args.common.logger.debug({
        msg: "Logs matched by factory contained child addresses that could not be extracted",
        failure_count: childAddressDecodeFailureCount,
        success_count: childAddressDecodeSuccessCount,
      });
    }
    return childAddresses;
  };
  return {
    async syncBlockRangeData({
      interval,
      requiredIntervals,
      requiredFactoryIntervals,
      syncStore,
    }) {
      const context = {
        logger: args.common.logger.child({ action: "fetch_block_data" }),
      };
      const endClock = startClock();
      const childAddresses: ChildAddresses = new Map();
      // Dedupe factory intervals by factory id
      const factoryIntervalsById: Map<
        Factory["id"],
        { factory: Factory; interval: Interval }
      > = new Map();
      for (const { factory, interval } of requiredFactoryIntervals) {
        if (factoryIntervalsById.has(factory.id)) {
          const existingInterval = factoryIntervalsById.get(
            factory.id,
          )!.interval;
          factoryIntervalsById.get(factory.id)!.interval = intervalBounds([
            existingInterval,
            interval,
          ]);
        } else {
          factoryIntervalsById.set(factory.id, { factory, interval });
        }
      }
      requiredFactoryIntervals = Array.from(factoryIntervalsById.values());
      await Promise.all(
        requiredFactoryIntervals.map(async ({ factory, interval }) => {
          childAddresses.set(
            factory.id,
            await syncAddressFactory(factory, interval, context)!,
          );
        }),
      );
      const mergedEthGetLogsParams: Map<string, EthGetLogsParams> = new Map();
      const singleEthGetLogsParams: EthGetLogsParams[] = [];
      for (const { filter, interval } of requiredIntervals) {
        if (filter.type !== "log") continue;
        const hasAddress = filter.address !== undefined;
        const hasTopic1 = filter.topic1 !== undefined;
        const hasTopic2 = filter.topic2 !== undefined;
        const hasTopic3 = filter.topic3 !== undefined;
        if (hasAddress === false || hasTopic1 || hasTopic2 || hasTopic3) {
          if (isAddressFactory(filter.address)) {
            const childAddresses = args.childAddresses.get(filter.address.id)!;
            singleEthGetLogsParams.push({
              address:
                childAddresses.size >=
                args.common.options.factoryAddressCountThreshold
                  ? undefined
                  : Array.from(childAddresses.keys()),
              topic0: filter.topic0,
              topic1: filter.topic1,
              topic2: filter.topic2,
              topic3: filter.topic3,
              interval,
            });
          } else {
            singleEthGetLogsParams.push({
              address: filter.address,
              topic0: filter.topic0,
              topic1: filter.topic1,
              topic2: filter.topic2,
              topic3: filter.topic3,
              interval,
            });
          }
          continue;
        }
        let addressKey: string;
        if (isAddressFactory(filter.address)) {
          addressKey = filter.address.id;
        } else if (Array.isArray(filter.address)) {
          addressKey = filter.address.join("_");
        } else {
          addressKey = filter.address as Address;
        }
        if (mergedEthGetLogsParams.has(addressKey) === false) {
          if (isAddressFactory(filter.address)) {
            const childAddresses = args.childAddresses.get(filter.address.id)!;
            mergedEthGetLogsParams.set(addressKey, {
              address:
                childAddresses.size >=
                args.common.options.factoryAddressCountThreshold
                  ? undefined
                  : Array.from(childAddresses.keys()),
              topic0: filter.topic0,
              topic1: filter.topic1,
              topic2: filter.topic2,
              topic3: filter.topic3,
              interval,
            });
          } else {
            mergedEthGetLogsParams.set(addressKey, {
              address: filter.address,
              topic0: filter.topic0,
              topic1: filter.topic1,
              topic2: filter.topic2,
              topic3: filter.topic3,
              interval,
            });
          }
        } else {
          const existingInterval =
            mergedEthGetLogsParams.get(addressKey)!.interval;
          const existingTopic0 = mergedEthGetLogsParams.get(addressKey)!
            .topic0 as Hex | Hex[];
          if (Array.isArray(existingTopic0)) {
            if (existingTopic0.includes(filter.topic0) === false) {
              mergedEthGetLogsParams.get(addressKey)!.topic0 = [
                ...existingTopic0,
                filter.topic0,
              ];
            }
          } else {
            if (existingTopic0 !== filter.topic0) {
              mergedEthGetLogsParams.get(addressKey)!.topic0 = [
                existingTopic0,
                filter.topic0,
              ];
            }
          }
          mergedEthGetLogsParams.get(addressKey)!.interval = intervalBounds([
            existingInterval,
            interval,
          ]);
        }
      }
      const ethGetLogsParams = dedupe(
        [
          ...singleEthGetLogsParams,
          ...Array.from(mergedEthGetLogsParams.values()),
        ],
        (params) => JSON.stringify(params),
      );
      let logs: SyncLog[] = [];
      await Promise.all(
        ethGetLogsParams.map(async (params) => {
          const _logs = await syncLogsDynamic(params, context);
          logs.push(..._logs);
        }),
      );
      // Remove duplicate logs that may have resulted from intersecting
      // filters.
      logs = dedupe(logs, (log) => `${log.blockNumber}_${log.logIndex}`);
      for (const log of logs) {
        if (log.transactionHash === zeroHash) {
          args.common.logger.warn({
            msg: "Detected log with empty transaction hash. This is expected for some chains like ZKsync.",
            action: "fetch_block_data",
            chain: args.chain.name,
            chain_id: args.chain.id,
            number: hexToNumber(log.blockNumber),
            hash: log.blockHash,
            logIndex: hexToNumber(log.logIndex),
          });
        }
      }
      let childAddressCount = 0;
      for (const { size } of childAddresses.values()) {
        childAddressCount += size;
      }
      args.common.logger.debug(
        {
          msg: "Fetched block range data",
          chain: args.chain.name,
          chain_id: args.chain.id,
          block_range: JSON.stringify(interval),
          log_count: logs.length,
          child_address_count: childAddressCount,
          duration: endClock(),
        },
        ["chain", "block_range"],
      );
      await promiseAllSettledWithThrow(
        Array.from(childAddresses.entries()).map(
          ([factoryId, childAddresses]) =>
            syncStore.insertChildAddresses(
              {
                factory: factoryIntervalsById.get(factoryId)!.factory,
                childAddresses,
                chainId: args.chain.id,
              },
              context,
            ),
        ),
      );
      return logs;
    },
    async syncBlockData({ syncStore, interval, requiredIntervals, logs }) {
      const context = {
        logger: args.common.logger.child({ action: "fetch_block_data" }),
      };
      const endClock = startClock();
      const blockFilters: BlockFilter[] = [];
      const transactionFilters: TransactionFilter[] = [];
      const traceFilters: TraceFilter[] = [];
      const logFilters: LogFilter[] = [];
      const transferFilters: TransferFilter[] = [];
      for (const { filter } of requiredIntervals) {
        switch (filter.type) {
          case "block": {
            blockFilters.push(filter as BlockFilter);
            break;
          }
          case "transaction": {
            transactionFilters.push(filter as TransactionFilter);
            break;
          }
          case "trace": {
            traceFilters.push(filter as TraceFilter);
            break;
          }
          case "log": {
            logFilters.push(filter as LogFilter);
            break;
          }
          case "transfer": {
            transferFilters.push(filter as TransferFilter);
            break;
          }
        }
      }
      const perBlockLogs = new Map<number, SyncLog[]>();
      for (const log of logs) {
        const blockNumber = hexToNumber(log.blockNumber);
        if (perBlockLogs.has(blockNumber) === false) {
          perBlockLogs.set(blockNumber, []);
        }
        perBlockLogs.get(blockNumber)!.push(log);
      }
      let closestToTipBlock: SyncBlock | undefined;
      const syncBlockData = async (blockNumber: number) => {
        let block: SyncBlock | undefined;
        const requiredTransactions = new Set<Hash>();
        const requiredTransactionReceipts = new Set<Hash>();
        ////////
        // Logs
        ////////
        let logs: SyncLog[] = [];
        if (perBlockLogs.has(blockNumber)) {
          block = await eth_getBlockByNumber(
            args.rpc,
            [numberToHex(blockNumber), true],
            context,
          );
          logs = perBlockLogs.get(blockNumber)!.filter((log) => {
            let isMatched = false;
            for (const filter of logFilters) {
              if (
                isLogFilterMatched({ filter, log }) &&
                (isAddressFactory(filter.address)
                  ? isAddressMatched({
                      address: log.address,
                      blockNumber,
                      childAddresses: args.childAddresses.get(
                        filter.address.id,
                      )!,
                    })
                  : true)
              ) {
                isMatched = true;
                requiredTransactions.add(log.transactionHash);
                if (filter.hasTransactionReceipt) {
                  requiredTransactionReceipts.add(log.transactionHash);
                  // skip to next log
                  break;
                }
              }
            }
            return isMatched;
          });
          if (logs.length > 0) {
            // Note: `logsRequest` could be more accurate by tracking the exact
            // request made to include `address` and `topics`.
            validateLogsAndBlock(
              logs,
              block,
              {
                method: "eth_getLogs",
                params: [
                  {
                    fromBlock: toHex(blockNumber),
                    toBlock: toHex(blockNumber),
                  },
                ],
              },
              {
                method: "eth_getBlockByNumber",
                params: [toHex(blockNumber), true],
              },
            );
          }
        }
        ////////
        // Traces
        ////////
        const shouldRequestTraces =
          traceFilters.some((filter) => isBlockInFilter(filter, blockNumber)) ||
          transferFilters.some((filter) =>
            isBlockInFilter(filter, blockNumber),
          );
        let traces: SyncTrace[] = [];
        if (shouldRequestTraces) {
          if (block === undefined) {
            [block, traces] = await Promise.all([
              eth_getBlockByNumber(
                args.rpc,
                [numberToHex(blockNumber), true],
                context,
              ),
              debug_traceBlockByNumber(
                args.rpc,
                [numberToHex(blockNumber), { tracer: "callTracer" }],
                context,
              ),
            ]);
          } else {
            traces = await debug_traceBlockByNumber(
              args.rpc,
              [numberToHex(blockNumber), { tracer: "callTracer" }],
              context,
            );
          }
          traces = traces.filter((trace) => {
            let isMatched = false;
            for (const filter of transferFilters) {
              if (
                isTransferFilterMatched({
                  filter,
                  trace: trace.trace,
                  block: { number: BigInt(blockNumber) },
                }) &&
                (isAddressFactory(filter.fromAddress)
                  ? isAddressMatched({
                      address: trace.trace.from,
                      blockNumber,
                      childAddresses: args.childAddresses.get(
                        filter.fromAddress.id,
                      )!,
                    })
                  : true) &&
                (isAddressFactory(filter.toAddress)
                  ? isAddressMatched({
                      address: trace.trace.to,
                      blockNumber,
                      childAddresses: args.childAddresses.get(
                        filter.toAddress.id,
                      )!,
                    })
                  : true)
              ) {
                isMatched = true;
                requiredTransactions.add(trace.transactionHash);
                if (filter.hasTransactionReceipt) {
                  requiredTransactionReceipts.add(trace.transactionHash);
                  // skip to next trace
                  break;
                }
              }
            }
            for (const filter of traceFilters) {
              if (
                isTraceFilterMatched({
                  filter,
                  trace: trace.trace,
                  block: { number: BigInt(blockNumber) },
                }) &&
                (isAddressFactory(filter.fromAddress)
                  ? isAddressMatched({
                      address: trace.trace.from,
                      blockNumber,
                      childAddresses: args.childAddresses.get(
                        filter.fromAddress.id,
                      )!,
                    })
                  : true) &&
                (isAddressFactory(filter.toAddress)
                  ? isAddressMatched({
                      address: trace.trace.to,
                      blockNumber,
                      childAddresses: args.childAddresses.get(
                        filter.toAddress.id,
                      )!,
                    })
                  : true)
              ) {
                isMatched = true;
                requiredTransactions.add(trace.transactionHash);
                if (filter.hasTransactionReceipt) {
                  requiredTransactionReceipts.add(trace.transactionHash);
                  // skip to next trace
                  break;
                }
              }
            }
            return isMatched;
          });
          if (traces.length > 0) {
            validateTracesAndBlock(
              traces,
              block,
              {
                method: "debug_traceBlockByNumber",
                params: [toHex(blockNumber), { tracer: "callTracer" }],
              },
              {
                method: "eth_getBlockByNumber",
                params: [toHex(blockNumber), true],
              },
            );
          }
        }
        ////////
        // Block
        ////////
        if (
          block === undefined &&
          blockFilters.some((filter) =>
            isBlockFilterMatched({
              filter,
              block: { number: BigInt(blockNumber) },
            }),
          )
        ) {
          block = await eth_getBlockByNumber(
            args.rpc,
            [numberToHex(blockNumber), true],
            context,
          );
        }
        ////////
        // Transactions
        ////////
        // Return early if no data is fetched
        if (
          block === undefined &&
          transactionFilters.some((filter) =>
            isBlockInFilter(filter, blockNumber),
          ) === false
        ) {
          return;
        }
        if (block === undefined) {
          block = await eth_getBlockByNumber(
            args.rpc,
            [numberToHex(blockNumber), true],
            context,
          );
        }
        if (
          closestToTipBlock === undefined ||
          hexToNumber(block.number) > hexToNumber(closestToTipBlock.number)
        ) {
          closestToTipBlock = block;
        }
        const transactions = block.transactions.filter((transaction) => {
          let isMatched = requiredTransactions.has(transaction.hash);
          for (const filter of transactionFilters) {
            if (
              isTransactionFilterMatched({ filter, transaction }) &&
              (isAddressFactory(filter.fromAddress)
                ? isAddressMatched({
                    address: transaction.from,
                    blockNumber,
                    childAddresses: args.childAddresses.get(
                      filter.fromAddress.id,
                    )!,
                  })
                : true) &&
              (isAddressFactory(filter.toAddress)
                ? isAddressMatched({
                    address: transaction.to ?? undefined,
                    blockNumber,
                    childAddresses: args.childAddresses.get(
                      filter.toAddress.id,
                    )!,
                  })
                : true)
            ) {
              requiredTransactionReceipts.add(transaction.hash);
              isMatched = true;
            }
          }
          return isMatched;
        });
        if (transactions.length > 0) {
          validateTransactionsAndBlock(block, {
            method: "eth_getBlockByNumber",
            params: [toHex(blockNumber), true],
          });
        }
        const transactionsByHash = new Map<Hash, SyncTransaction>();
        for (const transaction of transactions) {
          transactionsByHash.set(transaction.hash, transaction);
        }
        ////////
        // Transaction Receipts
        ////////
        const transactionReceipts = await syncTransactionReceipts(
          block,
          requiredTransactionReceipts,
        );
        blockCount += 1;
        transactionCount += transactions.length;
        receiptCount += transactionReceipts.length;
        traceCount += traces.length;
        // Free memory of all unused transactions
        block.transactions = transactions;
        await promiseAllSettledWithThrow([
          syncStore.insertBlocks({ blocks: [block], chainId: args.chain.id }),
          syncStore.insertTransactions({
            transactions,
            chainId: args.chain.id,
          }),
          syncStore.insertTransactionReceipts({
            transactionReceipts,
            chainId: args.chain.id,
          }),
          syncStore.insertTraces({
            traces: traces.map((trace) => ({
              trace,
              block: block!,
              transaction: transactionsByHash.get(trace.transactionHash)!,
            })),
            chainId: args.chain.id,
          }),
          syncStore.insertLogs({ logs, chainId: args.chain.id }),
        ]);
      };
      let blockCount = 0;
      let transactionCount = 0;
      let receiptCount = 0;
      let traceCount = 0;
      // Same memory usage as `sync-realtime`.
      const MAX_BLOCKS_IN_MEM = Math.max(
        args.chain.finalityBlockCount * 2,
        100,
      );
      if (requiredIntervals.length > 0) {
        const queue = createQueue({
          browser: false,
          initialStart: true,
          concurrency: MAX_BLOCKS_IN_MEM,
          worker: syncBlockData,
        });
        await Promise.all(
          intervalRange(interval).map((blockNumber) => queue.add(blockNumber)),
        );
      }
      args.common.logger.debug(
        {
          msg: "Fetched block data",
          chain: args.chain.name,
          chain_id: args.chain.id,
          block_range: JSON.stringify(interval),
          block_count: blockCount,
          transaction_count: transactionCount,
          receipt_count: receiptCount,
          trace_count: traceCount,
          duration: endClock(),
        },
        ["chain", "block_range"],
      );
      return closestToTipBlock;
    },
  };
};
</file>

<file path="packages/core/src/sync-realtime/bloom.test.ts">
import { EMPTY_LOG_FILTER } from "@/_test/constants.js";
import type { LogFactory, LogFilter } from "@/internal/types.js";
import type { Hex } from "viem";
import { expect, test } from "vitest";
import { isFilterInBloom, isInBloom } from "./bloom.js";
test("isInBloom", () => {
  let bloom =
    "0x00000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002020000000000000000000000000000000000000000000008000000001000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000" as Hex;
  const address = "0xef2d6d194084c2de36e0dabfce45d046b37d1106";
  let topic =
    "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc" as Hex;
  expect(isInBloom(bloom, address)).toBe(true);
  expect(isInBloom(bloom, topic)).toBe(true);
  bloom =
    "0x00000000000000000000008000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000044000200000000000000000002000000000000000000000040000000000000000000000000000020000000000000000000800000000000800000000000800000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000808002000000000400000000000000000000000060000000000000000000000000000000000000000000000100000000000002000000" as Hex;
  expect(isInBloom(bloom, address)).toBe(false);
  expect(isInBloom(bloom, topic)).toBe(false);
  topic =
    "0x4a39dc06d4c0dbc64b70af90fd698a233a518aa5d07e595d983b8c0526c8f7fb" as Hex;
  expect(isInBloom(bloom, topic)).toBe(true);
});
test("isFilterInBloom returns false for out of range blocks", () => {
  const block = {
    number: "0x5",
    logsBloom:
      "0x00000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002020000000000000000000000000000000000000000000008000000001000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
  } as const;
  const filter = {
    ...EMPTY_LOG_FILTER,
    fromBlock: 10,
    toBlock: 20,
  } satisfies LogFilter;
  expect(isFilterInBloom({ block, filter })).toBe(false);
});
test("isFilterInBloom returns false for missing topics", () => {
  const block = {
    number: "0x5",
    logsBloom:
      "0x00000000000000000000008000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000044000200000000000000000002000000000000000000000040000000000000000000000000000020000000000000000000800000000000800000000000800000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000808002000000000400000000000000000000000060000000000000000000000000000000000000000000000100000000000002000000",
  } as const;
  const filter = {
    ...EMPTY_LOG_FILTER,
    topic0:
      "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
  } satisfies LogFilter;
  expect(isFilterInBloom({ block, filter })).toBe(false);
});
test("isFilterInBloom returns true for undefined address", () => {
  const block = {
    number: "0x5",
    logsBloom:
      "0x00000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002020000000000000000000000000000000000000000000008000000001000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
  } as const;
  const filter: LogFilter = {
    ...EMPTY_LOG_FILTER,
    // @ts-expect-error
    topic0: undefined,
  };
  expect(isFilterInBloom({ block, filter })).toBe(true);
});
test("isFilterInBloom returns true for factory with new child address", () => {
  const block = {
    number: "0x5",
    logsBloom:
      "0x00000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002020000000000000000000000000000000000000000000008000000001000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
  } as const;
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: {
      id: `log_${"0xef2d6d194084c2de36e0dabfce45d046b37d1106"}_${1}_topic${1}_${"0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc"}_${"undefined"}_${"undefined"}`,
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
      eventSelector:
        "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
      childAddressLocation: "topic1",
      fromBlock: undefined,
      toBlock: undefined,
    } satisfies LogFactory,
  } satisfies LogFilter;
  expect(isFilterInBloom({ block, filter })).toBe(true);
});
test("isFilterInBloom returns true for factory without new child address", () => {
  const block = {
    number: "0x5",
    logsBloom:
      "0x00000000000000000000008000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000044000200000000000000000002000000000000000000000040000000000000000000000000000020000000000000000000800000000000800000000000800000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000808002000000000400000000000000000000000060000000000000000000000000000000000000000000000100000000000002000000",
  } as const;
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: {
      id: `log_${"0xef2d6d194084c2de36e0dabfce45d046b37d1106"}_${1}_topic${1}_${"0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc"}_${"undefined"}_${"undefined"}`,
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
      eventSelector:
        "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
      childAddressLocation: "topic1",
      fromBlock: undefined,
      toBlock: undefined,
    } satisfies LogFactory,
    topic0:
      "0x4a39dc06d4c0dbc64b70af90fd698a233a518aa5d07e595d983b8c0526c8f7fb",
  } satisfies LogFilter;
  expect(isFilterInBloom({ block, filter })).toBe(true);
});
test("isFilterInBloom returns true for array of addresses", () => {
  const block = {
    number: "0x5",
    logsBloom:
      "0x00000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002020000000000000000000000000000000000000000000008000000001000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
  } as const;
  const filter: LogFilter = {
    ...EMPTY_LOG_FILTER,
    address: ["0xef2d6d194084c2de36e0dabfce45d046b37d1106"],
    // @ts-expect-error
    topic0: undefined,
  };
  expect(isFilterInBloom({ block, filter })).toBe(true);
});
</file>

<file path="packages/core/src/sync-realtime/bloom.ts">
import type { LogFilter, SyncBlock } from "@/internal/types.js";
import {
  getFilterFromBlock,
  getFilterToBlock,
  isAddressFactory,
} from "@/runtime/filter.js";
import { type Hex, hexToBytes, hexToNumber, keccak256 } from "viem";
export const zeroLogsBloom =
  "0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000";
const BLOOM_SIZE_BYTES = 256;
export const isInBloom = (_bloom: Hex, input: Hex): boolean => {
  const bloom = hexToBytes(_bloom);
  const hash = hexToBytes(keccak256(input));
  for (const i of [0, 2, 4]) {
    const bit = (hash[i + 1]! + (hash[i]! << 8)) & 0x7ff;
    if (
      (bloom[BLOOM_SIZE_BYTES - 1 - Math.floor(bit / 8)]! &
        (1 << (bit % 8))) ===
      0
    )
      return false;
  }
  return true;
};
/**
 * Return true if `filter` is in `bloom`.
 *
 * A filter with an address of type `LogFactory` is matched
 * if the address filter is matched (new child contract) or the log
 * filter is matched (log on child contract).
 *
 * Note: False positives are possible.
 */
export function isFilterInBloom({
  block,
  filter,
}: {
  block: Pick<SyncBlock, "number" | "logsBloom">;
  filter: LogFilter;
}): boolean {
  // Return `false` for out of range blocks
  if (
    hexToNumber(block.number) < getFilterFromBlock(filter) ||
    hexToNumber(block.number) > getFilterToBlock(filter)
  ) {
    return false;
  }
  const isTopicsInBloom = [
    filter.topic0,
    filter.topic1,
    filter.topic2,
    filter.topic3,
  ].every((topic) => {
    if (topic === null || topic === undefined) {
      return true;
    } else if (Array.isArray(topic)) {
      return topic.some((t) => isInBloom(block.logsBloom, t));
    } else {
      return isInBloom(block.logsBloom, topic);
    }
  });
  let isAddressInBloom: boolean;
  if (filter.address === undefined) isAddressInBloom = true;
  else if (isAddressFactory(filter.address)) {
    // Return true if the `Factory` is matched.
    if (
      (filter.address.address === undefined ||
        (Array.isArray(filter.address.address)
          ? filter.address.address.some((address) =>
              isInBloom(block.logsBloom, address),
            )
          : isInBloom(block.logsBloom, filter.address.address))) &&
      isInBloom(block.logsBloom, filter.address.eventSelector)
    ) {
      return true;
    }
    isAddressInBloom = true;
  } else if (Array.isArray(filter.address)) {
    if (filter.address.length === 0) {
      isAddressInBloom = true;
    } else {
      isAddressInBloom = filter.address.some((address) =>
        isInBloom(block.logsBloom, address),
      );
    }
  } else {
    // single address case
    isAddressInBloom = isInBloom(block.logsBloom, filter.address);
  }
  return isAddressInBloom && isTopicsInBloom;
}
</file>

<file path="packages/core/src/sync-realtime/index.test.ts">
import { ALICE, BOB } from "@/_test/constants.js";
import {
  context,
  setupAnvil,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import {
  createPair,
  deployErc20,
  deployFactory,
  mintErc20,
  simulateBlock,
  swapPair,
  transferErc20,
  transferEth,
} from "@/_test/simulate.js";
import {
  getAccountsIndexingBuild,
  getBlocksIndexingBuild,
  getChain,
  getErc20IndexingBuild,
  getPairWithFactoryIndexingBuild,
} from "@/_test/utils.js";
import type { LogFactory, LogFilter } from "@/internal/types.js";
import { eth_getBlockByNumber } from "@/rpc/actions.js";
import { createRpc } from "@/rpc/index.js";
import { drainAsyncGenerator } from "@/utils/generators.js";
import { parseEther } from "viem";
import { beforeEach, expect, test, vi } from "vitest";
import { type RealtimeSyncEvent, createRealtimeSync } from "./index.js";
beforeEach(setupCommon);
beforeEach(setupAnvil);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
test("createRealtimeSync()", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ common, chain });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  expect(realtimeSync).toBeDefined();
});
test("sync() handles block", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  const blockData = await simulateBlock();
  const syncResult = await drainAsyncGenerator(
    realtimeSync.sync(blockData.block),
  );
  expect(syncResult).toHaveLength(1);
  expect(syncResult[0]!.type).toBe("block");
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(1);
});
test("sync() no-op when receiving same block twice", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain();
  const rpc = createRpc({ chain, common });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  const blockData = await simulateBlock();
  await drainAsyncGenerator(realtimeSync.sync(blockData.block));
  const syncResult = await drainAsyncGenerator(
    realtimeSync.sync(blockData.block),
  );
  expect(syncResult).toHaveLength(0);
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(1);
});
test("sync() gets missing block", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({ common, chain });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  await simulateBlock();
  const blockData = await simulateBlock();
  const syncResult = await drainAsyncGenerator(
    realtimeSync.sync(blockData.block),
  );
  expect(syncResult).toHaveLength(2);
  expect(syncResult[0]!.type).toBe("block");
  expect(syncResult[1]!.type).toBe("block");
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(2);
});
test("sync() catches error", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({ common, chain });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  const blockData = await simulateBlock();
  const requestSpy = vi.spyOn(rpc, "request");
  requestSpy.mockRejectedValueOnce(new Error());
  const syncResult = await drainAsyncGenerator(
    realtimeSync.sync(blockData.block),
  );
  expect(syncResult).toHaveLength(0);
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(0);
});
test("handleBlock() block event with log", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({ common, chain });
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getErc20IndexingBuild({
    address,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x1", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  const block = await eth_getBlockByNumber(rpc, ["0x2", true]);
  const syncResult = await drainAsyncGenerator(realtimeSync.sync(block));
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(1);
  expect(syncResult).toHaveLength(1);
  expect(syncResult[0]).toStrictEqual({
    type: "block",
    blockCallback: undefined,
    hasMatchedFilter: true,
    block: expect.any(Object),
    logs: expect.any(Object),
    transactions: expect.any(Object),
    traces: expect.any(Object),
    transactionReceipts: expect.any(Object),
    childAddresses: expect.any(Object),
  });
  expect(
    (syncResult[0] as Extract<RealtimeSyncEvent, { type: "block" }>)?.block
      .number,
  ).toBe("0x2");
  expect(
    (syncResult[0] as Extract<RealtimeSyncEvent, { type: "block" }>)?.logs,
  ).toHaveLength(1);
  expect(
    (syncResult[0] as Extract<RealtimeSyncEvent, { type: "block" }>)?.traces,
  ).toHaveLength(0);
  expect(
    (syncResult[0] as Extract<RealtimeSyncEvent, { type: "block" }>)
      ?.transactions,
  ).toHaveLength(1);
});
test("handleBlock() block event with log factory", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({ common, chain });
  const { address } = await deployFactory({ sender: ALICE });
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  await swapPair({
    pair,
    amount0Out: 1n,
    amount1Out: 1n,
    to: ALICE,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({
    address,
  });
  const filter = eventCallbacks[0]!.filter as LogFilter<LogFactory>;
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x1", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map([[filter.address.id, new Map()]]),
  });
  let block = await eth_getBlockByNumber(rpc, ["0x2", true]);
  const syncResult1 = await drainAsyncGenerator(realtimeSync.sync(block));
  block = await eth_getBlockByNumber(rpc, ["0x3", true]);
  const syncResult2 = await drainAsyncGenerator(realtimeSync.sync(block));
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(2);
  expect(syncResult1).toHaveLength(1);
  expect(syncResult2).toHaveLength(1);
  const data = [...syncResult1, ...syncResult2] as Extract<
    RealtimeSyncEvent,
    { type: "block" }
  >[];
  expect(data[0]).toStrictEqual({
    type: "block",
    blockCallback: undefined,
    hasMatchedFilter: false,
    block: expect.any(Object),
    logs: expect.any(Object),
    transactions: expect.any(Object),
    traces: expect.any(Object),
    transactionReceipts: expect.any(Object),
    childAddresses: expect.any(Object),
  });
  expect(data[1]).toStrictEqual({
    type: "block",
    blockCallback: undefined,
    hasMatchedFilter: true,
    block: expect.any(Object),
    logs: expect.any(Object),
    transactions: expect.any(Object),
    traces: expect.any(Object),
    transactionReceipts: expect.any(Object),
    childAddresses: expect.any(Object),
  });
  expect(data[0]?.block.number).toBe("0x2");
  expect(data[1]?.block.number).toBe("0x3");
  expect(data[0]?.logs).toHaveLength(0);
  expect(data[1]?.logs).toHaveLength(1);
  expect(data[0]?.childAddresses).toMatchObject(
    new Map([
      [
        {
          address: "0x5fbdb2315678afecb367f032d93f642f64180aa3",
          chainId: 1,
          childAddressLocation: "topic1",
          eventSelector:
            "0x17aa8d0e85db1d0531a8181b5bb84e1d4ed744db1cadd8814acd3d181ff30137",
          fromBlock: undefined,
          id: "log_0x5fbdb2315678afecb367f032d93f642f64180aa3_1_topic1_0x17aa8d0e85db1d0531a8181b5bb84e1d4ed744db1cadd8814acd3d181ff30137_undefined_undefined",
          sourceId: "Pair",
          toBlock: undefined,
          type: "log",
        },
        new Set(["0xa16e02e87b7454126e5e10d957a927a7f5b5d2be"]),
      ],
    ]),
  );
  expect(data[1]?.childAddresses).toMatchObject(
    new Map([
      [
        {
          address: "0x5fbdb2315678afecb367f032d93f642f64180aa3",
          chainId: 1,
          childAddressLocation: "topic1",
          eventSelector:
            "0x17aa8d0e85db1d0531a8181b5bb84e1d4ed744db1cadd8814acd3d181ff30137",
          fromBlock: undefined,
          id: "log_0x5fbdb2315678afecb367f032d93f642f64180aa3_1_topic1_0x17aa8d0e85db1d0531a8181b5bb84e1d4ed744db1cadd8814acd3d181ff30137_undefined_undefined",
          sourceId: "Pair",
          toBlock: undefined,
          type: "log",
        },
        new Set(),
      ],
    ]),
  );
  expect(data[0]?.traces).toHaveLength(0);
  expect(data[1]?.traces).toHaveLength(0);
  expect(data[0]?.transactions).toHaveLength(0);
  expect(data[1]?.transactions).toHaveLength(1);
});
test("handleBlock() block event with log factory and no address", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({ common, chain });
  const { address } = await deployFactory({ sender: ALICE });
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  await swapPair({
    pair,
    amount0Out: 1n,
    amount1Out: 1n,
    to: ALICE,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({
    address,
  });
  const filter = eventCallbacks[0]!.filter as LogFilter<LogFactory>;
  filter.address.address = undefined;
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x1", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map([[filter.address.id, new Map()]]),
  });
  let block = await eth_getBlockByNumber(rpc, ["0x2", true]);
  const syncResult1 = await drainAsyncGenerator(realtimeSync.sync(block));
  block = await eth_getBlockByNumber(rpc, ["0x3", true]);
  const syncResult2 = await drainAsyncGenerator(realtimeSync.sync(block));
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(2);
  expect(syncResult1).toHaveLength(1);
  expect(syncResult2).toHaveLength(1);
  const data = [...syncResult1, ...syncResult2] as Extract<
    RealtimeSyncEvent,
    { type: "block" }
  >[];
  expect(data[0]).toStrictEqual({
    type: "block",
    blockCallback: undefined,
    hasMatchedFilter: false,
    block: expect.any(Object),
    logs: expect.any(Object),
    transactions: expect.any(Object),
    traces: expect.any(Object),
    transactionReceipts: expect.any(Object),
    childAddresses: expect.any(Object),
  });
  expect(data[1]).toStrictEqual({
    type: "block",
    blockCallback: undefined,
    hasMatchedFilter: true,
    block: expect.any(Object),
    logs: expect.any(Object),
    transactions: expect.any(Object),
    traces: expect.any(Object),
    transactionReceipts: expect.any(Object),
    childAddresses: expect.any(Object),
  });
  expect(data[0]?.block.number).toBe("0x2");
  expect(data[1]?.block.number).toBe("0x3");
  expect(data[0]?.logs).toHaveLength(0);
  expect(data[1]?.logs).toHaveLength(1);
  expect(data[0]?.childAddresses.size).toBe(1);
  expect(data[0]?.childAddresses).toMatchObject(
    new Map([
      [
        {
          address: undefined,
          chainId: 1,
          childAddressLocation: "topic1",
          eventSelector:
            "0x17aa8d0e85db1d0531a8181b5bb84e1d4ed744db1cadd8814acd3d181ff30137",
          fromBlock: undefined,
          id: "log_0x5fbdb2315678afecb367f032d93f642f64180aa3_1_topic1_0x17aa8d0e85db1d0531a8181b5bb84e1d4ed744db1cadd8814acd3d181ff30137_undefined_undefined",
          sourceId: "Pair",
          toBlock: undefined,
          type: "log",
        },
        new Set(["0xa16e02e87b7454126e5e10d957a927a7f5b5d2be"]),
      ],
    ]),
  );
  expect(data[1]?.childAddresses).toMatchObject(
    new Map([
      [
        {
          address: undefined,
          chainId: 1,
          childAddressLocation: "topic1",
          eventSelector:
            "0x17aa8d0e85db1d0531a8181b5bb84e1d4ed744db1cadd8814acd3d181ff30137",
          fromBlock: undefined,
          id: "log_0x5fbdb2315678afecb367f032d93f642f64180aa3_1_topic1_0x17aa8d0e85db1d0531a8181b5bb84e1d4ed744db1cadd8814acd3d181ff30137_undefined_undefined",
          sourceId: "Pair",
          toBlock: undefined,
          type: "log",
        },
        new Set(),
      ],
    ]),
  );
  expect(data[0]?.traces).toHaveLength(0);
  expect(data[1]?.traces).toHaveLength(0);
  expect(data[0]?.transactions).toHaveLength(0);
  expect(data[1]?.transactions).toHaveLength(1);
});
test("handleBlock() block event with log factory error", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({ common, chain });
  const { address } = await deployFactory({ sender: ALICE });
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  await swapPair({
    pair,
    amount0Out: 1n,
    amount1Out: 1n,
    to: ALICE,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({
    address,
  });
  const filter = eventCallbacks[0]!.filter as LogFilter<LogFactory>;
  filter.address.address = undefined;
  // Invalid child address location causes extracting child address to throw an error
  filter.address.childAddressLocation = "topic3";
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x1", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map([[filter.address.id, new Map()]]),
  });
  let block = await eth_getBlockByNumber(rpc, ["0x2", true]);
  const syncResult1 = await drainAsyncGenerator(realtimeSync.sync(block));
  block = await eth_getBlockByNumber(rpc, ["0x3", true]);
  const syncResult2 = await drainAsyncGenerator(realtimeSync.sync(block));
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(2);
  expect(syncResult1).toHaveLength(1);
  expect(syncResult2).toHaveLength(1);
  const data = [...syncResult1, ...syncResult2] as Extract<
    RealtimeSyncEvent,
    { type: "block" }
  >[];
  expect(data[0]).toStrictEqual({
    type: "block",
    blockCallback: undefined,
    hasMatchedFilter: false,
    block: expect.any(Object),
    logs: expect.any(Object),
    transactions: expect.any(Object),
    traces: expect.any(Object),
    transactionReceipts: expect.any(Object),
    childAddresses: expect.any(Object),
  });
  expect(data[1]).toStrictEqual({
    type: "block",
    blockCallback: undefined,
    hasMatchedFilter: false,
    block: expect.any(Object),
    logs: expect.any(Object),
    transactions: expect.any(Object),
    traces: expect.any(Object),
    transactionReceipts: expect.any(Object),
    childAddresses: expect.any(Object),
  });
  expect(data[0]?.block.number).toBe("0x2");
  expect(data[1]?.block.number).toBe("0x3");
  expect(data[0]?.logs).toHaveLength(0);
  expect(data[1]?.logs).toHaveLength(0);
  expect(data[0]?.traces).toHaveLength(0);
  expect(data[1]?.traces).toHaveLength(0);
  expect(data[0]?.transactions).toHaveLength(0);
  expect(data[1]?.transactions).toHaveLength(0);
});
test("handleBlock() block event with block", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({ common, chain });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  const blockData = await simulateBlock();
  const syncResult = await drainAsyncGenerator(
    realtimeSync.sync(blockData.block),
  );
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(1);
  expect(syncResult).toHaveLength(1);
  expect(syncResult[0]).toStrictEqual({
    type: "block",
    blockCallback: undefined,
    hasMatchedFilter: true,
    block: expect.any(Object),
    logs: expect.any(Object),
    transactions: expect.any(Object),
    traces: expect.any(Object),
    transactionReceipts: expect.any(Object),
    childAddresses: expect.any(Object),
  });
  const data = syncResult as Extract<RealtimeSyncEvent, { type: "block" }>[];
  expect(data[0]?.block.number).toBe("0x1");
  expect(data[0]?.logs).toHaveLength(0);
  expect(data[0]?.traces).toHaveLength(0);
  expect(data[0]?.transactions).toHaveLength(0);
});
test("handleBlock() block event with transaction", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({ common, chain });
  await transferEth({
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getAccountsIndexingBuild({
    address: ALICE,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks: eventCallbacks.filter(
      ({ filter }) => filter.type === "transaction",
    ),
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  const block = await eth_getBlockByNumber(rpc, ["0x1", true]);
  const syncResult = await drainAsyncGenerator(realtimeSync.sync(block));
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(1);
  expect(syncResult).toHaveLength(1);
  expect(syncResult[0]).toStrictEqual({
    type: "block",
    blockCallback: undefined,
    hasMatchedFilter: true,
    block: expect.any(Object),
    logs: expect.any(Object),
    transactions: expect.any(Object),
    traces: expect.any(Object),
    transactionReceipts: expect.any(Object),
    childAddresses: expect.any(Object),
  });
  const data = syncResult as Extract<RealtimeSyncEvent, { type: "block" }>[];
  expect(data[0]?.block.number).toBe("0x1");
  expect(data[0]?.logs).toHaveLength(0);
  expect(data[0]?.traces).toHaveLength(0);
  expect(data[0]?.transactions).toHaveLength(1);
  expect(data[0]?.transactionReceipts).toHaveLength(1);
});
test("handleBlock() block event with transfer", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({ common, chain });
  const blockData = await transferEth({
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getAccountsIndexingBuild({
    address: ALICE,
  });
  const request = async (request: any) => {
    if (request.method === "debug_traceBlockByHash") {
      return Promise.resolve([
        {
          txHash: blockData.trace.transactionHash,
          result: blockData.trace.trace,
        },
      ]);
    }
    return rpc.request(request);
  };
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc: {
      // @ts-ignore
      request,
    },
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  const block = await eth_getBlockByNumber(rpc, ["0x1", true]);
  const syncResult = await drainAsyncGenerator(realtimeSync.sync(block));
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(1);
  expect(syncResult).toHaveLength(1);
  expect(syncResult[0]).toStrictEqual({
    type: "block",
    blockCallback: undefined,
    hasMatchedFilter: true,
    block: expect.any(Object),
    logs: expect.any(Object),
    transactions: expect.any(Object),
    traces: expect.any(Object),
    transactionReceipts: expect.any(Object),
    childAddresses: expect.any(Object),
  });
  const data = syncResult as Extract<RealtimeSyncEvent, { type: "block" }>[];
  expect(data[0]?.block.number).toBe("0x1");
  expect(data[0]?.logs).toHaveLength(0);
  expect(data[0]?.traces).toHaveLength(1);
  expect(data[0]?.transactions).toHaveLength(1);
  expect(data[0]?.transactionReceipts).toHaveLength(1);
});
test("handleBlock() block event with trace", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({ chain, common });
  const { address } = await deployErc20({ sender: ALICE });
  const blockData2 = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const blockData3 = await transferErc20({
    erc20: address,
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const { eventCallbacks } = getErc20IndexingBuild({
    address,
    includeCallTraces: true,
  });
  const request = async (request: any) => {
    if (request.method === "debug_traceBlockByHash") {
      if (request.params[0] === blockData2.block.hash) {
        return Promise.resolve([
          {
            txHash: blockData2.transaction.hash,
            result: {
              type: "CREATE",
              from: ALICE,
              gas: "0x0",
              gasUsed: "0x0",
              input: "0x0",
              value: "0x0",
            },
          },
        ]);
      }
      if (request.params[0] === blockData3.block.hash) {
        return Promise.resolve([
          {
            txHash: blockData3.trace.transactionHash,
            result: blockData3.trace.trace,
          },
        ]);
      }
      return Promise.resolve([]);
    }
    return rpc.request(request);
  };
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x1", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc: {
      ...rpc,
      // @ts-ignore
      request,
    },
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  const syncResult1 = await drainAsyncGenerator(
    realtimeSync.sync(blockData2.block),
  );
  const syncResult2 = await drainAsyncGenerator(
    realtimeSync.sync(blockData3.block),
  );
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(2);
  expect(syncResult1).toHaveLength(1);
  expect(syncResult2).toHaveLength(1);
  const data = [...syncResult1, ...syncResult2] as Extract<
    RealtimeSyncEvent,
    { type: "block" }
  >[];
  expect(data[0]?.block.number).toBe("0x2");
  expect(data[1]?.block.number).toBe("0x3");
  expect(data[0]?.logs).toHaveLength(1);
  expect(data[1]?.logs).toHaveLength(1);
  expect(data[0]?.traces).toHaveLength(0);
  expect(data[1]?.traces).toHaveLength(1);
  expect(data[0]?.transactions).toHaveLength(1);
  expect(data[1]?.transactions).toHaveLength(1);
  expect(data[0]?.transactionReceipts).toHaveLength(0);
  expect(data[1]?.transactionReceipts).toHaveLength(0);
});
test("handleBlock() finalize event", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({
    chain,
    common,
  });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  let blockData = await simulateBlock();
  await drainAsyncGenerator(realtimeSync.sync(blockData.block));
  blockData = await simulateBlock();
  await drainAsyncGenerator(realtimeSync.sync(blockData.block));
  blockData = await simulateBlock();
  await drainAsyncGenerator(realtimeSync.sync(blockData.block));
  blockData = await simulateBlock();
  const syncResult = await drainAsyncGenerator(
    realtimeSync.sync(blockData.block),
  );
  expect(syncResult).toHaveLength(2);
  expect(syncResult[1]).toStrictEqual({
    type: "finalize",
    block: expect.any(Object),
  });
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(2);
  expect(
    (syncResult[1] as Extract<RealtimeSyncEvent, { type: "finalize" }>).block
      .number,
  ).toBe("0x2");
});
test("handleReorg() finds common ancestor", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({
    chain,
    common,
  });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  const blockData1 = await simulateBlock();
  await drainAsyncGenerator(realtimeSync.sync(blockData1.block));
  const blockData2 = await simulateBlock();
  await drainAsyncGenerator(realtimeSync.sync(blockData2.block));
  const blockData3 = await simulateBlock();
  await drainAsyncGenerator(realtimeSync.sync(blockData3.block));
  const syncResult = await drainAsyncGenerator(
    realtimeSync.sync(blockData2.block),
  );
  expect(syncResult).toHaveLength(1);
  expect(syncResult[0]).toStrictEqual({
    type: "reorg",
    block: expect.any(Object),
    reorgedBlocks: [expect.any(Object), expect.any(Object)],
  });
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(1);
});
test("handleReorg() throws error for deep reorg", async () => {
  const { common } = context;
  await setupDatabaseServices();
  const chain = getChain({ finalityBlockCount: 2 });
  const rpc = createRpc({
    chain,
    common,
  });
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  const finalizedBlock = await eth_getBlockByNumber(rpc, ["0x0", true]);
  const realtimeSync = createRealtimeSync({
    common,
    chain,
    rpc,
    eventCallbacks,
    syncProgress: { finalized: finalizedBlock },
    childAddresses: new Map(),
  });
  const blockData1 = await simulateBlock();
  await drainAsyncGenerator(realtimeSync.sync(blockData1.block));
  const blockData2 = await simulateBlock();
  await drainAsyncGenerator(realtimeSync.sync(blockData2.block));
  const blockData3 = await simulateBlock();
  await drainAsyncGenerator(realtimeSync.sync(blockData3.block));
  await drainAsyncGenerator(
    realtimeSync.sync({
      ...blockData3.block,
      number: "0x4",
      hash: "0x0000000000000000000000000000000000000000000000000000000000000000",
      parentHash: realtimeSync.unfinalizedBlocks[1]!.hash,
    }),
  );
  // block 4 is not added to `unfinalizedBlocks`
  expect(realtimeSync.unfinalizedBlocks).toHaveLength(3);
});
</file>

<file path="packages/core/src/sync-realtime/index.ts">
import type { Common } from "@/internal/common.js";
import { ShutdownError } from "@/internal/errors.js";
import type {
  BlockFilter,
  Chain,
  EventCallback,
  Factory,
  FactoryId,
  Filter,
  LightBlock,
  LogFilter,
  SyncBlock,
  SyncBlockHeader,
  SyncLog,
  SyncTrace,
  SyncTransaction,
  SyncTransactionReceipt,
  TraceFilter,
  TransactionFilter,
  TransferFilter,
} from "@/internal/types.js";
import {
  debug_traceBlockByHash,
  eth_getBlockByHash,
  eth_getBlockByNumber,
  eth_getBlockReceipts,
  eth_getLogs,
  eth_getTransactionReceipt,
  validateLogsAndBlock,
  validateReceiptsAndBlock,
  validateTracesAndBlock,
  validateTransactionsAndBlock,
} from "@/rpc/actions.js";
import type { Rpc } from "@/rpc/index.js";
import {
  getChildAddress,
  getFilterFactories,
  isAddressFactory,
  isAddressMatched,
  isBlockFilterMatched,
  isLogFactoryMatched,
  isLogFilterMatched,
  isTraceFilterMatched,
  isTransactionFilterMatched,
  isTransferFilterMatched,
} from "@/runtime/filter.js";
import type { SyncProgress } from "@/runtime/index.js";
import { createLock } from "@/utils/mutex.js";
import { range } from "@/utils/range.js";
import { startClock } from "@/utils/timer.js";
import {
  type Address,
  type Hash,
  hexToNumber,
  numberToHex,
  zeroHash,
} from "viem";
import { isFilterInBloom, isInBloom, zeroLogsBloom } from "./bloom.js";
export type RealtimeSync = {
  /**
   * Fetch block event data and reconcile it into the local chain.
   *
   * @param block - The block to reconcile.
   */
  sync(
    block: SyncBlock | SyncBlockHeader,
    blockCallback?: (isAccepted: boolean) => void,
  ): AsyncGenerator<RealtimeSyncEvent>;
  onError(error: Error): void;
  /** Local chain of blocks that have not been finalized. */
  unfinalizedBlocks: LightBlock[];
};
export type BlockWithEventData = {
  block: SyncBlock | SyncBlockHeader;
  transactions: SyncTransaction[];
  transactionReceipts: SyncTransactionReceipt[];
  logs: SyncLog[];
  traces: SyncTrace[];
  childAddresses: Map<Factory, Set<Address>>;
};
export type RealtimeSyncEvent =
  | ({
      type: "block";
      hasMatchedFilter: boolean;
      blockCallback?: (isAccepted: boolean) => void;
    } & BlockWithEventData)
  | { type: "finalize"; block: LightBlock }
  | { type: "reorg"; block: LightBlock; reorgedBlocks: LightBlock[] };
type CreateRealtimeSyncParameters = {
  common: Common;
  chain: Chain;
  rpc: Rpc;
  eventCallbacks: EventCallback[];
  syncProgress: Pick<SyncProgress, "finalized">;
  childAddresses: Map<FactoryId, Map<Address, number>>;
};
const MAX_LATEST_BLOCK_ATTEMPT_MS = 10 * 60 * 1000; // 10 minutes
const MAX_QUEUED_BLOCKS = 50;
export const createRealtimeSync = (
  args: CreateRealtimeSyncParameters,
): RealtimeSync => {
  let isBlockReceipts = true;
  let finalizedBlock: LightBlock = args.syncProgress.finalized;
  const childAddresses = args.childAddresses;
  /** Annotates `childAddresses` for efficient lookup by block number */
  const childAddressesPerBlock = new Map<
    number,
    BlockWithEventData["childAddresses"]
  >();
  /**
   * Blocks that have been ingested and are
   * waiting to be finalized. It is an invariant that
   * all blocks are linked to each other,
   * `parentHash` => `hash`.
   */
  let unfinalizedBlocks: LightBlock[] = [];
  /** Closest-to-tip block that has been fetched but not yet reconciled. */
  let latestFetchedBlock: LightBlock | undefined;
  let fetchAndReconcileLatestBlockErrorCount = 0;
  const realtimeSyncLock = createLock();
  const factories: Factory[] = [];
  const logFilters: LogFilter[] = [];
  const traceFilters: TraceFilter[] = [];
  const transactionFilters: TransactionFilter[] = [];
  const transferFilters: TransferFilter[] = [];
  const blockFilters: BlockFilter[] = [];
  for (const eventCallback of args.eventCallbacks) {
    if (
      eventCallback.filter.toBlock &&
      eventCallback.filter.toBlock <= hexToNumber(finalizedBlock.number)
    ) {
      continue;
    }
    // Collect filters from event callbacks
    if (eventCallback.filter.type === "log") {
      logFilters.push(eventCallback.filter);
    } else if (eventCallback.filter.type === "trace") {
      traceFilters.push(eventCallback.filter);
    } else if (eventCallback.filter.type === "transaction") {
      transactionFilters.push(eventCallback.filter);
    } else if (eventCallback.filter.type === "transfer") {
      transferFilters.push(eventCallback.filter);
    } else if (eventCallback.filter.type === "block") {
      blockFilters.push(eventCallback.filter);
    }
    for (const factory of getFilterFactories(eventCallback.filter)) {
      if (
        factory.toBlock &&
        factory.toBlock <= hexToNumber(finalizedBlock.number)
      ) {
        continue;
      }
      factories.push(factory);
    }
  }
  const syncTransactionReceipts = async (
    block: SyncBlock,
    transactionHashes: Set<Hash>,
    ethGetBlockMethod: "eth_getBlockByHash" | "eth_getBlockByNumber",
    context?: Parameters<Rpc["request"]>[1],
  ): Promise<SyncTransactionReceipt[]> => {
    if (transactionHashes.size === 0) {
      return [];
    }
    if (isBlockReceipts === false) {
      const transactionReceipts = await Promise.all(
        Array.from(transactionHashes).map(async (hash) => {
          const receipt = await eth_getTransactionReceipt(
            args.rpc,
            [hash],
            context,
          );
          validateReceiptsAndBlock(
            [receipt],
            block,
            {
              method: "eth_getTransactionReceipt",
              params: [hash],
            },
            ethGetBlockMethod === "eth_getBlockByNumber"
              ? {
                  method: "eth_getBlockByNumber",
                  params: [block.number, true],
                }
              : {
                  method: "eth_getBlockByHash",
                  params: [block.hash, true],
                },
          );
          return receipt;
        }),
      );
      return transactionReceipts;
    }
    let blockReceipts: SyncTransactionReceipt[];
    try {
      blockReceipts = await eth_getBlockReceipts(
        args.rpc,
        [block.hash],
        context,
      );
    } catch (_error) {
      const error = _error as Error;
      args.common.logger.warn({
        msg: "Caught eth_getBlockReceipts error, switching to eth_getTransactionReceipt method",
        action: "fetch block data",
        chain: args.chain.name,
        chain_id: args.chain.id,
        error,
      });
      isBlockReceipts = false;
      return syncTransactionReceipts(
        block,
        transactionHashes,
        ethGetBlockMethod,
        context,
      );
    }
    validateReceiptsAndBlock(
      blockReceipts,
      block,
      {
        method: "eth_getBlockReceipts",
        params: [block.hash],
      },
      ethGetBlockMethod === "eth_getBlockByNumber"
        ? {
            method: "eth_getBlockByNumber",
            params: [block.number, true],
          }
        : {
            method: "eth_getBlockByHash",
            params: [block.hash, true],
          },
    );
    const transactionReceipts = blockReceipts.filter((receipt) =>
      transactionHashes.has(receipt.transactionHash),
    );
    return transactionReceipts;
  };
  const getLatestUnfinalizedBlock = () => {
    if (unfinalizedBlocks.length === 0) {
      return finalizedBlock;
    } else return unfinalizedBlocks[unfinalizedBlocks.length - 1]!;
  };
  /**
   * Fetch all data (logs, traces, receipts) for the specified block required by `args.sources`
   *
   * @dev The data returned by this function may include false positives. This
   * is due to the fact that factory addresses are unknown and are always
   * treated as "matched".
   */
  const fetchBlockEventData = async (
    maybeBlockHeader: SyncBlock | SyncBlockHeader,
  ): Promise<BlockWithEventData> => {
    const context = {
      logger: args.common.logger.child({ action: "fetch_block_data" }),
    };
    const endClock = startClock();
    let block: SyncBlock | undefined;
    let ethGetBlockMethod: "eth_getBlockByHash" | "eth_getBlockByNumber";
    if (maybeBlockHeader.transactions !== undefined) {
      block = maybeBlockHeader;
      ethGetBlockMethod = "eth_getBlockByNumber";
    } else {
      ethGetBlockMethod = "eth_getBlockByHash";
    }
    ////////
    // Logs
    ////////
    // "eth_getLogs" calls can be skipped if no filters match `newHeadBlock.logsBloom`.
    const shouldRequestLogs =
      maybeBlockHeader.logsBloom === zeroLogsBloom ||
      logFilters.some((filter) =>
        isFilterInBloom({ block: maybeBlockHeader, filter }),
      );
    let logs: SyncLog[] = [];
    if (shouldRequestLogs) {
      if (block === undefined) {
        [block, logs] = await Promise.all([
          eth_getBlockByHash(args.rpc, [maybeBlockHeader.hash, true], context),
          eth_getLogs(
            args.rpc,
            [{ blockHash: maybeBlockHeader.hash }],
            context,
          ),
        ]);
      } else {
        logs = await eth_getLogs(
          args.rpc,
          [{ blockHash: block.hash }],
          context,
        );
      }
      validateLogsAndBlock(
        logs,
        block,
        {
          method: "eth_getLogs",
          params: [{ blockHash: block.hash }],
        },
        ethGetBlockMethod === "eth_getBlockByNumber"
          ? {
              method: "eth_getBlockByNumber",
              params: [block.number, true],
            }
          : {
              method: "eth_getBlockByHash",
              params: [block.hash, true],
            },
      );
      // Note: Exact `logsBloom` validations were considered too strict to add to `validateLogsAndBlock`.
      let isInvalidLogsBloom = false;
      for (const log of logs) {
        if (isInBloom(block.logsBloom, log.address) === false) {
          isInvalidLogsBloom = true;
        }
        if (
          log.topics[0] &&
          isInBloom(block.logsBloom, log.topics[0]) === false
        ) {
          isInvalidLogsBloom = true;
        }
        if (
          log.topics[1] &&
          isInBloom(block.logsBloom, log.topics[1]) === false
        ) {
          isInvalidLogsBloom = true;
        }
        if (
          log.topics[2] &&
          isInBloom(block.logsBloom, log.topics[2]) === false
        ) {
          isInvalidLogsBloom = true;
        }
        if (
          log.topics[3] &&
          isInBloom(block.logsBloom, log.topics[3]) === false
        ) {
          isInvalidLogsBloom = true;
        }
        if (isInvalidLogsBloom) {
          args.common.logger.warn({
            msg: "Detected inconsistent RPC responses. Log not found in block.logsBloom.",
            action: "fetch_block_data",
            chain: args.chain.name,
            chain_id: args.chain.id,
            number: hexToNumber(block.number),
            hash: block.hash,
            logIndex: hexToNumber(log.logIndex),
          });
          break;
        }
      }
      for (const log of logs) {
        if (log.transactionHash === zeroHash) {
          args.common.logger.warn({
            msg: "Detected log with empty transaction hash. This is expected for some chains like ZKsync.",
            action: "fetch_block_data",
            chain: args.chain.name,
            chain_id: args.chain.id,
            number: hexToNumber(block.number),
            hash: block.hash,
            logIndex: hexToNumber(log.logIndex),
          });
        }
      }
    }
    if (shouldRequestLogs === false && logFilters.length > 0) {
      args.common.logger.trace({
        msg: "Skipped eth_getLogs request due to bloom filter result",
        action: "fetch_block_data",
        chain: args.chain.name,
        chain_id: args.chain.id,
        number: hexToNumber(maybeBlockHeader.number),
        hash: maybeBlockHeader.hash,
      });
    }
    ////////
    // Traces
    ////////
    const shouldRequestTraces =
      traceFilters.length > 0 || transferFilters.length > 0;
    let traces: SyncTrace[] = [];
    if (shouldRequestTraces) {
      if (block === undefined) {
        [block, traces] = await Promise.all([
          eth_getBlockByHash(args.rpc, [maybeBlockHeader.hash, true], context),
          debug_traceBlockByHash(
            args.rpc,
            [maybeBlockHeader.hash, { tracer: "callTracer" }],
            context,
          ),
        ]);
      } else {
        traces = await debug_traceBlockByHash(
          args.rpc,
          [block.hash, { tracer: "callTracer" }],
          context,
        );
      }
      validateTracesAndBlock(
        traces,
        block,
        {
          method: "debug_traceBlockByNumber",
          params: [block.number, { tracer: "callTracer" }],
        },
        ethGetBlockMethod === "eth_getBlockByNumber"
          ? {
              method: "eth_getBlockByNumber",
              params: [block.number, true],
            }
          : {
              method: "eth_getBlockByHash",
              params: [block.hash, true],
            },
      );
    }
    ////////
    // Get Matched
    ////////
    // Record `blockChildAddresses` that contain factory child addresses
    const blockChildAddresses = new Map<Factory, Set<Address>>();
    const childAddressDecodeFailureIds = new Set<string>();
    let childAddressDecodeFailureCount = 0;
    let childAddressDecodeSuccessCount = 0;
    for (const factory of factories) {
      blockChildAddresses.set(factory, new Set<Address>());
      for (const log of logs) {
        if (isLogFactoryMatched({ factory, log })) {
          let address: Address;
          try {
            address = getChildAddress({ log, factory });
            childAddressDecodeSuccessCount++;
          } catch (error) {
            if (factory.address === undefined) {
              childAddressDecodeFailureCount++;
              if (childAddressDecodeFailureIds.has(factory.id) === false) {
                childAddressDecodeFailureIds.add(factory.id);
                args.common.logger.debug({
                  msg: "Failed to extract child address from log matched by factory using the provided ABI item",
                  chain: args.chain.name,
                  chain_id: args.chain.id,
                  factory: factory.sourceId,
                  block_number: hexToNumber(log.blockNumber),
                  log_index: hexToNumber(log.logIndex),
                  data: log.data,
                  topics: JSON.stringify(log.topics),
                });
              }
              continue;
            } else {
              throw error;
            }
          }
          blockChildAddresses.get(factory)!.add(address);
        }
      }
    }
    if (childAddressDecodeFailureCount > 0) {
      args.common.logger.debug({
        msg: "Logs matched by factory contained child addresses that could not be extracted",
        failure_count: childAddressDecodeFailureCount,
        success_count: childAddressDecodeSuccessCount,
      });
    }
    const requiredTransactions = new Set<Hash>();
    const requiredTransactionReceipts = new Set<Hash>();
    // Remove logs that don't match a filter, recording required transactions
    logs = logs.filter((log) => {
      let isMatched = false;
      for (const filter of logFilters) {
        if (isLogFilterMatched({ filter, log })) {
          isMatched = true;
          if (log.transactionHash !== zeroHash) {
            requiredTransactions.add(log.transactionHash);
            if (filter.hasTransactionReceipt) {
              requiredTransactionReceipts.add(log.transactionHash);
              // skip to next log
              break;
            }
          }
        }
      }
      return isMatched;
    });
    // Initial weak trace filtering before full filtering with factory addresses in handleBlock
    traces = traces.filter((trace) => {
      let isMatched = false;
      for (const filter of transferFilters) {
        if (
          isTransferFilterMatched({
            filter,
            trace: trace.trace,
            block: maybeBlockHeader,
          })
        ) {
          requiredTransactions.add(trace.transactionHash);
          isMatched = true;
          if (filter.hasTransactionReceipt) {
            requiredTransactionReceipts.add(trace.transactionHash);
            // skip to next trace
            break;
          }
        }
      }
      for (const filter of traceFilters) {
        if (
          isTraceFilterMatched({
            filter,
            trace: trace.trace,
            block: maybeBlockHeader,
          })
        ) {
          requiredTransactions.add(trace.transactionHash);
          isMatched = true;
          if (filter.hasTransactionReceipt) {
            requiredTransactionReceipts.add(trace.transactionHash);
            // skip to next trace
            break;
          }
        }
      }
      return isMatched;
    });
    ////////
    // Transactions
    ////////
    // exit early if no logs or traces were requested and no transactions are required
    if (block === undefined && transactionFilters.length === 0) {
      args.common.logger.debug(
        {
          msg: "Fetched block data",
          chain: args.chain.name,
          chain_id: args.chain.id,
          number: hexToNumber(maybeBlockHeader.number),
          hash: maybeBlockHeader.hash,
          transaction_count: 0,
          receipt_count: 0,
          trace_count: 0,
          log_count: 0,
          child_address_count: 0,
          duration: endClock(),
        },
        ["chain", "number", "hash"],
      );
      return {
        block: maybeBlockHeader,
        transactions: [],
        transactionReceipts: [],
        logs: [],
        traces: [],
        childAddresses: blockChildAddresses,
      };
    }
    if (block === undefined) {
      block = await eth_getBlockByHash(
        args.rpc,
        [maybeBlockHeader.hash, true],
        context,
      );
    }
    validateTransactionsAndBlock(
      block,
      ethGetBlockMethod === "eth_getBlockByNumber"
        ? {
            method: "eth_getBlockByNumber",
            params: [block.number, true],
          }
        : {
            method: "eth_getBlockByHash",
            params: [block.hash, true],
          },
    );
    const transactions = block.transactions.filter((transaction) => {
      let isMatched = requiredTransactions.has(transaction.hash);
      for (const filter of transactionFilters) {
        if (isTransactionFilterMatched({ filter, transaction })) {
          requiredTransactions.add(transaction.hash);
          requiredTransactionReceipts.add(transaction.hash);
          isMatched = true;
        }
      }
      return isMatched;
    });
    ////////
    // Transaction Receipts
    ////////
    const transactionReceipts = await syncTransactionReceipts(
      block,
      requiredTransactionReceipts,
      ethGetBlockMethod,
      context,
    );
    let childAddressCount = 0;
    for (const childAddresses of blockChildAddresses.values()) {
      childAddressCount += childAddresses.size;
    }
    args.common.logger.debug(
      {
        msg: "Fetched block data",
        chain: args.chain.name,
        chain_id: args.chain.id,
        number: hexToNumber(block.number),
        hash: block.hash,
        transaction_count: transactions.length,
        log_count: logs.length,
        trace_count: traces.length,
        receipt_count: transactionReceipts.length,
        child_address_count: childAddressCount,
        duration: endClock(),
      },
      ["chain", "number", "hash"],
    );
    return {
      block,
      transactions,
      transactionReceipts,
      logs,
      traces,
      childAddresses: blockChildAddresses,
    };
  };
  /**
   * Filter the block event data using the filters and child addresses.
   */
  const filterBlockEventData = ({
    block,
    logs,
    traces,
    transactions,
    transactionReceipts,
    childAddresses: blockChildAddresses,
  }: BlockWithEventData): BlockWithEventData & {
    matchedFilters: Set<Filter>;
  } => {
    // Update `childAddresses`
    for (const factory of factories) {
      const factoryId = factory.id;
      for (const address of blockChildAddresses.get(factory)!) {
        if (childAddresses.get(factoryId)!.has(address) === false) {
          childAddresses
            .get(factoryId)!
            .set(address, hexToNumber(block.number));
        } else {
          blockChildAddresses.get(factory)!.delete(address);
        }
      }
    }
    // Save per block child addresses so that they can be undone in the event of a reorg.
    childAddressesPerBlock.set(hexToNumber(block.number), blockChildAddresses);
    /**
     * `logs` and `callTraces` must be filtered again (already filtered in `extract`)
     *  because `extract` doesn't have factory address information.
     */
    const matchedFilters = new Set<Filter>();
    // Remove logs that don't match a filter, accounting for factory addresses
    logs = logs.filter((log) => {
      let isMatched = false;
      for (const filter of logFilters) {
        if (
          isLogFilterMatched({ filter, log }) &&
          (isAddressFactory(filter.address)
            ? isAddressMatched({
                address: log.address,
                blockNumber: hexToNumber(block.number),
                childAddresses: childAddresses.get(filter.address.id)!,
              })
            : true)
        ) {
          matchedFilters.add(filter);
          isMatched = true;
        }
      }
      return isMatched;
    });
    traces = traces.filter((trace) => {
      let isMatched = false;
      for (const filter of transferFilters) {
        if (
          isTransferFilterMatched({
            filter,
            trace: trace.trace,
            block,
          }) &&
          (isAddressFactory(filter.fromAddress)
            ? isAddressMatched({
                address: trace.trace.from,
                blockNumber: hexToNumber(block.number),
                childAddresses: childAddresses.get(filter.fromAddress.id)!,
              })
            : true) &&
          (isAddressFactory(filter.toAddress)
            ? isAddressMatched({
                address: trace.trace.to,
                blockNumber: hexToNumber(block.number),
                childAddresses: childAddresses.get(filter.toAddress.id)!,
              })
            : true)
        ) {
          matchedFilters.add(filter);
          isMatched = true;
        }
      }
      for (const filter of traceFilters) {
        if (
          isTraceFilterMatched({
            filter,
            trace: trace.trace,
            block,
          }) &&
          (isAddressFactory(filter.fromAddress)
            ? isAddressMatched({
                address: trace.trace.from,
                blockNumber: hexToNumber(block.number),
                childAddresses: childAddresses.get(filter.fromAddress.id)!,
              })
            : true) &&
          (isAddressFactory(filter.toAddress)
            ? isAddressMatched({
                address: trace.trace.to,
                blockNumber: hexToNumber(block.number),
                childAddresses: childAddresses.get(filter.toAddress.id)!,
              })
            : true)
        ) {
          matchedFilters.add(filter);
          isMatched = true;
        }
      }
      return isMatched;
    });
    // Remove transactions and transaction receipts that may have been filtered out
    const transactionHashes = new Set<Hash>();
    for (const log of logs) {
      transactionHashes.add(log.transactionHash);
    }
    for (const trace of traces) {
      transactionHashes.add(trace.transactionHash);
    }
    transactions = transactions.filter((transaction) => {
      let isMatched = transactionHashes.has(transaction.hash);
      for (const filter of transactionFilters) {
        if (
          isTransactionFilterMatched({ filter, transaction }) &&
          (isAddressFactory(filter.fromAddress)
            ? isAddressMatched({
                address: transaction.from,
                blockNumber: hexToNumber(block.number),
                childAddresses: childAddresses.get(filter.fromAddress.id)!,
              })
            : true) &&
          (isAddressFactory(filter.toAddress)
            ? isAddressMatched({
                address: transaction.to ?? undefined,
                blockNumber: hexToNumber(block.number),
                childAddresses: childAddresses.get(filter.toAddress.id)!,
              })
            : true)
        ) {
          matchedFilters.add(filter);
          isMatched = true;
        }
      }
      return isMatched;
    });
    for (const transaction of transactions) {
      transactionHashes.add(transaction.hash);
    }
    transactionReceipts = transactionReceipts.filter((t) =>
      transactionHashes.has(t.transactionHash),
    );
    // Record matched block filters
    for (const filter of blockFilters) {
      if (isBlockFilterMatched({ filter, block })) {
        matchedFilters.add(filter);
      }
    }
    return {
      matchedFilters,
      block,
      logs,
      transactions,
      transactionReceipts,
      traces,
      childAddresses: blockChildAddresses,
    };
  };
  /**
   * Traverse the remote chain until we find a block that is
   * compatible with our local chain.
   *
   * @param block Block that caused reorg to be detected.
   * Must be at most 1 block ahead of the local chain.
   */
  const reconcileReorg = async (
    block: SyncBlock | SyncBlockHeader,
  ): Promise<Extract<RealtimeSyncEvent, { type: "reorg" }>> => {
    const context = {
      logger: args.common.logger.child({ action: "reconcile_reorg" }),
    };
    const endClock = startClock();
    args.common.logger.debug({
      msg: "Detected reorg in local chain",
      chain: args.chain.name,
      chain_id: args.chain.id,
      number: hexToNumber(block.number),
      hash: block.hash,
    });
    // Record blocks that have been removed from the local chain.
    const reorgedBlocks = unfinalizedBlocks.filter(
      (lb) => hexToNumber(lb.number) >= hexToNumber(block.number),
    );
    // Prune the local chain of blocks that have been reorged out
    unfinalizedBlocks = unfinalizedBlocks.filter(
      (lb) => hexToNumber(lb.number) < hexToNumber(block.number),
    );
    // Block we are attempting to fit into the local chain.
    let remoteBlock: LightBlock = block;
    while (true) {
      const parentBlock = getLatestUnfinalizedBlock();
      if (parentBlock.hash === remoteBlock.parentHash) break;
      if (unfinalizedBlocks.length === 0) {
        // No compatible block was found in the local chain, must be a deep reorg.
        // Note: reorgedBlocks aren't removed from `unfinalizedBlocks` because we are "bailing"
        // from this attempt to reconcile the reorg, we need to reset the local chain state back
        // to what it was before we started.
        unfinalizedBlocks = reorgedBlocks;
        args.common.logger.warn({
          msg: "Encountered unrecoverable reorg",
          chain: args.chain.name,
          chain_id: args.chain.id,
          finalized_block: hexToNumber(finalizedBlock.number),
          duration: endClock(),
        });
        throw new Error(
          `Encountered unrecoverable '${args.chain.name}' reorg beyond finalized block ${hexToNumber(finalizedBlock.number)}`,
        );
      } else {
        remoteBlock = await eth_getBlockByHash(
          args.rpc,
          [remoteBlock.parentHash, false],
          context,
        );
        // Add tip to `reorgedBlocks`
        reorgedBlocks.unshift(unfinalizedBlocks.pop()!);
      }
    }
    const commonAncestor = getLatestUnfinalizedBlock();
    args.common.logger.debug({
      msg: "Reconciled reorg in local chain",
      chain: args.chain.name,
      chain_id: args.chain.id,
      reorg_depth: reorgedBlocks.length,
      common_ancestor_block: hexToNumber(commonAncestor.number),
      duration: endClock(),
    });
    // remove reorged blocks from `childAddresses`
    for (const block of reorgedBlocks) {
      for (const factory of factories) {
        const addresses = childAddressesPerBlock
          .get(hexToNumber(block.number))!
          .get(factory)!;
        for (const address of addresses) {
          childAddresses.get(factory.id)!.delete(address);
        }
      }
      childAddressesPerBlock.delete(hexToNumber(block.number));
    }
    return {
      type: "reorg",
      block: commonAncestor,
      reorgedBlocks,
    };
  };
  /**
   * Finish syncing a block.
   *
   * The four cases are:
   * 1) Block is the same as the one just processed, no-op.
   * 2) Block is behind the last processed. This is a sign that
   *    a reorg has occurred.
   * 3) Block is more than one ahead of the last processed,
   *    fetch all intermediate blocks and enqueue them again.
   * 4) Block is exactly one block ahead of the last processed,
   *    handle this new block (happy path).
   *
   * @dev `blockCallback` is guaranteed to be called exactly once or an error is thrown.
   * @dev It is an invariant that the correct events are generated or an error is thrown.
   */
  const reconcileBlock = async function* (
    blockWithEventData: BlockWithEventData,
    blockCallback?: (isAccepted: boolean) => void,
  ): AsyncGenerator<RealtimeSyncEvent> {
    const endClock = startClock();
    const latestBlock = getLatestUnfinalizedBlock();
    const block = blockWithEventData.block;
    // We already saw and handled this block. No-op.
    if (latestBlock.hash === block.hash) {
      args.common.logger.trace({
        msg: "Detected duplicate block",
        chain: args.chain.name,
        chain_id: args.chain.id,
        number: hexToNumber(block.number),
        hash: block.hash,
      });
      blockCallback?.(false);
      return;
    }
    // Quickly check for a reorg by comparing block numbers. If the block
    // number has not increased, a reorg must have occurred.
    if (hexToNumber(latestBlock.number) >= hexToNumber(block.number)) {
      const reorgEvent = await reconcileReorg(block);
      blockCallback?.(false);
      yield reorgEvent;
      return;
    }
    // Blocks are missing. They should be fetched and enqueued.
    if (hexToNumber(latestBlock.number) + 1 < hexToNumber(block.number)) {
      args.common.logger.trace({
        msg: "Missing blocks from local chain",
        chain: args.chain.name,
        chain_id: args.chain.id,
        block_range: JSON.stringify([
          hexToNumber(latestBlock.number) + 1,
          hexToNumber(block.number) - 1,
        ]),
      });
      // Retrieve missing blocks, but only fetch a certain amount.
      const missingBlockRange = range(
        hexToNumber(latestBlock.number) + 1,
        Math.min(
          hexToNumber(block.number),
          hexToNumber(latestBlock.number) + MAX_QUEUED_BLOCKS,
        ),
      );
      const pendingBlocks = await Promise.all(
        missingBlockRange.map((blockNumber) =>
          eth_getBlockByNumber(args.rpc, [numberToHex(blockNumber), true], {
            logger: args.common.logger.child({
              action: "fetch_missing_blocks",
            }),
          }).then((block) => fetchBlockEventData(block)),
        ),
      );
      args.common.logger.debug({
        msg: "Fetched missing blocks",
        chain: args.chain.name,
        chain_id: args.chain.id,
        block_range: JSON.stringify([
          hexToNumber(latestBlock.number) + 1,
          Math.min(
            hexToNumber(block.number) - 1,
            hexToNumber(latestBlock.number) + MAX_QUEUED_BLOCKS,
          ),
        ]),
      });
      for (const pendingBlock of pendingBlocks) {
        yield* reconcileBlock(pendingBlock);
      }
      if (
        hexToNumber(block.number) - hexToNumber(latestBlock.number) >
        MAX_QUEUED_BLOCKS
      ) {
        args.common.logger.trace({
          msg: "Latest block too far ahead of local chain",
          chain: args.chain.name,
          chain_id: args.chain.id,
          number: hexToNumber(block.number),
          hash: block.hash,
        });
        blockCallback?.(false);
      } else {
        yield* reconcileBlock(blockWithEventData, blockCallback);
      }
      return;
    }
    // Check if a reorg occurred by validating the chain of block hashes.
    if (block.parentHash !== latestBlock.hash) {
      const reorgEvent = await reconcileReorg(block);
      blockCallback?.(false);
      yield reorgEvent;
      return;
    }
    // New block is exactly one block ahead of the local chain.
    // Attempt to ingest it.
    const blockWithFilteredEventData = filterBlockEventData(blockWithEventData);
    let childAddressCount = 0;
    for (const childAddresses of blockWithFilteredEventData.childAddresses.values()) {
      childAddressCount += childAddresses.size;
    }
    args.common.logger.debug(
      {
        msg: "Added block to local chain",
        chain: args.chain.name,
        chain_id: args.chain.id,
        number: hexToNumber(block.number),
        hash: block.hash,
        transaction_count: blockWithFilteredEventData.transactions.length,
        log_count: blockWithFilteredEventData.logs.length,
        trace_count: blockWithFilteredEventData.traces.length,
        receipt_count: blockWithFilteredEventData.transactionReceipts.length,
        child_address_count: childAddressCount,
        duration: endClock(),
      },
      ["chain", "number", "hash"],
    );
    unfinalizedBlocks.push({
      hash: block.hash,
      parentHash: block.parentHash,
      number: block.number,
      timestamp: block.timestamp,
    });
    // Make sure `transactions` can be garbage collected
    blockWithEventData.block.transactions =
      blockWithFilteredEventData.block.transactions;
    yield {
      type: "block",
      hasMatchedFilter: blockWithFilteredEventData.matchedFilters.size > 0,
      block: blockWithFilteredEventData.block,
      logs: blockWithFilteredEventData.logs,
      transactions: blockWithFilteredEventData.transactions,
      transactionReceipts: blockWithFilteredEventData.transactionReceipts,
      traces: blockWithFilteredEventData.traces,
      childAddresses: blockWithFilteredEventData.childAddresses,
      blockCallback,
    };
    // Determine if a new range has become finalized by evaluating if the
    // latest block number is 2 * finalityBlockCount >= finalized block number.
    // Essentially, there is a range the width of finalityBlockCount that is entirely
    // finalized.
    const blockMovesFinality =
      hexToNumber(block.number) >=
      hexToNumber(finalizedBlock.number) + 2 * args.chain.finalityBlockCount;
    if (blockMovesFinality) {
      const pendingFinalizedBlock = unfinalizedBlocks.find(
        (lb) =>
          hexToNumber(lb.number) ===
          hexToNumber(block.number) - args.chain.finalityBlockCount,
      )!;
      args.common.logger.debug({
        msg: "Removed finalized blocks from local chain",
        chain: args.chain.name,
        chain_id: args.chain.id,
        block_count:
          hexToNumber(pendingFinalizedBlock.number) -
          hexToNumber(finalizedBlock.number),
        block_range: JSON.stringify([
          hexToNumber(finalizedBlock.number) + 1,
          hexToNumber(pendingFinalizedBlock.number),
        ]),
      });
      const finalizedBlocks = unfinalizedBlocks.filter(
        (lb) =>
          hexToNumber(lb.number) <= hexToNumber(pendingFinalizedBlock.number),
      );
      unfinalizedBlocks = unfinalizedBlocks.filter(
        (lb) =>
          hexToNumber(lb.number) > hexToNumber(pendingFinalizedBlock.number),
      );
      for (const block of finalizedBlocks) {
        childAddressesPerBlock.delete(hexToNumber(block.number));
      }
      finalizedBlock = pendingFinalizedBlock;
      yield {
        type: "finalize",
        block: pendingFinalizedBlock,
      };
    }
  };
  const onError = (error: Error, block?: SyncBlock | SyncBlockHeader) => {
    if (args.common.shutdown.isKilled) {
      throw new ShutdownError();
    }
    if (block) {
      args.common.logger.warn({
        msg: "Failed to fetch latest block",
        chain: args.chain.name,
        chain_id: args.chain.id,
        number: hexToNumber(block.number),
        hash: block.hash,
        retry_count: fetchAndReconcileLatestBlockErrorCount,
        error,
      });
    } else {
      args.common.logger.warn({
        msg: "Failed to fetch latest block",
        chain: args.chain.name,
        chain_id: args.chain.id,
        retry_count: fetchAndReconcileLatestBlockErrorCount,
        error,
      });
    }
    fetchAndReconcileLatestBlockErrorCount += 1;
    // Number of retries is max(10, `MAX_LATEST_BLOCK_ATTEMPT_MS` / `args.chain.pollingInterval`)
    if (
      fetchAndReconcileLatestBlockErrorCount >= 10 &&
      fetchAndReconcileLatestBlockErrorCount * args.chain.pollingInterval >
        MAX_LATEST_BLOCK_ATTEMPT_MS
    ) {
      throw error;
    }
  };
  return {
    async *sync(block, blockCallback) {
      try {
        args.common.logger.debug({
          msg: "Received new head block",
          chain: args.chain.name,
          chain_id: args.chain.id,
          number: hexToNumber(block.number),
          hash: block.hash,
        });
        const latestBlock = getLatestUnfinalizedBlock();
        // We already saw and handled this block. No-op.
        if (
          latestBlock.hash === block.hash ||
          latestFetchedBlock?.hash === block.hash
        ) {
          args.common.logger.trace({
            msg: "Detected duplicate block",
            chain: args.chain.name,
            chain_id: args.chain.id,
            number: hexToNumber(block.number),
            hash: block.hash,
          });
          blockCallback?.(false);
          return;
        }
        // Note: It's possible that a block with the same hash as `block` is
        // currently being fetched but hasn't been fully reconciled. `latestFetchedBlock`
        // is used to handle this case.
        latestFetchedBlock = block;
        const blockWithEventData = await fetchBlockEventData(block);
        // Note: `reconcileBlock` must be called serially.
        await realtimeSyncLock.lock();
        try {
          yield* reconcileBlock(blockWithEventData, blockCallback);
        } finally {
          realtimeSyncLock.unlock();
        }
        latestFetchedBlock = undefined;
        fetchAndReconcileLatestBlockErrorCount = 0;
      } catch (_error) {
        blockCallback?.(false);
        onError(_error as Error, block);
      }
    },
    onError,
    get unfinalizedBlocks() {
      return unfinalizedBlocks;
    },
  };
};
</file>

<file path="packages/core/src/sync-store/encode.ts">
import type {
  SyncBlock,
  SyncBlockHeader,
  SyncLog,
  SyncTrace,
  SyncTransaction,
  SyncTransactionReceipt,
} from "@/internal/types.js";
import { toLowerCase } from "@/utils/lowercase.js";
import type { Hex } from "viem";
import { hexToBigInt, hexToNumber } from "viem";
import type * as ponderSyncSchema from "./schema.js";
export const encodeBlock = ({
  block,
  chainId,
}: {
  block: SyncBlock | SyncBlockHeader;
  chainId: number;
}): typeof ponderSyncSchema.blocks.$inferInsert => ({
  chainId: BigInt(chainId),
  number: hexToBigInt(block.number),
  timestamp: hexToBigInt(block.timestamp),
  hash: block.hash,
  parentHash: block.parentHash,
  logsBloom: block.logsBloom!,
  miner: toLowerCase(block.miner),
  gasUsed: hexToBigInt(block.gasUsed),
  gasLimit: hexToBigInt(block.gasLimit),
  baseFeePerGas: block.baseFeePerGas ? hexToBigInt(block.baseFeePerGas) : null,
  nonce: block.nonce ?? null,
  mixHash: block.mixHash ?? null,
  stateRoot: block.stateRoot,
  receiptsRoot: block.receiptsRoot,
  transactionsRoot: block.transactionsRoot,
  sha3Uncles: block.sha3Uncles ?? null,
  size: hexToBigInt(block.size),
  difficulty: hexToBigInt(block.difficulty),
  totalDifficulty: block.totalDifficulty
    ? hexToBigInt(block.totalDifficulty)
    : null,
  extraData: block.extraData,
});
export const encodeLog = ({
  log,
  chainId,
}: {
  log: SyncLog;
  chainId: number;
}): typeof ponderSyncSchema.logs.$inferInsert => ({
  chainId: BigInt(chainId),
  blockNumber: hexToBigInt(log.blockNumber),
  logIndex: hexToNumber(log.logIndex),
  transactionIndex: hexToNumber(log.transactionIndex),
  blockHash: log.blockHash,
  transactionHash: log.transactionHash,
  address: toLowerCase(log.address),
  topic0: log.topics[0] ? log.topics[0] : null,
  topic1: log.topics[1] ? log.topics[1] : null,
  topic2: log.topics[2] ? log.topics[2] : null,
  topic3: log.topics[3] ? log.topics[3] : null,
  data: log.data,
});
export const encodeTransaction = ({
  transaction,
  chainId,
}: {
  transaction: SyncTransaction;
  chainId: number;
}): typeof ponderSyncSchema.transactions.$inferInsert => ({
  chainId: BigInt(chainId),
  blockNumber: hexToBigInt(transaction.blockNumber),
  transactionIndex: hexToNumber(transaction.transactionIndex),
  hash: transaction.hash,
  blockHash: transaction.blockHash,
  from: toLowerCase(transaction.from),
  to: transaction.to ? toLowerCase(transaction.to) : null,
  input: transaction.input,
  value: hexToBigInt(transaction.value),
  nonce: hexToNumber(transaction.nonce),
  r: transaction.r ?? null,
  s: transaction.s ?? null,
  v: transaction.v ? hexToBigInt(transaction.v) : null,
  type: transaction.type ?? "0x0",
  gas: hexToBigInt(transaction.gas),
  gasPrice: transaction.gasPrice ? hexToBigInt(transaction.gasPrice) : null,
  maxFeePerGas: transaction.maxFeePerGas
    ? hexToBigInt(transaction.maxFeePerGas)
    : null,
  maxPriorityFeePerGas: transaction.maxPriorityFeePerGas
    ? hexToBigInt(transaction.maxPriorityFeePerGas)
    : null,
  accessList: transaction.accessList
    ? JSON.stringify(transaction.accessList)
    : null,
});
export const encodeTransactionReceipt = ({
  transactionReceipt,
  chainId,
}: {
  transactionReceipt: SyncTransactionReceipt;
  chainId: number;
}): typeof ponderSyncSchema.transactionReceipts.$inferInsert => ({
  chainId: BigInt(chainId),
  blockNumber: hexToBigInt(transactionReceipt.blockNumber),
  transactionIndex: hexToNumber(transactionReceipt.transactionIndex),
  transactionHash: transactionReceipt.transactionHash,
  blockHash: transactionReceipt.blockHash,
  from: toLowerCase(transactionReceipt.from),
  to: transactionReceipt.to ? toLowerCase(transactionReceipt.to) : null,
  contractAddress: transactionReceipt.contractAddress
    ? toLowerCase(transactionReceipt.contractAddress)
    : null,
  logsBloom: transactionReceipt.logsBloom,
  gasUsed: hexToBigInt(transactionReceipt.gasUsed),
  cumulativeGasUsed: hexToBigInt(transactionReceipt.cumulativeGasUsed),
  effectiveGasPrice: hexToBigInt(transactionReceipt.effectiveGasPrice),
  status: transactionReceipt.status,
  type: transactionReceipt.type as Hex,
});
export const encodeTrace = ({
  trace,
  block,
  transaction,
  chainId,
}: {
  trace: SyncTrace;
  block: Pick<SyncBlock, "number">;
  transaction: Pick<SyncTransaction, "transactionIndex">;
  chainId: number;
}): typeof ponderSyncSchema.traces.$inferInsert => ({
  chainId: BigInt(chainId),
  blockNumber: hexToBigInt(block.number),
  transactionIndex: hexToNumber(transaction.transactionIndex),
  traceIndex: trace.trace.index,
  from: toLowerCase(trace.trace.from),
  to: trace.trace.to ? toLowerCase(trace.trace.to) : null,
  input: trace.trace.input,
  output: trace.trace.output ?? null,
  value: trace.trace.value ? hexToBigInt(trace.trace.value) : null,
  type: trace.trace.type,
  gas: hexToBigInt(trace.trace.gas),
  gasUsed: hexToBigInt(trace.trace.gasUsed),
  error: trace.trace.error ? trace.trace.error.replace(/\0/g, "") : null,
  revertReason: trace.trace.revertReason
    ? trace.trace.revertReason.replace(/\0/g, "")
    : null,
  subcalls: trace.trace.subcalls,
});
</file>

<file path="packages/core/src/sync-store/index.test.ts">
import {
  ALICE,
  BOB,
  EMPTY_BLOCK_FILTER,
  EMPTY_LOG_FILTER,
} from "@/_test/constants.js";
import {
  setupAnvil,
  setupCleanup,
  setupCommon,
  setupDatabaseServices,
  setupIsolatedDatabase,
} from "@/_test/setup.js";
import {
  createPair,
  deployErc20,
  deployFactory,
  mintErc20,
  simulateBlock,
  transferErc20,
} from "@/_test/simulate.js";
import {
  getBlocksIndexingBuild,
  getChain,
  getErc20IndexingBuild,
  getPairWithFactoryIndexingBuild,
} from "@/_test/utils.js";
import type { Factory, LogFilter } from "@/internal/types.js";
import { orderObject } from "@/utils/order.js";
import { sql } from "drizzle-orm";
import { hexToBigInt, hexToNumber, parseEther, zeroAddress } from "viem";
import { beforeEach, expect, test } from "vitest";
import * as ponderSyncSchema from "./schema.js";
beforeEach(setupCommon);
beforeEach(setupAnvil);
beforeEach(setupIsolatedDatabase);
beforeEach(setupCleanup);
test("getIntervals() empty", async () => {
  const { syncStore } = await setupDatabaseServices();
  const filter = EMPTY_BLOCK_FILTER;
  const intervals = await syncStore.getIntervals({
    filters: [filter],
  });
  expect(intervals).toMatchInlineSnapshot(`
    Map {
      {
        "chainId": 1,
        "fromBlock": undefined,
        "hasTransactionReceipt": false,
        "include": [],
        "interval": 1,
        "offset": 0,
        "sourceId": "test",
        "toBlock": undefined,
        "type": "block",
      } => [
        {
          "fragment": {
            "chainId": 1,
            "interval": 1,
            "offset": 0,
            "type": "block",
          },
          "intervals": [],
        },
      ],
    }
  `);
});
test("getIntervals() returns intervals", async () => {
  const { syncStore } = await setupDatabaseServices();
  const filter = EMPTY_BLOCK_FILTER;
  await syncStore.insertIntervals({
    intervals: [
      {
        filter,
        interval: [0, 4],
      },
    ],
    factoryIntervals: [],
    chainId: 1,
  });
  const intervals = await syncStore.getIntervals({
    filters: [filter],
  });
  expect(intervals).toMatchInlineSnapshot(`
    Map {
      {
        "chainId": 1,
        "fromBlock": undefined,
        "hasTransactionReceipt": false,
        "include": [],
        "interval": 1,
        "offset": 0,
        "sourceId": "test",
        "toBlock": undefined,
        "type": "block",
      } => [
        {
          "fragment": {
            "chainId": 1,
            "interval": 1,
            "offset": 0,
            "type": "block",
          },
          "intervals": [
            [
              0,
              4,
            ],
          ],
        },
      ],
    }
  `);
});
test("getIntervals() merges intervals", async () => {
  const { syncStore } = await setupDatabaseServices();
  const filter = EMPTY_BLOCK_FILTER;
  await syncStore.insertIntervals({
    intervals: [
      {
        filter,
        interval: [0, 4],
      },
    ],
    factoryIntervals: [],
    chainId: 1,
  });
  await syncStore.insertIntervals({
    intervals: [
      {
        filter,
        interval: [5, 8],
      },
    ],
    factoryIntervals: [],
    chainId: 1,
  });
  const intervals = await syncStore.getIntervals({
    filters: [filter],
  });
  expect(intervals).toMatchInlineSnapshot(`
    Map {
      {
        "chainId": 1,
        "fromBlock": undefined,
        "hasTransactionReceipt": false,
        "include": [],
        "interval": 1,
        "offset": 0,
        "sourceId": "test",
        "toBlock": undefined,
        "type": "block",
      } => [
        {
          "fragment": {
            "chainId": 1,
            "interval": 1,
            "offset": 0,
            "type": "block",
          },
          "intervals": [
            [
              0,
              8,
            ],
          ],
        },
      ],
    }
  `);
});
test("getIntervals() adjacent intervals", async () => {
  const { syncStore } = await setupDatabaseServices();
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: [zeroAddress],
  } satisfies LogFilter;
  await syncStore.insertIntervals({
    intervals: [
      {
        filter,
        interval: [0, 4],
      },
    ],
    factoryIntervals: [],
    chainId: 1,
  });
  await syncStore.insertIntervals({
    intervals: [
      {
        // @ts-ignore
        filter: { ...filter, address: undefined },
        interval: [5, 8],
      },
    ],
    factoryIntervals: [],
    chainId: 1,
  });
  const intervals = await syncStore.getIntervals({
    filters: [filter],
  });
  expect(intervals).toMatchInlineSnapshot(`
    Map {
      {
        "address": [
          "0x0000000000000000000000000000000000000000",
        ],
        "chainId": 1,
        "fromBlock": undefined,
        "hasTransactionReceipt": false,
        "include": [],
        "sourceId": "test",
        "toBlock": undefined,
        "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
        "topic1": null,
        "topic2": null,
        "topic3": null,
        "type": "log",
      } => [
        {
          "fragment": {
            "address": "0x0000000000000000000000000000000000000000",
            "chainId": 1,
            "includeTransactionReceipts": false,
            "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "topic1": null,
            "topic2": null,
            "topic3": null,
            "type": "log",
          },
          "intervals": [
            [
              0,
              8,
            ],
          ],
        },
      ],
    }
  `);
});
test("getIntervals() 0.15 migration", async () => {
  const { syncStore } = await setupDatabaseServices();
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: {
      id: "id",
      type: "log",
      chainId: 1,
      sourceId: "factory",
      address: "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
      eventSelector:
        "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
      childAddressLocation: "topic1",
      fromBlock: 10,
      toBlock: 20,
    },
    fromBlock: 10,
    toBlock: 20,
  } satisfies LogFilter;
  await syncStore.insertIntervals({
    intervals: [
      {
        filter,
        interval: [10, 20],
      },
    ],
    factoryIntervals: [],
    chainId: 1,
  });
  const intervals = await syncStore.getIntervals({
    filters: [filter],
  });
  expect(intervals).toMatchInlineSnapshot(`
    Map {
      {
        "address": {
          "address": "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
          "chainId": 1,
          "childAddressLocation": "topic1",
          "eventSelector": "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
          "fromBlock": 10,
          "id": "id",
          "sourceId": "factory",
          "toBlock": 20,
          "type": "log",
        },
        "chainId": 1,
        "fromBlock": 10,
        "hasTransactionReceipt": false,
        "include": [],
        "sourceId": "test",
        "toBlock": 20,
        "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
        "topic1": null,
        "topic2": null,
        "topic3": null,
        "type": "log",
      } => [
        {
          "fragment": {
            "address": {
              "address": "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
              "childAddressLocation": "topic1",
              "eventSelector": "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
            },
            "chainId": 1,
            "includeTransactionReceipts": false,
            "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "topic1": null,
            "topic2": null,
            "topic3": null,
            "type": "log",
          },
          "intervals": [
            [
              10,
              20,
            ],
          ],
        },
      ],
      {
        "address": "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
        "chainId": 1,
        "childAddressLocation": "topic1",
        "eventSelector": "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
        "fromBlock": 10,
        "id": "id",
        "sourceId": "factory",
        "toBlock": 20,
        "type": "log",
      } => [
        {
          "fragment": {
            "address": "0xef2d6d194084c2de36e0dabfce45d046b37d1106",
            "chainId": 1,
            "childAddressLocation": "topic1",
            "eventSelector": "0x02c69be41d0b7e40352fc85be1cd65eb03d40ef8427a0ca4596b1ead9a00e9fc",
            "fromBlock": 10,
            "toBlock": 20,
            "type": "factory_log",
          },
          "intervals": [
            [
              10,
              20,
            ],
          ],
        },
      ],
    }
  `);
});
test("insertIntervals() merges duplicates", async () => {
  const { syncStore } = await setupDatabaseServices();
  const filter = EMPTY_BLOCK_FILTER;
  await syncStore.insertIntervals({
    intervals: [
      {
        filter,
        interval: [0, 4],
      },
    ],
    factoryIntervals: [],
    chainId: 1,
  });
  await syncStore.insertIntervals({
    intervals: [
      {
        filter,
        interval: [5, 6],
      },
      {
        filter,
        interval: [5, 8],
      },
    ],
    factoryIntervals: [],
    chainId: 1,
  });
  const intervals = await syncStore.getIntervals({
    filters: [filter],
  });
  expect(intervals).toMatchInlineSnapshot(`
    Map {
      {
        "chainId": 1,
        "fromBlock": undefined,
        "hasTransactionReceipt": false,
        "include": [],
        "interval": 1,
        "offset": 0,
        "sourceId": "test",
        "toBlock": undefined,
        "type": "block",
      } => [
        {
          "fragment": {
            "chainId": 1,
            "interval": 1,
            "offset": 0,
            "type": "block",
          },
          "intervals": [
            [
              0,
              8,
            ],
          ],
        },
      ],
    }
  `);
});
test("insertIntervals() preserves fragments", async () => {
  const { syncStore } = await setupDatabaseServices();
  const filter = {
    ...EMPTY_LOG_FILTER,
    address: [zeroAddress, ALICE],
  } satisfies LogFilter;
  await syncStore.insertIntervals({
    intervals: [
      {
        filter,
        interval: [0, 4],
      },
    ],
    factoryIntervals: [],
    chainId: 1,
  });
  const intervals = await syncStore.getIntervals({
    filters: [filter],
  });
  expect(intervals).toMatchInlineSnapshot(`
    Map {
      {
        "address": [
          "0x0000000000000000000000000000000000000000",
          "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
        ],
        "chainId": 1,
        "fromBlock": undefined,
        "hasTransactionReceipt": false,
        "include": [],
        "sourceId": "test",
        "toBlock": undefined,
        "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
        "topic1": null,
        "topic2": null,
        "topic3": null,
        "type": "log",
      } => [
        {
          "fragment": {
            "address": "0x0000000000000000000000000000000000000000",
            "chainId": 1,
            "includeTransactionReceipts": false,
            "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "topic1": null,
            "topic2": null,
            "topic3": null,
            "type": "log",
          },
          "intervals": [
            [
              0,
              4,
            ],
          ],
        },
        {
          "fragment": {
            "address": "0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266",
            "chainId": 1,
            "includeTransactionReceipts": false,
            "topic0": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "topic1": null,
            "topic2": null,
            "topic3": null,
            "type": "log",
          },
          "intervals": [
            [
              0,
              4,
            ],
          ],
        },
      ],
    }
  `);
});
test("getChildAddresses()", async () => {
  const { syncStore } = await setupDatabaseServices();
  const { address } = await deployFactory({ sender: ALICE });
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({ address });
  const filter = eventCallbacks[0]!.filter as LogFilter<Factory>;
  await syncStore.insertChildAddresses({
    factory: filter.address,
    childAddresses: new Map([[pair, 0]]),
    chainId: 1,
  });
  const addresses = await syncStore.getChildAddresses({
    factory: filter.address,
  });
  expect(addresses).toMatchInlineSnapshot(`
    Map {
      "0xa16e02e87b7454126e5e10d957a927a7f5b5d2be" => 0,
    }
  `);
});
test("getChildAddresses() empty", async () => {
  const { syncStore } = await setupDatabaseServices();
  const { address } = await deployFactory({ sender: ALICE });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({ address });
  const filter = eventCallbacks[0]!.filter as LogFilter<Factory>;
  const addresses = await syncStore.getChildAddresses({
    factory: filter.address,
  });
  expect(addresses).toMatchInlineSnapshot("Map {}");
});
test("getChildAddresses() distinct", async () => {
  const { syncStore } = await setupDatabaseServices();
  const { address } = await deployFactory({ sender: ALICE });
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({ address });
  const filter = eventCallbacks[0]!.filter as LogFilter<Factory>;
  await syncStore.insertChildAddresses({
    factory: filter.address,
    childAddresses: new Map([[pair, 0]]),
    chainId: 1,
  });
  await syncStore.insertChildAddresses({
    factory: filter.address,
    childAddresses: new Map([[pair, 3]]),
    chainId: 1,
  });
  const addresses = await syncStore.getChildAddresses({
    factory: filter.address,
  });
  expect(addresses).toMatchInlineSnapshot(`
    Map {
      "0xa16e02e87b7454126e5e10d957a927a7f5b5d2be" => 0,
    }
  `);
});
test("getCrashRecoveryBlock()", async () => {
  const { syncStore } = await setupDatabaseServices();
  const chain = getChain();
  const blockData1 = await simulateBlock();
  const blockData2 = await simulateBlock();
  const blockData3 = await simulateBlock();
  const blockData4 = await simulateBlock();
  blockData1.block.timestamp = blockData1.block.number;
  blockData2.block.timestamp = blockData2.block.number;
  blockData3.block.timestamp = blockData3.block.number;
  blockData4.block.timestamp = blockData4.block.number;
  await syncStore.insertBlocks({
    blocks: [
      blockData1.block,
      blockData2.block,
      blockData3.block,
      blockData4.block,
    ],
    chainId: 1,
  });
  const result = await syncStore.getSafeCrashRecoveryBlock({
    chainId: chain.id,
    timestamp: hexToNumber(blockData3.block.timestamp),
  });
  expect(result).toEqual({
    number: hexToBigInt(blockData2.block.number),
    timestamp: hexToBigInt(blockData2.block.timestamp),
  });
});
test("insertChildAddresses()", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const { address } = await deployFactory({ sender: ALICE });
  const { address: pair } = await createPair({
    factory: address,
    sender: ALICE,
  });
  const { eventCallbacks } = getPairWithFactoryIndexingBuild({ address });
  const filter = eventCallbacks[0]!.filter as LogFilter<Factory>;
  await syncStore.insertChildAddresses({
    factory: filter.address,
    childAddresses: new Map([[pair, 0]]),
    chainId: 1,
  });
  await syncStore.insertChildAddresses({
    factory: filter.address,
    childAddresses: new Map([[pair, 3]]),
    chainId: 1,
  });
  const factories = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.factories).execute(),
  );
  const factoryAddresses = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.factoryAddresses).execute(),
  );
  expect(factories).toHaveLength(1);
  expect(factoryAddresses).toHaveLength(2);
});
test("insertLogs()", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await syncStore.insertLogs({ logs: [blockData.log], chainId: 1 });
  const logs = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.logs).execute(),
  );
  expect(logs).toHaveLength(1);
});
test("insertLogs() with duplicates", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await syncStore.insertLogs({ logs: [blockData.log], chainId: 1 });
  await syncStore.insertLogs({ logs: [blockData.log], chainId: 1 });
  const logs = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.logs).execute(),
  );
  expect(logs).toHaveLength(1);
});
test("insertBlocks()", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const blockData = await simulateBlock();
  await syncStore.insertBlocks({ blocks: [blockData.block], chainId: 1 });
  const blocks = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.blocks).execute(),
  );
  expect(blocks).toHaveLength(1);
});
test("insertBlocks() with duplicates", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const blockData = await simulateBlock();
  await syncStore.insertBlocks({ blocks: [blockData.block], chainId: 1 });
  await syncStore.insertBlocks({ blocks: [blockData.block], chainId: 1 });
  const blocks = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.blocks).execute(),
  );
  expect(blocks).toHaveLength(1);
});
test("insertTransactions()", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await syncStore.insertTransactions({
    transactions: [blockData.transaction],
    chainId: 1,
  });
  const transactions = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.transactions).execute(),
  );
  expect(transactions).toHaveLength(1);
});
test("insertTransactions() with duplicates", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await syncStore.insertTransactions({
    transactions: [blockData.transaction],
    chainId: 1,
  });
  await syncStore.insertTransactions({
    transactions: [blockData.transaction],
    chainId: 1,
  });
  const transactions = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.transactions).execute(),
  );
  expect(transactions).toHaveLength(1);
});
test("insertTransactionReceipts()", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await syncStore.insertTransactionReceipts({
    transactionReceipts: [blockData.transactionReceipt],
    chainId: 1,
  });
  const transactionReceipts = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.transactionReceipts).execute(),
  );
  expect(transactionReceipts).toHaveLength(1);
});
test("insertTransactionReceipts() with duplicates", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await syncStore.insertTransactionReceipts({
    transactionReceipts: [blockData.transactionReceipt],
    chainId: 1,
  });
  await syncStore.insertTransactionReceipts({
    transactionReceipts: [blockData.transactionReceipt],
    chainId: 1,
  });
  const transactionReceipts = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.transactionReceipts).execute(),
  );
  expect(transactionReceipts).toHaveLength(1);
});
test("insertTraces()", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const blockData = await transferErc20({
    erc20: address,
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await syncStore.insertTraces({
    traces: [
      {
        trace: blockData.trace,
        block: blockData.block,
        transaction: blockData.transaction,
      },
    ],
    chainId: 1,
  });
  const traces = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.traces).execute(),
  );
  expect(traces).toHaveLength(1);
});
test("insertTraces() with duplicates", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const blockData = await transferErc20({
    erc20: address,
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await syncStore.insertTraces({
    traces: [
      {
        trace: blockData.trace,
        block: blockData.block,
        transaction: blockData.transaction,
      },
    ],
    chainId: 1,
  });
  await syncStore.insertTraces({
    traces: [
      {
        trace: blockData.trace,
        block: blockData.block,
        transaction: blockData.transaction,
      },
    ],
    chainId: 1,
  });
  const traces = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.traces).execute(),
  );
  expect(traces).toHaveLength(1);
});
test("getEventBlockData() returns events", async () => {
  const { syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await syncStore.insertLogs({
    logs: [blockData.log],
    chainId: 1,
  });
  await syncStore.insertTransactions({
    transactions: [blockData.transaction],
    chainId: 1,
  });
  await syncStore.insertBlocks({ blocks: [blockData.block], chainId: 1 });
  const filter = EMPTY_LOG_FILTER;
  const { blocks } = await syncStore.getEventData({
    filters: [filter],
    fromBlock: 0,
    toBlock: 10,
    chainId: 1,
    limit: 10,
  });
  expect(blocks).toHaveLength(1);
});
test("getEventBlockData() pagination", async () => {
  const { syncStore } = await setupDatabaseServices();
  const blockData1 = await simulateBlock();
  const blockData2 = await simulateBlock();
  const { eventCallbacks } = getBlocksIndexingBuild({
    interval: 1,
  });
  await syncStore.insertBlocks({ blocks: [blockData1.block], chainId: 1 });
  await syncStore.insertBlocks({ blocks: [blockData2.block], chainId: 1 });
  const { blocks, cursor } = await syncStore.getEventData({
    filters: [eventCallbacks[0].filter],
    fromBlock: 0,
    toBlock: 10,
    chainId: 1,
    limit: 1,
  });
  expect(blocks).toHaveLength(1);
  const { blocks: blocks2 } = await syncStore.getEventData({
    filters: [eventCallbacks[0].filter],
    fromBlock: cursor,
    toBlock: 10,
    chainId: 1,
    limit: 1,
  });
  expect(blocks2).toHaveLength(1);
});
test("insertRpcRequestResults() ", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  await syncStore.insertRpcRequestResults({
    requests: [
      {
        // @ts-ignore
        request: { method: "eth_call", params: ["0x1"] },
        blockNumber: 1,
        result: "0x1",
      },
    ],
    chainId: 1,
  });
  const result = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.rpcRequestResults).execute(),
  );
  expect(result).toHaveLength(1);
  expect(result[0]!.requestHash).toBe("39d5ace8093d42c1bd00ce7781a7891a");
  expect(result[0]!.result).toBe("0x1");
});
test("insertRpcRequestResults() hash matches postgres", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  await syncStore.insertRpcRequestResults({
    requests: [
      {
        // @ts-ignore
        request: { method: "eth_call", params: ["0x1"] },
        blockNumber: 1,
        result: "0x1",
      },
    ],
    chainId: 1,
  });
  const jsHash = await database.syncQB
    .wrap((db) =>
      db.select().from(ponderSyncSchema.rpcRequestResults).execute(),
    )
    .then((result) => result[0]!.requestHash);
  const psqlHash = await database.syncQB.wrap((db) =>
    db.execute(
      sql`SELECT MD5(${JSON.stringify(orderObject({ method: "eth_call", params: ["0x1"] }))}) as request_hash`,
    ),
  );
  expect(jsHash).toBe(psqlHash.rows[0]!.request_hash);
});
test("getRpcRequestResults()", async () => {
  const { syncStore } = await setupDatabaseServices();
  await syncStore.insertRpcRequestResults({
    requests: [
      {
        // @ts-ignore
        request: { method: "eth_call", params: ["0x1"] },
        blockNumber: 1,
        result: "0x1",
      },
    ],
    chainId: 1,
  });
  const result = await syncStore.getRpcRequestResults({
    requests: [
      // @ts-ignore
      { method: "eth_call", params: ["0x1"] },
      // @ts-ignore
      { method: "eth_call", params: ["0x2"] },
    ],
    chainId: 1,
  });
  expect(result).toMatchInlineSnapshot(`
    [
      "0x1",
      undefined,
    ]
  `);
});
test("getEventBlockData() pagination with multiple filters", async () => {
  const { syncStore } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData2 = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const blockData3 = await simulateBlock();
  const erc20IndexingBuild = getErc20IndexingBuild({
    address,
  });
  const blocksIndexingBuild = getBlocksIndexingBuild({
    interval: 1,
  });
  await syncStore.insertBlocks({ blocks: [blockData2.block], chainId: 1 });
  await syncStore.insertTransactions({
    transactions: [blockData2.transaction],
    chainId: 1,
  });
  await syncStore.insertLogs({
    logs: [blockData2.log],
    chainId: 1,
  });
  await syncStore.insertBlocks({ blocks: [blockData3.block], chainId: 1 });
  const { blocks, cursor } = await syncStore.getEventData({
    filters: [
      erc20IndexingBuild.eventCallbacks[0].filter,
      blocksIndexingBuild.eventCallbacks[0].filter,
    ],
    fromBlock: 0,
    toBlock: 10,
    chainId: 1,
    limit: 3,
  });
  expect(blocks).toHaveLength(2);
  expect(cursor).toBe(10);
});
test("pruneRpcRequestResult", async () => {
  const { database, syncStore } = await setupDatabaseServices();
  await syncStore.insertRpcRequestResults({
    requests: [
      {
        // @ts-ignore
        request: { method: "eth_call", params: ["0x1"] },
        blockNumber: 1,
        result: "0x1",
      },
      {
        // @ts-ignore
        request: { method: "eth_call", params: ["0x2"] },
        blockNumber: 2,
        result: "0x2",
      },
      {
        // @ts-ignore
        request: { method: "eth_call", params: ["0x3"] },
        blockNumber: 3,
        result: "0x3",
      },
      {
        // @ts-ignore
        request: { method: "eth_call", params: ["0x4"] },
        blockNumber: 4,
        result: "0x4",
      },
    ],
    chainId: 1,
  });
  await syncStore.pruneRpcRequestResults({
    blocks: [{ number: "0x2" }, { number: "0x4" }],
    chainId: 1,
  });
  const requestResults = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.rpcRequestResults).execute(),
  );
  expect(requestResults).toHaveLength(2);
});
test("pruneByChain deletes blocks, logs, traces, transactions", async () => {
  const { syncStore, database } = await setupDatabaseServices();
  const { address } = await deployErc20({ sender: ALICE });
  const blockData2 = await mintErc20({
    erc20: address,
    to: ALICE,
    amount: parseEther("1"),
    sender: ALICE,
  });
  const blockData3 = await transferErc20({
    erc20: address,
    to: BOB,
    amount: parseEther("1"),
    sender: ALICE,
  });
  await syncStore.insertBlocks({ blocks: [blockData2.block], chainId: 1 });
  await syncStore.insertTransactions({
    transactions: [blockData2.transaction],
    chainId: 1,
  });
  await syncStore.insertLogs({
    logs: [blockData2.log],
    chainId: 1,
  });
  await syncStore.insertTransactionReceipts({
    transactionReceipts: [blockData2.transactionReceipt],
    chainId: 1,
  });
  await syncStore.insertBlocks({ blocks: [blockData3.block], chainId: 1 });
  await syncStore.insertTransactions({
    transactions: [blockData3.transaction],
    chainId: 1,
  });
  await syncStore.insertLogs({
    logs: [blockData3.log],
    chainId: 1,
  });
  await syncStore.insertTransactionReceipts({
    transactionReceipts: [blockData3.transactionReceipt],
    chainId: 1,
  });
  await syncStore.insertTraces({
    traces: [
      {
        trace: blockData3.trace,
        block: blockData3.block,
        transaction: blockData3.transaction,
      },
    ],
    chainId: 1,
  });
  await syncStore.pruneByChain({ chainId: 1 });
  const logs = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.logs).execute(),
  );
  const blocks = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.blocks).execute(),
  );
  const traces = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.traces).execute(),
  );
  const transactions = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.transactions).execute(),
  );
  const transactionReceipts = await database.syncQB.wrap((db) =>
    db.select().from(ponderSyncSchema.transactionReceipts).execute(),
  );
  expect(logs).toHaveLength(0);
  expect(blocks).toHaveLength(0);
  expect(traces).toHaveLength(0);
  expect(transactions).toHaveLength(0);
  expect(transactionReceipts).toHaveLength(0);
});
</file>

<file path="packages/core/src/sync-store/index.ts">
import crypto from "node:crypto";
import type { QB } from "@/database/queryBuilder.js";
import { extractBlockNumberParam } from "@/indexing/client.js";
import type { Common } from "@/internal/common.js";
import type { Logger } from "@/internal/logger.js";
import type {
  BlockFilter,
  Factory,
  Filter,
  Fragment,
  FragmentId,
  InternalBlock,
  InternalLog,
  InternalTrace,
  InternalTransaction,
  InternalTransactionReceipt,
  LightBlock,
  LogFilter,
  RequiredInternalBlockColumns,
  RequiredInternalTraceColumns,
  RequiredInternalTransactionColumns,
  RequiredInternalTransactionReceiptColumns,
  SyncBlock,
  SyncBlockHeader,
  SyncLog,
  SyncTrace,
  SyncTransaction,
  SyncTransactionReceipt,
  TraceFilter,
  TransactionFilter,
  TransferFilter,
} from "@/internal/types.js";
import type { RequestParameters } from "@/rpc/index.js";
import {
  getFilterFactories,
  isAddressFactory,
  unionFilterIncludeBlock,
  unionFilterIncludeTrace,
  unionFilterIncludeTransaction,
  unionFilterIncludeTransactionReceipt,
} from "@/runtime/filter.js";
import {
  encodeFragment,
  getFactoryFragments,
  getFragments,
} from "@/runtime/fragments.js";
import type {
  IntervalWithFactory,
  IntervalWithFilter,
} from "@/runtime/index.js";
import type { Interval } from "@/utils/interval.js";
import { intervalUnion } from "@/utils/interval.js";
import { toLowerCase } from "@/utils/lowercase.js";
import { orderObject } from "@/utils/order.js";
import { startClock } from "@/utils/timer.js";
import {
  type SQL,
  and,
  asc,
  desc,
  eq,
  gte,
  inArray,
  isNull,
  lt,
  lte,
  or,
  sql,
} from "drizzle-orm";
import {
  type PgColumn,
  type PgSelectBase,
  unionAll,
} from "drizzle-orm/pg-core";
import { type Address, hexToNumber, isHex } from "viem";
import {
  encodeBlock,
  encodeLog,
  encodeTrace,
  encodeTransaction,
  encodeTransactionReceipt,
} from "./encode.js";
import * as PONDER_SYNC from "./schema.js";
export type SyncStore = {
  insertIntervals(
    args: {
      intervals: IntervalWithFilter[];
      factoryIntervals: IntervalWithFactory[];
      chainId: number;
    },
    context?: { logger?: Logger },
  ): Promise<void>;
  getIntervals(
    args: { filters: Filter[] },
    context?: { logger?: Logger },
  ): Promise<
    Map<Filter | Factory, { fragment: Fragment; intervals: Interval[] }[]>
  >;
  insertChildAddresses(
    args: {
      factory: Factory;
      childAddresses: Map<Address, number>;
      chainId: number;
    },
    context?: { logger?: Logger },
  ): Promise<void>;
  getChildAddresses(
    args: { factory: Factory },
    context?: { logger?: Logger },
  ): Promise<Map<Address, number>>;
  getSafeCrashRecoveryBlock(
    args: {
      chainId: number;
      timestamp: number;
    },
    context?: { logger?: Logger },
  ): Promise<{ number: bigint; timestamp: bigint } | undefined>;
  insertLogs(
    args: { logs: SyncLog[]; chainId: number },
    context?: { logger?: Logger },
  ): Promise<void>;
  insertBlocks(
    args: {
      blocks: (SyncBlock | SyncBlockHeader)[];
      chainId: number;
    },
    context?: { logger?: Logger },
  ): Promise<void>;
  insertTransactions(
    args: {
      transactions: SyncTransaction[];
      chainId: number;
    },
    context?: { logger?: Logger },
  ): Promise<void>;
  insertTransactionReceipts(
    args: {
      transactionReceipts: SyncTransactionReceipt[];
      chainId: number;
    },
    context?: { logger?: Logger },
  ): Promise<void>;
  insertTraces(
    args: {
      traces: {
        trace: SyncTrace;
        block: SyncBlock;
        transaction: SyncTransaction;
      }[];
      chainId: number;
    },
    context?: { logger?: Logger },
  ): Promise<void>;
  getEventData(
    args: {
      filters: Filter[];
      fromBlock: number;
      toBlock: number;
      chainId: number;
      limit: number;
    },
    context?: { logger?: Logger },
  ): Promise<{
    blocks: InternalBlock[];
    logs: InternalLog[];
    transactions: InternalTransaction[];
    transactionReceipts: InternalTransactionReceipt[];
    traces: InternalTrace[];
    cursor: number;
  }>;
  insertRpcRequestResults(
    args: {
      requests: {
        request: RequestParameters;
        blockNumber: number | undefined;
        result: string;
      }[];
      chainId: number;
    },
    context?: { logger?: Logger },
  ): Promise<void>;
  getRpcRequestResults(
    args: {
      requests: RequestParameters[];
      chainId: number;
    },
    context?: { logger?: Logger },
  ): Promise<(string | undefined)[]>;
  pruneRpcRequestResults(
    args: {
      blocks: Pick<LightBlock, "number">[];
      chainId: number;
    },
    context?: { logger?: Logger },
  ): Promise<void>;
  pruneByChain(
    args: { chainId: number },
    context?: { logger?: Logger },
  ): Promise<void>;
};
export const createSyncStore = ({
  common,
  qb,
}: { common: Common; qb: QB<typeof PONDER_SYNC> }): SyncStore => {
  const syncStore = {
    insertIntervals: async (
      { intervals, factoryIntervals, chainId },
      context,
    ) => {
      if (intervals.length === 0 && factoryIntervals.length === 0) return;
      const perFragmentIntervals = new Map<FragmentId, Interval[]>();
      const values: (typeof PONDER_SYNC.intervals.$inferInsert)[] = [];
      // dedupe and merge matching fragments
      for (const { filter, interval } of intervals) {
        for (const fragment of getFragments(filter)) {
          const fragmentId = encodeFragment(fragment.fragment);
          if (perFragmentIntervals.has(fragmentId) === false) {
            perFragmentIntervals.set(fragmentId, []);
          }
          perFragmentIntervals.get(fragmentId)!.push(interval);
        }
      }
      for (const { factory, interval } of factoryIntervals) {
        for (const fragment of getFactoryFragments(factory)) {
          const fragmentId = encodeFragment(fragment);
          if (perFragmentIntervals.has(fragmentId) === false) {
            perFragmentIntervals.set(fragmentId, []);
          }
          perFragmentIntervals.get(fragmentId)!.push(interval);
        }
      }
      // NOTE: In order to force proper range union behavior, `interval[1]` must
      // be rounded up.
      for (const [fragmentId, intervals] of perFragmentIntervals) {
        const numranges = intervals
          .map((interval) => {
            const start = interval[0];
            const end = interval[1] + 1;
            return `numrange(${start}, ${end}, '[]')`;
          })
          .join(", ");
        values.push({
          fragmentId: fragmentId,
          chainId: BigInt(chainId),
          // @ts-expect-error
          blocks: sql.raw(`nummultirange(${numranges})`),
        });
      }
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters / 3,
      );
      for (let i = 0; i < values.length; i += batchSize) {
        await qb.wrap(
          { label: "insert_intervals" },
          (db) =>
            db
              .insert(PONDER_SYNC.intervals)
              .values(values.slice(i, i + batchSize))
              .onConflictDoUpdate({
                target: PONDER_SYNC.intervals.fragmentId,
                set: { blocks: sql`intervals.blocks + excluded.blocks` },
              }),
          context,
        );
      }
    },
    getIntervals: async ({ filters }, context) => {
      const queries: PgSelectBase<
        "unnested",
        {
          mergedBlocks: SQL.Aliased<string>;
          fragment: SQL.Aliased<unknown>;
        },
        "partial"
      >[] = [];
      let index = 0;
      for (const filter of filters) {
        const fragments = getFragments(filter);
        for (const fragment of fragments) {
          queries.push(
            qb.raw
              .select({
                mergedBlocks: sql<string>`range_agg(unnested.blocks)`.as(
                  "merged_blocks",
                ),
                fragment: sql.raw(`'${index++}'`).as("fragment"),
              })
              .from(
                qb.raw
                  .select({ blocks: sql.raw("unnest(blocks)").as("blocks") })
                  .from(PONDER_SYNC.intervals)
                  .where(
                    sql.raw(
                      `fragment_id IN (${fragment.adjacentIds.map((id) => `'${id}'`).join(", ")})`,
                    ),
                  )
                  .as("unnested"),
              ),
          );
        }
        for (const factory of getFilterFactories(filter)) {
          for (const fragment of getFactoryFragments(factory)) {
            queries.push(
              qb.raw
                .select({
                  mergedBlocks: sql<string>`range_agg(unnested.blocks)`.as(
                    "merged_blocks",
                  ),
                  fragment: sql.raw(`'${index++}'`).as("fragment"),
                })
                .from(
                  qb.raw
                    .select({
                      blocks: sql.raw("unnest(blocks)").as("blocks"),
                    })
                    .from(PONDER_SYNC.intervals)
                    .where(
                      sql.raw(`fragment_id = '${encodeFragment(fragment)}'`),
                    )
                    .as("unnested"),
                ),
            );
          }
        }
      }
      let rows: Awaited<(typeof queries)[number]> = [];
      if (queries.length > 1) {
        // Note: This query has no parameters, but there is a bug with
        // drizzle causing a "maximum call stack size exceeded" error.
        // Related: https://github.com/drizzle-team/drizzle-orm/issues/1740
        const batchSize = 200;
        for (let i = 0; i < queries.length; i += batchSize) {
          const _rows = await qb.wrap(
            { label: "select_intervals" },
            () =>
              // @ts-expect-error
              unionAll(...queries.slice(i, i + batchSize)),
            context,
          );
          if (i === 0) {
            rows = _rows;
          } else {
            rows.push(..._rows);
          }
        }
      } else {
        rows = await qb.wrap(
          { label: "select_intervals" },
          () => queries[0]!.execute(),
          context,
        );
      }
      const result = new Map<
        Filter | Factory,
        { fragment: Fragment; intervals: Interval[] }[]
      >();
      // NOTE: `interval[1]` must be rounded down in order to offset the previous
      // rounding.
      index = 0;
      for (const filter of filters) {
        const fragments = getFragments(filter);
        result.set(filter, []);
        for (const fragment of fragments) {
          const intervals = rows
            .filter((row) => row.fragment === `${index}`)
            .map((row) =>
              (row.mergedBlocks
                ? (JSON.parse(
                    `[${row.mergedBlocks.slice(1, -1)}]`,
                  ) as Interval[])
                : []
              ).map((interval) => [interval[0], interval[1] - 1] as Interval),
            )[0]!;
          index += 1;
          result.get(filter)!.push({ fragment: fragment.fragment, intervals });
        }
        for (const factory of getFilterFactories(filter)) {
          result.set(factory, []);
          for (const fragment of getFactoryFragments(factory)) {
            const intervals = rows
              .filter((row) => row.fragment === `${index}`)
              .map((row) =>
                (row.mergedBlocks
                  ? (JSON.parse(
                      `[${row.mergedBlocks.slice(1, -1)}]`,
                    ) as Interval[])
                  : []
                ).map((interval) => [interval[0], interval[1] - 1] as Interval),
              )[0]!;
            index += 1;
            result.get(factory)!.push({ fragment, intervals });
          }
          // Note: This is a stand-in for a migration to the `intervals` table
          // required in `v0.15`. It is an invariant that filter with factories
          // have a row in the intervals table for both the filter and the factory.
          // If this invariant is broken, it must be because of the migration from
          // `v0.14` to `v0.15`. In this case, we can assume that the factory interval
          // is the same as the filter interval.
          const filterIntervals = intervalUnion(
            result.get(filter)!.flatMap(({ intervals }) => intervals),
          );
          const factoryIntervals = intervalUnion(
            result.get(factory)!.flatMap(({ intervals }) => intervals),
          );
          if (
            filterIntervals.length > 0 &&
            factoryIntervals.length === 0 &&
            filter.fromBlock === factory.fromBlock &&
            filter.toBlock === factory.toBlock
          ) {
            for (const factoryInterval of result.get(factory)!) {
              factoryInterval.intervals = filterIntervals;
            }
          }
        }
      }
      return result;
    },
    insertChildAddresses: async (
      { factory, childAddresses, chainId },
      context,
    ) => {
      if (childAddresses.size === 0) return;
      const { id, sourceId: _sourceId, ..._factory } = factory;
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters / 3,
      );
      const values: (typeof PONDER_SYNC.factoryAddresses.$inferInsert)[] = [];
      const factoryInsert = qb.raw.$with("factory_insert").as(
        qb.raw
          .insert(PONDER_SYNC.factories)
          .values({ factory: _factory })
          // @ts-expect-error bug with drizzle-orm
          .returning({ id: PONDER_SYNC.factories.id })
          .onConflictDoUpdate({
            target: PONDER_SYNC.factories.factory,
            set: { factory: sql`excluded.factory` },
          }),
      );
      for (const [address, blockNumber] of childAddresses) {
        values.push({
          // @ts-expect-error
          factoryId: sql`(SELECT id FROM factory_insert)`,
          chainId: BigInt(chainId),
          blockNumber: BigInt(blockNumber),
          address,
        });
      }
      for (let i = 0; i < values.length; i += batchSize) {
        await qb.wrap(
          { label: "insert_child_addresses" },
          (db) =>
            db
              .with(factoryInsert)
              .insert(PONDER_SYNC.factoryAddresses)
              .values(values.slice(i, i + batchSize)),
          context,
        );
      }
    },
    getChildAddresses: ({ factory }, context) => {
      const { id, sourceId: _sourceId, ..._factory } = factory;
      const factoryInsert = qb.raw.$with("factory_insert").as(
        qb.raw
          .insert(PONDER_SYNC.factories)
          .values({ factory: _factory })
          // @ts-expect-error bug with drizzle-orm
          .returning({ id: PONDER_SYNC.factories.id })
          .onConflictDoUpdate({
            target: PONDER_SYNC.factories.factory,
            set: { factory: sql`excluded.factory` },
          }),
      );
      return qb
        .wrap(
          { label: "select_child_addresses" },
          (db) =>
            db
              .with(factoryInsert)
              .select({
                address: PONDER_SYNC.factoryAddresses.address,
                blockNumber: PONDER_SYNC.factoryAddresses.blockNumber,
              })
              .from(PONDER_SYNC.factoryAddresses)
              .where(
                eq(
                  PONDER_SYNC.factoryAddresses.factoryId,
                  qb.raw.select({ id: factoryInsert.id }).from(factoryInsert),
                ),
              ),
          context,
        )
        .then((rows) => {
          const result = new Map<Address, number>();
          for (const { address, blockNumber } of rows) {
            if (
              result.has(address) === false ||
              result.get(address)! > Number(blockNumber)
            ) {
              result.set(address, Number(blockNumber));
            }
          }
          return result;
        });
    },
    getSafeCrashRecoveryBlock: async ({ chainId, timestamp }, context) => {
      const rows = await qb.wrap(
        { label: "select_crash_recovery_block" },
        (db) =>
          db
            .select({
              number: PONDER_SYNC.blocks.number,
              timestamp: PONDER_SYNC.blocks.timestamp,
            })
            .from(PONDER_SYNC.blocks)
            .where(
              and(
                eq(PONDER_SYNC.blocks.chainId, BigInt(chainId)),
                lt(PONDER_SYNC.blocks.timestamp, BigInt(timestamp)),
              ),
            )
            .orderBy(desc(PONDER_SYNC.blocks.number))
            .limit(1),
        context,
      );
      return rows[0];
    },
    insertLogs: async ({ logs, chainId }, context) => {
      if (logs.length === 0) return;
      // Calculate `batchSize` based on how many parameters the
      // input will have
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters /
          Object.keys(encodeLog({ log: logs[0]!, chainId })).length,
      );
      // As an optimization, logs that are matched by a factory do
      // not contain a checkpoint, because not corresponding block is
      // fetched (no block.timestamp). However, when a log is matched by
      // both a log filter and a factory, the checkpoint must be included
      // in the db.
      for (let i = 0; i < logs.length; i += batchSize) {
        await qb.wrap(
          { label: "insert_logs" },
          (db) =>
            db
              .insert(PONDER_SYNC.logs)
              .values(
                logs
                  .slice(i, i + batchSize)
                  .map((log) => encodeLog({ log, chainId })),
              )
              .onConflictDoNothing({
                target: [
                  PONDER_SYNC.logs.chainId,
                  PONDER_SYNC.logs.blockNumber,
                  PONDER_SYNC.logs.logIndex,
                ],
              }),
          context,
        );
      }
    },
    insertBlocks: async ({ blocks, chainId }, context) => {
      if (blocks.length === 0) return;
      // Calculate `batchSize` based on how many parameters the
      // input will have
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters /
          Object.keys(encodeBlock({ block: blocks[0]!, chainId })).length,
      );
      for (let i = 0; i < blocks.length; i += batchSize) {
        await qb.wrap(
          { label: "insert_blocks" },
          (db) =>
            db
              .insert(PONDER_SYNC.blocks)
              .values(
                blocks
                  .slice(i, i + batchSize)
                  .map((block) => encodeBlock({ block, chainId })),
              )
              .onConflictDoNothing({
                target: [PONDER_SYNC.blocks.chainId, PONDER_SYNC.blocks.number],
              }),
          context,
        );
      }
    },
    insertTransactions: async ({ transactions, chainId }, context) => {
      if (transactions.length === 0) return;
      // Calculate `batchSize` based on how many parameters the
      // input will have
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters /
          Object.keys(
            encodeTransaction({
              transaction: transactions[0]!,
              chainId,
            }),
          ).length,
      );
      for (let i = 0; i < transactions.length; i += batchSize) {
        await qb.wrap(
          { label: "insert_transactions" },
          (db) =>
            db
              .insert(PONDER_SYNC.transactions)
              .values(
                transactions
                  .slice(i, i + batchSize)
                  .map((transaction) =>
                    encodeTransaction({ transaction, chainId }),
                  ),
              )
              .onConflictDoNothing({
                target: [
                  PONDER_SYNC.transactions.chainId,
                  PONDER_SYNC.transactions.blockNumber,
                  PONDER_SYNC.transactions.transactionIndex,
                ],
              }),
          context,
        );
      }
    },
    insertTransactionReceipts: async (
      { transactionReceipts, chainId },
      context,
    ) => {
      if (transactionReceipts.length === 0) return;
      // Calculate `batchSize` based on how many parameters the
      // input will have
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters /
          Object.keys(
            encodeTransactionReceipt({
              transactionReceipt: transactionReceipts[0]!,
              chainId,
            }),
          ).length,
      );
      for (let i = 0; i < transactionReceipts.length; i += batchSize) {
        await qb.wrap(
          { label: "insert_transaction_receipts" },
          (db) =>
            db
              .insert(PONDER_SYNC.transactionReceipts)
              .values(
                transactionReceipts
                  .slice(i, i + batchSize)
                  .map((transactionReceipt) =>
                    encodeTransactionReceipt({
                      transactionReceipt,
                      chainId,
                    }),
                  ),
              )
              .onConflictDoNothing({
                target: [
                  PONDER_SYNC.transactionReceipts.chainId,
                  PONDER_SYNC.transactionReceipts.blockNumber,
                  PONDER_SYNC.transactionReceipts.transactionIndex,
                ],
              }),
          context,
        );
      }
    },
    insertTraces: async ({ traces, chainId }, context) => {
      if (traces.length === 0) return;
      // Calculate `batchSize` based on how many parameters the
      // input will have
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters /
          Object.keys(
            encodeTrace({
              trace: traces[0]!.trace,
              block: traces[0]!.block,
              transaction: traces[0]!.transaction,
              chainId,
            }),
          ).length,
      );
      for (let i = 0; i < traces.length; i += batchSize) {
        await qb.wrap(
          { label: "insert_traces" },
          (db) =>
            db
              .insert(PONDER_SYNC.traces)
              .values(
                traces
                  .slice(i, i + batchSize)
                  .map(({ trace, block, transaction }) =>
                    encodeTrace({ trace, block, transaction, chainId }),
                  ),
              )
              .onConflictDoNothing({
                target: [
                  PONDER_SYNC.traces.chainId,
                  PONDER_SYNC.traces.blockNumber,
                  PONDER_SYNC.traces.transactionIndex,
                  PONDER_SYNC.traces.traceIndex,
                ],
              }),
          context,
        );
      }
    },
    getEventData: async (
      { filters, fromBlock, toBlock, chainId, limit },
      context,
    ): Promise<{
      blocks: InternalBlock[];
      logs: InternalLog[];
      transactions: InternalTransaction[];
      transactionReceipts: InternalTransactionReceipt[];
      traces: InternalTrace[];
      cursor: number;
    }> => {
      const logFilters = filters.filter(
        (f): f is LogFilter => f.type === "log",
      );
      const transactionFilters = filters.filter(
        (f): f is TransactionFilter => f.type === "transaction",
      );
      const traceFilters = filters.filter(
        (f): f is TraceFilter => f.type === "trace",
      );
      const transferFilters = filters.filter(
        (f): f is TransferFilter => f.type === "transfer",
      );
      const shouldQueryBlocks = true;
      const shouldQueryLogs = logFilters.length > 0;
      const shouldQueryTraces =
        traceFilters.length > 0 || transferFilters.length > 0;
      const shouldQueryTransactions =
        transactionFilters.length > 0 || shouldQueryLogs || shouldQueryTraces;
      const shouldQueryTransactionReceipts = filters.some(
        (filter) => filter.hasTransactionReceipt,
      );
      type BlockSelect = {
        [P in RequiredInternalBlockColumns]: (typeof PONDER_SYNC.blocks)[P];
      } & {
        [P in Exclude<
          keyof typeof PONDER_SYNC.blocks.$inferSelect,
          RequiredInternalBlockColumns
        >]?: (typeof PONDER_SYNC.blocks)[P];
      };
      type TransactionSelect = {
        [P in RequiredInternalTransactionColumns]: (typeof PONDER_SYNC.transactions)[P];
      } & {
        [P in Exclude<
          keyof typeof PONDER_SYNC.transactions.$inferSelect,
          RequiredInternalTransactionColumns
        >]?: (typeof PONDER_SYNC.transactions)[P];
      };
      type TransactionReceiptSelect = {
        [P in RequiredInternalTransactionReceiptColumns]: (typeof PONDER_SYNC.transactionReceipts)[P];
      } & {
        [P in Exclude<
          keyof typeof PONDER_SYNC.transactionReceipts.$inferSelect,
          RequiredInternalTransactionReceiptColumns
        >]?: (typeof PONDER_SYNC.transactionReceipts)[P];
      };
      type TraceSelect = {
        [P in RequiredInternalTraceColumns]: (typeof PONDER_SYNC.traces)[P];
      } & {
        [P in Exclude<
          keyof typeof PONDER_SYNC.traces.$inferSelect,
          RequiredInternalTraceColumns
        >]?: (typeof PONDER_SYNC.traces)[P];
      };
      // Note: `LogSelect` doesn't exist because all log columns are required.
      const blockSelect: BlockSelect = {
        number: PONDER_SYNC.blocks.number,
        hash: PONDER_SYNC.blocks.hash,
        timestamp: PONDER_SYNC.blocks.timestamp,
      };
      for (const column of unionFilterIncludeBlock(filters)) {
        // @ts-ignore
        blockSelect[column] = PONDER_SYNC.blocks[column];
      }
      const blocksQuery = qb.raw
        .select(blockSelect)
        .from(PONDER_SYNC.blocks)
        .where(
          and(
            eq(PONDER_SYNC.blocks.chainId, BigInt(chainId)),
            gte(PONDER_SYNC.blocks.number, BigInt(fromBlock)),
            lte(PONDER_SYNC.blocks.number, BigInt(toBlock)),
          ),
        )
        .orderBy(asc(PONDER_SYNC.blocks.number))
        .limit(limit);
      const transactionSelect: TransactionSelect = {
        blockNumber: PONDER_SYNC.transactions.blockNumber,
        transactionIndex: PONDER_SYNC.transactions.transactionIndex,
        from: PONDER_SYNC.transactions.from,
        to: PONDER_SYNC.transactions.to,
        hash: PONDER_SYNC.transactions.hash,
        type: PONDER_SYNC.transactions.type,
      };
      for (const column of unionFilterIncludeTransaction(filters)) {
        // @ts-ignore
        transactionSelect[column] = PONDER_SYNC.transactions[column];
      }
      const transactionsQuery = qb.raw
        .select(transactionSelect)
        .from(PONDER_SYNC.transactions)
        .where(
          and(
            eq(PONDER_SYNC.transactions.chainId, BigInt(chainId)),
            gte(PONDER_SYNC.transactions.blockNumber, BigInt(fromBlock)),
            lte(PONDER_SYNC.transactions.blockNumber, BigInt(toBlock)),
          ),
        )
        .orderBy(
          asc(PONDER_SYNC.transactions.blockNumber),
          asc(PONDER_SYNC.transactions.transactionIndex),
        )
        .limit(limit);
      const transactionReceiptSelect: TransactionReceiptSelect = {
        blockNumber: PONDER_SYNC.transactionReceipts.blockNumber,
        transactionIndex: PONDER_SYNC.transactionReceipts.transactionIndex,
        status: PONDER_SYNC.transactionReceipts.status,
        from: PONDER_SYNC.transactionReceipts.from,
        to: PONDER_SYNC.transactionReceipts.to,
      };
      for (const column of unionFilterIncludeTransactionReceipt(filters)) {
        // @ts-ignore
        transactionReceiptSelect[column] =
          PONDER_SYNC.transactionReceipts[column];
      }
      const transactionReceiptsQuery = qb.raw
        .select(transactionReceiptSelect)
        .from(PONDER_SYNC.transactionReceipts)
        .where(
          and(
            eq(PONDER_SYNC.transactionReceipts.chainId, BigInt(chainId)),
            gte(PONDER_SYNC.transactionReceipts.blockNumber, BigInt(fromBlock)),
            lte(PONDER_SYNC.transactionReceipts.blockNumber, BigInt(toBlock)),
          ),
        )
        .orderBy(
          asc(PONDER_SYNC.transactionReceipts.blockNumber),
          asc(PONDER_SYNC.transactionReceipts.transactionIndex),
        )
        .limit(limit);
      const traceSelect: TraceSelect = {
        blockNumber: PONDER_SYNC.traces.blockNumber,
        transactionIndex: PONDER_SYNC.traces.transactionIndex,
        from: PONDER_SYNC.traces.from,
        to: PONDER_SYNC.traces.to,
        input: PONDER_SYNC.traces.input,
        output: PONDER_SYNC.traces.output,
        value: PONDER_SYNC.traces.value,
        type: PONDER_SYNC.traces.type,
        error: PONDER_SYNC.traces.error,
        traceIndex: PONDER_SYNC.traces.traceIndex,
      };
      for (const column of unionFilterIncludeTrace(filters)) {
        // @ts-ignore
        traceSelect[column] = PONDER_SYNC.traces[column];
      }
      const tracesQuery = qb.raw
        .select(traceSelect)
        .from(PONDER_SYNC.traces)
        .where(
          and(
            eq(PONDER_SYNC.traces.chainId, BigInt(chainId)),
            gte(PONDER_SYNC.traces.blockNumber, BigInt(fromBlock)),
            lte(PONDER_SYNC.traces.blockNumber, BigInt(toBlock)),
            or(
              ...traceFilters.map((filter) => traceFilter(filter)),
              ...transferFilters.map((filter) => transferFilter(filter)),
            ),
          ),
        )
        .orderBy(
          asc(PONDER_SYNC.traces.blockNumber),
          asc(PONDER_SYNC.traces.transactionIndex),
          asc(PONDER_SYNC.traces.traceIndex),
        )
        .limit(limit);
      const logsQuery = qb.raw
        .select({
          blockNumber: PONDER_SYNC.logs.blockNumber,
          logIndex: PONDER_SYNC.logs.logIndex,
          transactionIndex: PONDER_SYNC.logs.transactionIndex,
          address: PONDER_SYNC.logs.address,
          topic0: PONDER_SYNC.logs.topic0,
          topic1: PONDER_SYNC.logs.topic1,
          topic2: PONDER_SYNC.logs.topic2,
          topic3: PONDER_SYNC.logs.topic3,
          data: PONDER_SYNC.logs.data,
        })
        .from(PONDER_SYNC.logs)
        .where(
          and(
            eq(PONDER_SYNC.logs.chainId, BigInt(chainId)),
            gte(PONDER_SYNC.logs.blockNumber, BigInt(fromBlock)),
            lte(PONDER_SYNC.logs.blockNumber, BigInt(toBlock)),
            or(...logFilters.map((filter) => logFilter(filter))),
          ),
        )
        .orderBy(
          asc(PONDER_SYNC.logs.blockNumber),
          asc(PONDER_SYNC.logs.logIndex),
        )
        .limit(limit);
      let endClock = startClock();
      const [
        blocksRows,
        transactionsRows,
        transactionReceiptsRows,
        logsRows,
        tracesRows,
      ] = await Promise.all([
        shouldQueryBlocks
          ? qb.wrap({ label: "select_blocks" }, () => blocksQuery)
          : [],
        shouldQueryTransactions
          ? qb.wrap(
              { label: "select_transactions" },
              () => transactionsQuery,
              context,
            )
          : [],
        shouldQueryTransactionReceipts
          ? qb.wrap(
              { label: "select_transaction_receipts" },
              () => transactionReceiptsQuery,
              context,
            )
          : [],
        shouldQueryLogs
          ? qb.wrap({ label: "select_logs" }, () => logsQuery, context)
          : [],
        shouldQueryTraces
          ? qb.wrap({ label: "select_traces" }, () => tracesQuery, context)
          : [],
      ]);
      const supremum = Math.min(
        blocksRows.length < limit
          ? Number.POSITIVE_INFINITY
          : Number(blocksRows[blocksRows.length - 1]!.number),
        transactionsRows.length < limit
          ? Number.POSITIVE_INFINITY
          : Number(transactionsRows[transactionsRows.length - 1]!.blockNumber),
        transactionReceiptsRows.length < limit
          ? Number.POSITIVE_INFINITY
          : Number(
              transactionReceiptsRows[transactionReceiptsRows.length - 1]!
                .blockNumber,
            ),
        logsRows.length < limit
          ? Number.POSITIVE_INFINITY
          : Number(logsRows[logsRows.length - 1]!.blockNumber),
        tracesRows.length < limit
          ? Number.POSITIVE_INFINITY
          : Number(tracesRows[tracesRows.length - 1]!.blockNumber),
      );
      endClock = startClock();
      let cursor: number;
      if (
        Math.max(
          blocksRows.length,
          transactionsRows.length,
          transactionReceiptsRows.length,
          logsRows.length,
          tracesRows.length,
        ) !== limit
      ) {
        cursor = toBlock;
      } else if (
        blocksRows.length === limit &&
        Math.max(
          transactionsRows.length,
          transactionReceiptsRows.length,
          logsRows.length,
          tracesRows.length,
        ) !== limit
      ) {
        // all events for `supremum` block have been extracted
        cursor = supremum;
      } else {
        // there may be events for `supremum` block that have not been extracted
        cursor = supremum - 1;
        if (cursor < fromBlock) {
          return syncStore.getEventData(
            {
              filters,
              fromBlock,
              toBlock,
              chainId,
              limit: limit * 2,
            },
            context,
          );
        }
      }
      endClock = startClock();
      for (let i = 0; i < blocksRows.length; i++) {
        if (Number(blocksRows[i]!.number) > cursor) {
          blocksRows.length = i;
          break;
        }
        const block = blocksRows[i]!;
        if (block.miner) {
          block.miner = toLowerCase(block.miner);
        }
      }
      for (let i = 0; i < transactionsRows.length; i++) {
        if (Number(transactionsRows[i]!.blockNumber) > cursor) {
          transactionsRows.length = i;
          break;
        }
        const transaction = transactionsRows[i]!;
        const internalTransaction =
          transaction as unknown as InternalTransaction;
        internalTransaction.blockNumber = Number(transaction.blockNumber);
        internalTransaction.from = toLowerCase(transaction.from);
        if (transaction.to !== null) {
          internalTransaction.to = toLowerCase(transaction.to);
        }
        if (transaction.type === "0x0") {
          internalTransaction.type = "legacy";
          internalTransaction.accessList = undefined;
          internalTransaction.maxFeePerGas = undefined;
          internalTransaction.maxPriorityFeePerGas = undefined;
        } else if (transaction.type === "0x1") {
          internalTransaction.type = "eip2930";
          internalTransaction.accessList =
            transaction.accessList === undefined
              ? undefined
              : JSON.parse(transaction.accessList!);
          internalTransaction.maxFeePerGas = undefined;
          internalTransaction.maxPriorityFeePerGas = undefined;
        } else if (transaction.type === "0x2") {
          internalTransaction.type = "eip1559";
          internalTransaction.gasPrice = undefined;
          internalTransaction.accessList = undefined;
        } else if (transaction.type === "0x7e") {
          internalTransaction.type = "deposit";
          internalTransaction.gasPrice = undefined;
          internalTransaction.accessList = undefined;
        }
      }
      for (let i = 0; i < transactionReceiptsRows.length; i++) {
        if (Number(transactionReceiptsRows[i]!.blockNumber) > cursor) {
          transactionReceiptsRows.length = i;
          break;
        }
        const transactionReceipt = transactionReceiptsRows[i]!;
        const internalTransactionReceipt =
          transactionReceipt as unknown as InternalTransactionReceipt;
        internalTransactionReceipt.blockNumber = Number(
          transactionReceipt.blockNumber,
        );
        if (transactionReceipt.contractAddress) {
          internalTransactionReceipt.contractAddress = toLowerCase(
            transactionReceipt.contractAddress,
          );
        }
        internalTransactionReceipt.from = toLowerCase(transactionReceipt.from);
        if (transactionReceipt.to !== null) {
          internalTransactionReceipt.to = toLowerCase(transactionReceipt.to);
        }
        internalTransactionReceipt.status =
          transactionReceipt.status === "0x1"
            ? "success"
            : transactionReceipt.status === "0x0"
              ? "reverted"
              : (transactionReceipt.status as InternalTransactionReceipt["status"]);
        internalTransactionReceipt.type =
          transactionReceipt.type === "0x0"
            ? "legacy"
            : transactionReceipt.type === "0x1"
              ? "eip2930"
              : transactionReceipt.type === "0x2"
                ? "eip1559"
                : transactionReceipt.type === "0x7e"
                  ? "deposit"
                  : transactionReceipt.type;
      }
      for (let i = 0; i < tracesRows.length; i++) {
        if (Number(tracesRows[i]!.blockNumber) > cursor) {
          tracesRows.length = i;
          break;
        }
        const trace = tracesRows[i]!;
        const internalTrace = trace as unknown as InternalTrace;
        internalTrace.blockNumber = Number(trace.blockNumber);
        internalTrace.from = toLowerCase(trace.from);
        if (trace.to !== null) {
          internalTrace.to = toLowerCase(trace.to);
        }
        if (trace.output === null) {
          internalTrace.output = undefined;
        }
        if (trace.error === null) {
          internalTrace.error = undefined;
        }
        if (trace.revertReason === null) {
          internalTrace.revertReason = undefined;
        }
      }
      for (let i = 0; i < logsRows.length; i++) {
        if (Number(logsRows[i]!.blockNumber) > cursor) {
          logsRows.length = i;
          break;
        }
        const log = logsRows[i]!;
        const internalLog = log as unknown as InternalLog;
        internalLog.blockNumber = Number(log.blockNumber);
        internalLog.address = toLowerCase(log.address);
        internalLog.removed = false;
        internalLog.topics = [
          // @ts-ignore
          log.topic0,
          log.topic1,
          log.topic2,
          log.topic3,
        ];
        // @ts-ignore
        log.topic0 = undefined;
        // @ts-ignore
        log.topic1 = undefined;
        // @ts-ignore
        log.topic2 = undefined;
        // @ts-ignore
        log.topic3 = undefined;
      }
      common.metrics.ponder_historical_extract_duration.inc(
        { step: "format" },
        endClock(),
      );
      await new Promise(setImmediate);
      return {
        blocks: blocksRows as InternalBlock[],
        logs: logsRows as InternalLog[],
        transactions: transactionsRows as InternalTransaction[],
        transactionReceipts:
          transactionReceiptsRows as InternalTransactionReceipt[],
        traces: tracesRows as InternalTrace[],
        cursor,
      };
    },
    insertRpcRequestResults: async ({ requests, chainId }, context) => {
      if (requests.length === 0) return;
      const values = requests.map(({ request, blockNumber, result }) => ({
        requestHash: crypto
          .createHash("md5")
          .update(toLowerCase(JSON.stringify(orderObject(request))))
          .digest("hex"),
        chainId: BigInt(chainId),
        blockNumber: blockNumber ? BigInt(blockNumber) : undefined,
        result,
      }));
      await qb.wrap(
        { label: "insert_rpc_requests" },
        (db) =>
          db
            .insert(PONDER_SYNC.rpcRequestResults)
            .values(values)
            .onConflictDoNothing({
              target: [
                PONDER_SYNC.rpcRequestResults.requestHash,
                PONDER_SYNC.rpcRequestResults.chainId,
              ],
            }),
        context,
      );
    },
    getRpcRequestResults: async ({ requests, chainId }, context) => {
      if (requests.length === 0) return [];
      // Optimized fast path for high number of `requests` using a range of block numbers
      // rather than querying each request individually.
      const blockNumbersByRequest: (number | undefined)[] = new Array(
        requests.length,
      );
      const requestHashes: string[] = new Array(requests.length);
      for (let i = 0; i < requests.length; i++) {
        const request = requests[i]!;
        const blockNumber = extractBlockNumberParam(request);
        // Note: "latest" is not considered a block number
        if (isHex(blockNumber)) {
          blockNumbersByRequest[i] = hexToNumber(blockNumber);
        } else {
          blockNumbersByRequest[i] = undefined;
        }
        const requestHash = crypto
          .createHash("md5")
          .update(toLowerCase(JSON.stringify(orderObject(request))))
          .digest("hex");
        requestHashes[i] = requestHash;
      }
      const blockNumbers = blockNumbersByRequest.filter(
        (blockNumber): blockNumber is number => blockNumber !== undefined,
      );
      if (blockNumbers.length > 100) {
        const minBlockNumber = Math.min(...blockNumbers);
        const maxBlockNumber = Math.max(...blockNumbers);
        const nonBlockRequestHashes = requestHashes.filter(
          (_, i) => blockNumbersByRequest[i] === undefined,
        );
        const result = await Promise.all([
          qb.wrap(
            { label: "select_rpc_requests" },
            (db) =>
              db
                .select({
                  request_hash: PONDER_SYNC.rpcRequestResults.requestHash,
                  result: PONDER_SYNC.rpcRequestResults.result,
                })
                .from(PONDER_SYNC.rpcRequestResults)
                .where(
                  and(
                    eq(PONDER_SYNC.rpcRequestResults.chainId, BigInt(chainId)),
                    gte(
                      PONDER_SYNC.rpcRequestResults.blockNumber,
                      BigInt(minBlockNumber),
                    ),
                    lte(
                      PONDER_SYNC.rpcRequestResults.blockNumber,
                      BigInt(maxBlockNumber),
                    ),
                  ),
                ),
            context,
          ),
          nonBlockRequestHashes.length === 0
            ? []
            : qb.wrap(
                { label: "select_rpc_requests" },
                (db) =>
                  db
                    .select({
                      request_hash: PONDER_SYNC.rpcRequestResults.requestHash,
                      result: PONDER_SYNC.rpcRequestResults.result,
                    })
                    .from(PONDER_SYNC.rpcRequestResults)
                    .where(
                      and(
                        eq(
                          PONDER_SYNC.rpcRequestResults.chainId,
                          BigInt(chainId),
                        ),
                        inArray(
                          PONDER_SYNC.rpcRequestResults.requestHash,
                          nonBlockRequestHashes,
                        ),
                      ),
                    ),
                context,
              ),
        ]);
        const results = new Map<string, string | undefined>();
        for (const row of result[0]!) {
          results.set(row.request_hash, row.result);
        }
        for (const row of result[1]!) {
          results.set(row.request_hash, row.result);
        }
        return requestHashes.map((requestHash) => results.get(requestHash));
      }
      const result = await qb.wrap(
        { label: "select_rpc_requests" },
        (db) =>
          db
            .select({
              request_hash: PONDER_SYNC.rpcRequestResults.requestHash,
              result: PONDER_SYNC.rpcRequestResults.result,
            })
            .from(PONDER_SYNC.rpcRequestResults)
            .where(
              and(
                eq(PONDER_SYNC.rpcRequestResults.chainId, BigInt(chainId)),
                inArray(
                  PONDER_SYNC.rpcRequestResults.requestHash,
                  requestHashes,
                ),
              ),
            ),
        context,
      );
      const results = new Map<string, string | undefined>();
      for (const row of result) {
        results.set(row.request_hash, row.result);
      }
      return requestHashes.map((requestHash) => results.get(requestHash));
    },
    pruneRpcRequestResults: async ({ blocks, chainId }, context) => {
      if (blocks.length === 0) return;
      const numbers = blocks.map(({ number }) => BigInt(hexToNumber(number)));
      await qb.wrap(
        { label: "delete_rpc_requests" },
        (db) =>
          db
            .delete(PONDER_SYNC.rpcRequestResults)
            .where(
              and(
                eq(PONDER_SYNC.rpcRequestResults.chainId, BigInt(chainId)),
                inArray(PONDER_SYNC.rpcRequestResults.blockNumber, numbers),
              ),
            ),
        context,
      );
    },
    pruneByChain: async ({ chainId }, context) =>
      qb.transaction(async (tx) => {
        await tx.wrap(
          { label: "delete_logs" },
          (db) =>
            db
              .delete(PONDER_SYNC.logs)
              .where(eq(PONDER_SYNC.logs.chainId, BigInt(chainId)))
              .execute(),
          context,
        );
        await tx.wrap(
          { label: "delete_blocks" },
          (db) =>
            db
              .delete(PONDER_SYNC.blocks)
              .where(eq(PONDER_SYNC.blocks.chainId, BigInt(chainId)))
              .execute(),
          context,
        );
        await tx.wrap(
          { label: "delete_traces" },
          (db) =>
            db
              .delete(PONDER_SYNC.traces)
              .where(eq(PONDER_SYNC.traces.chainId, BigInt(chainId)))
              .execute(),
          context,
        );
        await tx.wrap(
          { label: "delete_transactions" },
          (db) =>
            db
              .delete(PONDER_SYNC.transactions)
              .where(eq(PONDER_SYNC.transactions.chainId, BigInt(chainId)))
              .execute(),
          context,
        );
        await tx.wrap(
          { label: "delete_transaction_receipts" },
          (db) =>
            db
              .delete(PONDER_SYNC.transactionReceipts)
              .where(
                eq(PONDER_SYNC.transactionReceipts.chainId, BigInt(chainId)),
              )
              .execute(),
          context,
        );
        await tx.wrap(
          { label: "delete_factory_addresses" },
          (db) =>
            db
              .delete(PONDER_SYNC.factoryAddresses)
              .where(eq(PONDER_SYNC.factoryAddresses.chainId, BigInt(chainId)))
              .execute(),
          context,
        );
      }),
  } satisfies SyncStore;
  return syncStore;
};
const addressFilter = (
  address:
    | LogFilter["address"]
    | TransactionFilter["fromAddress"]
    | TransactionFilter["toAddress"],
  column: PgColumn,
): SQL => {
  // `factory` filtering is handled in-memory
  if (isAddressFactory(address)) return sql`true`;
  // @ts-ignore
  if (Array.isArray(address)) return inArray(column, address);
  // @ts-ignore
  if (typeof address === "string") return eq(column, address);
  return sql`true`;
};
export const logFilter = (filter: LogFilter): SQL => {
  const conditions: SQL[] = [];
  for (const idx of [0, 1, 2, 3] as const) {
    // If it's an array of length 1, collapse it.
    const raw = filter[`topic${idx}`] ?? null;
    if (raw === null) continue;
    const topic = Array.isArray(raw) && raw.length === 1 ? raw[0]! : raw;
    if (Array.isArray(topic)) {
      conditions.push(inArray(PONDER_SYNC.logs[`topic${idx}`], topic));
    } else {
      conditions.push(eq(PONDER_SYNC.logs[`topic${idx}`], topic));
    }
  }
  conditions.push(addressFilter(filter.address, PONDER_SYNC.logs.address));
  if (filter.fromBlock !== undefined) {
    conditions.push(
      gte(PONDER_SYNC.logs.blockNumber, BigInt(filter.fromBlock!)),
    );
  }
  if (filter.toBlock !== undefined) {
    conditions.push(lte(PONDER_SYNC.logs.blockNumber, BigInt(filter.toBlock!)));
  }
  return and(...conditions)!;
};
export const blockFilter = (filter: BlockFilter): SQL => {
  const conditions: SQL[] = [];
  conditions.push(
    sql`(blocks.number - ${filter.offset}) % ${filter.interval} = 0`,
  );
  if (filter.fromBlock !== undefined) {
    conditions.push(gte(PONDER_SYNC.blocks.number, BigInt(filter.fromBlock!)));
  }
  if (filter.toBlock !== undefined) {
    conditions.push(lte(PONDER_SYNC.blocks.number, BigInt(filter.toBlock!)));
  }
  return and(...conditions)!;
};
export const transactionFilter = (filter: TransactionFilter): SQL => {
  const conditions: SQL[] = [];
  conditions.push(
    addressFilter(filter.fromAddress, PONDER_SYNC.transactions.from),
  );
  conditions.push(addressFilter(filter.toAddress, PONDER_SYNC.transactions.to));
  if (filter.fromBlock !== undefined) {
    conditions.push(
      gte(PONDER_SYNC.transactions.blockNumber, BigInt(filter.fromBlock!)),
    );
  }
  if (filter.toBlock !== undefined) {
    conditions.push(
      lte(PONDER_SYNC.transactions.blockNumber, BigInt(filter.toBlock!)),
    );
  }
  return and(...conditions)!;
};
export const transferFilter = (filter: TransferFilter): SQL => {
  const conditions: SQL[] = [];
  conditions.push(addressFilter(filter.fromAddress, PONDER_SYNC.traces.from));
  conditions.push(addressFilter(filter.toAddress, PONDER_SYNC.traces.to));
  if (filter.includeReverted === false) {
    conditions.push(isNull(PONDER_SYNC.traces.error));
  }
  if (filter.fromBlock !== undefined) {
    conditions.push(
      gte(PONDER_SYNC.traces.blockNumber, BigInt(filter.fromBlock!)),
    );
  }
  if (filter.toBlock !== undefined) {
    conditions.push(
      lte(PONDER_SYNC.traces.blockNumber, BigInt(filter.toBlock!)),
    );
  }
  return and(...conditions)!;
};
export const traceFilter = (filter: TraceFilter): SQL => {
  const conditions: SQL[] = [];
  conditions.push(addressFilter(filter.fromAddress, PONDER_SYNC.traces.from));
  conditions.push(addressFilter(filter.toAddress, PONDER_SYNC.traces.to));
  if (filter.includeReverted === false) {
    conditions.push(isNull(PONDER_SYNC.traces.error));
  }
  if (filter.callType !== undefined) {
    conditions.push(eq(PONDER_SYNC.traces.type, filter.callType));
  }
  if (filter.functionSelector !== undefined) {
    conditions.push(
      eq(sql`substring(traces.input from 1 for 10)`, filter.functionSelector),
    );
  }
  if (filter.fromBlock !== undefined) {
    conditions.push(
      gte(PONDER_SYNC.traces.blockNumber, BigInt(filter.fromBlock!)),
    );
  }
  if (filter.toBlock !== undefined) {
    conditions.push(
      lte(PONDER_SYNC.traces.blockNumber, BigInt(filter.toBlock!)),
    );
  }
  return and(...conditions)!;
};
</file>

<file path="packages/core/src/sync-store/migrations.ts">
import { type Logger, createNoopLogger } from "@/internal/logger.js";
import type { Kysely, Migration, MigrationProvider } from "kysely";
import { sql } from "kysely";
import { maxUint256 } from "viem";
let logger = createNoopLogger();
class StaticMigrationProvider implements MigrationProvider {
  async getMigrations() {
    return migrations;
  }
}
export function buildMigrationProvider(logger_: Logger) {
  logger = logger_;
  const migrationProvider = new StaticMigrationProvider();
  return migrationProvider;
}
const migrations: Record<string, Migration> = {
  "2023_05_15_0_initial": {
    async up(db: Kysely<any>) {
      await db.schema
        .createTable("blocks")
        .addColumn("baseFeePerGas", sql`bytea`) // BigInt
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("difficulty", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("extraData", "text", (col) => col.notNull())
        .addColumn("finalized", "integer", (col) => col.notNull()) // Boolean (0 or 1).
        .addColumn("gasLimit", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("gasUsed", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("hash", "text", (col) => col.notNull().primaryKey())
        .addColumn("logsBloom", "text", (col) => col.notNull())
        .addColumn("miner", "text", (col) => col.notNull())
        .addColumn("mixHash", "text", (col) => col.notNull())
        .addColumn("nonce", "text", (col) => col.notNull())
        .addColumn("number", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("parentHash", "text", (col) => col.notNull())
        .addColumn("receiptsRoot", "text", (col) => col.notNull())
        .addColumn("sha3Uncles", "text", (col) => col.notNull())
        .addColumn("size", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("stateRoot", "text", (col) => col.notNull())
        .addColumn("timestamp", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("totalDifficulty", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("transactionsRoot", "text", (col) => col.notNull())
        .execute();
      await db.schema
        .createTable("transactions")
        .addColumn("accessList", "text")
        .addColumn("blockHash", "text", (col) => col.notNull())
        .addColumn("blockNumber", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("finalized", "integer", (col) => col.notNull()) // Boolean (0 or 1).
        .addColumn("from", "text", (col) => col.notNull())
        .addColumn("gas", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("gasPrice", sql`bytea`) // BigInt
        .addColumn("hash", "text", (col) => col.notNull().primaryKey())
        .addColumn("input", "text", (col) => col.notNull())
        .addColumn("maxFeePerGas", sql`bytea`) // BigInt
        .addColumn("maxPriorityFeePerGas", sql`bytea`) // BigInt
        .addColumn("nonce", "integer", (col) => col.notNull())
        .addColumn("r", "text", (col) => col.notNull())
        .addColumn("s", "text", (col) => col.notNull())
        .addColumn("to", "text")
        .addColumn("transactionIndex", "integer", (col) => col.notNull())
        .addColumn("type", "text", (col) => col.notNull())
        .addColumn("value", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("v", sql`bytea`, (col) => col.notNull()) // BigInt
        .execute();
      await db.schema
        .createTable("logs")
        .addColumn("address", "text", (col) => col.notNull())
        .addColumn("blockHash", "text", (col) => col.notNull())
        .addColumn("blockNumber", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("data", "text", (col) => col.notNull())
        .addColumn("finalized", "integer", (col) => col.notNull()) // Boolean (0 or 1).
        .addColumn("id", "text", (col) => col.notNull().primaryKey())
        .addColumn("logIndex", "integer", (col) => col.notNull())
        .addColumn("topic0", "text")
        .addColumn("topic1", "text")
        .addColumn("topic2", "text")
        .addColumn("topic3", "text")
        .addColumn("transactionHash", "text", (col) => col.notNull())
        .addColumn("transactionIndex", "integer", (col) => col.notNull())
        .execute();
      await db.schema
        .createTable("contractReadResults")
        .addColumn("address", "text", (col) => col.notNull())
        .addColumn("blockNumber", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("data", "text", (col) => col.notNull())
        .addColumn("finalized", "integer", (col) => col.notNull()) // Boolean (0 or 1).
        .addColumn("result", "text", (col) => col.notNull())
        .addPrimaryKeyConstraint("contractReadResultPrimaryKey", [
          "chainId",
          "blockNumber",
          "address",
          "data",
        ])
        .execute();
      await db.schema
        .createTable("logFilterCachedRanges")
        .addColumn("endBlock", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("endBlockTimestamp", sql`bytea`, (col) => col.notNull()) // BigInt
        .addColumn("filterKey", "text", (col) => col.notNull())
        // The `id` column should not be included in INSERT statements.
        // This column uses Postgres SERIAL type which autoincrements.
        .addColumn("id", "serial", (col) => col.notNull().primaryKey())
        .addColumn("startBlock", sql`bytea`, (col) => col.notNull()) // BigInt
        .execute();
    },
  },
  "2023_06_20_0_indices": {
    async up(db: Kysely<any>) {
      await db.schema
        .createIndex("log_events_index")
        .on("logs")
        .columns(["address", "chainId", "blockHash"])
        .execute();
      await db.schema
        .createIndex("blocks_index")
        .on("blocks")
        .columns(["timestamp", "number"])
        .execute();
      await db.schema
        .createIndex("logFilterCachedRanges_index")
        .on("logFilterCachedRanges")
        .columns(["filterKey"])
        .execute();
    },
  },
  "2023_07_18_0_better_indices": {
    async up(db: Kysely<any>) {
      // Drop old indices.
      await db.schema.dropIndex("log_events_index").execute();
      await db.schema.dropIndex("blocks_index").execute();
      // Block hash is a join key.
      await db.schema
        .createIndex("log_block_hash_index")
        .on("logs")
        .column("blockHash")
        .execute();
      // Chain ID, address and topic0 are all used in WHERE clauses.
      await db.schema
        .createIndex("log_chain_id_index")
        .on("logs")
        .column("chainId")
        .execute();
      await db.schema
        .createIndex("log_address_index")
        .on("logs")
        .column("address")
        .execute();
      await db.schema
        .createIndex("log_topic0_index")
        .on("logs")
        .column("topic0")
        .execute();
      // Block timestamp and number are both used in WHERE and SORT clauses.
      await db.schema
        .createIndex("block_timestamp_index")
        .on("blocks")
        .column("timestamp")
        .execute();
      await db.schema
        .createIndex("block_number_index")
        .on("blocks")
        .column("number")
        .execute();
    },
  },
  "2023_07_24_0_drop_finalized": {
    async up(db: Kysely<any>) {
      await db.schema.alterTable("blocks").dropColumn("finalized").execute();
      await db.schema
        .alterTable("transactions")
        .dropColumn("finalized")
        .execute();
      await db.schema.alterTable("logs").dropColumn("finalized").execute();
      await db.schema
        .alterTable("contractReadResults")
        .dropColumn("finalized")
        .execute();
    },
  },
  "2023_09_19_0_new_sync_design": {
    async up(db: Kysely<any>) {
      /** This table is no longer being used. */
      await db.schema.dropTable("logFilterCachedRanges").execute();
      /** Drop and re-create all tables to fix bigint encoding. */
      await db.schema.dropTable("blocks").execute();
      await db.schema
        .createTable("blocks")
        .addColumn("baseFeePerGas", "numeric(78, 0)")
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("difficulty", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("extraData", "text", (col) => col.notNull())
        .addColumn("gasLimit", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("gasUsed", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("hash", "varchar(66)", (col) => col.notNull().primaryKey())
        .addColumn("logsBloom", "varchar(514)", (col) => col.notNull())
        .addColumn("miner", "varchar(42)", (col) => col.notNull())
        .addColumn("mixHash", "varchar(66)", (col) => col.notNull())
        .addColumn("nonce", "varchar(18)", (col) => col.notNull())
        .addColumn("number", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("parentHash", "varchar(66)", (col) => col.notNull())
        .addColumn("receiptsRoot", "varchar(66)", (col) => col.notNull())
        .addColumn("sha3Uncles", "varchar(66)", (col) => col.notNull())
        .addColumn("size", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("stateRoot", "varchar(66)", (col) => col.notNull())
        .addColumn("timestamp", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("totalDifficulty", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("transactionsRoot", "varchar(66)", (col) => col.notNull())
        .execute();
      await db.schema
        .createIndex("blockTimestampIndex")
        .on("blocks")
        .column("timestamp")
        .execute();
      await db.schema
        .createIndex("blockNumberIndex")
        .on("blocks")
        .column("number")
        .execute();
      await db.schema.dropTable("transactions").execute();
      await db.schema
        .createTable("transactions")
        .addColumn("accessList", "text")
        .addColumn("blockHash", "varchar(66)", (col) => col.notNull())
        .addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("from", "varchar(42)", (col) => col.notNull())
        .addColumn("gas", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("gasPrice", "numeric(78, 0)")
        .addColumn("hash", "varchar(66)", (col) => col.notNull().primaryKey())
        .addColumn("input", "text", (col) => col.notNull())
        .addColumn("maxFeePerGas", "numeric(78, 0)")
        .addColumn("maxPriorityFeePerGas", "numeric(78, 0)")
        .addColumn("nonce", "integer", (col) => col.notNull())
        .addColumn("r", "varchar(66)", (col) => col.notNull())
        .addColumn("s", "varchar(66)", (col) => col.notNull())
        .addColumn("to", "varchar(42)")
        .addColumn("transactionIndex", "integer", (col) => col.notNull())
        .addColumn("type", "text", (col) => col.notNull())
        .addColumn("value", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("v", "numeric(78, 0)", (col) => col.notNull())
        .execute();
      await db.schema.dropTable("logs").execute();
      await db.schema
        .createTable("logs")
        .addColumn("address", "varchar(42)", (col) => col.notNull())
        .addColumn("blockHash", "varchar(66)", (col) => col.notNull())
        .addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("data", "text", (col) => col.notNull())
        .addColumn("id", "text", (col) => col.notNull().primaryKey())
        .addColumn("logIndex", "integer", (col) => col.notNull())
        .addColumn("topic0", "varchar(66)")
        .addColumn("topic1", "varchar(66)")
        .addColumn("topic2", "varchar(66)")
        .addColumn("topic3", "varchar(66)")
        .addColumn("transactionHash", "varchar(66)", (col) => col.notNull())
        .addColumn("transactionIndex", "integer", (col) => col.notNull())
        .execute();
      await db.schema
        .createIndex("logBlockHashIndex")
        .on("logs")
        .column("blockHash")
        .execute();
      await db.schema
        .createIndex("logChainIdIndex")
        .on("logs")
        .column("chainId")
        .execute();
      await db.schema
        .createIndex("logAddressIndex")
        .on("logs")
        .column("address")
        .execute();
      await db.schema
        .createIndex("logTopic0Index")
        .on("logs")
        .column("topic0")
        .execute();
      await db.schema.dropTable("contractReadResults").execute();
      await db.schema
        .createTable("contractReadResults")
        .addColumn("address", "varchar(42)", (col) => col.notNull())
        .addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("data", "text", (col) => col.notNull())
        .addColumn("result", "text", (col) => col.notNull())
        .addPrimaryKeyConstraint("contractReadResultPrimaryKey", [
          "chainId",
          "blockNumber",
          "address",
          "data",
        ])
        .execute();
      /** Add new log filter and factory contract interval tables. */
      await db.schema
        .createTable("logFilters")
        .addColumn("id", "text", (col) => col.notNull().primaryKey()) // `${chainId}_${address}_${topic0}_${topic1}_${topic2}_${topic3}`
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("address", "varchar(66)")
        .addColumn("topic0", "varchar(66)")
        .addColumn("topic1", "varchar(66)")
        .addColumn("topic2", "varchar(66)")
        .addColumn("topic3", "varchar(66)")
        .execute();
      await db.schema
        .createTable("logFilterIntervals")
        .addColumn("id", "serial", (col) => col.notNull().primaryKey()) // Auto-increment
        .addColumn("logFilterId", "text", (col) =>
          col.notNull().references("logFilters.id"),
        )
        .addColumn("startBlock", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("endBlock", "numeric(78, 0)", (col) => col.notNull())
        .execute();
      await db.schema
        .createIndex("logFilterIntervalsLogFilterId")
        .on("logFilterIntervals")
        .column("logFilterId")
        .execute();
      await db.schema
        .createTable("factories")
        .addColumn("id", "text", (col) => col.notNull().primaryKey()) // `${chainId}_${address}_${eventSelector}_${childAddressLocation}`
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("address", "varchar(42)", (col) => col.notNull())
        .addColumn("eventSelector", "varchar(66)", (col) => col.notNull())
        .addColumn("childAddressLocation", "text", (col) => col.notNull()) // `topic${number}` or `offset${number}`
        .addColumn("topic0", "varchar(66)")
        .addColumn("topic1", "varchar(66)")
        .addColumn("topic2", "varchar(66)")
        .addColumn("topic3", "varchar(66)")
        .execute();
      await db.schema
        .createTable("factoryLogFilterIntervals")
        .addColumn("id", "serial", (col) => col.notNull().primaryKey()) // Auto-increment
        .addColumn("factoryId", "text", (col) =>
          col.notNull().references("factories.id"),
        )
        .addColumn("startBlock", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("endBlock", "numeric(78, 0)", (col) => col.notNull())
        .execute();
      await db.schema
        .createIndex("factoryLogFilterIntervalsFactoryId")
        .on("factoryLogFilterIntervals")
        .column("factoryId")
        .execute();
    },
  },
  "2023_11_06_0_new_rpc_cache_design": {
    async up(db: Kysely<any>) {
      await db.schema.dropTable("contractReadResults").execute();
      /**
       * Formatting for "request" field values:
       *
       * eth_call: eth_call_{to}_{data}
       * eth_getBalance: eth_getBalance_{address}
       * eth_getCode: eth_getCode_{address}
       * eth_getStorageAt: eth_getStorageAt_{address}_{slot}
       */
      await db.schema
        .createTable("rpcRequestResults")
        .addColumn("request", "text", (col) => col.notNull())
        .addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("result", "text", (col) => col.notNull())
        .addPrimaryKeyConstraint("rpcRequestResultPrimaryKey", [
          "request",
          "chainId",
          "blockNumber",
        ])
        .execute();
    },
  },
  "2024_01_30_0_change_chain_id_type": {
    async up(db: Kysely<any>) {
      await db.schema
        .alterTable("blocks")
        .alterColumn("chainId", (col) => col.setDataType("int8"))
        .execute();
      await db.schema
        .alterTable("transactions")
        .alterColumn("chainId", (col) => col.setDataType("int8"))
        .execute();
      await db.schema
        .alterTable("logs")
        .alterColumn("chainId", (col) => col.setDataType("int8"))
        .execute();
      await db.schema
        .alterTable("logFilters")
        .alterColumn("chainId", (col) => col.setDataType("int8"))
        .execute();
      await db.schema
        .alterTable("factories")
        .alterColumn("chainId", (col) => col.setDataType("int8"))
        .execute();
      await db.schema
        .alterTable("rpcRequestResults")
        .alterColumn("chainId", (col) => col.setDataType("int8"))
        .execute();
    },
  },
  "2024_02_1_0_nullable_block_columns": {
    async up(db: Kysely<any>) {
      await db.schema
        .alterTable("blocks")
        .alterColumn("mixHash", (col) => col.dropNotNull())
        .execute();
      await db.schema
        .alterTable("blocks")
        .alterColumn("nonce", (col) => col.dropNotNull())
        .execute();
    },
  },
  "2024_03_00_0_log_transaction_hash_index": {
    async up(db: Kysely<any>) {
      await db.schema
        .createIndex("log_transaction_hash_index")
        .on("logs")
        .column("transactionHash")
        .execute();
    },
  },
  "2024_03_13_0_nullable_block_columns_sha3uncles": {
    async up(db: Kysely<any>) {
      await db.schema
        .alterTable("blocks")
        .alterColumn("sha3Uncles", (col) => col.dropNotNull())
        .execute();
    },
  },
  "2024_03_14_0_nullable_transaction_rsv": {
    async up(db: Kysely<any>) {
      await db.schema
        .alterTable("transactions")
        .alterColumn("r", (col) => col.dropNotNull())
        .execute();
      await db.schema
        .alterTable("transactions")
        .alterColumn("s", (col) => col.dropNotNull())
        .execute();
      await db.schema
        .alterTable("transactions")
        .alterColumn("v", (col) => col.dropNotNull())
        .execute();
    },
  },
  "2024_03_20_0_checkpoint_in_logs_table": {
    async up(_db: Kysely<any>) {
      // no-op migration to avoid crashing databases that successfully ran this migration
      return;
    },
  },
  "2024_04_04_0_log_events_indexes": {
    async up(db: Kysely<any>) {
      await db.schema.dropIndex("blockNumberIndex").ifExists().execute();
      await db.schema.dropIndex("blockTimestampIndex").ifExists().execute();
      await db.schema
        .createIndex("logBlockNumberIndex")
        .on("logs")
        .column("blockNumber")
        .execute();
    },
  },
  "2024_04_14_0_nullable_block_total_difficulty": {
    async up(db: Kysely<any>) {
      await db.schema
        .alterTable("blocks")
        .alterColumn("totalDifficulty", (col) => col.dropNotNull())
        .execute();
    },
  },
  "2024_04_14_1_add_checkpoint_column_to_logs_table": {
    async up(db: Kysely<any>) {
      await db.executeQuery(
        sql`
        ALTER TABLE ponder_sync.logs 
        ADD COLUMN IF NOT EXISTS 
        checkpoint varchar(75)`.compile(db),
      );
    },
  },
  "2024_04_14_2_set_checkpoint_in_logs_table": {
    async up(db: Kysely<any>) {
      await db.executeQuery(sql`SET statement_timeout = 3600000;`.compile(db));
      await db.executeQuery(
        sql`
        CREATE TEMP TABLE cp_vals AS 
        SELECT
          logs.id,
          (lpad(blocks.timestamp::text, 10, '0') ||
          lpad(blocks."chainId"::text, 16, '0') ||
          lpad(blocks.number::text, 16, '0') ||
          lpad(logs."transactionIndex"::text, 16, '0') ||
          '5' ||
          lpad(logs."logIndex"::text, 16, '0')) AS checkpoint
        FROM ponder_sync.logs logs
        JOIN ponder_sync.blocks blocks ON logs."blockHash" = blocks.hash;
        `.compile(db),
      );
      await db.executeQuery(
        sql`
        CREATE INDEX ON cp_vals(id)
        `.compile(db),
      );
      await db.executeQuery(
        sql`
          UPDATE ponder_sync.logs
          SET checkpoint=cp_vals.checkpoint
          FROM cp_vals
          WHERE ponder_sync.logs.id = cp_vals.id
        `.compile(db),
      );
      await db.executeQuery(
        sql`DROP TABLE IF EXISTS cp_vals CASCADE;`.compile(db),
      );
    },
  },
  "2024_04_14_3_index_on_logs_checkpoint": {
    async up(db: Kysely<any>) {
      await db.schema
        .createIndex("logs_checkpoint_index")
        .ifNotExists()
        .on("logs")
        .column("checkpoint")
        .execute();
    },
  },
  "2024_04_22_0_transaction_receipts": {
    async up(db: Kysely<any>) {
      // Update the log filter ID keys to include the integer includeTransactionReceipts value.
      // Note that we have to remove the FK constraint, which is fine given our app logic.
      await db.schema
        .alterTable("logFilterIntervals")
        .dropConstraint("logFilterIntervals_logFilterId_fkey")
        .execute();
      await db
        .updateTable("logFilters")
        .set({ id: sql`"id" || '_0'` })
        .execute();
      await db
        .updateTable("logFilterIntervals")
        .set({ logFilterId: sql`"logFilterId" || '_0'` })
        .execute();
      // Add the includeTransactionReceipts column. By setting a default in the ADD COLUMN statement,
      // Postgres will automatically populate all existing rows with the default value. But, we don't
      // actually want a default (want to require a value on insertion), so immediately drop the default.
      await db.schema
        .alterTable("logFilters")
        .addColumn("includeTransactionReceipts", "integer", (col) =>
          col.notNull().defaultTo(0),
        )
        .execute();
      await db.schema
        .alterTable("logFilters")
        .alterColumn("includeTransactionReceipts", (col) => col.dropDefault())
        .execute();
      // Repeat the same 2 steps for the factory tables.
      await db.schema
        .alterTable("factoryLogFilterIntervals")
        .dropConstraint("factoryLogFilterIntervals_factoryId_fkey")
        .execute();
      await db
        .updateTable("factories")
        .set({ id: sql`"id" || '_0'` })
        .execute();
      await db
        .updateTable("factoryLogFilterIntervals")
        .set({ factoryId: sql`"factoryId" || '_0'` })
        .execute();
      await db.schema
        .alterTable("factories")
        .addColumn("includeTransactionReceipts", "integer", (col) =>
          col.notNull().defaultTo(0),
        )
        .execute();
      await db.schema
        .alterTable("factories")
        .alterColumn("includeTransactionReceipts", (col) => col.dropDefault())
        .execute();
      await db.schema
        .createTable("transactionReceipts")
        .addColumn("blockHash", "varchar(66)", (col) => col.notNull())
        .addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("contractAddress", "varchar(66)")
        .addColumn("cumulativeGasUsed", "numeric(78, 0)", (col) =>
          col.notNull(),
        )
        .addColumn("effectiveGasPrice", "numeric(78, 0)", (col) =>
          col.notNull(),
        )
        .addColumn("from", "varchar(42)", (col) => col.notNull())
        .addColumn("gasUsed", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("logs", "text", (col) => col.notNull())
        .addColumn("logsBloom", "varchar(514)", (col) => col.notNull())
        .addColumn("status", "text", (col) => col.notNull())
        .addColumn("to", "varchar(42)")
        .addColumn("transactionHash", "varchar(66)", (col) =>
          col.notNull().primaryKey(),
        )
        .addColumn("transactionIndex", "integer", (col) => col.notNull())
        .addColumn("type", "text", (col) => col.notNull())
        .execute();
    },
  },
  "2024_04_23_0_block_filters": {
    async up(db: Kysely<any>) {
      await db.schema
        .createTable("blockFilters")
        .addColumn("id", "text", (col) => col.notNull().primaryKey()) // `${chainId}_${interval}_${offset}`
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("interval", "integer", (col) => col.notNull())
        .addColumn("offset", "integer", (col) => col.notNull())
        .execute();
      await db.schema
        .createTable("blockFilterIntervals")
        .addColumn("id", "serial", (col) => col.notNull().primaryKey()) // Auto-increment
        .addColumn("blockFilterId", "text", (col) =>
          col.notNull().references("blockFilters.id"),
        )
        .addColumn("startBlock", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("endBlock", "numeric(78, 0)", (col) => col.notNull())
        .execute();
      await db.schema
        .createIndex("blockFilterIntervalsBlockFilterId")
        .on("blockFilterIntervals")
        .column("blockFilterId")
        .execute();
      await db.schema
        .alterTable("blocks")
        .addColumn("checkpoint", "varchar(75)")
        .execute();
      await db.executeQuery(
        sql`
          CREATE TEMP TABLE bcp_vals AS 
          SELECT
            blocks.hash,
            (lpad(blocks.timestamp::text, 10, '0') ||
            lpad(blocks."chainId"::text, 16, '0') ||
            lpad(blocks.number::text, 16, '0') ||
            '9999999999999999' ||
            '5' ||
            '0000000000000000') AS checkpoint
          FROM ponder_sync.blocks
          `.compile(db),
      );
      await db.executeQuery(
        sql`
          UPDATE ponder_sync.blocks
          SET checkpoint=bcp_vals.checkpoint
          FROM bcp_vals
          WHERE ponder_sync.blocks.hash = bcp_vals.hash
        `.compile(db),
      );
      await db.executeQuery(
        sql`DROP TABLE IF EXISTS bcp_vals CASCADE;`.compile(db),
      );
      await db.schema
        .alterTable("blocks")
        .alterColumn("checkpoint", (col) => col.setNotNull())
        .execute();
      // The blocks.number index supports getEvents and deleteRealtimeData
      await db.schema
        .createIndex("blockNumberIndex")
        .on("blocks")
        .column("number")
        .execute();
      // The blocks.chainId index supports getEvents and deleteRealtimeData
      await db.schema
        .createIndex("blockChainIdIndex")
        .on("blocks")
        .column("chainId")
        .execute();
      // The blocks.checkpoint index supports getEvents
      await db.schema
        .createIndex("blockCheckpointIndex")
        .on("blocks")
        .column("checkpoint")
        .execute();
    },
  },
  "2024_05_07_0_trace_filters": {
    async up(db: Kysely<any>) {
      // TODO(kyle) drop foreign key constraint on "blockFilterIntervals.blockFilterId".
      await db.schema
        .createTable("traceFilters")
        .addColumn("id", "text", (col) => col.notNull().primaryKey()) // `${chainId}_${fromAddress}_${toAddress}`
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("fromAddress", "varchar(42)")
        .addColumn("toAddress", "varchar(42)")
        .execute();
      await db.schema
        .createTable("traceFilterIntervals")
        .addColumn("id", "serial", (col) => col.notNull().primaryKey()) // Auto-increment
        .addColumn("traceFilterId", "text", (col) => col.notNull())
        .addColumn("startBlock", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("endBlock", "numeric(78, 0)", (col) => col.notNull())
        .execute();
      await db.schema
        .createIndex("traceFilterIntervalsTraceFilterId")
        .on("traceFilterIntervals")
        .column("traceFilterId")
        .execute();
      await db.schema
        .createTable("callTraces")
        .addColumn("id", "text", (col) => col.notNull().primaryKey())
        .addColumn("callType", "text", (col) => col.notNull())
        .addColumn("from", "varchar(42)", (col) => col.notNull())
        .addColumn("gas", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("input", "text", (col) => col.notNull())
        .addColumn("to", "varchar(42)", (col) => col.notNull())
        .addColumn("value", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("blockHash", "varchar(66)", (col) => col.notNull())
        .addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("error", "text")
        .addColumn("gasUsed", "numeric(78, 0)")
        .addColumn("output", "text")
        .addColumn("subtraces", "integer", (col) => col.notNull())
        .addColumn("traceAddress", "text", (col) => col.notNull())
        .addColumn("transactionHash", "varchar(66)", (col) => col.notNull())
        .addColumn("transactionPosition", "integer", (col) => col.notNull())
        .addColumn("functionSelector", "varchar(10)", (col) => col.notNull())
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("checkpoint", "varchar(75)", (col) => col.notNull())
        .execute();
      // The callTraces.blockNumber index supports getEvents and deleteRealtimeData
      await db.schema
        .createIndex("callTracesBlockNumberIndex")
        .on("callTraces")
        .column("blockNumber")
        .execute();
      // The callTraces.functionSelector index supports getEvents
      await db.schema
        .createIndex("callTracesFunctionSelectorIndex")
        .on("callTraces")
        .column("functionSelector")
        .execute();
      // The callTraces.error index supports getEvents
      await db.schema
        .createIndex("callTracesErrorIndex")
        .on("callTraces")
        .column("error")
        .execute();
      // The callTraces.blockHash index supports getEvents
      await db.schema
        .createIndex("callTracesBlockHashIndex")
        .on("callTraces")
        .column("blockHash")
        .execute();
      // The callTraces.transactionHash index supports getEvents
      await db.schema
        .createIndex("callTracesTransactionHashIndex")
        .on("callTraces")
        .column("transactionHash")
        .execute();
      // The callTraces.checkpoint index supports getEvents
      await db.schema
        .createIndex("callTracesCheckpointIndex")
        .on("callTraces")
        .column("checkpoint")
        .execute();
      // The callTraces.chainId index supports getEvents
      await db.schema
        .createIndex("callTracesChainIdIndex")
        .on("callTraces")
        .column("chainId")
        .execute();
      // The callTraces.from index supports getEvents
      await db.schema
        .createIndex("callTracesFromIndex")
        .on("callTraces")
        .column("from")
        .execute();
      // The callTraces.to index supports getEvents
      await db.schema
        .createIndex("callTracesToIndex")
        .on("callTraces")
        .column("to")
        .execute();
      await db.schema
        .alterTable("factories")
        .renameTo("factoryLogFilters")
        .execute();
      await db.schema
        .createTable("factoryTraceFilters")
        .addColumn("id", "text", (col) => col.notNull().primaryKey()) // `${chainId}_${address}_${eventSelector}_${childAddressLocation}_${fromAddress}`
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("address", "varchar(42)", (col) => col.notNull())
        .addColumn("eventSelector", "varchar(66)", (col) => col.notNull())
        .addColumn("childAddressLocation", "text", (col) => col.notNull()) // `topic${number}` or `offset${number}`
        .addColumn("fromAddress", "varchar(42)")
        .execute();
      await db.schema
        .createTable("factoryTraceFilterIntervals")
        .addColumn("id", "serial", (col) => col.notNull().primaryKey()) // Auto-increment
        .addColumn("factoryId", "text")
        .addColumn("startBlock", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("endBlock", "numeric(78, 0)", (col) => col.notNull())
        .execute();
      await db.schema
        .createIndex("factoryTraceFilterIntervalsFactoryId")
        .on("factoryTraceFilterIntervals")
        .column("factoryId")
        .execute();
    },
  },
  "2024_11_04_0_request_cache": {
    async up(db: Kysely<any>) {
      await db.schema
        .createTable("rpc_request_results")
        .addColumn("request", "text", (col) => col.notNull())
        .addColumn("block_number", "numeric(78, 0)")
        .addColumn("chain_id", "integer", (col) => col.notNull())
        .addColumn("result", "text", (col) => col.notNull())
        .addPrimaryKeyConstraint("rpc_request_result_primary_key", [
          "request",
          "chain_id",
        ])
        .execute();
      await db.executeQuery(
        sql`
INSERT INTO ponder_sync.rpc_request_results (request, block_number, chain_id, result)
SELECT 
  CONCAT (
    '{"method":"eth_getbalance","params":["',
    LOWER(SUBSTRING(request, 16)),
    '","0x',
    to_hex("blockNumber"::bigint),
    '"]}'
  ) as request,
  "blockNumber" as block_number,
  "chainId" as chain_id,
  result
FROM ponder_sync."rpcRequestResults"
WHERE ponder_sync."rpcRequestResults".request LIKE 'eth_getBalance_%'
AND ponder_sync."rpcRequestResults"."blockNumber" <= 9223372036854775807;
`.compile(db),
      );
      await db.executeQuery(
        sql`
INSERT INTO ponder_sync.rpc_request_results (request, block_number, chain_id, result)
SELECT 
  CONCAT (
    '{"method":"eth_call","params":[{"data":"',
    LOWER(SUBSTRING(request, 53)),
    '","to":"',
    LOWER(SUBSTRING(request, 10, 42)),
    '"},"0x',
    to_hex("blockNumber"::bigint),
    '"]}'
  ) as request,
  "blockNumber" as block_number,
  "chainId" as chain_id,
  result
FROM ponder_sync."rpcRequestResults"
WHERE ponder_sync."rpcRequestResults".request LIKE 'eth_call_%'
AND ponder_sync."rpcRequestResults"."blockNumber" <= 9223372036854775807;
`.compile(db),
      );
      await db.schema
        .dropTable("rpcRequestResults")
        .ifExists()
        .cascade()
        .execute();
    },
  },
  "2024_11_09_0_adjacent_interval": {
    async up(db: Kysely<any>) {
      await db.schema
        .createTable("intervals")
        .addColumn("fragment_id", "text", (col) => col.notNull().primaryKey())
        .addColumn("chain_id", "integer", (col) => col.notNull())
        .addColumn("blocks", sql`nummultirange`, (col) => col.notNull())
        .execute();
      await db
        .with("range(fragment_id, chain_id, blocks)", (db) =>
          db
            .selectFrom("logFilters as lf")
            .innerJoin("logFilterIntervals as lfi", "lf.id", "lfi.logFilterId")
            .select([
              sql<string>`concat('log', '_', lf.id)`.as("fragment_id"),
              "lf.chainId as chain_id",
              sql`numrange(lfi."startBlock", lfi."endBlock" + 1, '[]')`.as(
                "blocks",
              ),
            ]),
        )
        .insertInto("intervals")
        .columns(["fragment_id", "chain_id", "blocks"])
        .expression(
          sql.raw(`
SELECT
  fragment_id,
  chain_id,
  range_agg(range.blocks) as blocks
FROM range
GROUP BY fragment_id, chain_id
`),
        )
        .execute();
      await db.schema.dropTable("logFilters").ifExists().cascade().execute();
      await db.schema
        .dropTable("logFilterIntervals")
        .ifExists()
        .cascade()
        .execute();
      await db
        .with("range(fragment_id, chain_id, blocks)", (db) =>
          db
            .selectFrom("factoryLogFilters as flf")
            .innerJoin(
              "factoryLogFilterIntervals as flfi",
              "flf.id",
              "flfi.factoryId",
            )
            .select([
              sql<string>`concat('log', '_', flf.id)`.as("fragment_id"),
              "flf.chainId as chain_id",
              sql`numrange(flfi."startBlock", flfi."endBlock" + 1, '[]')`.as(
                "blocks",
              ),
            ]),
        )
        .insertInto("intervals")
        .columns(["fragment_id", "chain_id", "blocks"])
        .expression(
          sql.raw(`
  SELECT
    fragment_id,
    chain_id,
    range_agg(range.blocks) as blocks
  FROM range
  GROUP BY fragment_id, chain_id
  `),
        )
        .onConflict((oc) =>
          oc.column("fragment_id").doUpdateSet({
            blocks: sql`intervals.blocks + excluded.blocks`,
          }),
        )
        .execute();
      await db.schema
        .dropTable("factoryLogFilters")
        .ifExists()
        .cascade()
        .execute();
      await db.schema
        .dropTable("factoryLogFilterIntervals")
        .ifExists()
        .cascade()
        .execute();
      await db
        .with("range(fragment_id, chain_id, blocks)", (db) =>
          db
            .selectFrom("traceFilters as tf")
            .innerJoin(
              "traceFilterIntervals as tfi",
              "tf.id",
              "tfi.traceFilterId",
            )
            .select([
              sql<string>`concat('trace', '_', tf.id)`.as("fragment_id"),
              "tf.chainId as chain_id",
              sql`numrange(tfi."startBlock", tfi."endBlock" + 1, '[]')`.as(
                "blocks",
              ),
            ]),
        )
        .insertInto("intervals")
        .columns(["fragment_id", "chain_id", "blocks"])
        .expression(
          sql.raw(`
  SELECT
    fragment_id,
    chain_id,
    range_agg(range.blocks) as blocks
  FROM range
  GROUP BY fragment_id, chain_id
  `),
        )
        .onConflict((oc) =>
          oc.column("fragment_id").doUpdateSet({
            blocks: sql`intervals.blocks + excluded.blocks`,
          }),
        )
        .execute();
      await db.schema.dropTable("traceFilters").ifExists().cascade().execute();
      await db.schema
        .dropTable("traceFilterIntervals")
        .ifExists()
        .cascade()
        .execute();
      await db
        .with("range(fragment_id, chain_id, blocks)", (db) =>
          db
            .selectFrom("factoryTraceFilters as ftf")
            .innerJoin(
              "factoryTraceFilterIntervals as ftfi",
              "ftf.id",
              "ftfi.factoryId",
            )
            .select([
              sql<string>`concat('trace', '_', ftf.id)`.as("fragment_id"),
              "ftf.chainId as chain_id",
              sql`numrange(ftfi."startBlock", ftfi."endBlock" + 1, '[]')`.as(
                "blocks",
              ),
            ]),
        )
        .insertInto("intervals")
        .columns(["fragment_id", "chain_id", "blocks"])
        .expression(
          sql.raw(`
  SELECT
    fragment_id,
    chain_id,
    range_agg(range.blocks) as blocks
  FROM range
  GROUP BY fragment_id, chain_id
  `),
        )
        .onConflict((oc) =>
          oc.column("fragment_id").doUpdateSet({
            blocks: sql`intervals.blocks + excluded.blocks`,
          }),
        )
        .execute();
      await db.schema
        .dropTable("factoryTraceFilters")
        .ifExists()
        .cascade()
        .execute();
      await db.schema
        .dropTable("factoryTraceFilterIntervals")
        .ifExists()
        .cascade()
        .execute();
      await db
        .with("range(fragment_id, chain_id, blocks)", (db) =>
          db
            .selectFrom("blockFilters as bf")
            .innerJoin(
              "blockFilterIntervals as bfi",
              "bf.id",
              "bfi.blockFilterId",
            )
            .select([
              sql<string>`concat('block', '_', bf.id)`.as("fragment_id"),
              "bf.chainId as chain_id",
              sql`numrange(bfi."startBlock", bfi."endBlock" + 1, '[]')`.as(
                "blocks",
              ),
            ]),
        )
        .insertInto("intervals")
        .columns(["fragment_id", "chain_id", "blocks"])
        .expression(
          sql.raw(`
  SELECT
    fragment_id,
    chain_id,
    range_agg(range.blocks) as blocks
  FROM range
  GROUP BY fragment_id, chain_id
  `),
        )
        .onConflict((oc) =>
          oc.column("fragment_id").doUpdateSet({
            blocks: sql`intervals.blocks + excluded.blocks`,
          }),
        )
        .execute();
      await db.schema.dropTable("blockFilters").ifExists().cascade().execute();
      await db.schema
        .dropTable("blockFilterIntervals")
        .ifExists()
        .cascade()
        .execute();
    },
  },
  "2024_11_12_0_debug": {
    async up(db) {
      await db.schema.dropTable("callTraces").ifExists().cascade().execute();
      await db
        .deleteFrom("intervals")
        .where("fragment_id", "like", "trace_%")
        .execute();
      await db.schema
        .createTable("traces")
        .addColumn("id", "text", (col) => col.notNull().primaryKey())
        .addColumn("chainId", "integer", (col) => col.notNull())
        .addColumn("checkpoint", "varchar(75)", (col) => col.notNull())
        .addColumn("type", "text", (col) => col.notNull())
        .addColumn("transactionHash", "varchar(66)", (col) => col.notNull())
        .addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("blockHash", "varchar(66)", (col) => col.notNull())
        .addColumn("from", "varchar(42)", (col) => col.notNull())
        .addColumn("to", "varchar(42)")
        .addColumn("gas", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("gasUsed", "numeric(78, 0)", (col) => col.notNull())
        .addColumn("input", "text", (col) => col.notNull())
        .addColumn("functionSelector", "text", (col) => col.notNull())
        .addColumn("output", "text")
        .addColumn("error", "text")
        .addColumn("revertReason", "text")
        .addColumn("value", "numeric(78, 0)")
        .addColumn("index", "integer", (col) => col.notNull())
        .addColumn("subcalls", "integer", (col) => col.notNull())
        .addColumn("isReverted", "integer", (col) => col.notNull())
        .execute();
      // `getEvents` benefits from an index on
      // "blockNumber", "functionSelector", "blockHash"
      // "transactionHash", "checkpoint", "chainId", "from", "to",
      // "value", "type", and "isReverted"
      await db.schema
        .createIndex("trace_block_number_index")
        .on("traces")
        .column("blockNumber")
        .execute();
      await db.schema
        .createIndex("trace_function_selector_index")
        .on("traces")
        .column("functionSelector")
        .execute();
      await db.schema
        .createIndex("trace_is_reverted_index")
        .on("traces")
        .column("isReverted")
        .execute();
      await db.schema
        .createIndex("trace_block_hash_index")
        .on("traces")
        .column("blockHash")
        .execute();
      await db.schema
        .createIndex("trace_transaction_hash_index")
        .on("traces")
        .column("transactionHash")
        .execute();
      await db.schema
        .createIndex("trace_checkpoint_index")
        .on("traces")
        .column("checkpoint")
        .execute();
      await db.schema
        .createIndex("trace_chain_id_index")
        .on("traces")
        .column("chainId")
        .execute();
      await db.schema
        .createIndex("trace_value_index")
        .on("traces")
        .column("value")
        .execute();
      await db.schema
        .createIndex("trace_from_index")
        .on("traces")
        .column("from")
        .execute();
      await db.schema
        .createIndex("trace_to_index")
        .on("traces")
        .column("to")
        .execute();
      await db.schema
        .createIndex("trace_type_index")
        .on("traces")
        .column("type")
        .execute();
      // add `checkpoint` to `transactions`
      await db.schema
        .alterTable("transactions")
        .addColumn("checkpoint", "varchar(75)")
        .execute();
      await db.schema
        .createIndex("transactions_checkpoint_index")
        .on("transactions")
        .column("checkpoint")
        .execute();
      await db.schema
        .alterTable("transactionReceipts")
        .dropColumn("logs")
        .execute();
    },
  },
  "2024_12_02_0_request_cache": {
    async up(db) {
      await db.schema
        .alterTable("rpc_request_results")
        .addColumn("request_hash", "text", (col) =>
          col.generatedAlwaysAs(sql`MD5(request)`).stored().notNull(),
        )
        .execute();
      // Drop previous primary key constraint, on columns "request" and "chain_id"
      await db.schema
        .alterTable("rpc_request_results")
        .dropConstraint("rpc_request_result_primary_key")
        .execute();
      await db.schema
        .alterTable("rpc_request_results")
        .addPrimaryKeyConstraint("rpc_request_result_primary_key", [
          "request_hash",
          "chain_id",
        ])
        .execute();
    },
  },
  "2025_02_19_0_primary_key": {
    async up(db) {
      // 1. drop unused indexes
      // 2. update column types
      // 3. drop primary key
      // 4. drop unused columns
      // 5. rename tables and columns
      // 6. create new primary key
      // 7. reset metadata
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] started 2025_02_19_0_primary_key`,
      });
      await db.executeQuery(sql`SET statement_timeout = 3600000;`.compile(db));
      await db.schema.dropIndex("logAddressIndex").ifExists().execute();
      await db.schema.dropIndex("logBlockHashIndex").ifExists().execute();
      await db.schema.dropIndex("logBlockNumberIndex").ifExists().execute();
      await db.schema.dropIndex("logChainIdIndex").ifExists().execute();
      await db.schema.dropIndex("logTopic0Index").ifExists().execute();
      await db.schema
        .dropIndex("log_transaction_hash_index")
        .ifExists()
        .execute();
      await db.schema.dropIndex("logs_checkpoint_index").ifExists().execute();
      await db.schema.dropIndex("blockChainIdIndex").execute();
      await db.schema.dropIndex("blockCheckpointIndex").execute();
      await db.schema.dropIndex("blockNumberIndex").execute();
      await db.schema.dropIndex("transactions_checkpoint_index").execute();
      await db.schema.dropIndex("trace_block_hash_index").ifExists().execute();
      await db.schema
        .dropIndex("trace_block_number_index")
        .ifExists()
        .execute();
      await db.schema.dropIndex("trace_chain_id_index").ifExists().execute();
      await db.schema.dropIndex("trace_checkpoint_index").ifExists().execute();
      await db.schema.dropIndex("trace_from_index").ifExists().execute();
      await db.schema
        .dropIndex("trace_function_selector_index")
        .ifExists()
        .execute();
      await db.schema.dropIndex("trace_is_reverted_index").ifExists().execute();
      await db.schema.dropIndex("trace_to_index").ifExists().execute();
      await db.schema
        .dropIndex("trace_transaction_hash_index")
        .ifExists()
        .execute();
      await db.schema.dropIndex("trace_type_index").ifExists().execute();
      await db.schema.dropIndex("trace_value_index").ifExists().execute();
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] dropped indexes`,
      });
      await db.schema
        .alterTable("logs")
        .alterColumn("blockNumber", (qb) => qb.setDataType("bigint"))
        .execute();
      await db.schema
        .alterTable("blocks")
        .alterColumn("number", (qb) => qb.setDataType("bigint"))
        .execute();
      await db.schema
        .alterTable("blocks")
        .alterColumn("timestamp", (qb) => qb.setDataType("bigint"))
        .execute();
      await db.schema
        .alterTable("transactions")
        .alterColumn("blockNumber", (qb) => qb.setDataType("bigint"))
        .execute();
      await db.schema
        .alterTable("transactionReceipts")
        .alterColumn("blockNumber", (qb) => qb.setDataType("bigint"))
        .execute();
      await db.schema
        .alterTable("transactionReceipts")
        .alterColumn("chainId", (qb) => qb.setDataType("bigint"))
        .execute();
      await db.schema
        .alterTable("traces")
        .alterColumn("blockNumber", (qb) => qb.setDataType("bigint"))
        .execute();
      await db.schema
        .alterTable("traces")
        .alterColumn("chainId", (qb) => qb.setDataType("bigint"))
        .execute();
      await db.schema
        .alterTable("traces")
        .addColumn("transaction_index", "integer")
        .execute();
      await db
        .updateTable("traces")
        .set({ transaction_index: sql`SUBSTRING(checkpoint, 43, 16)::bigint` })
        .execute();
      await db.schema
        .alterTable("traces")
        .alterColumn("transaction_index", (col) => col.setNotNull())
        .execute();
      await db.schema
        .alterTable("intervals")
        .alterColumn("chain_id", (qb) => qb.setDataType("bigint"))
        .execute();
      await db.deleteFrom("logs").where("checkpoint", "=", null).execute();
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] updated column types`,
      });
      await db.schema.alterTable("logs").dropConstraint("logs_pkey").execute();
      await db.schema
        .alterTable("blocks")
        .dropConstraint("blocks_pkey")
        .execute();
      await db.schema
        .alterTable("transactions")
        .dropConstraint("transactions_pkey")
        .execute();
      await db.schema
        .alterTable("transactionReceipts")
        .dropConstraint("transactionReceipts_pkey")
        .execute();
      await db.schema
        .alterTable("traces")
        .dropConstraint("traces_pkey")
        .execute();
      await db.schema.alterTable("logs").dropColumn("checkpoint").execute();
      await db.schema.alterTable("logs").dropColumn("id").execute();
      await db.schema.alterTable("blocks").dropColumn("checkpoint").execute();
      await db.schema
        .alterTable("transactions")
        .dropColumn("checkpoint")
        .execute();
      await db.schema.alterTable("traces").dropColumn("id").execute();
      await db.schema.alterTable("traces").dropColumn("checkpoint").execute();
      await db.schema
        .alterTable("traces")
        .dropColumn("transactionHash")
        .execute();
      await db.schema.alterTable("traces").dropColumn("blockHash").execute();
      await db.schema
        .alterTable("traces")
        .dropColumn("functionSelector")
        .execute();
      await db.schema.alterTable("traces").dropColumn("isReverted").execute();
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] dropped columns`,
      });
      await db.schema
        .alterTable("logs")
        .renameColumn("chainId", "chain_id")
        .execute();
      await db.schema
        .alterTable("logs")
        .renameColumn("blockNumber", "block_number")
        .execute();
      await db.schema
        .alterTable("logs")
        .renameColumn("logIndex", "log_index")
        .execute();
      await db.schema
        .alterTable("logs")
        .renameColumn("transactionIndex", "transaction_index")
        .execute();
      await db.schema
        .alterTable("logs")
        .renameColumn("blockHash", "block_hash")
        .execute();
      await db.schema
        .alterTable("logs")
        .renameColumn("transactionHash", "transaction_hash")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("chainId", "chain_id")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("parentHash", "parent_hash")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("logsBloom", "logs_bloom")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("gasUsed", "gas_used")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("gasLimit", "gas_limit")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("baseFeePerGas", "base_fee_per_gas")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("mixHash", "mix_hash")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("stateRoot", "state_root")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("receiptsRoot", "receipts_root")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("transactionsRoot", "transactions_root")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("sha3Uncles", "sha3_uncles")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("totalDifficulty", "total_difficulty")
        .execute();
      await db.schema
        .alterTable("blocks")
        .renameColumn("extraData", "extra_data")
        .execute();
      await db.schema
        .alterTable("transactions")
        .renameColumn("chainId", "chain_id")
        .execute();
      await db.schema
        .alterTable("transactions")
        .renameColumn("blockNumber", "block_number")
        .execute();
      await db.schema
        .alterTable("transactions")
        .renameColumn("transactionIndex", "transaction_index")
        .execute();
      await db.schema
        .alterTable("transactions")
        .renameColumn("blockHash", "block_hash")
        .execute();
      await db.schema
        .alterTable("transactions")
        .renameColumn("gasPrice", "gas_price")
        .execute();
      await db.schema
        .alterTable("transactions")
        .renameColumn("maxFeePerGas", "max_fee_per_gas")
        .execute();
      await db.schema
        .alterTable("transactions")
        .renameColumn("maxPriorityFeePerGas", "max_priority_fee_per_gas")
        .execute();
      await db.schema
        .alterTable("transactions")
        .renameColumn("accessList", "access_list")
        .execute();
      await db.schema
        .alterTable("transactionReceipts")
        .renameTo("transaction_receipts")
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .renameColumn("chainId", "chain_id")
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .renameColumn("blockNumber", "block_number")
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .renameColumn("transactionIndex", "transaction_index")
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .renameColumn("blockHash", "block_hash")
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .renameColumn("transactionHash", "transaction_hash")
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .renameColumn("contractAddress", "contract_address")
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .renameColumn("logsBloom", "logs_bloom")
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .renameColumn("gasUsed", "gas_used")
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .renameColumn("cumulativeGasUsed", "cumulative_gas_used")
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .renameColumn("effectiveGasPrice", "effective_gas_price")
        .execute();
      await db.schema
        .alterTable("traces")
        .renameColumn("chainId", "chain_id")
        .execute();
      await db.schema
        .alterTable("traces")
        .renameColumn("blockNumber", "block_number")
        .execute();
      await db.schema
        .alterTable("traces")
        .renameColumn("index", "trace_index")
        .execute();
      await db.schema
        .alterTable("traces")
        .renameColumn("gasUsed", "gas_used")
        .execute();
      await db.schema
        .alterTable("traces")
        .renameColumn("revertReason", "revert_reason")
        .execute();
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] renamed columns`,
      });
      await db.schema
        .alterTable("logs")
        .addPrimaryKeyConstraint("logs_pkey", [
          "chain_id",
          "block_number",
          "log_index",
        ])
        .execute();
      await db.schema
        .alterTable("blocks")
        .addPrimaryKeyConstraint("blocks_pkey", ["chain_id", "number"])
        .execute();
      await db.schema
        .alterTable("transactions")
        .addPrimaryKeyConstraint("transactions_pkey", [
          "chain_id",
          "block_number",
          "transaction_index",
        ])
        .execute();
      await db.schema
        .alterTable("transaction_receipts")
        .addPrimaryKeyConstraint("transaction_receipts_pkey", [
          "chain_id",
          "block_number",
          "transaction_index",
        ])
        .execute();
      await db.schema
        .alterTable("traces")
        .addPrimaryKeyConstraint("traces_pkey", [
          "chain_id",
          "block_number",
          "transaction_index",
          "trace_index",
        ])
        .execute();
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] added primary keys`,
      });
      await sql`ANALYZE ponder_sync.logs`.execute(db);
      await sql`ANALYZE ponder_sync.blocks`.execute(db);
      await sql`ANALYZE ponder_sync.transactions`.execute(db);
      await sql`ANALYZE ponder_sync.transaction_receipts`.execute(db);
      await sql`ANALYZE ponder_sync.traces`.execute(db);
      await sql`REINDEX TABLE ponder_sync.logs`.execute(db);
      await sql`REINDEX TABLE ponder_sync.blocks`.execute(db);
      await sql`REINDEX TABLE ponder_sync.transactions`.execute(db);
      await sql`REINDEX TABLE ponder_sync.transaction_receipts`.execute(db);
      await sql`REINDEX TABLE ponder_sync.traces`.execute(db);
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] finished 2025_02_19_0_primary_key`,
      });
    },
  },
  "2025_02_26_0_factories": {
    async up(db) {
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] started 2025_02_26_0_factories`,
      });
      await db.executeQuery(sql`SET statement_timeout = 3600000;`.compile(db));
      // drop any intervals that contain a factory address
      await db
        .deleteFrom("intervals")
        .where((qb) =>
          qb.or([
            qb("fragment_id", "like", "%offset%"),
            qb("fragment_id", "like", "%topic%"),
          ]),
        )
        .execute();
      await db.schema
        .createTable("factories")
        .addColumn("id", "integer", (col) =>
          col.generatedAlwaysAsIdentity().primaryKey(),
        )
        .addColumn("factory", "jsonb", (col) => col.notNull().unique())
        .execute();
      await db.schema
        .createTable("factory_addresses")
        .addColumn("id", "integer", (col) =>
          col.generatedAlwaysAsIdentity().primaryKey(),
        )
        .addColumn("factory_id", "integer", (col) => col.notNull())
        .addColumn("chain_id", "bigint", (col) => col.notNull())
        .addColumn("block_number", "bigint", (col) => col.notNull())
        .addColumn("address", "text", (col) => col.notNull())
        .execute();
      await db.schema
        .createIndex("factories_factory_index")
        .on("factories")
        .column("factory")
        .execute();
      await db.schema
        .createIndex("factory_addresses_factory_id_index")
        .on("factory_addresses")
        .column("factory_id")
        .execute();
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] finished 2025_02_26_0_factories`,
      });
    },
  },
  "2025_02_26_1_rpc_request_results": {
    async up(db) {
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] started 2025_02_26_1_rpc_request_results`,
      });
      await db.executeQuery(sql`SET statement_timeout = 3600000;`.compile(db));
      await db.schema
        .alterTable("rpc_request_results")
        .addColumn("request_hash_temp", "text")
        .execute();
      await db
        .updateTable("rpc_request_results")
        .set({ request_hash_temp: sql`request_hash` })
        .execute();
      await db.schema
        .alterTable("rpc_request_results")
        .dropConstraint("rpc_request_result_primary_key")
        .execute();
      await db.schema
        .alterTable("rpc_request_results")
        .dropColumn("request_hash")
        .execute();
      await db.schema
        .alterTable("rpc_request_results")
        .renameColumn("request_hash_temp", "request_hash")
        .execute();
      await db.schema
        .alterTable("rpc_request_results")
        .addPrimaryKeyConstraint("rpc_request_results_pkey", [
          "chain_id",
          "request_hash",
        ])
        .execute();
      await db.schema
        .alterTable("rpc_request_results")
        .dropColumn("request")
        .execute();
      await db
        .updateTable("rpc_request_results")
        .set({ block_number: 0 })
        .where("block_number", "=", maxUint256)
        .execute();
      await db.schema
        .alterTable("rpc_request_results")
        .alterColumn("block_number", (col) => col.setDataType("bigint"))
        .execute();
      await db.schema
        .alterTable("rpc_request_results")
        .alterColumn("chain_id", (col) => col.setDataType("bigint"))
        .execute();
      await db.schema
        .createIndex("rpc_request_results_chain_id_block_number_index")
        .on("rpc_request_results")
        .columns(["chain_id", "block_number"])
        .execute();
      await db
        .deleteFrom("rpc_request_results")
        .where("result", "=", "0x")
        .execute();
      await sql`ANALYZE ponder_sync.rpc_request_results`.execute(db);
      logger.debug({
        msg: `${new Date().toISOString()} [ponder_sync migration] finished 2025_02_26_1_rpc_request_results`,
      });
    },
  },
};
</file>

<file path="packages/core/src/sync-store/schema.ts">
import type { Factory, FragmentId } from "@/internal/types.js";
import {
  customType,
  index,
  pgSchema,
  primaryKey,
  unique,
} from "drizzle-orm/pg-core";
import type { Address, Hash, Hex } from "viem";
const nummultirange = customType<{ data: string }>({
  dataType() {
    return "nummultirange";
  },
});
const numeric78 = customType<{ data: bigint; driverData: string }>({
  dataType() {
    return "numeric(78,0)";
  },
  fromDriver(value: string) {
    return BigInt(value);
  },
});
/**
 * Database schemas for the sync.
 *
 * @dev The order of the schemas represents the order of the migrations.
 * @dev The schemas must match the files in "./sql".
 */
export const PONDER_SYNC_SCHEMAS = ["ponder_sync"] as const;
/**
 * Latest database schema for the sync.
 */
export const PONDER_SYNC_SCHEMA =
  PONDER_SYNC_SCHEMAS[PONDER_SYNC_SCHEMAS.length - 1]!;
export const PONDER_SYNC = pgSchema(PONDER_SYNC_SCHEMA);
export const blocks = PONDER_SYNC.table(
  "blocks",
  (t) => ({
    chainId: t.bigint({ mode: "bigint" }).notNull(),
    number: t.bigint({ mode: "bigint" }).notNull(),
    timestamp: t.bigint({ mode: "bigint" }).notNull(),
    hash: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    parentHash: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    logsBloom: t.varchar({ length: 514 }).notNull().$type<Hex>(),
    miner: t.varchar({ length: 42 }).notNull().$type<Address>(),
    gasUsed: numeric78().notNull(),
    gasLimit: numeric78().notNull(),
    baseFeePerGas: numeric78(),
    nonce: t.varchar({ length: 18 }).$type<Hex>(),
    mixHash: t.varchar({ length: 66 }).$type<Hash>(),
    stateRoot: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    receiptsRoot: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    transactionsRoot: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    sha3Uncles: t.varchar({ length: 66 }).$type<Hash>(),
    size: numeric78().notNull(),
    difficulty: numeric78().notNull(),
    totalDifficulty: numeric78(),
    extraData: t.text().notNull().$type<Hex>(),
  }),
  (table) => [
    primaryKey({
      name: "blocks_pkey",
      columns: [table.chainId, table.number],
    }),
  ],
);
export const transactions = PONDER_SYNC.table(
  "transactions",
  (t) => ({
    chainId: t.bigint({ mode: "bigint" }).notNull(),
    blockNumber: t.bigint({ mode: "bigint" }).notNull(),
    transactionIndex: t.integer().notNull(),
    hash: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    blockHash: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    from: t.varchar({ length: 42 }).notNull().$type<Address>(),
    to: t.varchar({ length: 42 }).$type<Address>(),
    input: t.text().notNull().$type<Hex>(),
    value: numeric78().notNull(),
    nonce: t.integer().notNull(),
    r: t.varchar({ length: 66 }).$type<Hex>(),
    s: t.varchar({ length: 66 }).$type<Hex>(),
    v: numeric78(),
    type: t.text().notNull().$type<Hex>(),
    gas: numeric78().notNull(),
    gasPrice: numeric78(),
    maxFeePerGas: numeric78(),
    maxPriorityFeePerGas: numeric78(),
    accessList: t.text(),
  }),
  (table) => [
    primaryKey({
      name: "transactions_pkey",
      columns: [table.chainId, table.blockNumber, table.transactionIndex],
    }),
  ],
);
export const transactionReceipts = PONDER_SYNC.table(
  "transaction_receipts",
  (t) => ({
    chainId: t.bigint({ mode: "bigint" }).notNull(),
    blockNumber: t.bigint({ mode: "bigint" }).notNull(),
    transactionIndex: t.integer().notNull(),
    transactionHash: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    blockHash: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    from: t.varchar({ length: 42 }).notNull().$type<Address>(),
    to: t.varchar({ length: 42 }).$type<Address>(),
    contractAddress: t.varchar({ length: 42 }).$type<Address>(), // Note: incorrect
    logsBloom: t.varchar({ length: 514 }).notNull().$type<Hex>(),
    gasUsed: numeric78().notNull(),
    cumulativeGasUsed: numeric78().notNull(),
    effectiveGasPrice: numeric78().notNull(),
    status: t.text().notNull().$type<Hex>(),
    type: t.text().notNull().$type<Hex>(),
  }),
  (table) => [
    primaryKey({
      name: "transaction_receipts_pkey",
      columns: [table.chainId, table.blockNumber, table.transactionIndex],
    }),
  ],
);
export const logs = PONDER_SYNC.table(
  "logs",
  (t) => ({
    chainId: t.bigint({ mode: "bigint" }).notNull(),
    blockNumber: t.bigint({ mode: "bigint" }).notNull(),
    logIndex: t.integer().notNull(),
    transactionIndex: t.integer().notNull(),
    blockHash: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    transactionHash: t.varchar({ length: 66 }).notNull().$type<Hash>(),
    address: t.varchar({ length: 42 }).notNull().$type<Address>(),
    topic0: t.varchar({ length: 66 }).$type<Hex>(),
    topic1: t.varchar({ length: 66 }).$type<Hex>(),
    topic2: t.varchar({ length: 66 }).$type<Hex>(),
    topic3: t.varchar({ length: 66 }).$type<Hex>(),
    data: t.text().notNull().$type<Hex>(),
  }),
  (table) => [
    primaryKey({
      name: "logs_pkey",
      columns: [table.chainId, table.blockNumber, table.logIndex],
    }),
  ],
);
export const traces = PONDER_SYNC.table(
  "traces",
  (t) => ({
    chainId: t.bigint({ mode: "bigint" }).notNull(),
    blockNumber: t.bigint({ mode: "bigint" }).notNull(),
    transactionIndex: t.integer().notNull(),
    traceIndex: t.integer().notNull(),
    from: t.varchar({ length: 42 }).notNull().$type<Address>(),
    to: t.varchar({ length: 42 }).$type<Address>(),
    input: t.text().notNull().$type<Hex>(),
    output: t.text().$type<Hex>(),
    value: numeric78(),
    type: t.text().notNull(),
    gas: numeric78().notNull(),
    gasUsed: numeric78().notNull(),
    error: t.text(),
    revertReason: t.text(),
    subcalls: t.integer().notNull(),
  }),
  (table) => [
    primaryKey({
      name: "traces_pkey",
      columns: [
        table.chainId,
        table.blockNumber,
        table.transactionIndex,
        table.traceIndex,
      ],
    }),
  ],
);
export const rpcRequestResults = PONDER_SYNC.table(
  "rpc_request_results",
  (t) => ({
    requestHash: t.text().notNull(),
    chainId: t.bigint({ mode: "bigint" }).notNull(),
    blockNumber: t.bigint({ mode: "bigint" }),
    result: t.text().notNull(),
  }),
  (table) => [
    primaryKey({
      name: "rpc_request_results_pkey",
      columns: [table.chainId, table.requestHash],
    }),
    index("rpc_request_results_chain_id_block_number_index").on(
      table.chainId,
      table.blockNumber,
    ),
  ],
);
export const intervals = PONDER_SYNC.table("intervals", (t) => ({
  fragmentId: t.text().notNull().$type<FragmentId>().primaryKey(),
  chainId: t.bigint({ mode: "bigint" }).notNull(),
  blocks: nummultirange().notNull(),
}));
export const factories = PONDER_SYNC.table(
  "factories",
  (t) => ({
    id: t.integer().primaryKey().generatedAlwaysAsIdentity(),
    factory: t
      .jsonb()
      .$type<
        Pick<
          Factory,
          | "type"
          | "chainId"
          | "address"
          | "eventSelector"
          | "childAddressLocation"
          | "fromBlock"
          | "toBlock"
        >
      >()
      .notNull(),
  }),
  (table) => [
    index("factories_factory_idx").on(table.factory),
    unique("factories_factory_key").on(table.factory),
  ],
);
export const factoryAddresses = PONDER_SYNC.table(
  "factory_addresses",
  (t) => ({
    id: t.integer().primaryKey().generatedAlwaysAsIdentity(),
    factoryId: t.integer().notNull(), // references `factories.id`
    chainId: t.bigint({ mode: "bigint" }).notNull(),
    blockNumber: t.bigint({ mode: "bigint" }).notNull(),
    address: t.text().$type<Address>().notNull(),
  }),
  (table) => [index("factory_addresses_factory_id_index").on(table.factoryId)],
);
</file>

<file path="packages/core/src/types/db.test-d.ts">
import { onchainTable, primaryKey } from "@/drizzle/onchain.js";
import { test } from "vitest";
import type { Delete, Find, Insert, Key, Update } from "./db.js";
test("composite primary key", () => {
  const table = onchainTable(
    "table",
    (t) => ({
      id: t.text().notNull(),
      other: t.integer().notNull(),
      otherOther: t.boolean(),
    }),
    (table) => ({
      pk: primaryKey({ columns: [table.id, table.other] }),
    }),
  );
  // @ts-ignore
  type _ = Key<typeof table>;
  //   ^?
});
test("find", () => {
  const table = onchainTable("table", (t) => ({
    id: t.text().primaryKey(),
    other: t.integer(),
  }));
  // @ts-ignore
  const find: Find = () => {};
  () => {
    // @ts-ignore
    const _ = find(table, { id: "kevin" });
    //    ^?
  };
});
test("insert", () => {
  const table = onchainTable("table", (t) => ({
    id: t.text().primaryKey(),
    other: t.integer(),
  }));
  // @ts-ignore
  const insert: Insert = () => {};
  () => {
    // @ts-ignore
    const t1 = insert(table).values({ id: "kevin" });
    //    ^?
    // @ts-ignore
    const t2 = insert(table).values({ id: "kevin" }).onConflictDoNothing();
    //    ^?
    // @ts-ignore
    const t3 = insert(table).values({ id: "kevin" }).onConflictDoUpdate({
      //  ^?
      other: 9,
    });
    // @ts-ignore
    const t4 = insert(table)
      //  ^?
      .values({ id: "kevin" })
      .onConflictDoUpdate((row) => ({
        other: row.other ?? 8,
      }));
    // @ts-ignore
    const t5 = insert(table)
      //  ^?
      .values([{ id: "kevin" }])
      .onConflictDoNothing();
  };
});
test("update", () => {
  const table = onchainTable("table", (t) => ({
    id: t.text().primaryKey(),
    other: t.integer(),
  }));
  // @ts-ignore
  const update: Update = () => {};
  () => {
    // @ts-ignore
    const _ = update(table, { id: "kevin" }).set({ other: 52 });
    //    ^?
  };
});
test("delete", () => {
  const table = onchainTable("table", (t) => ({
    id: t.text().primaryKey(),
    other: t.integer(),
  }));
  // @ts-ignore
  const _delete: Delete = () => {};
  () => {
    // @ts-ignore
    const _ = _delete(table, { id: "kevin" });
    //    ^?
  };
});
test("non-empty table name", () => {
  onchainTable("table", (t) => ({
    id: t.text().primaryKey(),
    other: t.integer(),
  }));
  // @ts-expect-error
  onchainTable("", (t) => ({
    id: t.text().primaryKey(),
    other: t.integer(),
  }));
});
</file>

<file path="packages/core/src/types/db.ts">
import { onchain } from "@/drizzle/onchain.js";
import type { OnchainTable, PrimaryKeyBuilder } from "@/drizzle/onchain.js";
import type { Schema } from "@/internal/types.js";
import type {
  Column,
  GetColumnData,
  InferInsertModel,
  InferSelectModel,
  Table,
} from "drizzle-orm";
import type { NodePgDatabase } from "drizzle-orm/node-postgres";
import type { PgTableExtraConfig, TableConfig } from "drizzle-orm/pg-core";
import type { PgliteDatabase } from "drizzle-orm/pglite";
import type { PonderTypeError, Prettify } from "./utils.js";
export type Drizzle<TSchema extends Schema = { [name: string]: never }> =
  | NodePgDatabase<TSchema>
  | PgliteDatabase<TSchema>;
export type ReadonlyDrizzle<
  TSchema extends Schema = { [name: string]: never },
> = Omit<
  Drizzle<TSchema>,
  | "insert"
  | "update"
  | "delete"
  | "transaction"
  | "refreshMaterializedView"
  | "_"
>;
export type Db<schema extends Schema> = {
  /**
   * Find a row
   *
   * - Docs: https://ponder.sh/docs/indexing/write#find
   *
   * @example
   * ```ts twoslash
   * const result = await db.find(table, { id: 10 });
   * ```
   *
   * @param table - The table to select from.
   * @param key - The primary key.
   * @returns The row if it exists or undefined if it doesn't.
   */
  find: Find;
  /**
   * Create new rows
   *
   * - Docs: https://ponder.sh/docs/indexing/write#insert
   *
   * @example
   * ```ts twoslash
   * await db.insert(table).values({ id: 10, name: "joe" });
   * ```
   *
   * @example
   * ```ts twoslash
   * await db.insert(table).values([
   *   { id: 10, name: "joe" },
   *   { id: 3, name: "rob" }
   * ]);
   * ```
   *
   * @example
   * ```ts twoslash
   * await db.insert(table).values({ id: 10, name: "joe" }).onConflictDoNothing();
   * ```
   *
   * @example
   * ```ts twoslash
   * await db
   *   .insert(table)
   *   .values({ id: 10, name: "joe" })
   *   .onConflictDoUpdate((row) => ({ age: row.age + 3 }));
   * ```
   *
   * @param table - The table to insert into.
   */
  insert: Insert;
  /**
   * Update a row
   *
   * - Docs: https://ponder.sh/docs/indexing/write#update
   *
   * @example
   * ```ts twoslash
   * await db
   *   .update(table, { id: 10 })
   *   .set({ age: 19 });
   * ```
   *
   * @example
   * ```ts twoslash
   * await db
   *   .update(table, { id: 10 })
   *   .set((row) => ({ age: row.age + 3 }));
   * ```
   *
   * @param table - The table to select from.
   * @param key - The primary key.
   */
  update: Update;
  /**
   * Delete a row
   *
   * - Docs: https://ponder.sh/docs/indexing/write#delete
   *
   * @example
   * ```ts twoslash
   * const deleted = await db.delete(table, { id: 10 });
   * ```
   *
   * @param table - The table to select from.
   * @param key - The primary key.
   * @returns `true` if the row existed.
   */
  delete: Delete;
  /**
   * Access the raw drizzle object
   *
   * - Docs: https://ponder.sh/docs/indexing/write#raw-sql
   */
  sql: Prettify<Omit<Drizzle<schema>, "refreshMaterializedView" | "_">>;
};
type InferPrimaryKey<
  table extends Table,
  ///
  columns extends Record<string, Column> = table["_"]["columns"],
  columnNames extends keyof columns & string = keyof columns & string,
> = columnNames extends columnNames
  ? columns[columnNames]["_"]["isPrimaryKey"] extends true
    ? columnNames
    : never
  : never;
export type Key<
  table extends Table,
  ///
  compositePrimaryKey extends // @ts-ignore
  keyof table["_"]["columns"] = InferCompositePrimaryKey<table>,
  primaryKey extends keyof table["_"]["columns"] = [
    compositePrimaryKey,
  ] extends [never]
    ? InferPrimaryKey<table>
    : compositePrimaryKey,
> = {
  [columnName in primaryKey]: GetColumnData<table["_"]["columns"][columnName]>;
};
export type InferCompositePrimaryKey<
  table extends OnchainTable<
    TableConfig & { extra: PgTableExtraConfig | undefined }
  >,
  ///
  extra extends PgTableExtraConfig | undefined = table["_"]["config"]["extra"],
  builders = extra[keyof extra],
> = builders extends builders
  ? builders extends PrimaryKeyBuilder
    ? builders["columnNames"]
    : never
  : never;
export type Find = <table extends Table>(
  table: table extends { [onchain]: true }
    ? table
    : PonderTypeError<`db.find() can only be used with onchain tables, and '${table["_"]["name"]}' is an offchain table or a view.`>,
  key: Key<table>,
) => Promise<InferSelectModel<table> | null>;
export type Insert = <
  table extends Table,
  ///
  insertModel = InferInsertModel<table>,
  selectModel = InferSelectModel<table>,
  updateModel = Prettify<Omit<insertModel, keyof Key<table>>>,
  updateFn = (row: selectModel) => Partial<updateModel>,
>(
  table: table extends { [onchain]: true }
    ? table
    : PonderTypeError<`Indexing functions can only write to onchain tables, and '${table["_"]["name"]}' is an offchain table or a view.`>,
) => {
  /**
   * Create new rows
   *
   * - Docs: https://ponder.sh/docs/indexing/write#insert
   *
   * @example
   * ```ts twoslash
   * await db.insert(table).values({ id: 10, name: "joe" });
   * ```
   *
   * @example
   * ```ts twoslash
   * await db.insert(table).values([
   *   { id: 10, name: "joe" },
   *   { id: 3, name: "rob" }
   * ]);
   * ```
   * @param table - The table to insert into.
   */
  values: <values extends insertModel | insertModel[]>(
    values: values,
  ) => Promise<values extends unknown[] ? selectModel[] : selectModel> & {
    /**
     * Create new rows, cancelling the insert if there is a conflict
     *
     * - Docs: https://ponder.sh/docs/indexing/write#onconflictdonothing
     * @example
     * ```ts twoslash
     * await db.insert(table).values({ id: 10, name: "joe" }).onConflictDoNothing();
     * ```
     * @param table - The table to insert into.
     */
    onConflictDoNothing: () => Promise<
      values extends unknown[] ? (selectModel | null)[] : selectModel | null
    >;
    /**
     * Create new rows, updating the row if there is a conflict
     *
     * - Docs: https://ponder.sh/docs/indexing/write#onconflictdoupdate
     *
     * @example
     * ```ts twoslash
     * await db
     *   .insert(table)
     *   .values({ id: 10, name: "joe" })
     *   .onConflictDoUpdate({ age: 24 });
     * ```
     *
     * @example
     * ```ts twoslash
     * await db
     *   .insert(table)
     *   .values({ id: 10, name: "joe" })
     *   .onConflictDoUpdate((row) => ({ age: row.age + 3 }));
     * ```
     *
     * @param table - The table to insert into.
     */
    onConflictDoUpdate: (
      values: Partial<updateModel> | updateFn,
    ) => Promise<values extends unknown[] ? selectModel[] : selectModel>;
  };
};
export type Update = <
  table extends Table,
  ///
  insertModel = InferInsertModel<table>,
  selectModel = InferSelectModel<table>,
  insertValues = Prettify<Omit<insertModel, keyof Key<table>>>,
  updateFn = (row: selectModel) => Partial<insertModel>,
>(
  table: table extends { [onchain]: true }
    ? table
    : PonderTypeError<`Indexing functions can only write to onchain tables, and '${table["_"]["name"]}' is an offchain table or a view.`>,
  key: Key<table>,
) => {
  /**
   * Update a row
   *
   * - Docs: https://ponder.sh/docs/indexing/write#update
   *
   * @example
   * ```ts twoslash
   * await db
   *   .update(table, { id: 10 })
   *   .set({ age: 19 });
   * ```
   *
   * @example
   * ```ts twoslash
   * await db
   *   .update(table, { id: 10 })
   *   .set((row) => ({ age: row.age + 3 }));
   * ```
   *
   * @param table - The table to select from.
   * @param key - The primary key.
   */
  set: (values: Partial<insertValues> | updateFn) => Promise<selectModel>;
};
export type Delete = <table extends Table>(
  table: table extends { [onchain]: true }
    ? table
    : PonderTypeError<`Indexing functions can only write to onchain tables, and '${table["_"]["name"]}' is an offchain table or a view.`>,
  key: Key<table>,
) => Promise<boolean>;
</file>

<file path="packages/core/src/types/eth.ts">
import type { AccessList, Address, Hash, Hex, TransactionType } from "viem";
import type { Prettify } from "./utils.js";
/**
 * A confirmed Ethereum block.
 *
 * @link https://docs.soliditylang.org/en/v0.8.20/introduction-to-smart-contracts.html#blocks
 */
export type Block = {
  /** Base fee per gas */
  baseFeePerGas: bigint | null;
  /** Difficulty for this block */
  difficulty: bigint;
  /** "Extra data" field of this block */
  extraData: Hex;
  /** Maximum gas allowed in this block */
  gasLimit: bigint;
  /** Total used gas by all transactions in this block */
  gasUsed: bigint;
  /** Block hash */
  hash: Hash;
  /** Logs bloom filter */
  logsBloom: Hex;
  /** Address that received this blocks mining rewards */
  miner: Address;
  /** Unique identifier for the block. */
  mixHash: Hash | null;
  /** Proof-of-work hash */
  nonce: Hex | null;
  /** Block number */
  number: bigint;
  /** Parent block hash */
  parentHash: Hash;
  /** Root of the this blocks receipts trie */
  receiptsRoot: Hex;
  /** SHA3 of the uncles data in this block */
  sha3Uncles: Hash | null;
  /** Size of this block in bytes */
  size: bigint;
  /** Root of this blocks final state trie */
  stateRoot: Hash;
  /** Unix timestamp of when this block was collated */
  timestamp: bigint;
  /** Total difficulty of the chain until this block */
  totalDifficulty: bigint | null;
  /** Root of this blocks transaction trie */
  transactionsRoot: Hash;
};
/**
 * A confirmed Ethereum transaction. Contains `legacy`, `EIP-1559`, or `EIP-2930` fee values depending on the transaction `type`.
 *
 * @link https://docs.soliditylang.org/en/v0.8.20/introduction-to-smart-contracts.html#transactions
 */
export type Transaction = Prettify<
  {
    /** Transaction sender */
    from: Address;
    /** Gas provided for transaction execution */
    gas: bigint;
    /** Hash of this transaction */
    hash: Hash;
    /** Contract code or a hashed method call */
    input: Hex;
    /** Unique number identifying this transaction */
    nonce: number;
    /** ECDSA signature r */
    r: Hex | null;
    /** ECDSA signature s */
    s: Hex | null;
    /** Transaction recipient or `null` if deploying a contract */
    to: Address | null;
    /** Index of this transaction in the block */
    transactionIndex: number;
    /** ECDSA recovery ID */
    v: bigint | null;
    /** Value in wei sent with this transaction */
    value: bigint;
  } & (
    | {
        /** Transaction type. */
        type: "legacy";
        accessList?: never;
        /** Base fee per gas. Only present in legacy and EIP-2930 transactions. */
        gasPrice: bigint;
        maxFeePerGas?: never;
        maxPriorityFeePerGas?: never;
      }
    | {
        /** Transaction type. */
        type: "eip2930";
        /** List of addresses and storage keys the transaction will access. */
        accessList: AccessList;
        /** Base fee per gas. Only present in legacy and EIP-2930 transactions. */
        gasPrice: bigint;
        maxFeePerGas?: never;
        maxPriorityFeePerGas?: never;
      }
    | {
        /** Transaction type. */
        type: "eip1559";
        accessList?: never;
        gasPrice?: never;
        /** Total fee per gas in wei (gasPrice/baseFeePerGas + maxPriorityFeePerGas). Only present in EIP-1559 transactions. */
        maxFeePerGas: bigint;
        /** Max priority fee per gas (in wei). Only present in EIP-1559 transactions. */
        maxPriorityFeePerGas: bigint;
      }
    | {
        /** Transaction type. */
        type: "deposit";
        accessList?: never;
        gasPrice?: never;
        /** Total fee per gas in wei (gasPrice/baseFeePerGas + maxPriorityFeePerGas). Only present in EIP-1559 transactions. */
        maxFeePerGas?: bigint;
        /** Max priority fee per gas (in wei). Only present in EIP-1559 transactions. */
        maxPriorityFeePerGas?: bigint;
      }
    | {
        /** Transaction type. */
        type: Hex;
        gasPrice?: never;
        accessList?: never;
        maxFeePerGas?: never;
        maxPriorityFeePerGas?: never;
      }
  )
>;
/**
 * A confirmed Ethereum log.
 *
 * @link https://docs.soliditylang.org/en/v0.8.20/abi-spec.html#events
 */
export type Log = {
  /** The address from which this log originated */
  address: Address;
  /** Contains the non-indexed arguments of the log */
  data: Hex;
  /** Index of this log within its block */
  logIndex: number;
  /**
   * Indicates if this log was removed in a chain reorganization.
   *
   * Ponder automatically handles reorgs, so this will always be `false`.
   */
  removed: boolean;
  /** List of order-dependent topics */
  topics: [Hex, ...Hex[]] | [];
};
/** A confirmed Ethereum transaction receipt. */
export type TransactionReceipt = {
  /** Address of new contract or `null` if no contract was created */
  contractAddress: Address | null;
  /** Gas used by this and all preceding transactions in this block */
  cumulativeGasUsed: bigint;
  /** Pre-London, it is equal to the transaction's gasPrice. Post-London, it is equal to the actual gas price paid for inclusion. */
  effectiveGasPrice: bigint;
  /** Transaction sender */
  from: Address;
  /** Gas used by this transaction */
  gasUsed: bigint;
  /** Logs bloom filter */
  logsBloom: Hex;
  /** `success` if this transaction was successful or `reverted` if it failed */
  status: "success" | "reverted";
  /** Transaction recipient or `null` if deploying a contract */
  to: Address | null;
  /** Transaction type */
  type: TransactionType;
};
export type Trace = {
  /** The type of the call. */
  type:
    | "CALL"
    | "CALLCODE"
    | "DELEGATECALL"
    | "STATICCALL"
    | "CREATE"
    | "CREATE2"
    | "SELFDESTRUCT";
  /** The address of that initiated the call. */
  from: Address;
  /** The address of the contract that was called. */
  to: Address | null;
  /** How much gas was left before the call. */
  gas: bigint;
  /** How much gas was used by the call. */
  gasUsed: bigint;
  /** Calldata input. */
  input: Hex;
  /** Output of the call, if any. */
  output?: Hex;
  /** Error message, if any. */
  error?: string;
  /** Why this call reverted, if it reverted. */
  revertReason?: string;
  /** Value transferred. */
  value: bigint | null;
  /** Index of this trace in the transaction. */
  traceIndex: number;
  /** Number of subcalls. */
  subcalls: number;
};
/** A native token transfer. */
export type Transfer = {
  /** The address that sent the transfer */
  from: Address;
  /** The address that received the transfer */
  to: Address;
  /** The amount of tokens transferred */
  value: bigint;
};
</file>

<file path="packages/core/src/types/utils.ts">
/**
 * @description Combines members of an intersection into a readable type.
 *
 * @link https://twitter.com/mattpocockuk/status/1622730173446557697?s=20&t=NdpAcmEFXY01xkqU3KO0Mg
 * @example
 * Prettify<{ a: string } | { b: string } | { c: number, d: bigint }>
 * => { a: string, b: string, c: number, d: bigint }
 */
export type Prettify<T> = {
  [K in keyof T]: T[K];
} & {};
/**
 * @description Creates a type with all keys K from T as non-null.
 */
export type NonNull<T> = {
  [P in keyof T]-?: NonNullable<T[P]>;
};
export type PonderTypeError<error extends string> = error;
export type DeepPartial<T> = {
  [P in keyof T]?: T[P] extends (infer U)[]
    ? DeepPartial<U>[]
    : T[P] extends object
      ? DeepPartial<T[P]>
      : T[P];
};
/**
 * @description Creates a partial type with the selected keys required.
 *
 * @example
 * type t = PartialExcept<{ a: string, b: boolean }, "a">
 * //   ^? t: { a: string, b?: boolean }
 */
export type PartialExcept<T, K extends keyof T> = {
  [P in K]: T[P];
} & {
  [P in keyof Omit<T, K>]?: T[P];
};
/**
 * @description Marks a property of an object as optional.
 */
export type MakeOptional<T, K extends keyof T> = Omit<T, K> &
  Partial<Pick<T, K>>;
</file>

<file path="packages/core/src/types/virtual.test-d.ts">
import { createConfig } from "@/config/index.js";
import { onchainTable } from "@/drizzle/onchain.js";
import { type Abi, type Address, type Hex, parseAbiItem } from "viem";
import { assertType, test } from "vitest";
import type { Db } from "./db.js";
import type {
  Block,
  Log,
  Trace,
  Transaction,
  TransactionReceipt,
} from "./eth.js";
import type { Virtual } from "./virtual.js";
const event0 = parseAbiItem(
  "event Event0(bytes32 indexed arg, bytes32 indexed arg1)",
);
const event1 = parseAbiItem("event Event1()");
const event1Overloaded = parseAbiItem("event Event1(bytes32)");
const func0 = parseAbiItem(
  "function func0(address) external returns (uint256)",
);
const func1 = parseAbiItem("function func1()");
const func1Overloaded = parseAbiItem("function func1(bytes32)");
type Event0 = typeof event0;
type Event1 = typeof event1;
type Event1Overloaded = typeof event1Overloaded;
type Func0 = typeof func0;
type Func1 = typeof func1;
type Func1Overloaded = typeof func1Overloaded;
type abi = readonly [
  Event0,
  Event1,
  Event1Overloaded,
  Func0,
  Func1,
  Func1Overloaded,
];
const config = createConfig({
  chains: {
    mainnet: {
      id: 1,
      rpc: "https://rpc.com",
    },
    optimism: {
      id: 10,
      rpc: "https://rpc.com",
    },
  },
  contracts: {
    c1: {
      abi: [event0, func0],
      chain: "mainnet",
      address: "0x",
      startBlock: 0,
      includeTransactionReceipts: false,
      includeCallTraces: true,
    },
    c2: {
      abi: [event1, event1Overloaded, func1, func1Overloaded],
      address: "0x69",
      chain: {
        mainnet: {
          startBlock: 1,
          includeTransactionReceipts: true,
          includeCallTraces: true,
        },
        optimism: {},
      },
    },
  },
  accounts: {
    a1: {
      address: "0x",
      chain: "mainnet",
    },
  },
  blocks: {
    b1: {
      interval: 2,
      startBlock: 1,
      chain: "mainnet",
    },
  },
});
const account = onchainTable("account", (p) => ({
  address: p.hex().primaryKey(),
  balance: p.bigint().notNull(),
}));
const schema = { account };
test("FormatEventNames", () => {
  type a = Virtual.FormatEventNames<
    // ^?
    {
      contract: { abi: abi; chain: "" };
    },
    {},
    {}
  >;
  type eventNames =
    | "contract:setup"
    | "contract:Event0"
    | "contract:Event1()"
    | "contract:Event1(bytes32)";
  assertType<a>({} as any as eventNames);
  assertType<eventNames>({} as any as a);
});
test("FormatEventNames with semi-weak abi", () => {
  type a = Virtual.FormatEventNames<
    // ^?
    {
      contract: { abi: abi[number][]; chain: "" };
    },
    {},
    {}
  >;
  type eventNames =
    | "contract:setup"
    | "contract:Event0"
    | "contract:Event1()"
    | "contract:Event1(bytes32)";
  assertType<a>({} as any as eventNames);
  assertType<eventNames>({} as any as a);
});
test("FormatEventNames with weak abi", () => {
  type a = Virtual.FormatEventNames<
    // ^?
    {
      contract: { abi: Abi; chain: "" };
    },
    {},
    {}
  >;
  assertType<a>({} as any as "contract:setup");
  assertType<"contract:setup">({} as any as a);
});
test("FormatEventNames with functions", () => {
  type a = Virtual.FormatEventNames<
    // ^?
    {
      contract: { abi: abi; chain: ""; includeCallTraces: true };
    },
    {},
    {}
  >;
  type eventNames =
    | "contract:setup"
    | "contract:Event0"
    | "contract:Event1()"
    | "contract:Event1(bytes32)"
    | "contract.func0()"
    | "contract.func1()"
    | "contract.func1(bytes32)";
  assertType<a>({} as any as eventNames);
  assertType<eventNames>({} as any as a);
});
test("FormatEventName with accounts", () => {
  type a = Virtual.FormatEventNames<
    // ^?
    {},
    { account: { address: "0x"; chain: "mainnet" } },
    {}
  >;
  assertType<a>(
    {} as any as
      | "account:transfer:from"
      | "account:transfer:to"
      | "account:transaction:from"
      | "account:transaction:to",
  );
  assertType<
    | "account:transfer:from"
    | "account:transfer:to"
    | "account:transaction:from"
    | "account:transaction:to"
  >({} as any as a);
});
test("FormatEventName with blocks", () => {
  type a = Virtual.FormatEventNames<
    // ^?
    {},
    {},
    { block: { interval: 2; startBlock: 1; chain: "mainnet" } }
  >;
  assertType<a>({} as any as "block:block");
  assertType<"block:block">({} as any as a);
});
test("Context db", () => {
  type a = Virtual.Context<typeof config, typeof schema, "c1:Event0">["db"];
  //   ^?
  assertType<a>({} as any as Db<typeof schema>);
  assertType<Db<typeof schema>>({} as any as a);
});
test("Context single chain", () => {
  type a = Virtual.Context<typeof config, typeof schema, "c1:Event0">["chain"];
  //   ^?
  type expectedChain = { name: "mainnet"; id: 1 };
  assertType<a>({} as any as expectedChain);
  assertType<expectedChain>({} as any as a);
});
test("Context multi chain", () => {
  type a = Virtual.Context<
    typeof config,
    typeof schema,
    "c2:Event1()"
  >["chain"];
  //   ^?
  type expectedChain =
    | { name: "mainnet"; id: 1 }
    | { name: "optimism"; id: 10 };
  assertType<a>({} as any as expectedChain);
  assertType<expectedChain>({} as any as a);
});
test("Context block chain", () => {
  type a = Virtual.Context<typeof config, typeof schema, "b1:block">["chain"];
  //   ^?
  type expectedChain = { name: "mainnet"; id: 1 };
  assertType<a>({} as any as expectedChain);
  assertType<expectedChain>({} as any as a);
});
test("Context client", () => {
  type a = Virtual.Context<
    typeof config,
    typeof schema,
    "c2:Event1()"
  >["client"];
  //   ^?
  type expectedFunctions =
    | "request"
    | "readContract"
    | "multicall"
    | "getStorageAt"
    | "getCode"
    | "getBalance"
    | "getEnsName";
  assertType<keyof a>({} as any as expectedFunctions);
});
test("Context contracts", () => {
  type a = Virtual.Context<
    typeof config,
    typeof schema,
    "c2:Event1()"
  >["contracts"]["c2"];
  //   ^?
  type expectedAbi = [Event1, Event1Overloaded, Func1, Func1Overloaded];
  type expectedStartBlock = 1 | undefined;
  type expectedEndBlock = undefined;
  type expectedAddress = "0x69";
  assertType<a["abi"]>({} as any as expectedAbi);
  assertType<expectedAbi>({} as any as a["abi"]);
  assertType<a["startBlock"]>({} as any as expectedStartBlock);
  assertType<expectedStartBlock>({} as any as a["startBlock"]);
  assertType<a["endBlock"]>({} as any as expectedEndBlock);
  assertType<expectedEndBlock>({} as any as a["endBlock"]);
  assertType<a["address"]>({} as any as expectedAddress);
  assertType<expectedAddress>({} as any as a["address"]);
});
test("Context chain without event", () => {
  type a = Virtual.Context<
    // ^?
    typeof config,
    typeof schema,
    Virtual.EventNames<typeof config>
  >["chain"];
  type expectedChain =
    | {
        name: "mainnet";
        id: 1;
      }
    | {
        name: "optimism";
        id: 10;
      };
  assertType<a>({} as any as expectedChain);
  assertType<expectedChain>({} as any as a);
});
test("Event", () => {
  type a = Virtual.Event<typeof config, "c1:Event0">;
  //   ^?
  type expectedEvent = {
    id: string;
    args: {
      arg: Hex;
      arg1: Hex;
    };
    log: Log;
    block: Block;
    transaction: Transaction;
  };
  assertType<a>({} as any as expectedEvent);
  assertType<expectedEvent>({} as any as a);
});
test("Event transaction receipt", () => {
  type a = Virtual.Event<typeof config, "c2:Event1()">;
  //   ^?
  type expectedEvent = {
    id: string;
    args: readonly [];
    log: Log;
    block: Block;
    transaction: Transaction;
    transactionReceipt?: TransactionReceipt;
  };
  assertType<a>({} as any as expectedEvent);
  assertType<expectedEvent>({} as any as a);
});
test("Event with unnamed parameters", () => {
  type a = Virtual.Event<typeof config, "c2:Event1(bytes32)">;
  //   ^?
  type expectedEvent = {
    id: string;
    args: readonly [Hex];
    log: Log;
    block: Block;
    transaction: Transaction;
  };
  assertType<a>({} as any as expectedEvent);
  assertType<expectedEvent>({} as any as a);
});
test("Event with functions", () => {
  type a = Virtual.Event<typeof config, "c1.func0()">;
  //   ^?
  type expectedEvent = {
    id: string;
    args: readonly [Address];
    result: bigint;
    trace: Trace;
    block: Block;
    transaction: Transaction;
  };
  assertType<a>({} as any as expectedEvent);
  assertType<expectedEvent>({} as any as a);
});
test("Event with functions and no inputs or outputs", () => {
  type a = Virtual.Event<typeof config, "c2.func1()">;
  //   ^?
  type expectedEvent = {
    id: string;
    args: never;
    result: never;
    trace: Trace;
    block: Block;
    transaction: Transaction;
  };
  assertType<a>({} as any as expectedEvent);
  assertType<expectedEvent>({} as any as a);
});
test("Event with account transaction", () => {
  type a = Virtual.Event<typeof config, "a1:transaction:from">;
  //   ^?
  type expectedEvent = {
    id: string;
    block: Block;
    transaction: Transaction;
    transactionReceipt: TransactionReceipt;
  };
  assertType<a>({} as any as expectedEvent);
  assertType<expectedEvent>({} as any as a);
});
test("Event with account transfer", () => {
  type a = Virtual.Event<typeof config, "a1:transfer:from">;
  //   ^?
  type expectedEvent = {
    id: string;
    transfer: {
      from: Address;
      to: Address;
      value: bigint;
    };
    block: Block;
    transaction: Transaction;
    trace: Trace;
  };
  assertType<a>({} as any as expectedEvent);
  assertType<expectedEvent>({} as any as a);
});
test("Event with block", () => {
  type a = Virtual.Event<typeof config, "b1:block">;
  //   ^?
  type expectedEvent = {
    id: string;
    block: Block;
  };
  assertType<a>({} as any as expectedEvent);
  assertType<expectedEvent>({} as any as a);
});
test("Registry", () => {
  const ponder = {} as any as Virtual.Registry<typeof config, typeof schema>;
  ponder.on("c1:Event0", async ({ event, context }) => {
    event.id;
    event.args;
    event.log;
    event.block;
    event.transaction;
    context.chain;
    context.db;
    context.client;
    context.contracts.c1;
    context.contracts.c2;
  });
});
</file>

<file path="packages/core/src/types/virtual.ts">
import type { Config } from "@/config/index.js";
import type {
  FormatEventArgs,
  FormatFunctionArgs,
  FormatFunctionResult,
  SafeEventNames,
  SafeFunctionNames,
} from "@/config/utilityTypes.js";
import type { ReadonlyClient } from "@/indexing/client.js";
import type { Schema } from "@/internal/types.js";
import type {
  Block,
  Log,
  Trace,
  Transaction,
  TransactionReceipt,
  Transfer,
} from "@/types/eth.js";
import type { Db } from "./db.js";
import type { Prettify } from "./utils.js";
export namespace Virtual {
  type Setup = "setup";
  type _FormatEventNames<
    contract extends Config["contracts"][string],
    ///
    safeEventNames = SafeEventNames<contract["abi"]>,
  > = string extends safeEventNames ? never : safeEventNames;
  type _FormatFunctionNames<
    contract extends Config["contracts"][string],
    ///
    safeFunctionNames = SafeFunctionNames<contract["abi"]>,
  > = string extends safeFunctionNames ? never : safeFunctionNames;
  /** "{ContractName}:{EventName}" | "{ContractName}.{FunctionName}()" | "{SourceName}:block" | "{SourceName}:transaction:from" . */
  export type FormatEventNames<
    contracts extends Config["contracts"],
    accounts extends Config["accounts"],
    blocks extends Config["blocks"],
  > =
    | {
        [name in keyof contracts]: `${name & string}:${_FormatEventNames<contracts[name]> | Setup}`;
      }[keyof contracts]
    | {
        [name in keyof accounts]: `${name & string}:${"transaction" | "transfer"}:${"from" | "to"}`;
      }[keyof accounts]
    | {
        [name in keyof blocks]: `${name & string}:block`;
      }[keyof blocks]
    | {
        [name in keyof contracts]: true extends ExtractOverridenProperty<
          contracts[name],
          "includeCallTraces"
        >
          ? `${name & string}.${_FormatFunctionNames<contracts[name]>}`
          : never;
      }[keyof contracts];
  type FormatTransactionReceipts<
    source extends Config["contracts" | "accounts"][string],
    ///
    includeTxr = ExtractOverridenProperty<source, "includeTransactionReceipts">,
  > = includeTxr extends includeTxr
    ? includeTxr extends true
      ? {
          transactionReceipt: Prettify<TransactionReceipt>;
        }
      : {
          transactionReceipt?: never;
        }
    : never;
  export type ExtractEventName<name extends string> =
    name extends `${string}:${infer EventName extends string}`
      ? EventName
      : name extends `${string}.${infer EventName extends string}`
        ? EventName
        : never;
  export type ExtractSourceName<name extends string> =
    name extends `${infer SourceName extends string}:${string}`
      ? SourceName
      : name extends `${infer SourceName extends string}.${string}`
        ? SourceName
        : never;
  export type EventNames<config extends Config> = FormatEventNames<
    config["contracts"],
    config["accounts"],
    config["blocks"]
  >;
  export type Event<
    config extends Config,
    name extends EventNames<config>,
    ///
    sourceName extends ExtractSourceName<name> = ExtractSourceName<name>,
    eventName extends ExtractEventName<name> = ExtractEventName<name>,
  > = name extends `${string}:block`
    ? // 1. block event
      {
        id: string;
        block: Prettify<Block>;
      }
    : name extends `${string}:transaction:${"from" | "to"}`
      ? // 2. transaction event
        {
          id: string;
          block: Prettify<Block>;
          transaction: Prettify<Transaction>;
          transactionReceipt: Prettify<TransactionReceipt>;
        }
      : name extends `${string}:transfer:${"from" | "to"}`
        ? // 3. transfer event
          {
            id: string;
            transfer: Prettify<Transfer>;
            block: Prettify<Block>;
            transaction: Prettify<Transaction>;
            trace: Prettify<Trace>;
          } & FormatTransactionReceipts<config["accounts"][sourceName]>
        : name extends `${string}.${string}`
          ? // 4. call trace event
            Prettify<
              {
                id: string;
                args: FormatFunctionArgs<
                  config["contracts"][sourceName]["abi"],
                  eventName
                >;
                result: FormatFunctionResult<
                  config["contracts"][sourceName]["abi"],
                  eventName
                >;
                trace: Prettify<Trace>;
                block: Prettify<Block>;
                transaction: Prettify<Transaction>;
              } & FormatTransactionReceipts<config["contracts"][sourceName]>
            >
          : eventName extends Setup
            ? // 5. setup event
              never
            : // 6. log event
              Prettify<
                {
                  id: string;
                  args: FormatEventArgs<
                    config["contracts"][sourceName]["abi"],
                    eventName
                  >;
                  log: Prettify<Log>;
                  block: Prettify<Block>;
                  transaction: Prettify<Transaction>;
                } & FormatTransactionReceipts<config["contracts"][sourceName]>
              >;
  type ContextContractProperty = Exclude<
    keyof Config["contracts"][string],
    "abi" | "chain" | "filter" | "factory"
  >;
  type ExtractOverridenProperty<
    contract extends Config["contracts" | "accounts"][string],
    property extends ContextContractProperty,
    ///
    base = Extract<contract, { [p in property]: unknown }>[property],
    override = Extract<
      contract["chain"][keyof contract["chain"]],
      { [p in property]: unknown }
    >[property],
  > = ([base] extends [never] ? undefined : base) | override;
  export type Context<
    config extends Config,
    schema extends Schema,
    name extends EventNames<config>,
    ///
    sourceName extends ExtractSourceName<name> = ExtractSourceName<name>,
    sourceChain = sourceName extends sourceName
      ?
          | (unknown extends config["contracts"][sourceName]["chain"]
              ? never
              : config["contracts"][sourceName]["chain"])
          | (unknown extends config["blocks"][sourceName]["chain"]
              ? never
              : config["blocks"][sourceName]["chain"])
      : never,
  > = {
    contracts: {
      [_contractName in keyof config["contracts"]]: {
        abi: config["contracts"][_contractName]["abi"];
        address: ExtractOverridenProperty<
          config["contracts"][_contractName],
          "address"
        >;
        startBlock: ExtractOverridenProperty<
          config["contracts"][_contractName],
          "startBlock"
        >;
        endBlock: ExtractOverridenProperty<
          config["contracts"][_contractName],
          "endBlock"
        >;
      };
    };
    chain: sourceChain extends string
      ? // 1. No chain overriding
        {
          name: sourceChain;
          id: config["chains"][sourceChain]["id"];
        }
      : // 2. Chain overrides
        {
          [key in keyof sourceChain]: {
            name: key;
            id: config["chains"][key & keyof config["chains"]]["id"];
          };
        }[keyof sourceChain];
    client: Prettify<ReadonlyClient>;
    db: Db<schema>;
  };
  export type IndexingFunctionArgs<
    config extends Config,
    schema extends Schema,
    name extends EventNames<config>,
  > = {
    event: Event<config, name>;
    context: Context<config, schema, name>;
  };
  export type Registry<config extends Config, schema extends Schema> = {
    on: <name extends EventNames<config>>(
      _name: name,
      indexingFunction: (
        args: { event: Event<config, name> } & {
          context: Prettify<Context<config, schema, name>>;
        },
      ) => Promise<void> | void,
    ) => void;
  };
}
</file>

<file path="packages/core/src/ui/app.ts">
import type {
  getAppProgress,
  getIndexingProgress,
  getSyncProgress,
} from "@/internal/metrics.js";
import { formatEta, formatPercentage } from "@/utils/format.js";
import pc from "picocolors";
export type UiState = {
  port: number;
  hostname: string;
  sync: Awaited<ReturnType<typeof getSyncProgress>>;
  indexing: Awaited<ReturnType<typeof getIndexingProgress>>;
  app: Awaited<ReturnType<typeof getAppProgress>>;
};
export const initialUiState: UiState = {
  port: 42069,
  hostname: "localhost",
  sync: [],
  indexing: {
    hasError: false,
    events: [],
  },
  app: {
    progress: 0,
    eta: undefined,
    mode: undefined,
  },
};
const buildProgressBar = (current: number, end: number, width = 48): string => {
  const fraction = current / end;
  const count = Math.min(Math.floor(width * fraction), width);
  return "".repeat(count) + "".repeat(width - count);
};
export const buildTable = (
  rows: { [key: string]: any }[],
  columns: {
    title: string;
    key: string;
    align: "left" | "right" | string;
    format?: (value: any, row: { [key: string]: any }) => string | number;
    maxWidth?: number;
  }[],
): string[] => {
  if (rows.length === 0) {
    return ["Waiting to start..."];
  }
  // Calculate column widths
  const DEFAULT_MAX_COLUMN_WIDTH = 24;
  const columnWidths = columns.map((column) => {
    const formattedRows = rows.map((row) => {
      const value = column.format
        ? column.format(row[column.key], row)
        : row[column.key];
      return value !== undefined ? String(value) : "";
    });
    const maxWidth = Math.max(
      ...formattedRows.map((val) => val.toString().length),
      column.title.length,
    );
    return Math.min(maxWidth, column.maxWidth ?? DEFAULT_MAX_COLUMN_WIDTH);
  });
  // Generate header row
  const headerRow = [
    " ",
    columns
      .map((col, i) => {
        const width = columnWidths[i] ?? 0;
        return col.title
          .padEnd(width, " ")
          .padStart(col.align === "right" ? width : width, " ");
      })
      .join("  "),
    " ",
  ].join("");
  // Generate separator
  const separator = [
    "",
    columnWidths.map((w) => "".repeat(w)).join(""),
    "",
  ].join("");
  // Generate data rows
  const dataRows = rows.map((row) => {
    return [
      " ",
      columns
        .map((col, i) => {
          const width = columnWidths[i] ?? 0;
          const value = col.format
            ? col.format(row[col.key], row)
            : row[col.key];
          const strValue = value !== undefined ? String(value) : "";
          return col.align === "right"
            ? strValue.padStart(width, " ")
            : strValue.padEnd(width, " ");
        })
        .join("  "),
      " ",
    ].join("");
  });
  return [headerRow, separator, ...dataRows];
};
export const buildUiLines = (ui: UiState): string[] => {
  const { sync, indexing, app, port, hostname } = ui;
  const lines: string[] = [];
  lines.push("");
  if (indexing.hasError) {
    lines.push(
      pc.cyan("Resolve the error and save your changes to reload the server."),
    );
    return lines;
  }
  lines.push(pc.bold("Chains"));
  lines.push("");
  if (sync.length === 0) {
    lines.push("Waiting to start...");
  } else {
    lines.push(
      ...buildTable(sync, [
        {
          title: "Chain",
          key: "chainName",
          align: "left",
        },
        {
          title: "Status",
          key: "status",
          align: "left",
          format: (_, row) =>
            row.status === "backfill"
              ? `${row.status} (${formatPercentage(row.progress)})`
              : row.status,
        },
        {
          title: "Block",
          key: "block",
          align: "right",
        },
        {
          title: "RPC (req/s)",
          key: "rps",
          align: "right",
          format: (_, row) => row.rps.toFixed(1),
        },
      ]),
    );
  }
  lines.push("");
  let indexingLabel = pc.bold("Indexing");
  if (app.mode !== undefined && app.progress !== 0) {
    const color = app.mode === "backfill" ? "yellowBright" : "greenBright";
    indexingLabel += ` (${pc[color](app.mode)})`;
  }
  lines.push(indexingLabel);
  lines.push("");
  if (indexing.events.length === 0) {
    lines.push("Waiting to start...");
  } else {
    lines.push(
      ...buildTable(indexing.events, [
        { title: "Event", key: "eventName", align: "left", maxWidth: 36 },
        { title: "Count", key: "count", align: "right" },
        {
          title: "Duration (ms)",
          key: "averageDuration",
          align: "right",
          format: (v) => (v > 0 ? (v < 0.001 ? "<0.001" : v.toFixed(3)) : "-"),
        },
      ]),
    );
  }
  if (app.mode !== "live") {
    const progressValue = app.progress ?? 0;
    const progressBar = buildProgressBar(progressValue, 1, 48);
    let progressText = `${progressBar} ${formatPercentage(progressValue)}`;
    if (app.eta !== undefined && app.eta !== 0) {
      progressText += ` (${formatEta(app.eta * 1_000)} eta)`;
    }
    lines.push("");
    lines.push(progressText);
  }
  lines.push("");
  lines.push(pc.bold("API endpoints"));
  lines.push(`Live at http://${hostname}:${port}`);
  return lines;
};
</file>

<file path="packages/core/src/ui/index.ts">
import type { Common } from "@/internal/common.js";
import {
  getAppProgress,
  getIndexingProgress,
  getSyncProgress,
} from "@/internal/metrics.js";
import { buildUiLines, initialUiState } from "./app.js";
import { patchWriteStreams } from "./patch.js";
export function createUi({ common }: { common: Common }) {
  const ui = initialUiState;
  const { refresh, shutdown } = patchWriteStreams({
    getLines: () => buildUiLines(ui),
  });
  // Update the UI state every 100ms (independent of write rate)
  const stateUpdateInterval = setInterval(async () => {
    ui.sync = await getSyncProgress(common.metrics);
    ui.indexing = await getIndexingProgress(common.metrics);
    ui.app = await getAppProgress(common.metrics);
    if (common.options.hostname) ui.hostname = common.options.hostname;
    if (common.metrics.port) ui.port = common.metrics.port;
  }, 100);
  // Refresh the UI every 32ms
  const refreshInterval = setInterval(() => {
    refresh();
  }, 32);
  common.buildShutdown.add(() => {
    clearInterval(stateUpdateInterval);
    clearInterval(refreshInterval);
    shutdown();
  });
}
</file>

<file path="packages/core/src/ui/patch.ts">
import util from "node:util";
import ansi from "ansi-escapes";
import terminalSize from "terminal-size";
export function patchWriteStreams({ getLines }: { getLines: () => string[] }) {
  const isBun = "bun" in process.versions;
  const originalStdoutWrite = process.stdout.write;
  const originalStderrWrite = process.stderr.write;
  let isWriting = false;
  let previousLineCount = 0;
  let terminalWidth = terminalSize().columns;
  const calculateLineCount = (lines: string[]): number => {
    let count = 0;
    // For each line, calculate how many terminal lines it will occupy
    for (const line of lines) {
      // Apply wrapping logic to get actual number of lines
      const visibleLength = line.replaceAll(ansiEscapeRegex, "").length;
      count += Math.max(1, Math.ceil(visibleLength / terminalWidth));
    }
    return count;
  };
  // Clear previous UI and render new UI
  const clearAndWriteLines = (lines: string[]) => {
    if (isWriting && !lines) return;
    const wasAlreadyWriting = isWriting;
    if (!wasAlreadyWriting) isWriting = true;
    try {
      const text = [...lines, ""].join("\n"); // Include trailing newline
      // Calculate line count after constructing the text to ensure accuracy
      const newLineCount = calculateLineCount(lines);
      // Move cursor up to the start of the previous UI
      if (previousLineCount > 0) {
        originalStdoutWrite.call(
          process.stdout,
          ansi.cursorUp(previousLineCount),
        );
      }
      // Clear all lines of the previous UI and below
      originalStdoutWrite.call(process.stdout, ansi.eraseDown);
      // Write the new UI
      originalStdoutWrite.call(process.stdout, text);
      previousLineCount = newLineCount;
    } finally {
      if (!wasAlreadyWriting) isWriting = false;
    }
  };
  // Monkey patch stdout and stderr to handle out-of-band output
  const handleOutput = function (
    this: NodeJS.WriteStream,
    buffer: string | Uint8Array,
    encoding?: BufferEncoding,
    cb?: (err?: Error) => void,
  ) {
    const originalWrite =
      this === process.stderr ? originalStderrWrite : originalStdoutWrite;
    // If we're already writing, use the original to avoid recursion
    if (isWriting) {
      return originalWrite.apply(this, [buffer, encoding, cb]);
    }
    // Clear the UI
    if (previousLineCount > 0) {
      originalStdoutWrite.call(
        process.stdout,
        ansi.cursorUp(previousLineCount) + ansi.eraseDown,
      );
      previousLineCount = 0;
    }
    // Write the new content
    const result = originalWrite.apply(this, [buffer, encoding, cb]);
    // Clear and write the latest UI below the new content
    const lines = getLines();
    clearAndWriteLines(lines);
    return result;
  };
  process.stdout.write = handleOutput as typeof process.stdout.write;
  process.stderr.write = handleOutput as typeof process.stderr.write;
  // On terminal resize, reset terminal width and force a re-render
  const resizeListener = () => {
    terminalWidth = terminalSize().columns;
    // Clear the UI
    if (previousLineCount > 0) {
      originalStdoutWrite.call(
        process.stdout,
        ansi.cursorUp(previousLineCount) + ansi.eraseDown,
      );
      previousLineCount = 0;
    }
    // Clear and write the latest UI
    const lines = getLines();
    clearAndWriteLines(lines);
  };
  process.stdout.on("resize", resizeListener);
  let shutdown: () => void;
  if (isBun) {
    // bun has a custom impl of console.* methods that do not use process.stdout/process.stderr
    // so need to patch them as well
    const originalConsoleLog = console.log;
    const originalConsoleInfo = console.info;
    const originalConsoleWarn = console.warn;
    const originalConsoleError = console.error;
    const makeConsoleWriter =
      (stream: NodeJS.WriteStream, _: typeof originalStdoutWrite) =>
      (...args: unknown[]) => {
        // biome-ignore lint/style/useTemplate:
        const formatted = util.format(...args) + "\n";
        // handleOutput so TUI is cleared + re-rendered
        handleOutput.call(stream, formatted, "utf8", undefined);
      };
    console.log = makeConsoleWriter(
      process.stdout,
      originalStdoutWrite,
    ) as typeof console.log;
    console.info = makeConsoleWriter(
      process.stdout,
      originalStdoutWrite,
    ) as typeof console.info;
    console.warn = makeConsoleWriter(
      process.stderr,
      originalStderrWrite,
    ) as typeof console.warn;
    console.error = makeConsoleWriter(
      process.stderr,
      originalStderrWrite,
    ) as typeof console.error;
    shutdown = () => {
      // Restore original write methods
      process.stdout.write = originalStdoutWrite;
      process.stderr.write = originalStderrWrite;
      console.log = originalConsoleLog;
      console.info = originalConsoleInfo;
      console.warn = originalConsoleWarn;
      console.error = originalConsoleError;
      // Clear the UI
      if (previousLineCount > 0) {
        originalStdoutWrite.call(
          process.stdout,
          ansi.cursorUp(previousLineCount) + ansi.eraseDown,
        );
      }
      process.stdout.removeListener("resize", resizeListener);
    };
  } else {
    shutdown = () => {
      // Restore original write methods
      process.stdout.write = originalStdoutWrite;
      process.stderr.write = originalStderrWrite;
      // Clear the UI
      if (previousLineCount > 0) {
        originalStdoutWrite.call(
          process.stdout,
          ansi.cursorUp(previousLineCount) + ansi.eraseDown,
        );
      }
      // Remove resize listener
      process.stdout.removeListener("resize", resizeListener);
    };
  }
  return {
    refresh: () => {
      const lines = getLines();
      clearAndWriteLines(lines);
    },
    shutdown,
  };
}
// Regex to strip ANSI escape sequences from a string
const ansiEscapeRegex = new RegExp(
  [
    "[\\u001B\\u009B][[\\]()#;?]*(?:(?:(?:(?:;[-a-zA-Z\\d\\/#&.:=?%@~_]+)*|[a-zA-Z\\d]+(?:;[-a-zA-Z\\d\\/#&.:=?%@~_]*)*)?\\u0007)",
    "(?:(?:\\d{1,4}(?:;\\d{0,4})*)?[\\dA-PR-TZcf-ntqry=><~]))",
  ].join("|"),
  "g",
);
</file>

<file path="packages/core/src/utils/abi.ts">
import { getDuplicateElements } from "@/utils/duplicates.js";
import {
  type Abi,
  type AbiEvent,
  type AbiFunction,
  formatAbiItem,
} from "abitype";
import {
  type GetEventArgs,
  type Hex,
  encodeEventTopics,
  getAbiItem,
  parseAbiItem,
} from "viem";
import type { Config } from "../config/index.js";
/**
 * Fix issue with Array.isArray not checking readonly arrays
 * {@link https://github.com/microsoft/TypeScript/issues/17002}
 */
declare global {
  interface ArrayConstructor {
    isArray(arg: ReadonlyArray<any> | any): arg is ReadonlyArray<any>;
  }
}
export const toSafeName = ({
  abi,
  item,
}: { abi: Abi; item: AbiEvent | AbiFunction }) => {
  if (item.type === "event") {
    const abiEvents = abi
      .filter((item): item is AbiEvent => item.type === "event")
      .filter(
        (item) => item.anonymous === undefined || item.anonymous === false,
      );
    const overloadedEventNames = getDuplicateElements(
      abiEvents.map((item) => item.name),
    );
    if (overloadedEventNames.has(item.name)) {
      return formatAbiItem(item).split("event ")[1]!;
    }
    return item.name;
  } else {
    const abiFunctions = abi.filter(
      (item): item is AbiFunction => item.type === "function",
    );
    const overloadedFunctionNames = getDuplicateElements(
      abiFunctions.map((item) => item.name),
    );
    if (overloadedFunctionNames.has(item.name)) {
      return formatAbiItem(item).split("function ")[1]!;
    }
    return `${item.name}()`;
  }
};
export function buildTopics(
  abi: Abi,
  filter: NonNullable<Config["contracts"][string]["filter"]>,
): {
  topic0: Hex;
  topic1: Hex | Hex[] | null;
  topic2: Hex | Hex[] | null;
  topic3: Hex | Hex[] | null;
}[] {
  const filters = Array.isArray(filter) ? filter : [filter];
  const topics = filters.map((filter) => {
    // Single event with args
    const topics = encodeEventTopics({
      abi: [findAbiEvent(abi, filter.event)],
      args: filter.args as GetEventArgs<Abi, string>,
    });
    return {
      topic0: topics[0],
      topic1: topics[1] ?? null,
      topic2: topics[2] ?? null,
      topic3: topics[3] ?? null,
    };
  });
  return topics;
}
/**
 * Finds the event ABI item for the event name or event signature.
 *
 * @param eventName Event name or event signature if there are duplicates
 */
const findAbiEvent = (abi: Abi, eventName: string): AbiEvent => {
  if (eventName.includes("(")) {
    // full event signature
    return parseAbiItem(`event ${eventName}`) as AbiEvent;
  } else {
    return getAbiItem({ abi, name: eventName }) as AbiEvent;
  }
};
</file>

<file path="packages/core/src/utils/bigint.ts">
/**
 * Returns the minimum BigInt value from an array of BigInts.
 *
 * @param {bigint[]} values - An array of BigInt values.
 * @returns {bigint} The minimum BigInt value from the array.
 */
export function bigIntMin(values: bigint[]): bigint {
  if (values.length === 0) {
    throw new Error("Input array must not be empty.");
  }
  let minVal: bigint = values[0]!;
  for (let i = 1; i < values.length; i++) {
    if (values[i]! < minVal) {
      minVal = values[i]!;
    }
  }
  return minVal;
}
/**
 * Returns the maximum BigInt value from an array of BigInts.
 *
 * @param {bigint[]} values - An array of BigInt values.
 * @returns {bigint} The maximum BigInt value from the array.
 */
export function bigIntMax(values: bigint[]): bigint {
  if (values.length === 0) {
    throw new Error("Input array must not be empty.");
  }
  let maxVal: bigint = values[0]!;
  for (let i = 1; i < values.length; i++) {
    if (values[i]! > maxVal) {
      maxVal = values[i]!;
    }
  }
  return maxVal;
}
</file>

<file path="packages/core/src/utils/chains.test.ts">
import { test } from "vitest";
import { chains } from "./chains.js";
test("test", () => {
  // Will throw on error
  Object.values(chains).map((c) => c.id);
});
</file>

<file path="packages/core/src/utils/chains.ts">
import * as _chains from "viem/chains";
import { defineChain } from "viem/utils";
export const chains = _chains as unknown as Record<string, _chains.Chain>;
// Note: wanchain testnet uses the same id as hyperliquid evm
export const hyperliquidEvm = defineChain({
  id: 999,
  name: "Hyperliquid EVM",
  nativeCurrency: { name: "HYPE", symbol: "HYPE", decimals: 18 },
  rpcUrls: {
    default: {
      http: ["https://rpc.hyperliquid.xyz/evm"],
    },
  },
  contracts: {
    multicall3: {
      address: "0xca11bde05977b3631167028862be2a173976ca11",
      blockCreated: 13051,
    },
  },
});
</file>

<file path="packages/core/src/utils/checkpoint.bench.ts">
import { bench, run } from "mitata";
import { encodeCheckpoint } from "./checkpoint.js";
bench("encodeCheckpoint", () => {
  // 115.84 ns/iter
  encodeCheckpoint({
    blockTimestamp: 1,
    chainId: 1,
    blockNumber: 1,
    transactionIndex: 1,
    eventType: 1,
    eventIndex: 1,
  });
}).gc("inner");
run();
</file>

<file path="packages/core/src/utils/checkpoint.test.ts">
import { expect, test } from "vitest";
import {
  type Checkpoint,
  MAX_CHECKPOINT,
  MAX_CHECKPOINT_STRING,
  checkpointMax,
  checkpointMin,
  decodeCheckpoint,
  encodeCheckpoint,
  isCheckpointEqual,
  isCheckpointGreaterThan,
} from "./checkpoint.js";
test("encodeCheckpoint produces expected encoding", () => {
  const checkpoint = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  } satisfies Checkpoint;
  const encoded = encodeCheckpoint(checkpoint);
  const expectedEncoding =
    // biome-ignore lint: string concat is more readable than template literal here
    "1".padStart(10, "0") +
    "1".toString().padStart(16, "0") +
    "1".toString().padStart(16, "0") +
    "1".toString().padStart(16, "0") +
    "1" +
    "1".toString().padStart(16, "0");
  expect(encoded).toEqual(expectedEncoding);
});
test("decodeCheckpoint produces expected object", () => {
  const encoded =
    // biome-ignore lint: string concat is more readable than template literal here
    "1".padStart(10, "0") +
    "1".toString().padStart(16, "0") +
    "1".toString().padStart(16, "0") +
    "1".toString().padStart(16, "0") +
    "1" +
    "1".toString().padStart(16, "0");
  const decodedCheckpoint = decodeCheckpoint(encoded);
  const expectedCheckpoint = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  expect(decodedCheckpoint).toMatchObject(expectedCheckpoint);
});
test("decodeCheckpoint decodes an encoded maxCheckpoint", () => {
  const encoded = MAX_CHECKPOINT_STRING;
  const decoded = decodeCheckpoint(encoded);
  expect(decoded).toMatchObject(MAX_CHECKPOINT);
});
test("isCheckpointEqual returns true if checkpoints are the same", () => {
  const checkpointOne = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  expect(isCheckpointEqual(checkpointOne, checkpointOne)).toBe(true);
});
test("isCheckpointEqual returns false if checkpoints are different", () => {
  const checkpoint = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  const isEqual = isCheckpointEqual(checkpoint, { ...checkpoint, chainId: 2n });
  expect(isEqual).toBe(false);
});
test("isCheckpointGreaterThan compares correctly on blockTimestamp", () => {
  const checkpointOne = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  const checkpointTwo = {
    blockTimestamp: 2n,
    chainId: 2n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  const isGreater = isCheckpointGreaterThan(checkpointOne, checkpointTwo);
  expect(isGreater).toBe(false);
});
test("isCheckpointGreaterThan compares correctly on chainId", () => {
  const checkpointOne = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  const checkpointTwo = {
    blockTimestamp: 1n,
    chainId: 2n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  const isGreater = isCheckpointGreaterThan(checkpointOne, checkpointTwo);
  expect(isGreater).toBe(false);
});
test("isCheckpointGreaterThan compares correctly on transactionIndex", () => {
  const checkpointOne = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 5n,
    eventType: 1,
    eventIndex: 1n,
  };
  const checkpointTwo = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 4n,
    eventType: 1,
    eventIndex: 1n,
  };
  const isGreater = isCheckpointGreaterThan(checkpointOne, checkpointTwo);
  expect(isGreater).toBe(true);
});
test("isCheckpointGreaterThan compares correctly on eventType", () => {
  const checkpointOne = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 5,
    eventIndex: 1n,
  };
  const checkpointTwo = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 4,
    eventIndex: 1n,
  };
  const isGreater = isCheckpointGreaterThan(checkpointOne, checkpointTwo);
  expect(isGreater).toBe(true);
});
test("isCheckpointGreaterThan compares correctly on eventIndex", () => {
  const checkpointOne = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 5n,
  };
  const checkpointTwo = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 4n,
  };
  const isGreater = isCheckpointGreaterThan(checkpointOne, checkpointTwo);
  expect(isGreater).toBe(true);
});
test("isCheckpointGreaterThan compares correctly with multiple values", () => {
  const checkpointOne = {
    blockTimestamp: 6n,
    chainId: 5n,
    blockNumber: 9n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 12n,
  };
  const checkpointTwo = {
    blockTimestamp: 6n,
    chainId: 5n,
    blockNumber: 10n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 4n,
  };
  const isGreater = isCheckpointGreaterThan(checkpointOne, checkpointTwo);
  expect(isGreater).toBe(false);
});
test("isCheckpointGreaterThan returns false for equal checkpoints", () => {
  const checkpointOne = {
    blockTimestamp: 6n,
    chainId: 5n,
    blockNumber: 9n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 12n,
  };
  const isGreater = isCheckpointGreaterThan(checkpointOne, checkpointOne);
  expect(isGreater).toBe(false);
});
test("checkpointMax returns correct value if only one checkpoint", () => {
  const checkpointOne = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  const max = checkpointMax(checkpointOne);
  expect(max).toMatchObject(checkpointOne);
});
test("checkpointMax compares properly on timestamp", () => {
  const checkpointOne = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  const checkpointTwo = {
    blockTimestamp: 2n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  const max = checkpointMax(checkpointOne, checkpointTwo);
  expect(max).toMatchObject(checkpointTwo);
});
test("checkpointMin compares properly on blockNumber", () => {
  const checkpointOne = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  const checkpointTwo = {
    blockTimestamp: 2n,
    chainId: 1n,
    blockNumber: 3n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 1n,
  };
  const checkpointThree = {
    blockTimestamp: 1n,
    chainId: 1n,
    blockNumber: 1n,
    transactionIndex: 1n,
    eventType: 1,
    eventIndex: 99n,
  };
  const max = checkpointMin(checkpointOne, checkpointTwo, checkpointThree);
  expect(max).toMatchObject(checkpointOne);
});
</file>

<file path="packages/core/src/utils/checkpoint.ts">
import type { LightBlock, SyncBlock } from "@/internal/types.js";
import { hexToBigInt } from "viem";
export type Checkpoint = {
  blockTimestamp: bigint;
  chainId: bigint;
  blockNumber: bigint;
  transactionIndex: bigint;
  eventType: number;
  eventIndex: bigint;
};
// 10 digits for unix timestamp gets us to the year 2277.
const BLOCK_TIMESTAMP_DIGITS = 10;
// Chain IDs are uint256. As of writing the largest Chain ID on https://chainlist.org
// is 13 digits. 16 digits should be enough (JavaScript's max safe integer).
const CHAIN_ID_DIGITS = 16;
// Same logic as chain ID.
const BLOCK_NUMBER_DIGITS = 16;
// Same logic as chain ID.
const TRANSACTION_INDEX_DIGITS = 16;
// At time of writing, we only have 2 event types planned, so one digit (10 types) is enough.
const EVENT_TYPE_DIGITS = 1;
// This could contain log index, trace index, etc. 16 digits should be enough.
const EVENT_INDEX_DIGITS = 16;
const CHECKPOINT_LENGTH =
  BLOCK_TIMESTAMP_DIGITS +
  CHAIN_ID_DIGITS +
  BLOCK_NUMBER_DIGITS +
  TRANSACTION_INDEX_DIGITS +
  EVENT_TYPE_DIGITS +
  EVENT_INDEX_DIGITS;
export const EVENT_TYPES = {
  transactions: 2,
  blocks: 5,
  logs: 5,
  traces: 7,
} as const;
const encodedChainIds = new Map<number | bigint, string>();
export const encodeCheckpoint = (
  checkpoint: Checkpoint | { [K in keyof Checkpoint]: number | bigint },
) => {
  const {
    blockTimestamp,
    chainId,
    blockNumber,
    transactionIndex,
    eventType,
    eventIndex,
  } = checkpoint;
  if (eventType < 0 || eventType > 9) {
    throw new Error(
      `Got invalid event type ${eventType}, expected a number from 0 to 9`,
    );
  }
  let encodedChainId = encodedChainIds.get(chainId);
  if (encodedChainId === undefined) {
    encodedChainId = chainId.toString().padStart(CHAIN_ID_DIGITS, "0");
    encodedChainIds.set(chainId, encodedChainId);
  }
  const result = `${blockTimestamp.toString().padStart(BLOCK_TIMESTAMP_DIGITS, "0")}${encodedChainId}${blockNumber.toString().padStart(BLOCK_NUMBER_DIGITS, "0")}${transactionIndex.toString().padStart(TRANSACTION_INDEX_DIGITS, "0")}${eventType.toString()}${eventIndex.toString().padStart(EVENT_INDEX_DIGITS, "0")}`;
  if (result.length !== CHECKPOINT_LENGTH) {
    throw new Error(`Invalid stringified checkpoint: ${result}`);
  }
  return result;
};
export const decodeCheckpoint = (checkpoint: string): Checkpoint => {
  let offset = 0;
  const blockTimestamp = BigInt(
    checkpoint.slice(offset, offset + BLOCK_TIMESTAMP_DIGITS),
  );
  offset += BLOCK_TIMESTAMP_DIGITS;
  const chainId = BigInt(checkpoint.slice(offset, offset + CHAIN_ID_DIGITS));
  offset += CHAIN_ID_DIGITS;
  const blockNumber = BigInt(
    checkpoint.slice(offset, offset + BLOCK_NUMBER_DIGITS),
  );
  offset += BLOCK_NUMBER_DIGITS;
  const transactionIndex = BigInt(
    checkpoint.slice(offset, offset + TRANSACTION_INDEX_DIGITS),
  );
  offset += TRANSACTION_INDEX_DIGITS;
  const eventType = +checkpoint.slice(offset, offset + EVENT_TYPE_DIGITS);
  offset += EVENT_TYPE_DIGITS;
  const eventIndex = BigInt(
    checkpoint.slice(offset, offset + EVENT_INDEX_DIGITS),
  );
  offset += EVENT_INDEX_DIGITS;
  return {
    blockTimestamp,
    chainId,
    blockNumber,
    transactionIndex,
    eventType,
    eventIndex,
  };
};
export const ZERO_CHECKPOINT: Checkpoint = {
  blockTimestamp: 0n,
  chainId: 0n,
  blockNumber: 0n,
  transactionIndex: 0n,
  eventType: 0,
  eventIndex: 0n,
};
export const MAX_CHECKPOINT: Checkpoint = {
  blockTimestamp: 99999_99999n,
  chainId: 9999_9999_9999_9999n,
  blockNumber: 9999_9999_9999_9999n,
  transactionIndex: 9999_9999_9999_9999n,
  eventType: 9,
  eventIndex: 9999_9999_9999_9999n,
};
export const ZERO_CHECKPOINT_STRING = encodeCheckpoint(ZERO_CHECKPOINT);
export const MAX_CHECKPOINT_STRING = encodeCheckpoint(MAX_CHECKPOINT);
/**
/**
 * Returns true if two checkpoints are equal.
 */
export const isCheckpointEqual = (a: Checkpoint, b: Checkpoint) =>
  encodeCheckpoint(a) === encodeCheckpoint(b);
/**
 * Returns true if checkpoint a is greater than checkpoint b.
 * Returns false if the checkpoints are equal.
 */
export const isCheckpointGreaterThan = (a: Checkpoint, b: Checkpoint) =>
  encodeCheckpoint(a) > encodeCheckpoint(b);
/**
 * Returns true if checkpoint a is greater than or equal to checkpoint b.|
 */
export const isCheckpointGreaterThanOrEqualTo = (
  a: Checkpoint,
  b: Checkpoint,
) => encodeCheckpoint(a) >= encodeCheckpoint(b);
export const checkpointMax = (...checkpoints: Checkpoint[]) =>
  checkpoints.reduce((max, checkpoint) => {
    return isCheckpointGreaterThan(checkpoint, max) ? checkpoint : max;
  });
export const checkpointMin = (...checkpoints: Checkpoint[]) =>
  checkpoints.reduce((min, checkpoint) => {
    return isCheckpointGreaterThan(min, checkpoint) ? checkpoint : min;
  });
export const LATEST = MAX_CHECKPOINT_STRING;
/** Compute the minimum checkpoint, filtering out undefined. */
export const min = (...checkpoints: (string | undefined)[]) => {
  return checkpoints.reduce((acc, cur) => {
    if (cur === undefined) return acc;
    if (acc === undefined) return cur;
    if (acc < cur) return acc;
    return cur;
  })!;
};
/** Compute the maximum checkpoint, filtering out undefined. */
export const max = (...checkpoints: (string | undefined)[]) => {
  return checkpoints.reduce((acc, cur) => {
    if (cur === undefined) return acc;
    if (acc === undefined) return cur;
    if (acc > cur) return acc;
    return cur;
  });
};
/** Convert `block` to a `Checkpoint`. */
export const blockToCheckpoint = (
  block: LightBlock | SyncBlock,
  chainId: number,
  rounding: "up" | "down",
): Checkpoint => {
  return {
    ...(rounding === "up" ? MAX_CHECKPOINT : ZERO_CHECKPOINT),
    blockTimestamp: hexToBigInt(block.timestamp),
    chainId: BigInt(chainId),
    blockNumber: hexToBigInt(block.number),
  };
};
</file>

<file path="packages/core/src/utils/chunk.test.ts">
import { expect, test } from "vitest";
import { chunk } from "./chunk.js";
test("chunk", () => {
  let result = chunk([1, 2, 3, 4, 5, 6, 7, 8, 9], 3);
  expect(result).toStrictEqual([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9],
  ]);
  result = chunk([], 3);
  expect(result).toStrictEqual([]);
});
</file>

<file path="packages/core/src/utils/chunk.ts">
export const chunk = <T>(array: T[], size: number): T[][] => {
  const chunks = [];
  for (let i = 0; i < array.length; i += size) {
    chunks.push(array.slice(i, i + size));
  }
  return chunks;
};
</file>

<file path="packages/core/src/utils/copy.bench.ts">
import { bench, group, run } from "mitata";
import { copy } from "./copy.js";
const obj = { a: 1, b: 2n, c: { d: 3 }, e: "hello" };
group("copy", () => {
  // 48.55 ns/iter
  bench("copy", () => {
    copy({ a: 1, b: 2 });
  }).gc("inner");
  // 540.76 ns/iter
  bench("structuredClone", () => {
    structuredClone(obj);
  }).gc("inner");
});
run();
</file>

<file path="packages/core/src/utils/copy.test.ts">
import { toBytes, zeroAddress } from "viem";
import { expect, test } from "vitest";
import { copy, copyOnWrite } from "./copy.js";
test("copyOnWrite", () => {
  const obj = { a: 1, b: 2 };
  const copiedObj = copyOnWrite(obj);
  expect(obj.a).toBe(1);
  expect(obj.b).toBe(2);
  expect(copiedObj.a).toBe(1);
  expect(copiedObj.b).toBe(2);
  copiedObj.a = 3;
  expect(obj.a).toBe(1);
  expect(obj.b).toBe(2);
  expect(copiedObj.a).toBe(3);
  expect(copiedObj.b).toBe(2);
  // @ts-expect-error
  copiedObj.c = 10;
  // @ts-expect-error
  expect(obj.c).toBeUndefined();
  // @ts-expect-error
  expect(copiedObj.c).toBe(10);
});
test("copyOnWrite nested", () => {
  const obj = { a: { c: 1 }, b: 2 };
  const copiedObj = copyOnWrite(obj);
  expect(obj.a.c).toBe(1);
  expect(obj.b).toBe(2);
  expect(copiedObj.a.c).toBe(1);
  expect(copiedObj.b).toBe(2);
  copiedObj.a.c = 2;
  expect(obj.a.c).toBe(1);
  expect(copiedObj.a.c).toBe(2);
});
test("copyOnWrite nested array", () => {
  const obj = { a: [] as number[] };
  const copiedObj = copyOnWrite(obj);
  copiedObj.a.push(1);
  expect(obj.a).toEqual([]);
  expect(copiedObj.a).toEqual([1]);
});
test("copy", () => {
  const obj = { a: 1, b: 2 };
  const copiedObj = copyOnWrite(obj);
  const copiedObj2 = copy(copiedObj);
  expect(copiedObj.a).toBe(1);
  expect(copiedObj.b).toBe(2);
  expect(copiedObj2.a).toBe(1);
  expect(copiedObj2.b).toBe(2);
  copiedObj.a = 3;
  expect(obj.a).toBe(1);
  expect(obj.b).toBe(2);
  expect(copiedObj.a).toBe(3);
  expect(copiedObj.b).toBe(2);
  copy([copiedObj]);
});
test("copy bytes", () => {
  const obj = {
    address: zeroAddress,
    calldata: toBytes(zeroAddress),
  };
  const copiedObj = copyOnWrite(obj);
  const copiedObj2 = copy(copiedObj);
  expect(copiedObj.calldata).toMatchInlineSnapshot(`
    Uint8Array [
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
    ]
  `);
  expect(copiedObj2.calldata).toMatchInlineSnapshot(`
    Uint8Array [
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
    ]
  `);
  expect(copiedObj.calldata).toBeInstanceOf(Uint8Array);
  expect(copiedObj2.calldata).toBeInstanceOf(Uint8Array);
});
test("copy timestamp", () => {
  const obj = {
    address: zeroAddress,
    timestamp: new Date(1742925862000),
  };
  const copiedObj = copy(obj);
  expect(copiedObj.timestamp).toBeInstanceOf(Date);
});
</file>

<file path="packages/core/src/utils/copy.ts">
import util from "node:util";
/**
 * Symbol used to mark objects that are copied on write.
 */
export const COPY_ON_WRITE = Symbol.for("ponder:copyOnWrite");
/**
 * Create a copy-on-write proxy for an object.
 */
export const copyOnWrite = <T extends object>(obj: T): T => {
  let copiedObject: T | undefined;
  // @ts-ignore
  obj[util.inspect.custom] = () => {
    return copiedObject ?? obj;
  };
  return new Proxy<T>(obj, {
    get(target, prop, receiver) {
      if (prop === COPY_ON_WRITE) {
        return target;
      }
      let result = Reflect.get(copiedObject ?? target, prop, receiver);
      if (
        typeof result === "object" &&
        result !== null &&
        copiedObject === undefined
      ) {
        copiedObject = structuredClone(target);
        result = Reflect.get(copiedObject, prop, receiver);
      }
      return result;
    },
    set(target, prop, newValue, receiver) {
      if (copiedObject === undefined) {
        copiedObject = structuredClone(target);
      }
      return Reflect.set(copiedObject!, prop, newValue, receiver);
    },
    deleteProperty(target, prop) {
      if (copiedObject === undefined) {
        copiedObject = structuredClone(target);
      }
      return Reflect.deleteProperty(copiedObject!, prop);
    },
    defineProperty(target, prop, descriptor) {
      if (copiedObject === undefined) {
        copiedObject = structuredClone(target);
      }
      return Reflect.defineProperty(copiedObject!, prop, descriptor);
    },
    ownKeys(target) {
      return Reflect.ownKeys(copiedObject ?? target);
    },
    has(target, prop) {
      return Reflect.has(copiedObject ?? target, prop);
    },
    getOwnPropertyDescriptor(target, prop) {
      return Reflect.getOwnPropertyDescriptor(copiedObject ?? target, prop);
    },
  });
};
/**
 * Create a deep copy of an object.
 *
 * @dev This function supports copying objects that
 * have been created with `copyOnWrite`.
 */
export const copy = <T>(obj: T): T => {
  if (obj === null || typeof obj !== "object") {
    return obj;
  }
  const hasProxy = (obj: any): boolean => {
    if (obj === null || typeof obj !== "object") {
      return false;
    }
    if (obj[COPY_ON_WRITE] !== undefined) {
      return true;
    }
    if (Array.isArray(obj)) {
      return obj.some((element) => hasProxy(element));
    }
    for (const value of Object.values(obj)) {
      if (hasProxy(value)) {
        return true;
      }
    }
    return false;
  };
  const isDeeplyNested = (obj: any, depth = 0): boolean => {
    if (obj === null || typeof obj !== "object") {
      return false;
    }
    if (depth > 0) {
      return true;
    }
    if (Array.isArray(obj)) {
      return obj.some((element) => isDeeplyNested(element, depth + 1));
    }
    for (const value of Object.values(obj)) {
      if (isDeeplyNested(value, depth + 1)) {
        return true;
      }
    }
    return false;
  };
  // @ts-expect-error
  const proxy = obj[COPY_ON_WRITE];
  if (proxy === undefined) {
    if (Array.isArray(obj)) {
      if (hasProxy(obj)) {
        // @ts-expect-error
        return obj.map((element) => copy(element));
      }
      if (isDeeplyNested(obj)) return structuredClone(obj);
      return [...obj] as T;
    }
    if (hasProxy(obj)) {
      const result = {} as T;
      for (const [key, value] of Object.entries(obj)) {
        // @ts-expect-error
        result[key] = copy(value);
      }
      return result;
    }
    // Note: spread operator is significantly faster than `structuredClone`
    if (isDeeplyNested(obj)) return structuredClone(obj);
    return { ...obj };
  }
  return proxy;
};
</file>

<file path="packages/core/src/utils/date.ts">
/** Formats a unix timestamp into a short date format (e.g. "Jan 1, 2021")
 *
 * @param unixTimestamp Unix timestamp to format
 * @returns Formatted date string
 */
export function formatShortDate(unixTimestamp: number) {
  const date = new Date(unixTimestamp * 1000);
  const year = date.getFullYear();
  const months = [
    "Jan",
    "Feb",
    "Mar",
    "Apr",
    "May",
    "Jun",
    "Jul",
    "Aug",
    "Sep",
    "Oct",
    "Nov",
    "Dec",
  ];
  const month = months[date.getMonth()];
  const day = date.getDate();
  return `${month} ${day}, ${year}`;
}
</file>

<file path="packages/core/src/utils/debug.ts">
import type { Address, Hash, Hex, LogTopic } from "viem";
/** @see https://github.com/alloy-rs/alloy/blob/main/crates/rpc-types-trace/src/geth/call.rs */
/** @see https://github.com/alloy-rs/alloy/blob/main/crates/rpc-types-trace/src/common.rs */
/** @see https://github.com/paradigmxyz/reth/blob/main/crates/rpc/rpc/src/debug.rs */
/** Result type for geth style transaction trace. */
export type Trace = {
  /** Transaction hash. */
  txHash: Hex;
  /** Trace results produced by the tracer.  */
  result: CallFrame;
};
/**
 * The response object for `debug_traceBlockByNumber` and `debug_traceBlockByHash`
 * with `"tracer": "callTracer"`.
 */
type CallFrame = {
  /** The type of the call. */
  type:
    | "CALL"
    | "CALLCODE"
    | "DELEGATECALL"
    | "STATICCALL"
    | "CREATE"
    | "CREATE2"
    | "SELFDESTRUCT";
  /** The address of that initiated the call. */
  from: Address;
  /** The address of the contract that was called. */
  to?: Address;
  /** How much gas was left before the call. */
  gas: Hex;
  /** How much gas was used by the call. */
  gasUsed: Hex;
  /** Calldata input. */
  input: Hex;
  /** Output of the call, if any. */
  output?: Hex;
  /** Error message, if any. */
  error?: string;
  /** Why this call reverted, if it reverted. */
  revertReason?: string;
  /** Recorded child calls. */
  calls?: CallFrame[];
  /** Logs emitted by this call. */
  logs?: CallLogFrame[];
  /** Value transferred. */
  value?: Hex;
};
/** Represents a recorded log that is emitted during a trace call. */
type CallLogFrame = {
  /** The address of the contract that was called. */
  address: Address;
  /** The topics of the log. */
  topics: LogTopic[];
  /** The data of the log. */
  data: Hex;
  /** The position of the log relative to subcalls within the same trace. */
  position: number;
};
/** The configuration for the call tracer. */
type CallConfig = {
  /** When set to true, this will only trace the primary (top-level) call and not any sub-calls. */
  onlyTopCall?: boolean;
  /** When set to true, this will include the logs emitted by the call. */
  withLog?: boolean;
};
export type DebugRpcSchema = [
  /**
   * @description Returns tracing results by executing all transactions in the block specified by the block hash
   *
   * @example
   * provider.request({ method: 'debug_traceBlockByHash', params: ['0x...', { tracer: "callTracer" }] })
   * // => {
   * //   txHash: '0x5a42...',
   * //   result: [...],
   * // }
   */
  {
    Method: "debug_traceBlockByHash";
    Parameters: [
      hash: Hash,
      tracingOptions: { tracer: "callTracer"; tracerConfig?: CallConfig },
    ];
    ReturnType: Trace[];
  },
  /**
   * @description Returns tracing results by executing all transactions in the block specified by the block hash
   *
   * @example
   * provider.request({ method: 'debug_traceBlockByNumber', params: ['0x1b4', { tracer: "callTracer" }] })
   * // => {
   * //   txHash: '0x5a42...',
   * //   result: [...],
   * // }
   */
  {
    Method: "debug_traceBlockByNumber";
    Parameters: [
      block: Hex,
      tracingOptions: { tracer: "callTracer"; tracerConfig?: CallConfig },
    ];
    ReturnType: Trace[];
  },
];
</file>

<file path="packages/core/src/utils/decodeAbiParameters.bench.ts">
import { bench, group, run, summary } from "mitata";
import {
  type DecodeAbiParametersReturnType,
  decodeAbiParameters as decodeAbiParametersViem,
  parseAbiParameters,
} from "viem";
import {
  decodeAbiParameter,
  decodeAbiParameters,
} from "./decodeAbiParameters.js";
// TODO(kyle) use generated values to disable jit
group("decodeAbiParameters [uint]", () => {
  summary(() => {
    const abi = [{ type: "uint" }] as const;
    const data =
      "0x0000000000000000000000000000000000000000000000000000000000010f2c";
    bench("viem", () => {
      decodeAbiParametersViem(abi, data);
    }).gc("inner");
    // 66.38 ns/iter
    bench("ponder", () => {
      decodeAbiParameter(abi[0], data);
    }).gc("inner");
  });
});
group("decodeAbiParameters large static", () => {
  summary(() => {
    const abi = parseAbiParameters(
      "((uint256 x,bool y,address z) foo,(uint256 x,bool y,address z) baz,uint8[2] x)",
    );
    const data =
      "0x00000000000000000000000000000000000000000000000000000000000001a40000000000000000000000000000000000000000000000000000000000000001000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac00000000000000000000000000000000000000000000000000000000000000450000000000000000000000000000000000000000000000000000000000000000000000000000000000000000c961145a54c96e3ae9baa048c4f4d6b04c13916b00000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000002";
    const out = [] as unknown as DecodeAbiParametersReturnType<typeof abi>;
    bench("viem", () => {
      decodeAbiParametersViem(abi, data);
    }).gc("inner");
    // 1.59 s/iter
    bench("ponder", () => {
      decodeAbiParameters(abi, data, { out });
      // @ts-expect-error
      out.length = 0;
    }).gc("inner");
  });
});
group("decodeAbiParameters large dynamic", () => {
  summary(() => {
    const abi = parseAbiParameters(
      "((uint256[] x,bool y,string[] z) foo,uint256 a,string[] b)",
    );
    const data =
      "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000001a4000000000000000000000000000000000000000000000000000000000000024000000000000000000000000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000000568656c6c6f0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005776f726c6400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000057761676d6900000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000047669656d00000000000000000000000000000000000000000000000000000000";
    const out = [] as unknown as DecodeAbiParametersReturnType<typeof abi>;
    bench("viem", () => {
      decodeAbiParametersViem(abi, data);
    }).gc("inner");
    // 4.17 s/iter
    bench("ponder", () => {
      decodeAbiParameters(abi, data, { out });
      // @ts-expect-error
      out.length = 0;
    }).gc("inner");
  });
});
run();
</file>

<file path="packages/core/src/utils/decodeAbiParameters.test.ts">
import {
  type Address,
  type Hex,
  encodeAbiParameters,
  getAbiItem,
  parseAbiParameters,
  zeroAddress,
} from "viem";
import { describe, expect, expectTypeOf, test } from "vitest";
import { decodeAbiParameters } from "./decodeAbiParameters.js";
import { toLowerCase } from "./lowercase.js";
/** Imported from Viem: https://github.com/wevm/viem/blob/38525bf1d55ec....test.ts */
describe("static", () => {
  test("blank", () => {
    const result = decodeAbiParameters([], "0x");
    expectTypeOf<readonly []>(result);
    expect(result).toEqual([]);
  });
  test("uint", () => {
    const result = decodeAbiParameters(
      [{ type: "uint" }],
      "0x0000000000000000000000000000000000000000000000000000000000010f2c",
    );
    expectTypeOf<readonly [bigint]>(result);
    expect(result).toEqual([69420n]);
  });
  test("uint8", () => {
    const result = decodeAbiParameters(
      parseAbiParameters("uint8"),
      "0x0000000000000000000000000000000000000000000000000000000000000020",
    );
    expectTypeOf<readonly [number]>(result);
    expect(result).toEqual([32]);
  });
  test("uint32", () => {
    const result = decodeAbiParameters(
      parseAbiParameters("uint32"),
      "0x0000000000000000000000000000000000000000000000000000000000010f2c",
    );
    expectTypeOf<readonly [number]>(result);
    expect(result).toEqual([69420]);
  });
  describe("int", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("int"),
        "0x0000000000000000000000000000000000000000000000000000000000010f2c",
      );
      expectTypeOf<readonly [bigint]>(result);
      expect(result).toEqual([69420n]);
    });
    test("negative (twos compliment)", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("int"),
        "0xfffffffffffffffffffffffffffffffffffffffffffffffffffffffffffef0d4",
      );
      expectTypeOf<readonly [bigint]>(result);
      expect(result).toEqual([-69420n]);
    });
  });
  describe("int8", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("int8"),
        "0x000000000000000000000000000000000000000000000000000000000000007f",
      );
      expectTypeOf<readonly [number]>(result);
      expect(result).toEqual([127]);
    });
    test("negative (twos compliment)", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("int8"),
        "0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff80",
      );
      expectTypeOf<readonly [number]>(result);
      expect(result).toEqual([-128]);
    });
  });
  describe("int32", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("int32"),
        "0x000000000000000000000000000000000000000000000000000000007fffffff",
      );
      expectTypeOf<readonly [number]>(result);
      expect(result).toEqual([2147483647]);
    });
    test("negative (twos compliment)", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("int32"),
        "0xffffffffffffffffffffffffffffffffffffffffffffffffffffffff80000000",
      );
      expectTypeOf<readonly [number]>(result);
      expect(result).toEqual([-2147483648]);
    });
  });
  describe("address", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("address"),
        "0x00000000000000000000000014dc79964da2c08b23698b3d3cc7ca32193d9955",
        { formatAddress: toLowerCase },
      );
      expectTypeOf<readonly [Address]>(result);
      expect(
        decodeAbiParameters(
          parseAbiParameters("address"),
          "0x00000000000000000000000014dc79964da2c08b23698b3d3cc7ca32193d9955",
          { formatAddress: toLowerCase },
        ),
      ).toMatchInlineSnapshot(`
        [
          "0x14dc79964da2c08b23698b3d3cc7ca32193d9955",
        ]
      `);
    });
  });
  describe("bytes8", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("bytes8"),
        "0x0123456789abcdef000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [Hex]>(result);
      expect(result).toEqual(["0x0123456789abcdef"]);
    });
  });
  describe("bytes16", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("bytes16"),
        "0x0123456789abcdef0123456789abcdef00000000000000000000000000000000",
      );
      expectTypeOf<readonly [Hex]>(result);
      expect(result).toEqual(["0x0123456789abcdef0123456789abcdef"]);
    });
  });
  describe("uint[3]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[3]"),
        "0x0000000000000000000000000000000000000000000000000000000000010f2c000000000000000000000000000000000000000000000000000000000000a45500000000000000000000000000000000000000000000000000000000190f1b44",
      );
      expectTypeOf<readonly [readonly [bigint, bigint, bigint]]>(result);
      expect(result).toEqual([[69420n, 42069n, 420420420n]]);
    });
  });
  describe("int[3]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("int[3]"),
        "0x0000000000000000000000000000000000000000000000000000000000010f2cffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff5bab00000000000000000000000000000000000000000000000000000000190f1b44",
      );
      expectTypeOf<readonly [readonly [bigint, bigint, bigint]]>(result);
      expect(result).toEqual([[69420n, -42069n, 420420420n]]);
    });
  });
  describe("address[2]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("address[2]"),
        "0x000000000000000000000000c961145a54c96e3ae9baa048c4f4d6b04c13916b000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
        { formatAddress: toLowerCase },
      );
      expectTypeOf<readonly [readonly [Address, Address]]>(result);
      expect(result).toMatchInlineSnapshot(`
        [
          [
            "0xc961145a54c96e3ae9baa048c4f4d6b04c13916b",
            "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
          ],
        ]
      `);
    });
  });
  describe("bool[2]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("bool[2]"),
        "0x00000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [readonly [boolean, boolean]]>(result);
      expect(result).toEqual([[true, false]]);
    });
  });
  describe("uint[3][2]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[3][2]"),
        "0x0000000000000000000000000000000000000000000000000000000000010f2c000000000000000000000000000000000000000000000000000000000000a45500000000000000000000000000000000000000000000000000000000190f1b4400000000000000000000000000000000000000000000000000000000000001a4000000000000000000000000000000000000000000000000000000000000002c00000000000000000000000000000000000000000000000000000000000001a6",
      );
      expectTypeOf<
        readonly [
          readonly [
            readonly [bigint, bigint, bigint],
            readonly [bigint, bigint, bigint],
          ],
        ]
      >(result);
      expect(result).toEqual([
        [
          [69420n, 42069n, 420420420n],
          [420n, 44n, 422n],
        ],
      ]);
    });
  });
  describe("uint[3][2][4]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint[3][2][4]"),
        "0x000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000050000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000700000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000009000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000000b000000000000000000000000000000000000000000000000000000000000000c000000000000000000000000000000000000000000000000000000000000000d000000000000000000000000000000000000000000000000000000000000000e000000000000000000000000000000000000000000000000000000000000000f000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000110000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000001300000000000000000000000000000000000000000000000000000000000000140000000000000000000000000000000000000000000000000000000000000015000000000000000000000000000000000000000000000000000000000000001600000000000000000000000000000000000000000000000000000000000000170000000000000000000000000000000000000000000000000000000000000018",
      );
      expectTypeOf<
        readonly [
          readonly [
            readonly [
              readonly [bigint, bigint, bigint],
              readonly [bigint, bigint, bigint],
            ],
            readonly [
              readonly [bigint, bigint, bigint],
              readonly [bigint, bigint, bigint],
            ],
            readonly [
              readonly [bigint, bigint, bigint],
              readonly [bigint, bigint, bigint],
            ],
            readonly [
              readonly [bigint, bigint, bigint],
              readonly [bigint, bigint, bigint],
            ],
          ],
        ]
      >(result);
      expect(result).toEqual([
        [
          [
            [1n, 2n, 3n],
            [4n, 5n, 6n],
          ],
          [
            [7n, 8n, 9n],
            [10n, 11n, 12n],
          ],
          [
            [13n, 14n, 15n],
            [16n, 17n, 18n],
          ],
          [
            [19n, 20n, 21n],
            [22n, 23n, 24n],
          ],
        ],
      ]);
    });
  });
  describe("struct: (uint256,bool,address)", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("(uint256 x, bool y, address z)"),
        "0x00000000000000000000000000000000000000000000000000000000000001a40000000000000000000000000000000000000000000000000000000000000001000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
        { formatAddress: toLowerCase },
      );
      expectTypeOf<
        readonly [
          {
            x: bigint;
            y: boolean;
            z: Hex;
          },
        ]
      >(result);
      expect(result).toMatchInlineSnapshot(`
        [
          {
            "x": 420n,
            "y": true,
            "z": "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
          },
        ]
      `);
    });
  });
  describe("struct: (uint256,bool,address)", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("(uint256, bool, address)"),
        "0x00000000000000000000000000000000000000000000000000000000000001a40000000000000000000000000000000000000000000000000000000000000001000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
        { formatAddress: toLowerCase },
      );
      expectTypeOf<readonly [readonly [bigint, boolean, Hex]]>(result);
      expect(result).toMatchInlineSnapshot(`
        [
          [
            420n,
            true,
            "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
          ],
        ]
      `);
    });
  });
  describe("struct: (uint256,bool,address)", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("(uint256 x, bool, address z)"),
        "0x00000000000000000000000000000000000000000000000000000000000001a40000000000000000000000000000000000000000000000000000000000000001000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
        { formatAddress: toLowerCase },
      );
      expectTypeOf<readonly [readonly [bigint, boolean, Hex]]>(result);
      expect(result).toMatchInlineSnapshot(`
        [
          [
            420n,
            true,
            "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
          ],
        ]
      `);
    });
  });
  describe("struct: ()", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        [{ type: "tuple", components: [] }],
        "0x00000000000000000000000000000000000000000000000000000000000001a40000000000000000000000000000000000000000000000000000000000000001000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
      );
      expectTypeOf<readonly [readonly []]>(result);
      expect(result).toEqual([[]]);
    });
  });
  describe("struct: ((uint256,bool,address),(uint256,bool,address),uint8[2])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters(
          "((uint256 x,bool y,address z) foo,(uint256 x,bool y,address z) baz,uint8[2] x)",
        ),
        "0x00000000000000000000000000000000000000000000000000000000000001a40000000000000000000000000000000000000000000000000000000000000001000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac00000000000000000000000000000000000000000000000000000000000000450000000000000000000000000000000000000000000000000000000000000000000000000000000000000000c961145a54c96e3ae9baa048c4f4d6b04c13916b00000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000002",
        { formatAddress: toLowerCase },
      );
      expectTypeOf<
        readonly [
          {
            foo: {
              x: bigint;
              y: boolean;
              z: Hex;
            };
            baz: {
              x: bigint;
              y: boolean;
              z: Hex;
            };
            x: readonly [number, number];
          },
        ]
      >(result);
      expect(result).toMatchInlineSnapshot(`
        [
          {
            "baz": {
              "x": 69n,
              "y": false,
              "z": "0xc961145a54c96e3ae9baa048c4f4d6b04c13916b",
            },
            "foo": {
              "x": 420n,
              "y": true,
              "z": "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
            },
            "x": [
              1,
              2,
            ],
          },
        ]
      `);
    });
  });
  describe("uint256[2],bool,string[3]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[2],bool,string[3]"),
        "0x00000000000000000000000000000000000000000000000000000000000001a4000000000000000000000000000000000000000000000000000000000000004500000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000000e000000000000000000000000000000000000000000000000000000000000000057761676d6900000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000047669656d0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000036c6f6c0000000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<
        readonly [
          readonly [bigint, bigint],
          boolean,
          readonly [string, string, string],
        ]
      >(result);
      expect(result).toEqual([[420n, 69n], true, ["wagmi", "viem", "lol"]]);
    });
  });
  describe("uint256[2],bool,string[3]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[2],bool,string[3]"),
        "0x00000000000000000000000000000000000000000000000000000000000001a4000000000000000000000000000000000000000000000000000000000000004500000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000000e000000000000000000000000000000000000000000000000000000000000000057761676d6900000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000047669656d0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000036c6f6c0000000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<
        readonly [
          readonly [bigint, bigint],
          boolean,
          readonly [string, string, string],
        ]
      >(result);
      expect(result).toEqual([[420n, 69n], true, ["wagmi", "viem", "lol"]]);
    });
  });
  describe("uint,bool,address", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint,bool,address"),
        "0x00000000000000000000000000000000000000000000000000000000000001a40000000000000000000000000000000000000000000000000000000000000001000000000000000000000000c961145a54c96e3ae9baa048c4f4d6b04c13916b",
        { formatAddress: toLowerCase },
      );
      expectTypeOf<readonly [bigint, boolean, string]>(result);
      expect(result).toMatchInlineSnapshot(`
        [
          420n,
          true,
          "0xc961145a54c96e3ae9baa048c4f4d6b04c13916b",
        ]
      `);
    });
  });
});
describe("dynamic", () => {
  describe("string", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("string"),
        "0x000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000057761676d69000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [string]>(result);
      expect(result).toEqual(["wagmi"]);
    });
    test("empty", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("string"),
        "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [string]>(result);
      expect(result).toEqual([""]);
    });
    test("default", () => {
      expect(
        decodeAbiParameters(
          parseAbiParameters("string"),
          "0x000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000002da4c6f72656d20697073756d20646f6c6f722073697420616d65742c20636f6e73656374657475722061646970697363696e6720656c69742e204e756e63206661756369627573206c6f72656d2061206c696265726f20617563746f7220636f6e64696d656e74756d2e20446f6e6563206f726e617265206d617373612072686f6e637573206c616375732072757472756d2c20656765742070756c76696e6172206172637520656c656d656e74756d2e204e756e63206d6175726973206c6f72656d2c20736f64616c65732065676574207669766572726120696e2c20657569736d6f642071756973206d692e205072616573656e74206e656320636f6d6d6f646f206c656f2e2050686173656c6c757320636f6e64696d656e74756d206d61757269732073656420616363756d73616e20656c656966656e642e205072616573656e7420616320626c616e6469742073656d2c2065742072757472756d20697073756d2e20457469616d20696e2074656c6c757320616320656e696d20666163696c6973697320756c7472696365732e20467573636520616320766573746962756c756d207175616d2e204475697320736564207075727573207363656c657269737175652c20736f6c6c696369747564696e20657261742061632c2070756c76696e6172206e6973692e2050656c6c656e746573717565206575207075727573206e65632073617069656e207665686963756c6120636f6e76616c6c69732075742076656c20656c69742e2053757370656e6469737365206567657420657820766974616520656e696d20766f6c7574706174207363656c657269737175652e20536564207175697320656c6974207472697374697175652065726174206c756374757320656765737461732061206163206f64696f2e2044756973207665686963756c6120656e696d206163206d6574757320677261766964612c2076656c206d6178696d7573206e69736920696d706572646965742e000000000000",
        ),
      ).toMatchInlineSnapshot(
        `
        [
          "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc faucibus lorem a libero auctor condimentum. Donec ornare massa rhoncus lacus rutrum, eget pulvinar arcu elementum. Nunc mauris lorem, sodales eget viverra in, euismod quis mi. Praesent nec commodo leo. Phasellus condimentum mauris sed accumsan eleifend. Praesent ac blandit sem, et rutrum ipsum. Etiam in tellus ac enim facilisis ultrices. Fusce ac vestibulum quam. Duis sed purus scelerisque, sollicitudin erat ac, pulvinar nisi. Pellentesque eu purus nec sapien vehicula convallis ut vel elit. Suspendisse eget ex vitae enim volutpat scelerisque. Sed quis elit tristique erat luctus egestas a ac odio. Duis vehicula enim ac metus gravida, vel maximus nisi imperdiet.",
        ]
      `,
      );
    });
    test("emojis", () => {
      expect(
        decodeAbiParameters(
          parseAbiParameters("string"),
          "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000026f09f91a8e2808df09f91a8e2808df09f91a6e2808df09f91a6f09f8fb4e2808de298a0efb88f0000000000000000000000000000000000000000000000000000",
        ),
      ).toMatchInlineSnapshot(
        `
        [
          "",
        ]
      `,
      );
    });
  });
  describe("string,uint,bool", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("string,uint,bool"),
        "0x000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000001a4000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000057761676d69000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [string, bigint, boolean]>(result);
      expect(result).toEqual(["wagmi", 420n, true]);
    });
  });
  describe("uint[2],bool,string", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint[2],bool,string"),
        "0x00000000000000000000000000000000000000000000000000000000000001a400000000000000000000000000000000000000000000000000000000000000450000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000057761676d69000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [readonly [bigint, bigint], boolean, string]>(
        result,
      );
      expect(result).toEqual([[420n, 69n], true, "wagmi"]);
    });
  });
  describe("bytes", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("bytes"),
        "0x000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000030420690000000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [Hex]>(result);
      expect(result).toEqual(["0x042069"]);
    });
  });
  describe("(uint256[][2])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[][2]"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000e0000000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000001",
      );
      expectTypeOf<readonly [readonly [readonly bigint[], readonly bigint[]]]>(
        result,
      );
      expect(result).toEqual([
        [
          [1n, 2n, 3n, 4n],
          [3n, 2n, 1n],
        ],
      ]);
    });
  });
  describe("uint256[2][][2]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[2][][2]"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000e000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000005000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000009000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000000b",
      );
      expectTypeOf<
        readonly [
          readonly [
            readonly (readonly [bigint, bigint])[],
            readonly (readonly [bigint, bigint])[],
          ],
        ]
      >(result);
      expect(result).toEqual([
        [
          [
            [1n, 2n],
            [4n, 5n],
          ],
          [
            [8n, 9n],
            [10n, 11n],
          ],
        ],
      ]);
    });
  });
  describe("uint256[][][2]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[][][2]"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000001c00000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000c00000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000005000000000000000000000000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000070000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000800000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000800000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000009000000000000000000000000000000000000000000000000000000000000000a",
      );
      expectTypeOf<
        readonly [
          readonly [
            readonly (readonly bigint[])[],
            readonly (readonly bigint[])[],
          ],
        ]
      >(result);
      expect(result).toEqual([
        [
          [
            [1n, 2n, 3n],
            [4n, 5n, 6n, 7n],
          ],
          [[8n], [9n, 10n]],
        ],
      ]);
    });
  });
  describe("uint256[][1][1]", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[][1][1]"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000003",
      );
      expectTypeOf<readonly [readonly [readonly [readonly bigint[]]]]>(result);
      expect(result).toEqual([[[[1n, 2n, 3n]]]]);
    });
  });
  describe("(uint256[][2][2])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[][2][2]"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000001a0000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000007000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000800000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000800000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000009000000000000000000000000000000000000000000000000000000000000000a",
      );
      expectTypeOf<
        readonly [
          readonly [
            readonly [readonly bigint[], readonly bigint[]],
            readonly [readonly bigint[], readonly bigint[]],
          ],
        ]
      >(result);
      expect(result).toEqual([
        [
          [
            [1n, 2n, 3n],
            [4n, 5n, 6n, 7n],
          ],
          [[8n], [9n, 10n]],
        ],
      ]);
    });
  });
  describe("(uint256[][3][2])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[][3][2]"),
        "0x000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000240000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000e00000000000000000000000000000000000000000000000000000000000000180000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000007000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000009000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000b0000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000c000000000000000000000000000000000000000000000000000000000000000d0000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000e000000000000000000000000000000000000000000000000000000000000000f00000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000011",
      );
      expectTypeOf<
        readonly [
          readonly [
            readonly [readonly bigint[], readonly bigint[], readonly bigint[]],
            readonly [readonly bigint[], readonly bigint[], readonly bigint[]],
          ],
        ]
      >(result);
      expect(result).toEqual([
        [
          [
            [1n, 2n, 3n],
            [4n, 5n, 6n, 7n],
            [8n, 9n, 10n],
          ],
          [[11n], [12n, 13n], [14n, 15n, 16n, 17n]],
        ],
      ]);
    });
  });
  describe("(uint256[][2][3])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[][2][3]"),
        "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000018000000000000000000000000000000000000000000000000000000000000002a0000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000a00000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000005000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000007000000000000000000000000000000000000000000000000000000000000000800000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000009000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000c00000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000b000000000000000000000000000000000000000000000000000000000000000c000000000000000000000000000000000000000000000000000000000000000d0000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000e000000000000000000000000000000000000000000000000000000000000000f00000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000011",
      );
      expectTypeOf<
        readonly [
          readonly [
            readonly [readonly bigint[], readonly bigint[]],
            readonly [readonly bigint[], readonly bigint[]],
            readonly [readonly bigint[], readonly bigint[]],
          ],
        ]
      >(result);
      expect(result).toEqual([
        [
          [
            [1n, 2n],
            [3n, 4n, 5n],
          ],
          [
            [6n, 7n, 8n],
            [9n, 10n],
          ],
          [
            [11n, 12n, 13n],
            [14n, 15n, 16n, 17n],
          ],
        ],
      ]);
    });
  });
  describe("(uint256[][2][3][4])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[][2][3][4]"),
        "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000007c00000000000000000000000000000000000000000000000000000000000000b80000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000001c000000000000000000000000000000000000000000000000000000000000002a0000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000007000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000800000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000800000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000009000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000a00000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000b000000000000000000000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000d0000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000018000000000000000000000000000000000000000000000000000000000000002a0000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000a00000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000e000000000000000000000000000000000000000000000000000000000000000f0000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000110000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000a00000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000a00000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000001c00000000000000000000000000000000000000000000000000000000000000280000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000e0000000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000050000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000800000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000c000000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000007000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000001a000000000000000000000000000000000000000000000000000000000000002e0000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000c000000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000009000000000000000000000000000000000000000000000000000000000000000800000000000000000000000000000000000000000000000000000000000000070000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000050000000000000000000000000000000000000000000000000000000000000005000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000c000000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000c000000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000005000000000000000000000000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000050000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000006",
      );
      expectTypeOf<
        readonly [
          readonly [
            readonly [
              readonly [readonly bigint[], readonly bigint[]],
              readonly [readonly bigint[], readonly bigint[]],
              readonly [readonly bigint[], readonly bigint[]],
            ],
            readonly [
              readonly [readonly bigint[], readonly bigint[]],
              readonly [readonly bigint[], readonly bigint[]],
              readonly [readonly bigint[], readonly bigint[]],
            ],
            readonly [
              readonly [readonly bigint[], readonly bigint[]],
              readonly [readonly bigint[], readonly bigint[]],
              readonly [readonly bigint[], readonly bigint[]],
            ],
            readonly [
              readonly [readonly bigint[], readonly bigint[]],
              readonly [readonly bigint[], readonly bigint[]],
              readonly [readonly bigint[], readonly bigint[]],
            ],
          ],
        ]
      >(result);
      expect(result).toMatchInlineSnapshot(`
        [
          [
            [
              [
                [
                  1n,
                  2n,
                  3n,
                ],
                [
                  4n,
                  5n,
                  6n,
                  7n,
                ],
              ],
              [
                [
                  8n,
                ],
                [
                  9n,
                  10n,
                ],
              ],
              [
                [
                  11n,
                  12n,
                ],
                [
                  13n,
                ],
              ],
            ],
            [
              [
                [
                  14n,
                  15n,
                ],
                [
                  16n,
                  17n,
                  18n,
                ],
              ],
              [
                [
                  0n,
                  1n,
                ],
                [
                  2n,
                  3n,
                  4n,
                ],
              ],
              [
                [
                  5n,
                  6n,
                ],
                [
                  1n,
                  2n,
                  1n,
                ],
              ],
            ],
            [
              [
                [
                  1n,
                  2n,
                  1n,
                  2n,
                ],
                [
                  1n,
                  5n,
                  6n,
                ],
              ],
              [
                [
                  1n,
                ],
                [
                  2n,
                ],
              ],
              [
                [
                  1n,
                  2n,
                  3n,
                ],
                [
                  5n,
                  6n,
                  7n,
                ],
              ],
            ],
            [
              [
                [
                  9n,
                  8n,
                  7n,
                ],
                [
                  5n,
                  5n,
                  5n,
                ],
              ],
              [
                [
                  1n,
                  2n,
                  3n,
                ],
                [
                  4n,
                  4n,
                  4n,
                ],
              ],
              [
                [
                  5n,
                  5n,
                  5n,
                ],
                [
                  6n,
                  6n,
                  6n,
                ],
              ],
            ],
          ],
        ]
      `);
    });
  });
  describe("(uint256[2][][2])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[2][][2]"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000e000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000005000000000000000000000000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000009000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000000b0000000000000000000000000000000000000000000000000000000000000009000000000000000000000000000000000000000000000000000000000000000a",
      );
      expectTypeOf<
        readonly [
          readonly [
            readonly (readonly [bigint, bigint])[],
            readonly (readonly [bigint, bigint])[],
          ],
        ]
      >(result);
      expect(result).toEqual([
        [
          [
            [1n, 2n],
            [4n, 5n],
          ],
          [
            [8n, 9n],
            [10n, 11n],
            [9n, 10n],
          ],
        ],
      ]);
    });
  });
  describe("(uint[])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[]"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000001a4000000000000000000000000000000000000000000000000000000000000004500000000000000000000000000000000000000000000000000000000000000160000000000000000000000000000000000000000000000000000000000000037",
      );
      expectTypeOf<readonly [readonly bigint[]]>(result);
      expect(result).toEqual([[420n, 69n, 22n, 55n]]);
    });
    test("empty", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[]"),
        "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [readonly bigint[]]>(result);
      expect(result).toEqual([[]]);
    });
  });
  describe("(uint[][])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[][]"),
        "0x000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000001a40000000000000000000000000000000000000000000000000000000000000045",
      );
      expectTypeOf<readonly [readonly (readonly bigint[])[]]>(result);
      expect(result).toEqual([[[420n, 69n]]]);
    });
    test("empty", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[][]"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [readonly (readonly bigint[])[]]>(result);
      expect(result).toEqual([[[]]]);
    });
    // cast abi-encode "a(uint[][])" "[[420,69],[22,55,22],[51,52,66,11]]"
    test("complex", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("uint256[][]"),
        "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000003000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000000c00000000000000000000000000000000000000000000000000000000000000140000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000001a4000000000000000000000000000000000000000000000000000000000000004500000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000016000000000000000000000000000000000000000000000000000000000000003700000000000000000000000000000000000000000000000000000000000000160000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000003300000000000000000000000000000000000000000000000000000000000000340000000000000000000000000000000000000000000000000000000000000042000000000000000000000000000000000000000000000000000000000000000b",
      );
      expectTypeOf<readonly [readonly (readonly bigint[])[]]>(result);
      expect(result).toEqual([
        [
          [420n, 69n],
          [22n, 55n, 22n],
          [51n, 52n, 66n, 11n],
        ],
      ]);
    });
  });
  describe("(string[2])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("string[2]"),
        "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000057761676d6900000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000047669656d00000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [readonly [string, string]]>(result);
      expect(result).toEqual([["wagmi", "viem"]]);
    });
  });
  describe("(string[2][3])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("string[2][3]"),
        "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000001e00000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000057761676d6900000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000047669656d000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000046a616b65000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003746f6d00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000036c6f6c000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000046861686100000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [readonly (readonly [string, string])[]]>(result);
      expect(result).toEqual([
        [
          ["wagmi", "viem"],
          ["jake", "tom"],
          ["lol", "haha"],
        ],
      ]);
    });
  });
  describe("(bytes[2])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("bytes[2]"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000800000000000000000000000000000000000000000000000000000000000000002123400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000044141414100000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<readonly [readonly [Hex, Hex]]>(result);
      expect(result).toEqual([["0x1234", "0x41414141"]]);
    });
  });
  describe("(uint256, string)", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("(uint256, string)"),
        "0x000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000170000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000000568656c6c6f000000000000000000000000000000000000000000000000000000",
      );
      expect(result).toEqual([[23n, "hello"]]);
    });
  });
  describe("((uint256, string))", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("((uint256, string))"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000170000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000000568656c6c6f000000000000000000000000000000000000000000000000000000",
      );
      expect(result).toEqual([[[23n, "hello"]]]);
    });
  });
  describe("((uint256[], string))", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("((uint256[], string))"),
        "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000017000000000000000000000000000000000000000000000000000000000000002e000000000000000000000000000000000000000000000000000000000000000568656c6c6f000000000000000000000000000000000000000000000000000000",
      );
      expect(result).toEqual([[[[23n, 46n], "hello"]]]);
    });
  });
  // cast abi-encode "a((uint256[],bool,string[]))" "([1,2,3,4],true,[hello,world])"
  describe("(uint256[],bool,string[])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("(uint256[] x,bool y,string[] z)"),
        "0x000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000000568656c6c6f0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005776f726c64000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<
        readonly [
          {
            x: readonly bigint[];
            y: boolean;
            z: readonly string[];
          },
        ]
      >(result);
      expect(result).toEqual([
        {
          x: [1n, 2n, 3n, 4n],
          y: true,
          z: ["hello", "world"],
        },
      ]);
    });
  });
  describe("((uint256[] x,bool y,string[] z) foo,uint256 a,string[] b)", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters(
          "((uint256[] x,bool y,string[] z) foo,uint256 a,string[] b)",
        ),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000001a4000000000000000000000000000000000000000000000000000000000000024000000000000000000000000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000000568656c6c6f0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005776f726c6400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000057761676d6900000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000047669656d00000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<
        readonly [
          {
            foo: {
              x: readonly bigint[];
              y: boolean;
              z: readonly string[];
            };
            a: bigint;
            b: readonly string[];
          },
        ]
      >(result);
      expect(result).toEqual([
        {
          foo: {
            x: [1n, 2n, 3n, 4n],
            y: true,
            z: ["hello", "world"],
          },
          a: 420n,
          b: ["wagmi", "viem"],
        },
      ]);
    });
  });
  describe("((uint256[],bool,string[]) foo,uint256,string[])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters("((uint256[],bool,string[]) foo,uint256,string[])"),
        "0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000001a4000000000000000000000000000000000000000000000000000000000000024000000000000000000000000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000000568656c6c6f0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005776f726c6400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000057761676d6900000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000047669656d00000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<
        readonly [
          readonly [
            readonly [readonly bigint[], boolean, readonly string[]],
            bigint,
            readonly string[],
          ],
        ]
      >(result);
      expect(result).toEqual([
        [[[1n, 2n, 3n, 4n], true, ["hello", "world"]], 420n, ["wagmi", "viem"]],
      ]);
    });
  });
  describe("(uint256[],bool,string[]),(((uint256[],bool,string[]),uint256,string[]),((uint256[],bool,string[]), uint256, string[]),uint256,string[])", () => {
    test("default", () => {
      const result = decodeAbiParameters(
        parseAbiParameters(
          "(uint256[] x, bool y, string[] z) bazIn, (((uint256[] x, bool y, string[] z) foo, uint256 a, string[] b) foo, ((uint256[] x, bool y, string[] z) foo, uint256 a, string[] b) bar, uint256 c, string[] d) gmiIn",
        ),
        "0x0000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000022000000000000000000000000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000030000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000000568656c6c6f0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005776f726c640000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000036000000000000000000000000000000000000000000000000000000000004026aa0000000000000000000000000000000000000000000000000000000000000700000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000000000000000001a400000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000001a4000000000000000000000000000000000000000000000000000000000000004500000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000046e696365000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004686168610000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000057761676d690000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000006616c6c646179000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000010f2c00000000000000000000000000000000000000000000000000000000000002c00000000000000000000000000000000000000000000000000000000000000060000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000001a400000000000000000000000000000000000000000000000000000000000001a40000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000001400000000000000000000000000000000000000000000000000000000000000004746869730000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000026973000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000161000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005706172616d000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000000568656c6c6f0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005746865726500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000036c6f6c000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000046861686100000000000000000000000000000000000000000000000000000000",
      );
      expectTypeOf<
        readonly [
          {
            x: readonly bigint[];
            y: boolean;
            z: readonly string[];
          },
          {
            foo: {
              foo: {
                x: readonly bigint[];
                y: boolean;
                z: readonly string[];
              };
              a: bigint;
              b: readonly string[];
            };
            bar: {
              foo: {
                x: readonly bigint[];
                y: boolean;
                z: readonly string[];
              };
              a: bigint;
              b: readonly string[];
            };
            c: bigint;
            d: readonly string[];
          },
        ]
      >(result);
      expect(result).toEqual([
        {
          x: [1n, 2n, 3n, 4n],
          y: true,
          z: ["hello", "world"],
        },
        {
          foo: {
            a: 420n,
            b: ["wagmi", "allday"],
            foo: {
              x: [420n, 69n],
              y: true,
              z: ["nice", "haha"],
            },
          },
          bar: {
            a: 69420n,
            b: ["hello", "there"],
            foo: {
              x: [420n, 420n],
              y: true,
              z: ["this", "is", "a", "param"],
            },
          },
          c: 4204202n,
          d: ["lol", "haha"],
        },
      ]);
    });
  });
});
const seaportContractConfigAbi = [
  {
    inputs: [
      {
        components: [
          { name: "offerer", type: "address" },
          { name: "zone", type: "address" },
          {
            components: [
              {
                name: "itemType",
                type: "uint8",
              },
              { name: "token", type: "address" },
              {
                name: "identifierOrCriteria",
                type: "uint256",
              },
              {
                name: "startAmount",
                type: "uint256",
              },
              { name: "endAmount", type: "uint256" },
            ],
            name: "offer",
            type: "tuple[]",
          },
          {
            components: [
              {
                name: "itemType",
                type: "uint8",
              },
              { name: "token", type: "address" },
              {
                name: "identifierOrCriteria",
                type: "uint256",
              },
              {
                name: "startAmount",
                type: "uint256",
              },
              { name: "endAmount", type: "uint256" },
              {
                name: "recipient",
                type: "address",
              },
            ],
            name: "consideration",
            type: "tuple[]",
          },
          {
            name: "orderType",
            type: "uint8",
          },
          { name: "startTime", type: "uint256" },
          { name: "endTime", type: "uint256" },
          { name: "zoneHash", type: "bytes32" },
          { name: "salt", type: "uint256" },
          { name: "conduitKey", type: "bytes32" },
          { name: "counter", type: "uint256" },
        ],
        name: "orders",
        type: "tuple[]",
      },
    ],
    name: "cancel",
    outputs: [{ name: "cancelled", type: "bool" }],
    stateMutability: "nonpayable",
    type: "function",
  },
  {
    inputs: [
      {
        components: [
          {
            components: [
              { name: "offerer", type: "address" },
              { name: "zone", type: "address" },
              {
                components: [
                  {
                    name: "itemType",
                    type: "uint8",
                  },
                  { name: "token", type: "address" },
                  {
                    name: "identifierOrCriteria",
                    type: "uint256",
                  },
                  {
                    name: "startAmount",
                    type: "uint256",
                  },
                  {
                    name: "endAmount",
                    type: "uint256",
                  },
                ],
                name: "offer",
                type: "tuple[]",
              },
              {
                components: [
                  {
                    name: "itemType",
                    type: "uint8",
                  },
                  { name: "token", type: "address" },
                  {
                    name: "identifierOrCriteria",
                    type: "uint256",
                  },
                  {
                    name: "startAmount",
                    type: "uint256",
                  },
                  {
                    name: "endAmount",
                    type: "uint256",
                  },
                  {
                    name: "recipient",
                    type: "address",
                  },
                ],
                name: "consideration",
                type: "tuple[]",
              },
              {
                name: "orderType",
                type: "uint8",
              },
              { name: "startTime", type: "uint256" },
              { name: "endTime", type: "uint256" },
              { name: "zoneHash", type: "bytes32" },
              { name: "salt", type: "uint256" },
              {
                name: "conduitKey",
                type: "bytes32",
              },
              {
                name: "totalOriginalConsiderationItems",
                type: "uint256",
              },
            ],
            name: "parameters",
            type: "tuple",
          },
          { name: "numerator", type: "uint120" },
          { name: "denominator", type: "uint120" },
          { name: "signature", type: "bytes" },
          { name: "extraData", type: "bytes" },
        ],
        name: "advancedOrder",
        type: "tuple",
      },
      {
        components: [
          { name: "orderIndex", type: "uint256" },
          { name: "side", type: "uint8" },
          { name: "index", type: "uint256" },
          { name: "identifier", type: "uint256" },
          {
            name: "criteriaProof",
            type: "bytes32[]",
          },
        ],
        name: "criteriaResolvers",
        type: "tuple[]",
      },
      {
        name: "fulfillerConduitKey",
        type: "bytes32",
      },
      { name: "recipient", type: "address" },
    ],
    name: "fulfillAdvancedOrder",
    outputs: [{ name: "fulfilled", type: "bool" }],
    stateMutability: "payable",
    type: "function",
  },
] as const;
describe("seaport", () => {
  test("cancel", () => {
    const cancel = getAbiItem({
      abi: seaportContractConfigAbi,
      name: "cancel",
    });
    const data = decodeAbiParameters(
      cancel.inputs,
      "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",
      { formatAddress: toLowerCase },
    );
    expect(data).toMatchInlineSnapshot(`
      [
        [
          {
            "conduitKey": "0x511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511a",
            "consideration": [
              {
                "endAmount": 420n,
                "identifierOrCriteria": 69n,
                "itemType": 10,
                "recipient": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
                "startAmount": 6n,
                "token": "0x0000000000000000000000000000000000000000",
              },
              {
                "endAmount": 141n,
                "identifierOrCriteria": 55n,
                "itemType": 16,
                "recipient": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
                "startAmount": 15n,
                "token": "0x0000000000000000000000000000000000000000",
              },
            ],
            "counter": 1234123123n,
            "endTime": 123123123123n,
            "offer": [
              {
                "endAmount": 420n,
                "identifierOrCriteria": 69n,
                "itemType": 10,
                "startAmount": 6n,
                "token": "0x0000000000000000000000000000000000000000",
              },
              {
                "endAmount": 11n,
                "identifierOrCriteria": 515n,
                "itemType": 10,
                "startAmount": 6n,
                "token": "0x5414d89a8bf7e99d732bc52f3e6a3ef461c0c078",
              },
              {
                "endAmount": 123123n,
                "identifierOrCriteria": 55555511n,
                "itemType": 10,
                "startAmount": 111n,
                "token": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
              },
            ],
            "offerer": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
            "orderType": 10,
            "salt": 1234123123n,
            "startTime": 123123123123n,
            "zone": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
            "zoneHash": "0x511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511a",
          },
          {
            "conduitKey": "0x511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511a",
            "consideration": [
              {
                "endAmount": 420n,
                "identifierOrCriteria": 69n,
                "itemType": 10,
                "recipient": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
                "startAmount": 6n,
                "token": "0x0000000000000000000000000000000000000000",
              },
              {
                "endAmount": 141n,
                "identifierOrCriteria": 55n,
                "itemType": 16,
                "recipient": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
                "startAmount": 15n,
                "token": "0x0000000000000000000000000000000000000000",
              },
            ],
            "counter": 1234123123n,
            "endTime": 123123123123n,
            "offer": [
              {
                "endAmount": 420n,
                "identifierOrCriteria": 69n,
                "itemType": 10,
                "startAmount": 6n,
                "token": "0x0000000000000000000000000000000000000000",
              },
              {
                "endAmount": 11n,
                "identifierOrCriteria": 515n,
                "itemType": 10,
                "startAmount": 6n,
                "token": "0x5414d89a8bf7e99d732bc52f3e6a3ef461c0c078",
              },
              {
                "endAmount": 123123n,
                "identifierOrCriteria": 55555511n,
                "itemType": 10,
                "startAmount": 111n,
                "token": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
              },
            ],
            "offerer": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
            "orderType": 10,
            "salt": 1234123123n,
            "startTime": 123123123123n,
            "zone": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
            "zoneHash": "0x511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511a",
          },
        ],
      ]
    `);
  });
  test("fulfillAdvancedOrder", () => {
    const fulfillAdvancedOrder = getAbiItem({
      abi: seaportContractConfigAbi,
      name: "fulfillAdvancedOrder",
    });
    const data = decodeAbiParameters(
      fulfillAdvancedOrder.inputs,
      "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",
      { formatAddress: toLowerCase },
    );
    expect(data).toMatchInlineSnapshot(`
      [
        {
          "denominator": 69n,
          "extraData": "0x123123",
          "numerator": 420n,
          "parameters": {
            "conduitKey": "0x511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511a",
            "consideration": [
              {
                "endAmount": 420n,
                "identifierOrCriteria": 69n,
                "itemType": 10,
                "recipient": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
                "startAmount": 6n,
                "token": "0x0000000000000000000000000000000000000000",
              },
              {
                "endAmount": 141n,
                "identifierOrCriteria": 55n,
                "itemType": 16,
                "recipient": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
                "startAmount": 15n,
                "token": "0x0000000000000000000000000000000000000000",
              },
            ],
            "endTime": 123123123123n,
            "offer": [
              {
                "endAmount": 420n,
                "identifierOrCriteria": 69n,
                "itemType": 10,
                "startAmount": 6n,
                "token": "0x0000000000000000000000000000000000000000",
              },
              {
                "endAmount": 11n,
                "identifierOrCriteria": 515n,
                "itemType": 10,
                "startAmount": 6n,
                "token": "0x5414d89a8bf7e99d732bc52f3e6a3ef461c0c078",
              },
              {
                "endAmount": 123123n,
                "identifierOrCriteria": 55555511n,
                "itemType": 10,
                "startAmount": 111n,
                "token": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
              },
            ],
            "offerer": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
            "orderType": 10,
            "salt": 1234123123n,
            "startTime": 123123123123n,
            "totalOriginalConsiderationItems": 69420n,
            "zone": "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
            "zoneHash": "0x511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511a",
          },
          "signature": "0x123123",
        },
        [
          {
            "criteriaProof": [
              "0x511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511a",
            ],
            "identifier": 4242n,
            "index": 1231n,
            "orderIndex": 11n,
            "side": 1,
          },
        ],
        "0x511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511aaa511a",
        "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
      ]
    `);
  });
});
/* [Multicall3](https://github.com/mds1/multicall) */
const multicall3Abi = [
  {
    inputs: [
      {
        components: [
          {
            name: "target",
            type: "address",
          },
          {
            name: "allowFailure",
            type: "bool",
          },
          {
            name: "callData",
            type: "bytes",
          },
        ],
        name: "calls",
        type: "tuple[]",
      },
    ],
    name: "aggregate3",
    outputs: [
      {
        components: [
          {
            name: "success",
            type: "bool",
          },
          {
            name: "returnData",
            type: "bytes",
          },
        ],
        name: "returnData",
        type: "tuple[]",
      },
    ],
    stateMutability: "view",
    type: "function",
  },
] as const;
describe("multicall3", () => {
  test("zero data", () => {
    expect(
      decodeAbiParameters(
        multicall3Abi[0].outputs,
        "0x000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000000",
      ),
    ).toMatchInlineSnapshot(`
      [
        [
          {
            "returnData": "0x",
            "success": true,
          },
        ],
      ]
    `);
  });
  test("zero data + non-zero data", () => {
    expect(
      decodeAbiParameters(
        multicall3Abi[0].outputs,
        "0x00000000000000000000000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000a000000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000006000000000000000000000000000000000000000000000000d752bd588f61926e40000000000000000000000000000000000000000000000002e0e220d3d6cf5630000000000000000000000000000000000000000000000000000000063fa0b5f",
      ),
    ).toMatchInlineSnapshot(`
      [
        [
          {
            "returnData": "0x",
            "success": true,
          },
          {
            "returnData": "0x00000000000000000000000000000000000000000000000d752bd588f61926e40000000000000000000000000000000000000000000000002e0e220d3d6cf5630000000000000000000000000000000000000000000000000000000063fa0b5f",
            "success": true,
          },
        ],
      ]
    `);
  });
});
test("data suffix", () => {
  expect(
    decodeAbiParameters(
      [
        {
          name: "xIn",
          type: "uint256",
        },
      ],
      "0x0000000000000000000000000000000000000000000000000000000000010f2cdeadbeef",
    ),
  ).toEqual([69420n]);
});
test.skip("data size too small", () => {
  expect(() =>
    decodeAbiParameters(
      [{ type: "uint256" }],
      "0x0000000000000000000000000000000000000000000000000000000000010f",
    ),
  ).toThrowError(
    "Data size of 31 bytes is too small for given parameters.\n\nParams: (uint256)\nData:   0x0000000000000000000000000000000000000000000000000000000000010f (31 bytes)",
  );
  expect(() =>
    decodeAbiParameters(
      [{ type: "uint256" }],
      "0x0000000000000000000000000000000000000000000000000000000000010f",
    ),
  ).toThrowError(
    "Data size of 31 bytes is too small for given parameters.\n\nParams: (uint256)\nData:   0x0000000000000000000000000000000000000000000000000000000000010f (31 bytes)",
  );
  expect(() =>
    decodeAbiParameters(
      [{ type: "uint256" }, { type: "uint256" }],
      "0x0000000000000000000000000000000000000000000000000000000000010f2c",
    ),
  ).toThrowError("Position `32` is out of bounds (`0 < position < 32`).");
});
test("invalid type", () => {
  expect(() =>
    decodeAbiParameters(
      [{ name: "x", type: "lol" }],
      "0x0000000000000000000000000000000000000000000000000000000000000000",
    ),
  ).toThrowError(
    `Type "lol" is not a valid decoding type.\nPlease provide a valid ABI type.`,
  );
});
test("error: zero data", () => {
  expect(() =>
    decodeAbiParameters(
      [
        {
          inputs: [],
          name: "foo",
          outputs: [
            {
              name: "x",
              type: "uint256",
            },
          ],
          stateMutability: "pure",
          type: "function",
        },
      ],
      "0x",
    ),
  ).toThrowError(`Cannot decode zero data ("0x") with ABI parameters.`);
});
test("error: recursive decode array", () => {
  const payload = `0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000a${"0000000000000000000000000000000000000000000000000000000000000020".repeat(
    64,
  )}`;
  expect(() =>
    decodeAbiParameters(
      [{ type: "uint256[][][][][][][][][][]" }],
      `0x${payload}`,
    ),
  ).toThrowError("Recursive read limit exceeded.");
});
test("zst", () => {
  const payload =
    "0x000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000FFFFFFFF" as const;
  expect(() =>
    decodeAbiParameters([{ type: "uint256[0][4294967295]" }], payload),
  ).toThrowError("Invalid data length.");
  expect(() =>
    decodeAbiParameters([{ type: "uint32[0][4294967295]" }], payload),
  ).toThrowError("Invalid data length.");
  expect(() =>
    decodeAbiParameters([{ type: "uint256[4294967295][4294967295]" }], payload),
  ).toThrowError("Invalid data length.");
  expect(() =>
    decodeAbiParameters([{ type: "uint32[4294967295][4294967295]" }], payload),
  ).toThrowError("Invalid data length.");
  expect(() =>
    decodeAbiParameters([{ type: "uint256[0][]" }], payload),
  ).toThrowError("Invalid data length.");
  expect(() =>
    decodeAbiParameters([{ type: "uint256[0][]" }], payload),
  ).toThrowError("Invalid data length.");
  expect(() =>
    decodeAbiParameters([{ type: "tuple[]", components: [] }], payload),
  ).toThrowError("Invalid data length.");
  expect(() =>
    decodeAbiParameters(
      [{ type: "tuple[]", components: [{ type: "tuple", components: [] }] }],
      payload,
    ),
  ).toThrowError("Invalid data length.");
  expect(() =>
    decodeAbiParameters(
      [{ type: "tuple[]", components: [{ type: "uint32[0]" }] }],
      payload,
    ),
  ).toThrowError("Invalid data length.");
  expect(() =>
    decodeAbiParameters(
      [
        {
          type: "tuple[]",
          components: [{ type: "tuple", components: [{ type: "uint32[0]" }] }],
        },
      ],
      payload,
    ),
  ).toThrowError("Invalid data length.");
});
test.skip("recursive", () => {
  const arr2 = parseAbiParameters("uint256[][]");
  const arr4 = parseAbiParameters("uint256[][][][]");
  const arr10 = parseAbiParameters("uint256[][][][][][][][][][]");
  const a = [[[], [], [], [], [], [], [], [], [], []]] as const;
  const p = encodeAbiParameters(arr2, a);
  expect(p).toEqual(
    "0x0000000000000000000000000000000000000000000000000000000000000020" + // ptr
      "000000000000000000000000000000000000000000000000000000000000000a" + // len=10
      "0000000000000000000000000000000000000000000000000000000000000140" +
      "0000000000000000000000000000000000000000000000000000000000000160" +
      "0000000000000000000000000000000000000000000000000000000000000180" +
      "00000000000000000000000000000000000000000000000000000000000001a0" +
      "00000000000000000000000000000000000000000000000000000000000001c0" +
      "00000000000000000000000000000000000000000000000000000000000001e0" +
      "0000000000000000000000000000000000000000000000000000000000000200" +
      "0000000000000000000000000000000000000000000000000000000000000220" +
      "0000000000000000000000000000000000000000000000000000000000000240" +
      "0000000000000000000000000000000000000000000000000000000000000260" + // ptrs end (10)
      "0000000000000000000000000000000000000000000000000000000000000000" +
      "0000000000000000000000000000000000000000000000000000000000000000" +
      "0000000000000000000000000000000000000000000000000000000000000000" +
      "0000000000000000000000000000000000000000000000000000000000000000" +
      "0000000000000000000000000000000000000000000000000000000000000000" +
      "0000000000000000000000000000000000000000000000000000000000000000" +
      "0000000000000000000000000000000000000000000000000000000000000000" +
      "0000000000000000000000000000000000000000000000000000000000000000" +
      "0000000000000000000000000000000000000000000000000000000000000000" +
      "0000000000000000000000000000000000000000000000000000000000000000", // 10 values
  );
  const payload =
    `0x0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000000a${"0000000000000000000000000000000000000000000000000000000000000020".repeat(
      64,
    )}` as const;
  expect(() => decodeAbiParameters(arr10, payload)).toThrowError(
    "Recursive read limit exceeded.",
  );
  const ptrArr = parseAbiParameters("uint256[]");
  // Try to break check
  const p2 = encodeAbiParameters(ptrArr, [
    Array.from({ length: 10 * 1024 }, (_, j) => BigInt(j + 1) * 32n),
  ]);
  expect(() => decodeAbiParameters(arr10, p2)).toThrowError(
    "Position `327744` is out of bounds (`0 < position < 327744`).",
  );
  expect(() => decodeAbiParameters(arr4, p2)).toThrowError();
  expect(() => decodeAbiParameters(arr2, p2)).toThrowError();
});
test("recursive 2", () => {
  const arr10 = parseAbiParameters("uint256[][][][][][][][][][]");
  const a = [[], [], [], [], [], [], [], [], [], []] as const;
  const ptrArr = parseAbiParameters("uint256[]");
  const mainPtr = encodeAbiParameters(ptrArr, [
    a.map((i: any) => BigInt(a.length - i + 1) * 32n),
  ]);
  expect(() =>
    decodeAbiParameters(arr10, `0x${mainPtr.slice(2).repeat(10 + 1)}`),
  ).toThrowError("Recursive read limit exceeded.");
});
test("struct: (uint256, string, uint256)", () => {
  const params = parseAbiParameters("(uint256 x, string, address z)");
  const data = encodeAbiParameters(params, [[2n, "hi", zeroAddress]]);
  expect(decodeAbiParameters(params, data)).toMatchInlineSnapshot(`
    [
      [
        2n,
        "hi",
        "0x0000000000000000000000000000000000000000",
      ],
    ]
  `);
});
</file>

<file path="packages/core/src/utils/decodeAbiParameters.ts">
import {
  type AbiParameter,
  type AbiParameterToPrimitiveType,
  type DecodeAbiParametersReturnType,
  type Hex,
  InvalidHexBooleanError,
} from "viem";
import {
  AbiDecodingDataSizeTooSmallError,
  AbiDecodingZeroDataError,
  InvalidAbiDecodingTypeError,
  checksumAddress,
  hexToBigInt,
  hexToNumber,
  hexToString,
} from "viem";
const TRUE_BOOL =
  "0x0000000000000000000000000000000000000000000000000000000000000001" as const;
const FALSE_BOOL =
  "0x0000000000000000000000000000000000000000000000000000000000000000" as const;
const FIXED_ARRAY_REGEX = /^(.*)\[(\d+)\]$/;
const DYNAMIC_ARRAY_REGEX = /^(.*)\[\]$/;
const cursor = { index: 2, offset: 2, length: 2, readCount: 0, readLimit: 0 };
function readWord(data: Hex): Hex {
  if (cursor.readCount > cursor.readLimit) {
    throw new Error("Recursive read limit exceeded.");
  }
  cursor.readCount++;
  if (data.length - cursor.index < 64) {
    throw new Error("Invalid data length.");
  }
  return `0x${data.slice(cursor.index, cursor.index + 64)}` as const;
}
/**
 * Decode a list of abi parameters.
 *
 * @param params - The abi parameters to decode.
 * @param data - The data to decode.
 * @param formatAddress - An optional function to format addresses.
 * @param out - An optional array to store the decoded parameters.
 *
 * @see https://github.com/wevm/viem/blob/38525bf1d55ec....ts
 */
export function decodeAbiParameters<
  const params extends readonly AbiParameter[],
>(
  params: params,
  data: Hex,
  {
    formatAddress = checksumAddress,
    out = [] as DecodeAbiParametersReturnType<params>,
  }: {
    formatAddress?: (address: Hex) => Hex;
    out?: DecodeAbiParametersReturnType<params>;
  } = {
    formatAddress: checksumAddress,
    out: [] as DecodeAbiParametersReturnType<params>,
  },
): DecodeAbiParametersReturnType<params> {
  if (data.length <= 2 && params.length > 0) {
    throw new AbiDecodingZeroDataError();
  }
  if (data.length > 2 && data.length < 66) {
    throw new AbiDecodingDataSizeTooSmallError({
      data,
      params: params as readonly AbiParameter[],
      size: (data.length - 2) / 2,
    });
  }
  cursor.index = 2;
  cursor.offset = 2;
  cursor.length = data.length;
  cursor.readCount = 0;
  cursor.readLimit = Math.floor(data.length / 64) + 8_192;
  for (const param of params) {
    if (data.length - cursor.index < 64) {
      throw new Error("Invalid data length.");
    }
    (out as unknown[]).push(_decodeAbiParameter(param, data, formatAddress));
  }
  return out;
}
function _decodeAbiParameter(
  param: AbiParameter,
  data: Hex,
  formatAddress: (address: Hex) => Hex = checksumAddress,
): unknown {
  if (isAbiParameterFixedArray(param)) {
    const _type = param.type;
    const [_, type, length] = param.type.match(FIXED_ARRAY_REGEX)!;
    param.type = type!;
    if (length === "0") {
      throw new Error("Invalid data length.");
    }
    if (isAbiParameterDeeplyStatic(param) === false) {
      const _offset = cursor.offset;
      const _index = cursor.index;
      const offset = readWord(data);
      cursor.index = cursor.offset + hexToNumber(offset) * 2;
      cursor.offset += hexToNumber(offset) * 2;
      const value: unknown[] = [];
      for (let i = 0; i < Number.parseInt(length!, 10); ++i) {
        cursor.index = cursor.offset + i * 64;
        value.push(_decodeAbiParameter(param, data, formatAddress));
      }
      cursor.offset = _offset;
      cursor.index = _index + 64;
      param.type = _type;
      return value;
    }
    const value: unknown[] = [];
    for (let i = 0; i < Number.parseInt(length!, 10); ++i) {
      if (data.length - cursor.index < 64) {
        throw new Error("Invalid data length.");
      }
      value.push(_decodeAbiParameter(param, data, formatAddress));
    }
    param.type = _type;
    return value;
  }
  if (isAbiParameterDynamicArray(param)) {
    const _offset = cursor.offset;
    const _index = cursor.index;
    const offset = readWord(data);
    cursor.index = cursor.offset + hexToNumber(offset) * 2;
    cursor.offset += hexToNumber(offset) * 2 + 64;
    const length = readWord(data);
    cursor.index += 64;
    const _type = param.type;
    const [_, type] = param.type.match(DYNAMIC_ARRAY_REGEX)!;
    param.type = type!;
    const deeplyStatic = isAbiParameterDeeplyStatic(param);
    const value: unknown[] = [];
    for (let i = 0; i < hexToNumber(length!); ++i) {
      if (deeplyStatic === false) {
        cursor.index = cursor.offset + i * 64;
      }
      if (data.length - cursor.index < 64) {
        throw new Error("Invalid data length.");
      }
      value.push(_decodeAbiParameter(param, data, formatAddress));
    }
    cursor.offset = _offset;
    cursor.index = _index + 64;
    param.type = _type;
    return value;
  }
  if (param.type === "tuple") {
    const components = (
      param as Extract<AbiParameter, { components: readonly AbiParameter[] }>
    ).components;
    const hasUnnamedChild =
      components.length === 0 ||
      components.some((component) => component.name === undefined);
    const value: any = hasUnnamedChild ? [] : {};
    if (isAbiParameterDeeplyStatic(param)) {
      for (let i = 0; i < components.length; ++i) {
        const component = components[i]!;
        const _value = _decodeAbiParameter(component, data, formatAddress);
        if (hasUnnamedChild) {
          value.push(_value);
        } else {
          value[component.name!] = _value;
        }
      }
      return value;
    }
    const _offset = cursor.offset;
    const _index = cursor.index;
    const offset = readWord(data);
    cursor.offset += hexToNumber(offset) * 2;
    cursor.index = cursor.offset;
    for (let i = 0; i < components.length; ++i) {
      const component = components[i]!;
      const _value = _decodeAbiParameter(component, data, formatAddress);
      if (hasUnnamedChild) {
        value.push(_value);
      } else {
        value[component.name!] = _value;
      }
    }
    cursor.offset = _offset;
    cursor.index = _index + 64;
    return value;
  }
  if (param.type === "address") {
    if (data.length - cursor.index < 64) {
      throw new Error("Invalid data length.");
    }
    const address =
      `0x${data.slice(cursor.index + 24, cursor.index + 64)}` as const;
    cursor.index += 64;
    return formatAddress(address);
  }
  if (param.type.startsWith("uint") || param.type.startsWith("int")) {
    const signed = param.type.startsWith("int");
    const size = Number.parseInt(param.type.split("int")[1] || "256", 10);
    const value = readWord(data);
    cursor.index += 64;
    return size > 48
      ? hexToBigInt(value, { signed })
      : hexToNumber(value, { signed });
  }
  if (param.type.startsWith("bytes") && param.type.length > 5) {
    const [_, size] = param.type.split("bytes");
    if (data.length - cursor.index < Number.parseInt(size!, 10) * 2) {
      throw new Error("Invalid data length.");
    }
    const value =
      `0x${data.slice(cursor.index, cursor.index + Number.parseInt(size!, 10) * 2)}` as const;
    cursor.index += 64;
    return value;
  }
  if (param.type === "bool") {
    const value = readWord(data);
    cursor.index += 64;
    if (value !== TRUE_BOOL && value !== FALSE_BOOL) {
      throw new InvalidHexBooleanError(value);
    }
    return value === TRUE_BOOL;
  }
  if (param.type === "string") {
    const _index = cursor.index;
    const offset = readWord(data);
    cursor.index = cursor.offset + hexToNumber(offset) * 2;
    const length = readWord(data);
    cursor.index += 64;
    if (hexToNumber(length) === 0) {
      cursor.index = _index + 64;
      return "";
    }
    if (data.length - cursor.index < hexToNumber(length) * 2) {
      throw new Error("Invalid data length.");
    }
    const value =
      `0x${data.slice(cursor.index, cursor.index + hexToNumber(length) * 2)}` as const;
    cursor.index = _index + 64;
    return hexToString(value);
  }
  if (param.type === "bytes") {
    const index = cursor.index;
    const offset = readWord(data);
    cursor.index = cursor.offset + hexToNumber(offset) * 2;
    const length = readWord(data);
    cursor.index += 64;
    if (hexToNumber(length) === 0) {
      cursor.index = index + 64;
      return "0x";
    }
    if (data.length - cursor.index < hexToNumber(length) * 2) {
      throw new Error("Invalid data length.");
    }
    const value =
      `0x${data.slice(cursor.index, cursor.index + hexToNumber(length) * 2)}` as const;
    cursor.index = index + 64;
    return value;
  }
  throw new InvalidAbiDecodingTypeError(param.type, {
    docsPath: "/docs/contract/decodeAbiParameters",
  });
}
/**
 * Decode a single abi parameter.
 *
 * @param param - The abi parameter to decode.
 * @param data - The data to decode.
 * @param formatAddress - An optional function to format addresses.
 *
 * @see https://github.com/wevm/viem/blob/38525bf1d55ec....ts
 */
export function decodeAbiParameter<const param extends AbiParameter>(
  param: param,
  data: Hex,
  {
    formatAddress = checksumAddress,
  }: {
    formatAddress?: (address: Hex) => Hex;
  } = {
    formatAddress: checksumAddress,
  },
): AbiParameterToPrimitiveType<param> {
  if (data.length <= 2) {
    throw new AbiDecodingZeroDataError();
  }
  if (data.length !== 66) {
    throw new Error(
      `Invalid data length. Expected 66 bytes, got ${data.length}`,
    );
  }
  if (param.type === "address") {
    const address = `0x${data.slice(2 + 12 * 2)}` as const;
    return formatAddress(address) as AbiParameterToPrimitiveType<param>;
  }
  if (param.type.startsWith("uint") || param.type.startsWith("int")) {
    const signed = param.type.startsWith("int");
    const size = Number.parseInt(param.type.split("int")[1] || "256", 10);
    return (
      size > 48 ? hexToBigInt(data, { signed }) : hexToNumber(data, { signed })
    ) as AbiParameterToPrimitiveType<param>;
  }
  if (param.type.startsWith("bytes") && param.type.length > 5) {
    const [_, size] = param.type.split("bytes");
    return data.slice(
      0,
      2 + Number.parseInt(size!, 10) * 2,
    ) as AbiParameterToPrimitiveType<param>;
  }
  if (param.type === "bool") {
    return (data === TRUE_BOOL) as AbiParameterToPrimitiveType<param>;
  }
  throw new InvalidAbiDecodingTypeError(param.type, {
    docsPath: "/docs/contract/decodeAbiParameters",
  });
}
function isAbiParameterFixedArray(param: AbiParameter) {
  return FIXED_ARRAY_REGEX.test(param.type);
}
function isAbiParameterDynamicArray(param: AbiParameter) {
  return DYNAMIC_ARRAY_REGEX.test(param.type);
}
function isAbiParameterDeeplyStatic(param: AbiParameter): boolean {
  const { type } = param;
  if (type === "string") return false;
  if (type === "bytes") return false;
  if (type.endsWith("[]")) return false;
  if (type === "tuple") {
    return (
      param as Extract<AbiParameter, { components: readonly AbiParameter[] }>
    ).components.every(isAbiParameterDeeplyStatic);
  }
  if (isAbiParameterFixedArray(param)) {
    const _type = param.type;
    const [_, type] = param.type.match(FIXED_ARRAY_REGEX)!;
    param.type = type!;
    const result = isAbiParameterDeeplyStatic(param);
    param.type = _type;
    return result;
  }
  return true;
}
</file>

<file path="packages/core/src/utils/decodeEventLog.test.ts">
import { toEventSelector } from "viem";
import { describe, expect, test } from "vitest";
import { decodeEventLog } from "./decodeEventLog.js";
test("named args: Transfer(address,address,uint256)", () => {
  const event = decodeEventLog({
    abiItem: {
      inputs: [
        {
          indexed: true,
          name: "from",
          type: "address",
        },
        {
          indexed: true,
          name: "to",
          type: "address",
        },
        {
          indexed: false,
          name: "tokenId",
          type: "uint256",
        },
      ],
      name: "Transfer",
      type: "event",
    },
    data: "0x0000000000000000000000000000000000000000000000000000000000000001",
    topics: [
      "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
      "0x000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
      "0x000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    ],
  });
  expect(event).toEqual({
    from: "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    to: "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    tokenId: 1n,
  });
});
test("named args with a missing name: Transfer(address,address,uint256)", () => {
  const event = decodeEventLog({
    abiItem: {
      inputs: [
        {
          indexed: true,
          name: "from",
          type: "address",
        },
        {
          indexed: true,
          name: "to",
          type: "address",
        },
        {
          indexed: false,
          name: "",
          type: "uint256",
        },
      ],
      name: "Transfer",
      type: "event",
    },
    data: "0x0000000000000000000000000000000000000000000000000000000000000001",
    topics: [
      "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
      "0x000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
      "0x000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    ],
  });
  expect(event).toEqual([
    "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    1n,
  ]);
});
test("unnamed args: Transfer(address,address,uint256)", () => {
  const event = decodeEventLog({
    abiItem: {
      inputs: [
        {
          indexed: true,
          type: "address",
        },
        {
          indexed: true,
          type: "address",
        },
        {
          indexed: false,
          type: "uint256",
        },
      ],
      name: "Transfer",
      type: "event",
    },
    data: "0x0000000000000000000000000000000000000000000000000000000000000001",
    topics: [
      "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
      "0x000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
      "0x000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    ],
  });
  expect(event).toEqual([
    "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    1n,
  ]);
});
test("unnamed args: mixed ordering of indexed args", () => {
  const event = decodeEventLog({
    abiItem: {
      inputs: [
        {
          indexed: true,
          type: "address",
        },
        {
          indexed: false,
          type: "uint256",
        },
        {
          indexed: true,
          type: "address",
        },
      ],
      name: "Transfer",
      type: "event",
    },
    data: "0x0000000000000000000000000000000000000000000000000000000000000001",
    topics: [
      "0x138dbc8474f748db86063dcef24cef1495bc73385a946f8d691128085e5ebec2",
      "0x000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
      "0x000000000000000000000000a5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    ],
  });
  expect(event).toEqual([
    "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
    1n,
    "0xa5cc3c03994db5b0d9a5eedd10cabab0813678ac",
  ]);
});
test("Foo(string)", () => {
  const event = decodeEventLog({
    abiItem: {
      inputs: [
        {
          indexed: true,
          name: "message",
          type: "string",
        },
      ],
      name: "Foo",
      type: "event",
    },
    data: "0x",
    topics: [
      "0x9f0b7f1630bdb7d474466e2dfef0fb9dff65f7a50eec83935b68f77d0808f08a",
      "0x1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8",
    ],
  });
  expect(event).toEqual({
    message:
      "0x1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8",
  });
});
test("args: eventName", () => {
  const event = decodeEventLog({
    abiItem: {
      inputs: [
        {
          indexed: true,
          name: "message",
          type: "string",
        },
      ],
      name: "Foo",
      type: "event",
    },
    data: "0x",
    topics: [
      "0x9f0b7f1630bdb7d474466e2dfef0fb9dff65f7a50eec83935b68f77d0808f08a",
      "0x1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8",
    ],
  });
  expect(event).toEqual({
    message:
      "0x1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8",
  });
});
test("args: data  named (address,address,uint256)", () => {
  const event = decodeEventLog({
    abiItem: {
      inputs: [
        {
          indexed: true,
          name: "from",
          type: "address",
        },
        {
          indexed: true,
          name: "to",
          type: "address",
        },
        {
          indexed: false,
          name: "tokenId",
          type: "uint256",
        },
      ],
      name: "Transfer",
      type: "event",
    },
    data: "0x0000000000000000000000000000000000000000000000000000000000000001",
    topics: [
      "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
      "0x000000000000000000000000d8da6bf26964af9d7eed9e03e53415d37aa96045",
      "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
    ],
  });
  expect(event).toEqual({
    from: "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
    to: "0xf39fd6e51aad88f6f4ce6ab8827279cfffb92266",
    tokenId: 1n,
  });
});
test("args: data  unnamed (address,address,uint256)", () => {
  const event = decodeEventLog({
    abiItem: {
      inputs: [
        {
          indexed: true,
          type: "address",
        },
        {
          indexed: true,
          type: "address",
        },
        {
          indexed: false,
          type: "uint256",
        },
      ],
      name: "Transfer",
      type: "event",
    },
    data: "0x0000000000000000000000000000000000000000000000000000000000000001",
    topics: [
      "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
      "0x000000000000000000000000d8da6bf26964af9d7eed9e03e53415d37aa96045",
      "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
    ],
  });
  expect(event).toEqual([
    "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
    "0xf39fd6e51aad88f6f4ce6ab8827279cfffb92266",
    1n,
  ]);
});
test("named: topics + event params mismatch", () => {
  expect(() =>
    decodeEventLog({
      abiItem: {
        inputs: [
          {
            indexed: true,
            name: "from",
            type: "address",
          },
          {
            indexed: false,
            name: "to",
            type: "address",
          },
          {
            indexed: true,
            name: "id",
            type: "uint256",
          },
        ],
        name: "Transfer",
        type: "event",
      },
      data: "0x",
      topics: [
        "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
        "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
      ],
    }),
  ).toThrowError(
    `Expected a topic for indexed event parameter "id" on event "Transfer(address from, address to, uint256 id)".`,
  );
});
test("unnamed: topics + event params mismatch", () => {
  expect(() =>
    decodeEventLog({
      abiItem: {
        inputs: [
          {
            indexed: true,
            type: "address",
          },
          {
            indexed: false,
            type: "address",
          },
          {
            indexed: true,
            type: "uint256",
          },
        ],
        name: "Transfer",
        type: "event",
      },
      data: "0x",
      topics: [
        "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
        "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
      ],
    }),
  ).toThrowError(
    `Expected a topic for indexed event parameter on event "Transfer(address, address, uint256)".`,
  );
});
test("data + event params mismatch", () => {
  expect(() =>
    decodeEventLog({
      abiItem: {
        anonymous: false,
        inputs: [
          {
            indexed: true,
            name: "from",
            type: "address",
          },
          {
            indexed: false,
            name: "to",
            type: "address",
          },
          {
            indexed: false,
            name: "id",
            type: "uint256",
          },
        ],
        name: "Transfer",
        type: "event",
      },
      data: "0x0000000000000000000000000000000000000000000000000000000023c34600",
      topics: [
        "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
        "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
        "0x00000000000000000000000070e8a65d014918798ba424110d5df658cde1cc58",
      ],
    }),
  ).toThrowError("Invalid data length");
  expect(() =>
    decodeEventLog({
      abiItem: {
        inputs: [
          {
            indexed: true,
            name: "from",
            type: "address",
          },
          {
            indexed: false,
            name: "to",
            type: "address",
          },
          {
            indexed: true,
            name: "id",
            type: "uint256",
          },
        ],
        name: "Transfer",
        type: "event",
      },
      data: "0x",
      topics: [
        "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
        "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
        "0x0000000000000000000000000000000000000000000000000000000000000001",
      ],
    }),
  ).toThrowError(
    "Data size of 0 bytes is too small for non-indexed event parameters.",
  );
  expect(() =>
    decodeEventLog({
      abiItem: {
        inputs: [
          {
            indexed: true,
            name: "from",
            type: "address",
          },
          {
            indexed: false,
            name: "to",
            type: "address",
          },
          {
            indexed: true,
            name: "id",
            type: "uint256",
          },
        ],
        name: "Transfer",
        type: "event",
      },
      data: "0x",
      topics: [
        "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
        "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
        "0x0000000000000000000000000000000000000000000000000000000000000001",
      ],
    }),
  ).toThrowError(
    "Data size of 0 bytes is too small for non-indexed event parameters.",
  );
});
describe("GitHub repros", () => {
  describe("https://github.com/wevm/viem/issues/168", () => {
    test("zero data string", () => {
      const result = decodeEventLog({
        abiItem: {
          anonymous: false,
          inputs: [
            {
              indexed: false,
              name: "voter",
              type: "address",
            },
            {
              indexed: false,
              name: "proposalId",
              type: "bytes32",
            },
            {
              indexed: false,
              name: "support",
              type: "uint256",
            },
            {
              indexed: false,
              name: "weight",
              type: "uint256",
            },
            {
              indexed: false,
              name: "reason",
              type: "string",
            },
          ],
          name: "VoteCast",
          type: "event",
        },
        data: "0x000000000000000000000000d1d1d4e36117ab794ec5d4c78cbd3a8904e691d04bdc559e89b88b73d8edeea6a767041d448d8076d070facc8340621555be3ac40000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000a00000000000000000000000000000000000000000000000000000000000000000",
        topics: [
          "0x0c165c85edbf8f9b99d51793c9429beb9dc2b608a7f81e64623052f829657af3",
        ],
      });
      expect(result).toMatchInlineSnapshot(`
        {
          "proposalId": "0x4bdc559e89b88b73d8edeea6a767041d448d8076d070facc8340621555be3ac4",
          "reason": "",
          "support": 1n,
          "voter": "0xd1d1d4e36117ab794ec5d4c78cbd3a8904e691d0",
          "weight": 1n,
        }
      `);
    });
  });
  describe("https://github.com/wevm/viem/issues/197", () => {
    test("topics + event params mismatch", () => {
      expect(() =>
        decodeEventLog({
          abiItem: {
            anonymous: false,
            inputs: [
              {
                indexed: true,
                name: "from",
                type: "address",
              },
              {
                indexed: true,
                name: "to",
                type: "address",
              },
              {
                indexed: true,
                name: "id",
                type: "uint256",
              },
            ],
            name: "Transfer",
            type: "event",
          },
          data: "0x0000000000000000000000000000000000000000000000000000000023c34600",
          topics: [
            "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
            "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
            "0x00000000000000000000000070e8a65d014918798ba424110d5df658cde1cc58",
          ],
        }),
      ).toThrowError(
        `Expected a topic for indexed event parameter "id" on event "Transfer(address from, address to, uint256 id)".`,
      );
    });
  });
  describe("https://github.com/wevm/viem/issues/323", () => {
    test("data + params mismatch", () => {
      expect(() =>
        decodeEventLog({
          abiItem: {
            anonymous: false,
            inputs: [
              {
                indexed: true,
                name: "from",
                type: "address",
              },
              {
                indexed: false,
                name: "to",
                type: "address",
              },
              {
                indexed: false,
                name: "id",
                type: "uint256",
              },
            ],
            name: "Transfer",
            type: "event",
          },
          data: "0x0000000000000000000000000000000000000000000000000000000023c34600",
          topics: [
            "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
            "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
            "0x00000000000000000000000070e8a65d014918798ba424110d5df658cde1cc58",
          ],
        }),
      ).toThrowError("Invalid data length.");
    });
  });
  describe("https://github.com/wevm/viem/issues/1336", () => {
    test("topics + event params mismatch", () => {
      expect(() =>
        decodeEventLog({
          abiItem: {
            anonymous: false,
            inputs: [
              {
                indexed: true,
                name: "nounId",
                type: "uint256",
              },
              {
                indexed: false,
                name: "startTime",
                type: "uint256",
              },
              {
                indexed: false,
                name: "endTime",
                type: "uint256",
              },
            ],
            name: "AuctionCreated",
            type: "event",
          },
          data: "0x00000000000000000000000000000000000000000000000000000000000000680000000000000000000000000000000000000000000000004563918244f400000000000000000000000000000000000000000000000000000000000062845fba",
          topics: [
            "0xd6eddd1118d71820909c1197aa966dbc15ed6f508554252169cc3d5ccac756ca",
          ],
        }),
      ).toThrowError(
        `Expected a topic for indexed event parameter "nounId" on event "AuctionCreated(uint256 nounId, uint256 startTime, uint256 endTime)".`,
      );
    });
  });
});
test("errors: no topics", () => {
  expect(() =>
    decodeEventLog({
      abiItem: {
        inputs: [
          {
            indexed: true,
            name: "message",
            type: "string",
          },
        ],
        name: "Bar",
        type: "event",
      },
      data: "0x",
      topics: [],
    }),
  ).toThrowError(
    `Expected a topic for indexed event parameter "message" on event "Bar(string message)".`,
  );
});
test("errors: invalid data size", () => {
  expect(() =>
    decodeEventLog({
      abiItem: {
        inputs: [
          {
            indexed: true,
            name: "from",
            type: "address",
          },
          {
            indexed: true,
            name: "to",
            type: "address",
          },
          {
            indexed: false,
            name: "tokenId",
            type: "uint256",
          },
        ],
        name: "Transfer",
        type: "event",
      },
      data: "0x1",
      topics: [
        "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
        "0x000000000000000000000000d8da6bf26964af9d7eed9e03e53415d37aa96045",
        "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
      ],
    }),
  ).toThrowError("Data size of 0.5 bytes is too small for given parameters.");
});
test("errors: invalid bool", () => {
  expect(() =>
    decodeEventLog({
      abiItem: {
        inputs: [
          {
            indexed: true,
            name: "from",
            type: "address",
          },
          {
            indexed: true,
            name: "to",
            type: "address",
          },
          {
            indexed: false,
            name: "sender",
            type: "bool",
          },
        ],
        name: "Transfer",
        type: "event",
      },
      data: "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
      topics: [
        toEventSelector("Transfer(address,address,bool)"),
        "0x000000000000000000000000d8da6bf26964af9d7eed9e03e53415d37aa96045",
        "0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266",
      ],
    }),
  ).toThrowError(
    `Hex value "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" is not a valid boolean. The hex value must be "0x0" (false) or "0x1" (true).`,
  );
});
test("errors: namehash", () => {
  expect(() =>
    decodeEventLog({
      abiItem: {
        anonymous: false,
        inputs: [
          {
            indexed: true,
            internalType: "bytes32",
            name: "node",
            type: "bytes32",
          },
          {
            indexed: true,
            internalType: "string",
            name: "indexedKey",
            type: "string",
          },
          {
            indexed: false,
            internalType: "string",
            name: "key",
            type: "string",
          },
        ],
        name: "TextChanged",
        type: "event",
      },
      // topics/data from https://etherscan.io/tx/0x1c852ec21dc816060052a232...#eventlog
      // @ts-ignore
      topics: [
        "0xd8c9334b1a9c2f9da342a0a2b32629c1a229b6445dad78947f674b44444a7550",
        "0x4aeacf8a996820a6609013324038be7f8d07ff9185f50063e7bf81915e6d2c08",
        null,
        null,
      ],
      data: "0x00000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000000375726c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000375726c0000000000000000000000000000000000000000000000000000000000",
    }),
  ).toThrow(/Expected a topic for indexed event parameter/);
});
</file>

<file path="packages/core/src/utils/decodeEventLog.ts">
import {
  type AbiEvent,
  type AbiParameter,
  type DecodeAbiParametersReturnType,
  DecodeLogDataMismatch,
  DecodeLogTopicsMismatch,
  type Hex,
} from "viem";
import {
  decodeAbiParameter,
  decodeAbiParameters,
} from "./decodeAbiParameters.js";
import { toLowerCase } from "./lowercase.js";
/**
 * Decode an event log.
 *
 * @see https://github.com/wevm/viem/blob/main/src/utils/abi/decodeEventLog.ts#L99
 */
export function decodeEventLog({
  abiItem,
  topics,
  data,
}: {
  abiItem: AbiEvent;
  topics: [signature: Hex, ...args: Hex[]] | [];
  data: Hex;
}): any {
  const { inputs } = abiItem;
  const isUnnamed = inputs?.some((x) => !("name" in x && x.name));
  const args: any = isUnnamed ? [] : {};
  // Decode topics (indexed args).
  const indexedInputs = inputs
    .map((x, i) => [x, i] as const)
    .filter(([x]) => "indexed" in x && x.indexed);
  for (let i = 0; i < indexedInputs.length; i++) {
    const [param, argIndex] = indexedInputs[i]!;
    const topic = topics[i + 1];
    if (!topic) {
      throw new DecodeLogTopicsMismatch({
        abiItem,
        param: param as AbiParameter & { indexed: boolean },
      });
    }
    args[isUnnamed ? argIndex : param.name || argIndex] = decodeTopic({
      param,
      value: topic,
    });
  }
  // Decode data (non-indexed args).
  const nonIndexedInputs = inputs.filter((x) => !("indexed" in x && x.indexed));
  if (nonIndexedInputs.length > 0) {
    if (data && data !== "0x") {
      const out = [] as DecodeAbiParametersReturnType<typeof nonIndexedInputs>;
      decodeAbiParameters(nonIndexedInputs, data, {
        out,
        formatAddress: toLowerCase,
      });
      if (out) {
        if (isUnnamed) {
          for (let i = 0; i < inputs.length; i++) {
            args[i] = args[i] ?? out.shift();
          }
        } else {
          for (let i = 0; i < nonIndexedInputs.length; i++) {
            args[nonIndexedInputs[i]!.name!] = out[i];
          }
        }
        out.length = 0;
      }
    } else {
      throw new DecodeLogDataMismatch({
        abiItem,
        data: "0x",
        params: nonIndexedInputs,
        size: 0,
      });
    }
  }
  return Object.values(args).length > 0 ? args : undefined;
}
const ARRAY_REGEX = /^(.*)\[(\d+)?\]$/;
function decodeTopic({ param, value }: { param: AbiParameter; value: Hex }) {
  if (
    param.type === "string" ||
    param.type === "bytes" ||
    param.type === "tuple" ||
    param.type.match(ARRAY_REGEX)
  ) {
    return value;
  }
  return decodeAbiParameter(param, value, { formatAddress: toLowerCase });
}
</file>

<file path="packages/core/src/utils/dedupe.ts">
/**
 * Remove duplicate values from an array.
 *
 * @param getId callback used to calculate a unique value for an element of the array.
 *
 * @example
 * dedupe([1,1,2,3]) // [1,2,3]
 *
 * dedupe(
 *   [
 *     { a: 1, b: 2 },
 *     { a: 1, b: 2 },
 *     { a: 2, b: 2 },
 *   ],
 *   (e) => `${e.a}_${e.b}`,
 * ) // [{a: 1, b: 2}, {a: 2, b: 2}]
 *
 */
export function dedupe<item, id>(
  arr: item[] | readonly item[],
  getId?: (x: item) => id,
): item[] {
  const seen = new Set<id | item>();
  return arr.filter((x) => {
    const id = getId ? getId(x) : x;
    if (seen.has(id)) return false;
    seen.add(id);
    return true;
  });
}
</file>

<file path="packages/core/src/utils/duplicates.ts">
/**
 * Returns a Set containing all the duplicate elements in an array of strings.
 * @param arr - The input array of strings.
 * @returns A Set object containing the duplicate elements found in the input array.
 */
export function getDuplicateElements(arr: string[]): Set<string> {
  const uniqueElements = new Set<string>();
  const duplicates = new Set<string>();
  arr.forEach((element: string) => {
    if (uniqueElements.has(element)) {
      duplicates.add(element);
    } else {
      uniqueElements.add(element);
    }
  });
  return duplicates;
}
</file>

<file path="packages/core/src/utils/estimate.ts">
export const estimate = ({
  from,
  to,
  target,
  result,
  min,
  max,
  prev,
  maxIncrease,
}: {
  from: number;
  to: number;
  target: number;
  result: number;
  min: number;
  max: number;
  prev: number;
  maxIncrease: number;
}) => {
  const density = (to - from) / (result || 1);
  // min <= estimate <= prev * maxIncrease or max
  return Math.min(
    Math.max(min, Math.round(target * density)),
    Math.round(prev * maxIncrease),
    max,
  );
};
</file>

<file path="packages/core/src/utils/finality.ts">
import type { Chain } from "viem";
/**
 * Returns the number of blocks that must pass before a block is considered final.
 * Note that a value of `0` indicates that blocks are considered final immediately.
 *
 * @param chain The chain to get the finality block count for.
 * @returns The finality block count.
 */
export function getFinalityBlockCount({ chain }: { chain: Chain | undefined }) {
  let finalityBlockCount: number;
  switch (chain?.id) {
    // Mainnet and mainnet testnets.
    case 1:
    case 3:
    case 4:
    case 5:
    case 42:
    case 11155111:
      finalityBlockCount = 65;
      break;
    // Polygon.
    case 137:
    case 80001:
      finalityBlockCount = 200;
      break;
    // Arbitrum.
    case 42161:
    case 42170:
    case 421611:
    case 421613:
      finalityBlockCount = 240;
      break;
    default:
      // Assume a 2-second block time, e.g. OP stack chains.
      finalityBlockCount = 30;
  }
  return finalityBlockCount;
}
</file>

<file path="packages/core/src/utils/format.ts">
export const formatEta = (ms: number) => {
  // If less than 1 second, return ms.
  if (ms < 1000) return `${Math.round(ms)}ms`;
  const seconds = Math.floor(ms / 1000);
  const h = Math.floor(seconds / 3600);
  const m = Math.floor((seconds - h * 3600) / 60);
  const s = seconds - h * 3600 - m * 60;
  const hstr = h > 0 ? `${h}h ` : "";
  const mstr = m > 0 || h > 0 ? `${m < 10 && h > 0 ? "0" : ""}${m}m ` : "";
  const sstr = s > 0 || m > 0 ? `${s < 10 && m > 0 ? "0" : ""}${s}s` : "";
  return `${hstr}${mstr}${sstr}`;
};
export const formatPercentage = (cacheRate: number) => {
  const decimal = Math.round(cacheRate * 1000) / 10;
  return Number.isInteger(decimal) && decimal < 100
    ? `${decimal}.0%`
    : `${decimal}%`;
};
</file>

<file path="packages/core/src/utils/generators.test.ts">
import { promiseWithResolvers } from "@/utils/promiseWithResolvers.js";
import { expect, test } from "vitest";
import {
  bufferAsyncGenerator,
  createCallbackGenerator,
  mergeAsyncGenerators,
} from "./generators.js";
test("mergeAsyncGenerators()", async () => {
  const p1 = promiseWithResolvers<number>();
  const p2 = promiseWithResolvers<number>();
  const p3 = promiseWithResolvers<number>();
  const p4 = promiseWithResolvers<number>();
  async function* generator1() {
    yield await p1.promise;
    yield await p2.promise;
  }
  async function* generator2() {
    yield await p3.promise;
    yield await p4.promise;
  }
  const results: number[] = [];
  const generator = mergeAsyncGenerators([generator1(), generator2()]);
  (async () => {
    for await (const result of generator) {
      results.push(result);
    }
  })();
  p1.resolve(1);
  p2.resolve(2);
  await new Promise((res) => setTimeout(res));
  p3.resolve(3);
  p4.resolve(4);
  await new Promise((res) => setTimeout(res));
  expect(results).toStrictEqual([1, 2, 3, 4]);
});
test("mergeAsyncGenerators() yields all results", async () => {
  const p1 = promiseWithResolvers<number>();
  const p2 = promiseWithResolvers<number>();
  const p3 = promiseWithResolvers<number>();
  const p4 = promiseWithResolvers<number>();
  async function* generator1() {
    yield await p1.promise;
    yield await p2.promise;
  }
  async function* generator2() {
    yield await p3.promise;
    yield await p4.promise;
  }
  const results: number[] = [];
  const generator = mergeAsyncGenerators([generator1(), generator2()]);
  const lock = promiseWithResolvers<void>();
  (async () => {
    for await (const result of generator) {
      await lock.promise;
      results.push(result);
    }
  })();
  p1.resolve(1);
  p2.resolve(2);
  await new Promise((res) => setTimeout(res));
  p3.resolve(3);
  p4.resolve(4);
  lock.resolve();
  await new Promise((res) => setTimeout(res));
  expect(results).toStrictEqual([1, 2, 3, 4]);
});
test("bufferAsyncGenerator() prefetches results", async () => {
  let sum = 0;
  async function* inputGenerator() {
    yield;
    sum += 1;
    yield;
    sum += 1;
    yield;
    sum += 1;
    yield;
    sum += 1;
  }
  const generator = bufferAsyncGenerator(inputGenerator(), 2);
  let result = await generator.next();
  expect(result.done).toBe(false);
  expect(sum).toBe(2);
  result = await generator.next();
  expect(result.done).toBe(false);
  expect(sum).toBe(3);
  result = await generator.next();
  expect(result.done).toBe(false);
  expect(sum).toBe(4);
  result = await generator.next();
  expect(result.done).toBe(false);
  expect(sum).toBe(4);
  result = await generator.next();
  expect(result.done).toBe(true);
});
test("bufferAsyncGenerator() yields all results", async () => {
  const p1 = promiseWithResolvers<number>();
  const p2 = promiseWithResolvers<number>();
  const p3 = promiseWithResolvers<number>();
  const p4 = promiseWithResolvers<number>();
  async function* inputGenerator() {
    yield await p1.promise;
    yield await p2.promise;
    yield await p3.promise;
    yield await p4.promise;
  }
  const generator = bufferAsyncGenerator(inputGenerator(), 2);
  let resultPromise = generator.next();
  p1.resolve(1);
  let result = await resultPromise;
  expect(result.done).toBe(false);
  expect(result.value).toBe(1);
  resultPromise = generator.next();
  p2.resolve(2);
  result = await resultPromise;
  expect(result.done).toBe(false);
  expect(result.value).toBe(2);
  resultPromise = generator.next();
  p3.resolve(3);
  result = await resultPromise;
  expect(result.done).toBe(false);
  expect(result.value).toBe(3);
  resultPromise = generator.next();
  p4.resolve(4);
  result = await resultPromise;
  expect(result.done).toBe(false);
  expect(result.value).toBe(4);
  result = await generator.next();
  expect(result.done).toBe(true);
});
test("createCallbackGenerator()", async () => {
  const { callback, generator } = createCallbackGenerator<number>();
  (async () => {
    for (let i = 0; i < 5; i++) {
      callback(i);
      await new Promise((res) => setTimeout(res, 100));
    }
  })();
  for await (const value of generator) {
    if (value === 4) break;
  }
});
</file>

<file path="packages/core/src/utils/generators.ts">
import { promiseWithResolvers } from "@/utils/promiseWithResolvers.js";
import { startClock } from "./timer.js";
/**
 * Merges multiple async generators into a single async generator.
 *
 * @param generators - The generators to merge.
 * @returns A single async generator that yields results from all input generators.
 */
export async function* mergeAsyncGenerators<T>(
  generators: AsyncGenerator<T>[],
): AsyncGenerator<T> {
  const promises = generators.map((gen) => gen.next());
  while (promises.length > 0) {
    const wrappedPromises = promises.map((promise, index) =>
      promise.then((result) => ({ index, result })),
    );
    const { result, index } = await Promise.race(wrappedPromises);
    if (result.done) {
      generators.splice(index, 1);
      promises.splice(index, 1);
    } else {
      const generator = generators[index]!;
      const promise = generator.next();
      promises.splice(index, 1);
      generators.splice(index, 1);
      generators.push(generator);
      promises.push(promise);
      yield result.value;
    }
  }
}
/**
 * Buffers the results of an async generator.
 *
 * @param generator - The generator to buffer.
 * @param size - The size of the buffer.
 * @returns An async generator that yields results from the input generator.
 */
export async function* bufferAsyncGenerator<T>(
  generator: AsyncGenerator<T>,
  size: number,
  bufferCallback?: (bufferSize: number) => void,
): AsyncGenerator<T> {
  const buffer: T[] = [];
  let done = false;
  let pwr1 = promiseWithResolvers<void>();
  let pwr2 = promiseWithResolvers<void>();
  (async () => {
    for await (const result of generator) {
      buffer.push(result);
      bufferCallback?.(buffer.length);
      pwr1.resolve();
      if (buffer.length >= size) await pwr2.promise;
      pwr2 = promiseWithResolvers<void>();
    }
    done = true;
    pwr1.resolve();
  })();
  while (done === false || buffer.length > 0) {
    if (buffer.length > 0) {
      pwr2.resolve();
      yield buffer.shift()!;
    } else {
      await pwr1.promise;
      pwr1 = promiseWithResolvers<void>();
    }
  }
}
/**
 * Drains an async generator into an array.
 *
 * @param asyncGenerator - The async generator to drain.
 * @returns An array of results from the input generator.
 */
export async function drainAsyncGenerator<T>(
  asyncGenerator: AsyncGenerator<T>,
): Promise<T[]> {
  const result: T[] = [];
  for await (const events of asyncGenerator) {
    result.push(events);
  }
  return result;
}
/**
 * Records the total time taken to yield results from an async generator.
 *
 * @param asyncGenerator - The async generator to record.
 * @param callback - A callback function that receives duration metrics.
 * @returns An async generator that yields results from the input generator.
 */
export async function* recordAsyncGenerator<T>(
  asyncGenerator: AsyncGenerator<T>,
  callback: (params: { await: number; yield: number; total: number }) => void,
): AsyncGenerator<T> {
  let endClockTotal = startClock();
  for await (const result of asyncGenerator) {
    const endClockInner = startClock();
    yield result;
    callback({
      await: endClockTotal() - endClockInner(),
      yield: endClockInner(),
      total: endClockTotal(),
    });
    endClockTotal = startClock();
  }
}
/**
 * Creates an async generator that yields values from a callback.
 */
export function createCallbackGenerator<T>(
  bufferCallback?: (bufferSize: number) => void,
): {
  callback: (value: T) => void;
  generator: AsyncGenerator<T, void, unknown>;
} {
  const buffer: T[] = [];
  let pwr = promiseWithResolvers<void>();
  const callback = (value: T) => {
    buffer.push(value);
    bufferCallback?.(buffer.length);
    pwr.resolve();
  };
  async function* generator() {
    while (true) {
      if (buffer.length > 0) {
        yield buffer.shift()!;
      } else {
        await pwr.promise;
        pwr = promiseWithResolvers<void>();
      }
    }
  }
  return { callback, generator: generator() };
}
</file>

<file path="packages/core/src/utils/hash.ts">
import { createHash } from "node:crypto";
// Adapted from https://stackoverflow.com/questions/77336994/stronger-type-for-value-in-json-stringifyvalue-any
type JSONSerializable =
  | string
  | number
  | boolean
  | null
  | JSONObject
  | JSONArray;
type JSONObject = { [key: string]: JSONSerializable };
type JSONArray = Array<JSONSerializable>;
/**
 * Generates a 10-character hexadecimal hash of a JSON-serializable value.
 */
export function hash(value: JSONSerializable): string {
  return createHash("sha256")
    .update(JSON.stringify(value))
    .digest("hex")
    .slice(0, 10);
}
</file>

<file path="packages/core/src/utils/interval.test.ts">
import { expect, test } from "vitest";
import {
  getChunks,
  intervalDifference,
  intervalIntersection,
  intervalSum,
  intervalUnion,
  sortIntervals,
} from "./interval.js";
test("intervalSum handles empty input", () => {
  const result = intervalSum([]);
  expect(result).toEqual(0);
});
test("intervalSum calculates the sum of intervals", () => {
  const result = intervalSum([
    [1, 3],
    [5, 7],
    [10, 12],
  ]);
  expect(result).toEqual(9);
});
test("intervalSum calculates the sum of single-point intervals", () => {
  const result = intervalSum([
    [1, 1],
    [3, 3],
    [5, 5],
  ]);
  expect(result).toEqual(3);
});
test("intervalUnion handles empty input", () => {
  const result = intervalUnion([]);
  expect(result).toEqual([]);
});
test("intervalUnion merges overlapping intervals", () => {
  const result = intervalUnion([
    [1, 3],
    [2, 4],
    [6, 7],
  ]);
  expect(result).toEqual([
    [1, 4],
    [6, 7],
  ]);
});
test("intervalUnion merges adjacent intervals", () => {
  const result = intervalUnion([
    [1, 3],
    [4, 5],
    [6, 6],
  ]);
  expect(result).toEqual([[1, 6]]);
});
test("intervalUnion handles non-overlapping intervals", () => {
  const result = intervalUnion([
    [1, 3],
    [5, 7],
    [9, 12],
  ]);
  expect(result).toEqual([
    [1, 3],
    [5, 7],
    [9, 12],
  ]);
});
test("intervalUnion removes duplicate intervals", () => {
  const result = intervalUnion([
    [1, 3],
    [1, 3],
    [2, 6],
    [1, 3],
  ]);
  expect(result).toEqual([[1, 6]]);
});
test("intervalUnion does not mutate inputs", () => {
  const intervals = [
    [3, 5],
    [1, 2],
    [4, 6],
  ] satisfies [number, number][];
  const originalIntervals = JSON.parse(JSON.stringify(intervals));
  intervalUnion(intervals);
  // Asserting that the original intervals array has not been modified
  expect(intervals).toEqual(originalIntervals);
});
test("intervalIntersection handles empty input", () => {
  const result = intervalIntersection([], []);
  expect(result).toEqual([]);
});
test("intervalIntersection correctly finds intersections", () => {
  const result = intervalIntersection(
    [
      [1, 5],
      [3, 7],
      [9, 12],
    ],
    [
      [2, 4],
      [4, 6],
      [10, 11],
    ],
  );
  expect(result).toEqual([
    [2, 6],
    [10, 11],
  ]);
});
test("intervalIntersection handles no intersections", () => {
  const result = intervalIntersection(
    [
      [1, 3],
      [5, 7],
      [9, 12],
    ],
    [
      [4, 4],
      [8, 8],
    ],
  );
  expect(result).toEqual([]);
});
test("intervalDifference handles empty inputs", () => {
  const result = intervalDifference([], []);
  expect(result).toEqual([]);
});
test("intervalDifference correctly finds differences", () => {
  const result = intervalDifference(
    [
      [1, 5],
      [7, 10],
    ],
    [
      [2, 4],
      [8, 10],
    ],
  );
  expect(result).toEqual([
    [1, 1],
    [5, 5],
    [7, 7],
  ]);
});
test("intervalDifference handles no difference", () => {
  const result = intervalDifference(
    [
      [1, 3],
      [5, 7],
    ],
    [
      [0, 0],
      [4, 4],
    ],
  );
  expect(result).toEqual([
    [1, 3],
    [5, 7],
  ]);
});
test("intervalDifference handles full difference", () => {
  const result = intervalDifference(
    [
      [1, 5],
      [4, 7],
    ],
    [[0, 8]],
  );
  expect(result).toEqual([]);
});
test("intervalDifference does not mutate inputs", () => {
  const initial = [6, 17] satisfies [number, number];
  const remove = [
    [0, 10],
    [12, 15],
  ] satisfies [number, number][];
  intervalDifference([initial], remove);
  expect(initial).toStrictEqual([6, 17]);
});
test("sortIntervals", () => {
  let result = sortIntervals([
    [1, 5],
    [4, 7],
  ]);
  expect(result).toStrictEqual([
    [1, 5],
    [4, 7],
  ]);
  result = sortIntervals([
    [4, 7],
    [1, 5],
  ]);
  expect(result).toStrictEqual([
    [1, 5],
    [4, 7],
  ]);
});
test("getChunks", () => {
  const result = getChunks({ interval: [1, 9], maxChunkSize: 2 });
  expect(result).toStrictEqual([
    [1, 2],
    [3, 4],
    [5, 6],
    [7, 8],
    [9, 9],
  ]);
});
</file>

<file path="packages/core/src/utils/interval.ts">
import { range } from "./range.js";
export type Interval = [number, number];
/**
 * Return the total sum of a list of numeric intervals.
 *
 * @param intervals List of numeric intervals to find the sum of.
 * @returns Sum of the intervals.
 */
export function intervalSum(intervals: Interval[]) {
  let totalSum = 0;
  for (const [start, end] of intervals) {
    totalSum += end - start + 1;
  }
  return totalSum;
}
/**
 * Return the union of a list of numeric intervals.
 *
 * @param intervals List of numeric intervals to find the union of.
 * @returns Union of the intervals, represented as a list of intervals.
 */
export function intervalUnion(intervals_: Interval[]) {
  if (intervals_.length === 0) return [];
  // Create copies to avoid mutating the originals.
  const intervals = intervals_.map((interval) => [...interval] as Interval);
  // Sort intervals based on the left end.
  intervals.sort((a, b) => a[0] - b[0]);
  const result: Interval[] = [];
  let currentInterval = intervals[0]!;
  for (let i = 1; i < intervals.length; i++) {
    const nextInterval = intervals[i]!;
    if (currentInterval[1] >= nextInterval[0] - 1) {
      // Merge overlapping intervals
      currentInterval[1] = Math.max(currentInterval[1], nextInterval[1]);
    } else {
      // No overlap, add current interval to result
      result.push(currentInterval);
      currentInterval = nextInterval;
    }
  }
  result.push(currentInterval); // Add the last interval
  return result;
}
/**
 * Return the intersection of two lists of numeric intervals.
 *
 * @param list1 First list of numeric intervals.
 * @param list2 Second list of numeric intervals.
 * @returns Intersection of the intervals, represented as a list of intervals.
 */
export function intervalIntersection(
  list1: Interval[],
  list2: Interval[],
): Interval[] {
  const result: Interval[] = [];
  let i = 0;
  let j = 0;
  while (i < list1.length && j < list2.length) {
    const [start1, end1] = list1[i]!;
    const [start2, end2] = list2[j]!;
    const intersectionStart = Math.max(start1, start2);
    const intersectionEnd = Math.min(end1, end2);
    if (intersectionStart <= intersectionEnd) {
      result.push([intersectionStart, intersectionEnd]);
    }
    if (end1 < end2) {
      i++;
    } else {
      j++;
    }
  }
  // Merge potentially overlapping intervals before returning.
  return intervalUnion(result);
}
/**
 * Return the intersection of many lists of numeric intervals.
 *
 * @param list1 First list of numeric intervals.
 * @param list2 Second list of numeric intervals.
 * @returns Intersection of the intervals, represented as a list of intervals.
 */
export function intervalIntersectionMany(lists: Interval[][]): Interval[] {
  if (lists.length === 0) return [];
  if (lists.length === 1) return lists[0]!;
  let result: Interval[] = lists[0]!;
  for (let i = 1; i < lists.length; i++) {
    result = intervalIntersection(result, lists[i]!);
  }
  return intervalUnion(result);
}
/**
 * Return the difference between two lists of numeric intervals (initial - remove).
 *
 * @param initial Starting/base list of numeric intervals.
 * @param remove List of numeric intervals to remove.
 * @returns Difference of the intervals, represented as a list of intervals.
 */
export function intervalDifference(
  initial: Interval[],
  remove: Interval[],
): Interval[] {
  // Create copies to avoid mutating the originals.
  const initial_ = initial.map((interval) => [...interval] as Interval);
  const remove_ = remove.map((interval) => [...interval] as Interval);
  const result: Interval[] = [];
  let i = 0;
  let j = 0;
  while (i < initial.length && j < remove.length) {
    const interval1 = initial_[i]!;
    const interval2 = remove_[j]!;
    if (interval1[1] < interval2[0]) {
      // No overlap, add interval1 to the result
      result.push(interval1);
      i++;
    } else if (interval2[1] < interval1[0]) {
      // No overlap, move to the next interval in remove
      j++;
    } else {
      // There is an overlap
      if (interval1[0] < interval2[0]) {
        // Add the left part of interval1
        result.push([interval1[0], interval2[0] - 1]);
      }
      if (interval1[1] > interval2[1]) {
        // Update interval1's start to exclude the overlap
        interval1[0] = interval2[1] + 1;
        j++;
      } else {
        // No more overlap, move to the next interval in initial
        i++;
      }
    }
  }
  // Add any remaining intervals from initial
  while (i < initial_.length) {
    result.push(initial_[i]!);
    i++;
  }
  return result;
}
/**
 * Return an interval that encompasses all the intervals in the list.
 *
 * @param intervals List of numeric intervals to find the bounds of.
 * @returns Bounds of the intervals.
 */
export function intervalBounds(intervals: Interval[]): Interval {
  const start = Math.min(...intervals.map((interval) => interval[0]));
  const end = Math.max(...intervals.map((interval) => interval[1]));
  return [start, end];
}
export function sortIntervals(intervals: Interval[]) {
  return intervals.sort((a, b) => (a[0] < b[0] ? -1 : 1));
}
export function getChunks({
  interval,
  maxChunkSize,
}: {
  interval: Interval;
  maxChunkSize: number;
}) {
  const _chunks: Interval[] = [];
  const [startBlock, endBlock] = interval;
  let fromBlock = startBlock;
  let toBlock = Math.min(fromBlock + maxChunkSize - 1, endBlock);
  while (fromBlock <= endBlock) {
    _chunks.push([fromBlock, toBlock]);
    fromBlock = toBlock + 1;
    toBlock = Math.min(fromBlock + maxChunkSize - 1, endBlock);
  }
  return _chunks;
}
export function intervalRange(interval: Interval) {
  return range(interval[0], interval[1] + 1);
}
</file>

<file path="packages/core/src/utils/lowercase.ts">
/**
 * Transforms the input string to lower case.
 */
export function toLowerCase<T extends string>(value: T) {
  return value.toLowerCase() as Lowercase<T>;
}
</file>

<file path="packages/core/src/utils/mutex.ts">
import {
  type PromiseWithResolvers,
  promiseWithResolvers,
} from "./promiseWithResolvers.js";
export const createLock = (): {
  lock: () => Promise<void>;
  unlock: () => void;
} => {
  const queue: PromiseWithResolvers<void>[] = [];
  let locked = false;
  return {
    lock: async () => {
      if (locked === false) {
        locked = true;
        return;
      }
      const pwr = promiseWithResolvers<void>();
      queue.push(pwr);
      return pwr.promise;
    },
    unlock: () => {
      if (queue.length > 0) {
        const pwr = queue.shift()!;
        pwr.resolve();
      } else {
        locked = false;
      }
    },
  };
};
</file>

<file path="packages/core/src/utils/never.ts">
export const never = (_x: never) => {
  throw "unreachable";
};
</file>

<file path="packages/core/src/utils/offset.test.ts">
import { expect, test } from "vitest";
import { parseAbiParameter } from "viem";
import {
  type TupleAbiParameter,
  getBytesConsumedByParam,
  getNestedParamOffset,
} from "./offset.js";
test("getBytesConsumedByParam returns 32 for static primitive types", () => {
  expect(getBytesConsumedByParam({ type: "uint" })).toBe(32);
  expect(getBytesConsumedByParam({ type: "uint256" })).toBe(32);
  expect(getBytesConsumedByParam({ type: "address" })).toBe(32);
});
test("getBytesConsumedByParam returns 32 for dynamic array types", () => {
  expect(getBytesConsumedByParam({ type: "int256[]" })).toBe(32);
  expect(getBytesConsumedByParam({ type: "address[]" })).toBe(32);
  expect(getBytesConsumedByParam({ type: "bytes[][4]" })).toBe(32);
});
test("getBytesConsumedByParam returns 32 for array type containing nested dynamic type", () => {
  expect(getBytesConsumedByParam({ type: "bytes[][4]" })).toBe(32);
});
test("getBytesConsumedByParam returns 32 for tuple type containing nested dynamic type", () => {
  expect(
    getBytesConsumedByParam({
      components: [
        { name: "x", type: "uint256[]" },
        { name: "y", type: "bool" },
        { name: "z", type: "address" },
      ],
      name: "fooOut",
      type: "tuple",
    }),
  ).toBe(32);
});
test("getBytesConsumedByParam returns expanded byte amount for static array type", () => {
  expect(getBytesConsumedByParam({ type: "int256[3]" })).toBe(32 * 3);
});
test("getBytesConsumedByParam returns expanded byte amount for static tuple type", () => {
  expect(
    getBytesConsumedByParam({
      components: [
        { name: "x", type: "uint256" },
        { name: "y", type: "bool" },
        { name: "z", type: "address" },
      ],
      name: "fooOut",
      type: "tuple",
    }),
  ).toBe(32 * 3);
});
test("getNestedParamOffset", () => {
  // fully static tuple
  let signature = [
    "struct Foo_a { address bar; bool v; int z; address y }",
    "Foo_a indexed foo",
  ];
  expect(
    getNestedParamOffset(
      parseAbiParameter(signature) as TupleAbiParameter,
      "y".split("."),
    ),
  ).toEqual(32 * 3);
  // fully static nested tuple
  signature = [
    "struct Bar_a { address x; address y; address z }",
    "struct Foo_b { address a; bool b; int c; Bar_a d; address e }",
    "Foo_b indexed foo",
  ];
  expect(
    getNestedParamOffset(
      parseAbiParameter(signature) as TupleAbiParameter,
      "d.y".split("."),
    ),
  ).toEqual(32 * 4);
  // dynamic nested tuple with dynamic parameter after
  signature = [
    "struct Bar_b { address x; address y; address z }",
    "struct Foo_c { address a; bool b; int c; Bar_b d; string e }",
    "Foo_c indexed foo",
  ];
  expect(
    getNestedParamOffset(
      parseAbiParameter(signature) as TupleAbiParameter,
      "d.y".split("."),
    ),
  ).toEqual(32 * 4);
  // dynamic nested tuple with dynamic parameter before
  signature = [
    "struct Bar_c { address x; address y; address z }",
    "struct Foo_d { string a; Bar_c b; int c; Bar_c d; address e }",
    "Foo_d indexed foo",
  ];
  expect(
    getNestedParamOffset(
      parseAbiParameter(signature) as TupleAbiParameter,
      "d.y".split("."),
    ),
  ).toEqual(32 * 6);
});
</file>

<file path="packages/core/src/utils/offset.ts">
import type { AbiParameter } from "abitype";
import { InvalidAbiDecodingTypeError } from "viem";
// Adapted from viem.
// https://github.com/wagmi-dev/viem/blob/5c95fafceffe7f399b....ts
export function getBytesConsumedByParam(param: AbiParameter): number {
  const arrayComponents = getArrayComponents(param.type);
  if (arrayComponents) {
    const [length, innerType] = arrayComponents;
    // If the array is dynamic or has dynamic children, it uses the
    // dynamic encoding scheme (32 byte header).
    if (!length || hasDynamicChild(param)) {
      return 32;
    }
    // If the length of the array is known in advance,
    // and the length of each element in the array is known,
    // the array data is encoded contiguously after the array.
    const bytesConsumedByInnerType = getBytesConsumedByParam({
      ...param,
      type: innerType,
    });
    return length * bytesConsumedByInnerType;
  }
  if (param.type === "tuple") {
    // If the tuple has dynamic children, it uses the dynamic encoding
    // scheme (32 byte header).
    if (hasDynamicChild(param)) {
      return 32;
    }
    // Otherwise the tuple has static children, so we can just decode
    // each component in sequence.
    let consumed = 0;
    for (const component of (param as any).components ?? []) {
      consumed += getBytesConsumedByParam(component);
    }
    return consumed;
  }
  // Otherwise, it's a dynamic string or bytes (32 bytes),
  // or a static number, address, or bool (32 bytes).
  if (
    param.type === "string" ||
    param.type.startsWith("bytes") ||
    param.type.startsWith("uint") ||
    param.type.startsWith("int") ||
    param.type === "address" ||
    param.type === "bool"
  ) {
    return 32;
  }
  throw new InvalidAbiDecodingTypeError(param.type, {
    docsPath: "/docs/contract/decodeAbiParameters",
  });
}
export type TupleAbiParameter = AbiParameter & {
  type: "tuple";
  components: readonly AbiParameter[];
};
export function getNestedParamOffset(
  param: TupleAbiParameter,
  names: string[],
): number {
  let consumed = 0;
  let isFound = false;
  for (const component of param.components) {
    if (component.name === names[0]) {
      // the end of the branch
      if (names.length === 1) {
        isFound = true;
        break;
      }
      // additional nesting
      if (component.type === "tuple") {
        const nestedConsumed = getNestedParamOffset(
          component as TupleAbiParameter,
          names.slice(1),
        );
        return consumed + nestedConsumed;
      } else {
        throw new Error(
          `Factory event parameter '${param.name}.${names.join(".")}' is not valid for ${param.name} struct signature.`,
        );
      }
    } else {
      consumed += getBytesConsumedByParam(component);
    }
  }
  if (!isFound) {
    throw new Error(
      `Factory event parameter '${param.name}.${names.join(".")}' not found in ${param.name} struct signature.`,
    );
  }
  return consumed;
}
function hasDynamicChild(param: AbiParameter) {
  const { type } = param;
  if (type === "string") return true;
  if (type === "bytes") return true;
  if (type.endsWith("[]")) return true;
  if (type === "tuple") return (param as any).components?.some(hasDynamicChild);
  const arrayComponents = getArrayComponents(param.type);
  if (
    arrayComponents &&
    hasDynamicChild({ ...param, type: arrayComponents[1] } as AbiParameter)
  )
    return true;
  return false;
}
function getArrayComponents(
  type: string,
): [length: number | null, innerType: string] | undefined {
  const matches = type.match(/^(.*)\[(\d+)?\]$/);
  return matches
    ? // Return `null` if the array is dynamic.
      [matches[2] ? Number(matches[2]) : null, matches[1]!]
    : undefined;
}
</file>

<file path="packages/core/src/utils/order.ts">
export const orderObject = (obj: any): any => {
  if (Array.isArray(obj)) return obj.map((x) => orderObject(x));
  if (typeof obj !== "object") return obj;
  const newObj = {} as any;
  for (const key of Object.keys(obj).sort()) {
    const val = obj[key];
    if (typeof val === "object") {
      newObj[key] = orderObject(obj[key]);
    } else {
      newObj[key] = obj[key];
    }
  }
  return newObj;
};
</file>

<file path="packages/core/src/utils/partition.test.ts">
import { partition } from "@/utils/partition.js";
import { expect, test } from "vitest";
test("partition", () => {
  let [left, right] = partition([1, 2, 3, 4, 5], (n) => n <= 2);
  expect(left).toStrictEqual([1, 2]);
  expect(right).toStrictEqual([3, 4, 5]);
  [left, right] = partition([1, 2, 3, 4, 5], (n) => n < 6);
  expect(left).toStrictEqual([1, 2, 3, 4, 5]);
  expect(right).toStrictEqual([]);
  [left, right] = partition([2, 5], (n) => n <= 5);
  expect(left).toStrictEqual([2, 5]);
  expect(right).toStrictEqual([]);
  [left, right] = partition([1], (n) => n > 1);
  expect(left).toStrictEqual([]);
  expect(right).toStrictEqual([1]);
  [left, right] = partition([1, 2, 3], (n) => n > 0);
  expect(left).toStrictEqual([1, 2, 3]);
  expect(right).toStrictEqual([]);
  [left, right] = partition([1, 2, 3], (n) => n > 5);
  expect(left).toStrictEqual([]);
  expect(right).toStrictEqual([1, 2, 3]);
});
</file>

<file path="packages/core/src/utils/partition.ts">
/**
 * Divides an array into two arrays, where the first array
 * contains all elements that satisfy the predicate, and the
 * second array contains all elements that do not satisfy the
 * predicate.
 *
 * Note: It is assumed that the array monotonically goes from true to false.
 *
 * @param array - The array to partition.
 * @param predicate - The predicate to partition the array by.
 *
 * @returns A tuple containing the left and right arrays.
 *
 * @example
 * ```ts
 * const [left, right] = partition([1, 2, 3, 4, 5], (n) => n <= 2);
 * // left = [1, 2]
 * // right = [3, 4, 5]
 * ```
 */
export const partition = <T>(
  array: T[],
  predicate: (item: T) => boolean,
): [T[], T[]] => {
  if (array.length === 0) {
    return [[], []];
  }
  if (predicate(array[0]!) === false) {
    return [[], array];
  }
  if (predicate(array[array.length - 1]!)) {
    return [array, []];
  }
  let low = 0;
  let high = array.length;
  while (low < high) {
    const mid = Math.floor((low + high) / 2);
    if (predicate(array[mid]!)) {
      low = mid + 1;
    } else {
      high = mid;
    }
  }
  const left = array.slice(0, low);
  const right = array.slice(low);
  return [left, right];
};
</file>

<file path="packages/core/src/utils/pg.test.ts">
import { setupCommon } from "@/_test/setup.js";
import { withStubbedEnv } from "@/_test/utils.js";
import { beforeEach, expect, test } from "vitest";
import { getDatabaseName } from "./pg.js";
beforeEach(setupCommon);
test("getDatabaseName() prioritizes connectionString values", () => {
  const name = getDatabaseName({
    connectionString: "postgres://testhost:5432/pg",
    host: "otherhost",
    port: 5666,
  });
  expect(name).toBe("testhost:5432/pg");
});
test("getDatabaseName() uses env vars", () => {
  withStubbedEnv(
    { PGHOST: "testhost", PGPORT: "5435", PGDATABASE: "testdb" },
    () => {
      const name = getDatabaseName({});
      expect(name).toBe("testhost:5435/testdb");
    },
  );
});
test("getDatabaseName() uses defaults when config/env missing", () => {
  const name = getDatabaseName({});
  expect(name).toBe("localhost:5432/");
});
test("getDatabaseName() prioritizes config over env", () => {
  withStubbedEnv({ PGDATABASE: "otherdatabase" }, () => {
    const name = getDatabaseName({
      database: "test",
      port: 5433,
      host: "testhost",
    });
    expect(name).toBe("testhost:5433/test");
  });
});
test("getDatabaseName() prioritizes connection string over other config values", () => {
  withStubbedEnv({ PGDATABASE: "otherdatabase" }, () => {
    const name = getDatabaseName({
      connectionString: "postgres://testhost:5435/database",
      database: "test",
      port: 5433,
      host: "testhost",
    });
    expect(name).toBe("testhost:5435/database");
  });
});
test("getDatabaseName() mixes defaults, env, and config", () => {
  withStubbedEnv({ PGHOST: "testhost" }, () => {
    const name = getDatabaseName({ database: "test" });
    expect(name).toBe("testhost:5432/test");
  });
});
</file>

<file path="packages/core/src/utils/pg.ts">
import type { Logger } from "@/internal/logger.js";
import pg, { type PoolConfig } from "pg";
import parse from "pg-connection-string";
// The default parser for numeric[] (1231) seems to parse values as Number
// or perhaps through JSON.parse(). Use the int8[] (1016) parser instead,
// which properly returns an array of strings.
const bigIntArrayParser = pg.types.getTypeParser(1016);
pg.types.setTypeParser(1231, bigIntArrayParser);
export const PG_BIGINT_MAX = 9223372036854775807n;
export const PG_INTEGER_MAX = 2147483647;
export function getDatabaseName(conf: PoolConfig) {
  const config = parseBaseConfig(conf);
  // https://github.com/brianc/node-postgres/blob/ecff60dc8aa0bd1ad5....js#L3-L73
  const defaults = {
    hostname: "localhost",
    port: "5432",
    database: undefined,
  };
  const envVars = {
    hostname: process.env.PGHOST,
    database: process.env.PGDATABASE,
    port: process.env.PGPORT,
  };
  // https://github.com/brianc/node-postgres/blob/ecff60dc8aa0bd1ad5...-parameters.js#L18
  // precedence is config > env vars > default
  const hostname = config.hostname || envVars.hostname || defaults.hostname;
  const database = config.database || envVars.database || defaults.database;
  const port = config.port || envVars.port || defaults.port;
  return `${hostname}:${port}/${database ?? ""}`;
}
const parseBaseConfig = (
  config: PoolConfig,
): {
  hostname?: string;
  port?: string;
  database?: string;
} => {
  const conf = {
    hostname: config.host,
    port: config.port?.toString(),
    database: config.database,
  };
  if (!config.connectionString) return conf;
  // https://github.com/brianc/node-postgres/blob/ecff60dc8aa0bd1ad5...-parameters.js#L53-L57
  // connString values override other values, even if a value from connstring is missing
  const parsed = (parse as unknown as typeof parse.parse)(
    config.connectionString,
  );
  return {
    hostname: parsed.host ?? undefined,
    database: parsed.database ?? undefined,
    port: parsed.port ?? undefined,
  };
};
export function createPool(config: PoolConfig, logger: Logger) {
  class Client extends pg.Client {
    // @ts-expect-error
    override connect(
      callback: (err: Error) => void | undefined,
    ): void | Promise<void> {
      if (callback) {
        super.connect(() => {
          this.query(
            `
            SET synchronous_commit = off;
            SET idle_in_transaction_session_timeout = 3600000;`,
            callback,
          );
        });
      } else {
        return super.connect().then(() =>
          this.query(`
            SET synchronous_commit = off;
            SET idle_in_transaction_session_timeout = 3600000;`).then(() => {}),
        );
      }
    }
  }
  const pool = new pg.Pool({
    // https://stackoverflow.com/questions/59155572/how-to-set-query-timeout-in-relation-to-statement-timeout
    statement_timeout: 2 * 60 * 1000, // 2 minutes
    // @ts-expect-error: The custom Client is an undocumented option.
    Client: Client,
    ...config,
  });
  function onPoolError(error: Error) {
    const client = (error as any).client as any | undefined;
    const pid = (client?.processID as number | undefined) ?? "unknown";
    const applicationName =
      (client?.connectionParameters?.application_name as string | undefined) ??
      "unknown";
    logger.warn({
      msg: "Postgres pool error",
      application_name: applicationName,
      pid,
      error,
    });
    // NOTE: Errors thrown here cause an uncaughtException. It's better to just log and ignore -
    // if the underlying problem persists, the process will crash due to downstream effects.
  }
  function onClientError(error: Error) {
    logger.warn({ msg: "Postgres client error", error });
    // NOTE: Errors thrown here cause an uncaughtException. It's better to just log and ignore -
    // if the underlying problem persists, the process will crash due to downstream effects.
  }
  function onNotice(notice: { message?: string; code?: string }) {
    const level =
      typeof notice.code === "string" &&
      ["42P06", "42P07"].includes(notice.code)
        ? "trace"
        : "debug";
    logger[level]({
      msg: "Postgres notice",
      message: notice.message,
      code: notice.code,
    });
  }
  pool.on("error", onPoolError);
  pool.on("connect", (client) => {
    client.on("notice", onNotice);
    client.on("error", onClientError);
  });
  return pool;
}
export function createReadonlyPool(
  config: PoolConfig,
  logger: Logger,
  namespace: string,
) {
  class ReadonlyClient extends pg.Client {
    // @ts-expect-error
    override connect(
      callback: (err: Error) => void | undefined,
    ): void | Promise<void> {
      if (callback) {
        super.connect(() => {
          this.query(
            `
          SET search_path = "${namespace}";
          SET SESSION CHARACTERISTICS AS TRANSACTION READ ONLY;
          SET work_mem = '512MB';
          SET lock_timeout = '500ms';`,
            callback,
          );
        });
      } else {
        return super.connect().then(() =>
          this.query(
            `
          SET search_path = "${namespace}";
          SET SESSION CHARACTERISTICS AS TRANSACTION READ ONLY;
          SET work_mem = '512MB';
          SET lock_timeout = '500ms';`,
          ).then(() => {}),
        );
      }
    }
  }
  const pool = new pg.Pool({
    // https://stackoverflow.com/questions/59155572/how-to-set-query-timeout-in-relation-to-statement-timeout
    statement_timeout: 30 * 1000, // 30s
    // @ts-expect-error: The custom Client is an undocumented option.
    Client: ReadonlyClient,
    ...config,
  });
  function onPoolError(error: Error) {
    const client = (error as any).client as any | undefined;
    const pid = (client?.processID as number | undefined) ?? "unknown";
    const applicationName =
      (client?.connectionParameters?.application_name as string | undefined) ??
      "unknown";
    logger.warn({
      msg: "Postgres pool error",
      application_name: applicationName,
      pid,
      error,
    });
    // NOTE: Errors thrown here cause an uncaughtException. It's better to just log and ignore -
    // if the underlying problem persists, the process will crash due to downstream effects.
  }
  function onClientError(error: Error) {
    logger.warn({ msg: "Postgres client error", error });
    // NOTE: Errors thrown here cause an uncaughtException. It's better to just log and ignore -
    // if the underlying problem persists, the process will crash due to downstream effects.
  }
  function onNotice(notice: { message?: string; code?: string }) {
    const level =
      typeof notice.code === "string" &&
      ["42P06", "42P07"].includes(notice.code)
        ? "trace"
        : "debug";
    logger[level]({
      msg: "Postgres notice",
      message: notice.message,
      code: notice.code,
    });
  }
  pool.on("error", onPoolError);
  pool.on("connect", (client) => {
    client.on("notice", onNotice);
    client.on("error", onClientError);
  });
  return pool;
}
</file>

<file path="packages/core/src/utils/pglite.ts">
import { mkdirSync } from "node:fs";
import type { Prettify } from "@/types/utils.js";
import { type PGliteOptions as Options, PGlite } from "@electric-sql/pglite";
import {
  CompiledQuery,
  type DatabaseConnection,
  type Dialect,
  type Kysely,
  PostgresAdapter,
  PostgresIntrospector,
  PostgresQueryCompiler,
  type QueryResult,
  type TransactionSettings,
} from "kysely";
export type PGliteOptions = Prettify<Options & { dataDir: string }>;
export function createPglite(options: PGliteOptions) {
  // PGlite uses the memory FS by default, and Windows doesn't like the
  // "memory://" path, so it's better to pass `undefined` here.
  if (options.dataDir === "memory://") {
    // @ts-expect-error
    options.dataDir = undefined;
  } else {
    mkdirSync(options.dataDir, { recursive: true });
  }
  return new PGlite(options);
}
// Adapted from dnlsandiego/kysely-pglite
// https://github.com/dnlsandiego/kysely-pglite/blob/3891a0c4d9327a21bff2...-pglite.ts
export function createPgliteKyselyDialect(instance: PGlite) {
  return {
    createAdapter: () => new PostgresAdapter(),
    createDriver: () => new PGliteDriver(instance),
    createIntrospector: (db: Kysely<any>) => new PostgresIntrospector(db),
    createQueryCompiler: () => new PostgresQueryCompiler(),
  } satisfies Dialect;
}
// Adapted from dnlsandiego/kysely-pglite
// https://github.com/dnlsandiego/kysely-pglite/blob/3891a0c4d9327a21bff2...-driver.ts
export class PGliteDriver {
  #client: PGlite;
  constructor(client: PGlite) {
    this.#client = client;
  }
  async acquireConnection(): Promise<DatabaseConnection> {
    return new PGliteConnection(this.#client);
  }
  async beginTransaction(
    connection: DatabaseConnection,
    _settings: TransactionSettings,
  ): Promise<void> {
    await connection.executeQuery(CompiledQuery.raw("BEGIN"));
  }
  async commitTransaction(connection: DatabaseConnection): Promise<void> {
    await connection.executeQuery(CompiledQuery.raw("COMMIT"));
  }
  async rollbackTransaction(connection: DatabaseConnection): Promise<void> {
    await connection.executeQuery(CompiledQuery.raw("ROLLBACK"));
  }
  async destroy(): Promise<void> {
    await this.#client.close();
  }
  async init(): Promise<void> {}
  async releaseConnection(_connection: DatabaseConnection): Promise<void> {}
}
class PGliteConnection implements DatabaseConnection {
  #client: PGlite;
  constructor(client: PGlite) {
    this.#client = client;
  }
  async executeQuery<R>(
    compiledQuery: CompiledQuery<any>,
  ): Promise<QueryResult<R>> {
    return await this.#client.query<R>(compiledQuery.sql, [
      ...compiledQuery.parameters,
    ]);
  }
  // biome-ignore lint/correctness/useYield: <explanation>
  async *streamQuery(): AsyncGenerator<never, void, unknown> {
    throw new Error("PGlite does not support streaming.");
  }
}
</file>

<file path="packages/core/src/utils/port.test.ts">
import { context, setupCommon } from "@/_test/setup.js";
import { beforeEach, expect, test } from "vitest";
import { getNextAvailablePort } from "./port.js";
beforeEach(setupCommon);
test("port", async () => {
  const port = await getNextAvailablePort({ common: context.common });
  expect(port).toBe(42069);
});
</file>

<file path="packages/core/src/utils/port.ts">
import http from "node:http";
import type { Common } from "@/internal/common.js";
export const getNextAvailablePort = async ({ common }: { common: Common }) => {
  const server = http.createServer();
  let port = common.options.port;
  return new Promise<number>((resolve, reject) => {
    server.once("error", (error: Error & { code: string }) => {
      if (error.code === "EADDRINUSE") {
        common.logger.warn({
          msg: "Port in use",
          port,
        });
        port += 1;
        setTimeout(() => {
          server.listen(port, common.options.hostname);
        }, 5);
      } else {
        reject(error);
      }
    });
    server.once("listening", () => {
      // Port is available
      resolve(port);
    });
    server.listen(port, common.options.hostname);
  }).finally(() => {
    server.close();
  });
};
</file>

<file path="packages/core/src/utils/print.ts">
// Adapted from viem.
// https://github.com/wagmi-dev/viem/blob/021ce8e5a3fb02db61....ts#L6-L19
export function prettyPrint(
  args?: Record<string, bigint | number | string | undefined | false | unknown>,
) {
  if (args === undefined) return "(undefined)";
  const entries = Object.entries(args)
    .map(([key, value]) => {
      if (value === undefined) return null;
      const trimmedValue =
        typeof value === "string" && value.length > 80
          ? value.slice(0, 80).concat("...")
          : value;
      return [key, trimmedValue];
    })
    .filter(Boolean) as [string, string][];
  if (entries.length === 0) return "  (empty object)";
  const maxLength = entries.reduce(
    (acc, [key]) => Math.max(acc, key.length),
    0,
  );
  return entries
    .map(([key, value]) => `  ${`${key}`.padEnd(maxLength + 1)}  ${value}`)
    .join("\n");
}
</file>

<file path="packages/core/src/utils/promiseAllSettledWithThrow.test.ts">
import { expect, test } from "vitest";
import { promiseAllSettledWithThrow } from "./promiseAllSettledWithThrow.js";
test("promiseAllSettledWithThrow", async () => {
  await expect(
    promiseAllSettledWithThrow([
      Promise.resolve(1),
      new Promise((resolve) => setTimeout(() => resolve(2), 10)),
      new Promise((_, reject) => setTimeout(() => reject(new Error("1")), 10)),
      new Promise((_, reject) => setTimeout(() => reject(new Error("2")), 20)),
    ]),
  ).rejects.toThrowError("1");
});
</file>

<file path="packages/core/src/utils/promiseAllSettledWithThrow.ts">
/**
 * Like `Promise.allSettled` but throws if any of the promises reject.
 *
 * @dev This is very useful when dealing with multiple concurrent promises
 * in a database transaction.
 */
export async function promiseAllSettledWithThrow<T>(
  promises: Promise<T>[],
): Promise<T[]> {
  let firstError: Error | undefined;
  const result = await Promise.all(
    promises.map((promise) =>
      promise.catch((error) => {
        if (firstError === undefined) {
          firstError = error;
        }
      }),
    ),
  );
  if (firstError === undefined) {
    return result as T[];
  }
  throw firstError;
}
</file>

<file path="packages/core/src/utils/promiseWithResolvers.test-d.ts">
import { assertType, test } from "vitest";
import { promiseWithResolvers } from "./promiseWithResolvers.js";
test("resolve type", () => {
  const { resolve } = promiseWithResolvers<number>();
  assertType<(arg: number) => void>(resolve);
});
test("promise type", () => {
  const { promise } = promiseWithResolvers<number>();
  assertType<Promise<number>>(promise);
});
</file>

<file path="packages/core/src/utils/promiseWithResolvers.test.ts">
import { expect, test } from "vitest";
import { promiseWithResolvers } from "./promiseWithResolvers.js";
test("resolves", async () => {
  const { promise, resolve } = promiseWithResolvers<number>();
  resolve(1);
  const value = await promise;
  expect(value).toBe(1);
});
test("rejects", async () => {
  let rejected = false;
  const { promise, reject } = promiseWithResolvers();
  promise.catch(() => {
    rejected = true;
  });
  await Promise.reject().catch(reject);
  expect(rejected).toBe(true);
});
</file>

<file path="packages/core/src/utils/promiseWithResolvers.ts">
export type PromiseWithResolvers<TPromise> = {
  resolve: (arg: TPromise) => void;
  reject: (error: Error) => void;
  promise: Promise<TPromise>;
};
/**
 * @description Application level polyfill.
 */
export const promiseWithResolvers = <
  TPromise,
>(): PromiseWithResolvers<TPromise> => {
  let resolve: (arg: TPromise) => void;
  let reject: (error: Error) => void;
  const promise = new Promise<TPromise>((_resolve, _reject) => {
    resolve = _resolve;
    reject = _reject;
  });
  return { resolve: resolve!, reject: reject!, promise };
};
</file>

<file path="packages/core/src/utils/queue.test-d.ts">
import { assertType, test } from "vitest";
import { createQueue } from "./queue.js";
test("add type", () => {
  const queue = createQueue({
    concurrency: 1,
    worker: (_arg: "a" | "b" | "c") => Promise.resolve(),
  });
  assertType<(task: "a" | "b" | "c") => Promise<void>>(queue.add);
});
</file>

<file path="packages/core/src/utils/queue.test.ts">
import { expect, test, vi } from "vitest";
import { promiseWithResolvers } from "./promiseWithResolvers.js";
import { createQueue } from "./queue.js";
test("add resolves", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    initialStart: true,
    browser: false,
    worker: () => Promise.resolve(1),
  });
  const promise = queue.add();
  expect(await promise).toBe(1);
});
test("add rejects", async () => {
  let rejected = false;
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.reject(),
  });
  const promise = queue.add();
  await queue.start();
  await promise.catch(() => {
    rejected = true;
  });
  expect(rejected).toBe(true);
});
test("size", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  expect(queue.size()).toBe(1);
  await queue.start();
  expect(queue.size()).toBe(0);
});
test("pending", async () => {
  const { promise, resolve } = promiseWithResolvers<void>();
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    initialStart: true,
    browser: false,
    worker: () => promise,
  });
  queue.add();
  expect(await queue.pending()).toBe(1);
  resolve();
  expect(await queue.pending()).toBe(0);
});
test("clear", () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  queue.add();
  queue.add();
  queue.clear();
  expect(queue.size()).toBe(0);
});
test("clear timer", async () => {
  const queue = createQueue({
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  queue.add();
  queue.add();
  await queue.start();
  queue.clear();
  await queue.onIdle();
  expect(queue.size()).toBe(0);
  expect(await queue.pending()).toBe(0);
});
test("isStarted", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  expect(queue.isStarted()).toBe(false);
  await queue.start();
  expect(queue.isStarted()).toBe(true);
});
test("initial start", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    initialStart: true,
    worker: () => Promise.resolve(),
  });
  expect(queue.isStarted()).toBe(true);
  await queue.add();
  expect(queue.size()).toBe(0);
});
test("start", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  const promise = queue.add();
  expect(queue.size()).toBe(1);
  await queue.start();
  expect(queue.isStarted()).toBe(true);
  await promise;
  expect(queue.size()).toBe(0);
});
test("pause", () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    initialStart: true,
    worker: () => Promise.resolve(),
  });
  queue.pause();
  queue.add();
  expect(queue.size()).toBe(1);
});
test.todo("restart");
test("onIdle short loop", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  await queue.onIdle();
});
test("onIdle", async () => {
  const queue = createQueue({
    concurrency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  const promise = queue.onIdle();
  await queue.start();
  await promise;
});
test("onIdle twice", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  queue.onIdle();
  await queue.start();
  queue.pause();
  queue.add();
  const promise = queue.onIdle();
  await queue.start();
  await promise;
});
test("onEmpty short loop", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  await queue.onEmpty();
});
test("onEmpty", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  const promise = queue.onEmpty();
  await queue.start();
  await promise;
});
test("onEmpty twice", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  queue.onEmpty();
  await queue.start();
  queue.pause();
  queue.add();
  const promise = queue.onEmpty();
  await queue.start();
  await promise;
});
test("concurrency", async () => {
  const func = vi.fn(() => Promise.resolve());
  const queue = createQueue({
    concurrency: 2,
    frequency: 5,
    browser: false,
    worker: func,
  });
  queue.add();
  queue.add();
  queue.add();
  queue.add();
  await queue.start();
  queue.pause();
  expect(queue.size()).toBe(2);
  expect(func).toHaveBeenCalledTimes(2);
});
test("frequency", async () => {
  const func = vi.fn(() => Promise.resolve());
  const queue = createQueue({
    frequency: 2,
    concurrency: 5,
    browser: false,
    worker: func,
  });
  queue.add();
  queue.add();
  queue.add();
  queue.add();
  await queue.start();
  expect(queue.size()).toBe(2);
  expect(func).toHaveBeenCalledTimes(2);
  await new Promise((resolve) => setTimeout(resolve, 1_010));
  expect(queue.size()).toBe(0);
  expect(func).toHaveBeenCalledTimes(4);
});
/**
 * Two queues running at the same time should alternate between events.
 * One queue running all its event in a row would mean the event loop
 * is being "starved".
 */
test("event loop", async () => {
  const out: number[] = [];
  const queue1 = createQueue({
    concurrency: 1,
    browser: false,
    worker: () => {
      out.push(1);
      return Promise.resolve();
    },
  });
  const queue2 = createQueue({
    concurrency: 1,
    browser: false,
    worker: () => {
      out.push(2);
      return Promise.resolve();
    },
  });
  for (let i = 0; i < 10; i++) {
    queue1.add();
    queue2.add();
  }
  await Promise.all([queue1.start(), queue2.start()]);
  await Promise.all([queue1.onIdle(), queue2.onIdle()]);
  const expectedOut: number[] = [];
  for (let i = 0; i < 10; i++) {
    expectedOut.push(1, 2);
  }
  expect(out).toStrictEqual(expectedOut);
});
test("update parameters", async () => {
  const func = vi.fn(() => Promise.resolve());
  const queue = createQueue({
    concurrency: 2,
    frequency: 5,
    browser: false,
    worker: func,
  });
  queue.add();
  queue.add();
  queue.add();
  queue.add();
  queue.add();
  queue.add();
  await queue.start();
  queue.pause();
  expect(queue.size()).toBe(4);
  expect(func).toHaveBeenCalledTimes(2);
  queue.setParameters({ concurrency: undefined, frequency: 8 });
  await queue.start();
  queue.pause();
  expect(queue.size()).toBe(0);
  expect(func).toHaveBeenCalledTimes(6);
});
</file>

<file path="packages/core/src/utils/queue.ts">
import {
  type PromiseWithResolvers,
  promiseWithResolvers,
} from "./promiseWithResolvers.js";
export type InnerQueue<returnType, taskType> = {
  task: taskType;
  resolve: (arg: returnType) => void;
  reject: (error: Error) => void;
}[];
export type Queue<returnType, taskType> = {
  size: () => number;
  pending: () => Promise<number>;
  add: (task: taskType) => Promise<returnType>;
  clear: (
    callback?: (e: InnerQueue<returnType, taskType>[number]) => void,
  ) => void;
  isStarted: () => boolean;
  start: () => Promise<void>;
  pause: () => void;
  onIdle: () => Promise<void>;
  onEmpty: () => Promise<void>;
  setParameters: (
    parameters: Pick<
      CreateQueueParameters<unknown, unknown>,
      "frequency" | "concurrency"
    >,
  ) => void;
};
export type CreateQueueParameters<returnType, taskType> = {
  worker: (task: taskType) => Promise<returnType>;
  initialStart?: boolean;
  browser?: boolean;
} & (
  | {
      concurrency: number;
      frequency: number;
    }
  | { concurrency: number; frequency?: undefined }
  | { concurrency?: undefined; frequency: number }
);
const validateParameters = ({
  concurrency,
  frequency,
}: Pick<
  CreateQueueParameters<unknown, unknown>,
  "frequency" | "concurrency"
>) => {
  if (concurrency === undefined && frequency === undefined) {
    throw new Error(
      "Invalid queue configuration, must specify either 'concurrency' or 'frequency'.",
    );
  }
  if (concurrency !== undefined && concurrency <= 0) {
    throw new Error(
      `Invalid value for queue 'concurrency' option. Got ${concurrency}, expected a number greater than zero.`,
    );
  }
  if (frequency !== undefined && frequency <= 0) {
    throw new Error(
      `Invalid value for queue 'frequency' option. Got ${frequency}, expected a number greater than zero.`,
    );
  }
};
export const createQueue = <returnType, taskType = void>({
  worker,
  initialStart = false,
  browser = true,
  ..._parameters
}: CreateQueueParameters<returnType, taskType>): Queue<
  returnType,
  taskType
> => {
  validateParameters(_parameters);
  const parameters: Pick<
    CreateQueueParameters<unknown, unknown>,
    "frequency" | "concurrency"
  > = _parameters;
  let queue = new Array<InnerQueue<returnType, taskType>[number]>();
  let pending = 0;
  let timestamp = 0;
  let requests = 0;
  let isStarted = initialStart;
  let timer: NodeJS.Timeout | undefined;
  let emptyPromiseWithResolvers:
    | (PromiseWithResolvers<void> & { completed: boolean })
    | undefined = undefined;
  let idlePromiseWithResolvers:
    | (PromiseWithResolvers<void> & { completed: boolean })
    | undefined = undefined;
  const next = () => {
    if (!isStarted) return;
    const _timestamp = Date.now();
    if (Math.floor(_timestamp / 1_000) !== timestamp) {
      requests = 0;
      timestamp = Math.floor(_timestamp / 1_000);
    }
    if (timer) return;
    while (
      (parameters.frequency !== undefined
        ? requests < parameters.frequency
        : true) &&
      (parameters.concurrency !== undefined
        ? pending < parameters.concurrency
        : true) &&
      queue.length > 0
    ) {
      const { task, resolve, reject } = queue.shift()!;
      requests++;
      pending++;
      worker(task)
        .then(resolve)
        .catch(reject)
        .finally(() => {
          pending--;
          if (
            idlePromiseWithResolvers !== undefined &&
            queue.length === 0 &&
            pending === 0
          ) {
            idlePromiseWithResolvers.resolve();
            idlePromiseWithResolvers.completed = true;
          }
          browser ? next() : process.nextTick(next);
        });
      if (emptyPromiseWithResolvers !== undefined && queue.length === 0) {
        emptyPromiseWithResolvers.resolve();
        emptyPromiseWithResolvers.completed = true;
      }
    }
    if (
      parameters.frequency !== undefined &&
      requests >= parameters.frequency
    ) {
      timer = setTimeout(
        () => {
          timer = undefined;
          next();
        },
        1_000 - (_timestamp % 1_000),
      );
      return;
    }
  };
  return {
    size: () => queue.length,
    pending: () => {
      if (browser) {
        return new Promise<number>((resolve) =>
          setTimeout(() => resolve(pending)),
        );
      } else {
        return new Promise<number>((resolve) =>
          setImmediate(() => resolve(pending)),
        );
      }
    },
    add: (task: taskType) => {
      const { promise, resolve, reject } = promiseWithResolvers<returnType>();
      queue.push({ task, resolve, reject });
      next();
      return promise;
    },
    clear: (callback) => {
      if (callback) {
        for (const e of queue) {
          callback(e);
        }
      }
      queue = new Array<InnerQueue<returnType, taskType>[number]>();
      clearTimeout(timer);
      timer = undefined;
    },
    isStarted: () => isStarted,
    start: () => {
      if (browser) {
        return new Promise<number>((resolve) =>
          setTimeout(() => resolve(pending)),
        ).then(() => {
          isStarted = true;
          next();
        });
      } else {
        return new Promise<number>((resolve) =>
          process.nextTick(() => resolve(pending)),
        ).then(() => {
          isStarted = true;
          next();
        });
      }
    },
    pause: () => {
      isStarted = false;
    },
    onIdle: () => {
      if (
        idlePromiseWithResolvers === undefined ||
        idlePromiseWithResolvers.completed
      ) {
        if (queue.length === 0 && pending === 0) return Promise.resolve();
        idlePromiseWithResolvers = {
          ...promiseWithResolvers<void>(),
          completed: false,
        };
      }
      return idlePromiseWithResolvers.promise;
    },
    onEmpty: () => {
      if (
        emptyPromiseWithResolvers === undefined ||
        emptyPromiseWithResolvers.completed
      ) {
        if (queue.length === 0) return Promise.resolve();
        emptyPromiseWithResolvers = {
          ...promiseWithResolvers<void>(),
          completed: false,
        };
      }
      return emptyPromiseWithResolvers.promise;
    },
    setParameters: (_parameters) => {
      validateParameters(_parameters);
      if ("frequency" in _parameters) {
        parameters.frequency = _parameters.frequency;
      }
      if ("concurrency" in _parameters) {
        parameters.concurrency = _parameters.concurrency;
      }
    },
  } as Queue<returnType, taskType>;
};
</file>

<file path="packages/core/src/utils/range.ts">
/**
 * Generates an array of integers between two bounds. Exclusive on the right.
 *
 * @param start Integer to start at.
 * @param stop Integer to stop at (exclusive).
 */
export const range = (start: number, stop: number) =>
  Array.from({ length: stop - start }, (_, i) => start + i);
</file>

<file path="packages/core/src/utils/result.ts">
export type Result<T> =
  | ([T] extends [never]
      ? { status: "success" }
      : { status: "success"; result: T })
  | { status: "error"; error: Error };
export type MergeResults<T extends readonly Result<any>[]> =
  T extends readonly [
    infer Head extends Result<unknown>,
    ...infer Tail extends Result<unknown>[],
  ]
    ? [Extract<Head, { status: "success" }>["result"], ...MergeResults<Tail>]
    : [];
export const mergeResults = <const T extends readonly Result<unknown>[]>(
  results: T,
): Result<MergeResults<T>> => {
  for (const result of results) {
    if (result.status === "error") {
      return result;
    }
  }
  // @ts-ignore
  return { status: "success", result: results.map((result) => result.result) };
};
</file>

<file path="packages/core/src/utils/sql-parse.test.ts">
import { eq, onchainTable, relations } from "@/index.js";
import { createClient } from "@ponder/client";
import { expect, test } from "vitest";
import {
  getSQLQueryRelations,
  parseSQLQuery,
  validateAllowableSQLQuery,
} from "./sql-parse.js";
test("getSQLQueryRelations", async () => {
  let relations = await getSQLQueryRelations(
    "SELECT col FROM users JOIN metadata ON users.id = metadata.id;",
  );
  expect(relations).toMatchInlineSnapshot(`
    Set {
      "users",
      "metadata",
    }
  `);
  relations = await getSQLQueryRelations("UPDATE users SET col = $1;");
  expect(relations).toMatchInlineSnapshot(`
    Set {
      "users",
    }
  `);
  relations = await getSQLQueryRelations(
    "DELETE FROM user_schema.table WHERE id = $1;",
  );
  expect(relations).toMatchInlineSnapshot(`
    Set {
      "table",
    }
  `);
});
test("validateAllowableSQLQuery()", async () => {
  await validateAllowableSQLQuery("SELECT * FROM users;");
  await validateAllowableSQLQuery("SELECT u.name FROM users as u;");
  await validateAllowableSQLQuery("SELECT col FROM users LIMIT 1;");
  await validateAllowableSQLQuery(
    "SELECT col FROM users JOIN users ON users.id = users.id;",
  );
  await validateAllowableSQLQuery(
    "SELECT col FROM users JOIN users ON users.id = users.id GROUP BY col ORDER BY col;",
  );
  await validateAllowableSQLQuery(
    "WITH cte AS (SELECT * FROM users) SELECT * FROM cte;",
  );
  await validateAllowableSQLQuery(
    "SELECT * FROM users UNION ALL SELECT * FROM admins;",
  );
  await validateAllowableSQLQuery("SELECT * FROM users WHERE col <> $1;");
});
test("validateAllowableSQLQuery() cache", async () => {
  await validateAllowableSQLQuery("SELECT * FROM users;");
  await validateAllowableSQLQuery("SELECT * FROM users;");
  await expect(
    validateAllowableSQLQuery(`SET statement_timeout = '1s';`),
  ).rejects.toThrow();
  await expect(
    validateAllowableSQLQuery(`SET statement_timeout = '1s';`),
  ).rejects.toThrow();
});
test("validateAllowableSQLQuery() select into", async () => {
  await expect(
    validateAllowableSQLQuery("SELECT * INTO users;"),
  ).rejects.toThrow();
});
test("validateAllowableSQLQuery() schema name", async () => {
  await expect(
    validateAllowableSQLQuery("SELECT * FROM offchain.metadata;"),
  ).rejects.toThrow();
});
test("validateAllowableSQLQuery() system tables", async () => {
  await expect(
    validateAllowableSQLQuery("SELECT * FROM pg_stat_activity;"),
  ).rejects.toThrow();
});
test("validateAllowableSQLQuery() recursive cte", async () => {
  await expect(
    validateAllowableSQLQuery(
      "WITH RECURSIVE infinite_cte AS (SELECT 1 AS num UNION ALL SELECT num + 1 FROM infinite_cte) SELECT * FROM infinite_cte;",
    ),
  ).rejects.toThrow();
});
test("validateAllowableSQLQuery() function call", async () => {
  await validateAllowableSQLQuery("SELECT count(*) from users;");
  await expect(
    validateAllowableSQLQuery("SELECT blow_up();"),
  ).rejects.toThrow();
});
test("validateAllowableSQLQuery() window def", async () => {
  await validateAllowableSQLQuery(`
  SELECT
    ohlc.pool,
    p.market_cap_usd,
    p.liquidity,
    t.name,
    t.symbol,
    t.address,
    ohlc.volume as volumeUsd,
    p.holder_count,
    p.created_at,
    p.integrator,
    p.type,
    p.chain_id,
    ((ohlc.close - ohlc.open) / ohlc.open * 100) as "percent_day_change",
    p.graduation_percentage,
    p.max_threshold,
    p.graduation_balance,
    ohlc.open,
    ohlc.close,
    p.last_swap_timestamp,
    p.migrated
  FROM (
  WITH base AS (
    SELECT
      fmbu.pool,
      fmbu.minute_id,
      fmbu.volume_usd,
      fmbu.open,
      fmbu.close
    FROM fifteen_minute_bucket_usd AS fmbu
    WHERE
      fmbu.minute_id > $1
      AND fmbu.volume_usd > 1001941466175525071
      AND fmbu.open  > 0
      AND fmbu.close > 0
  )
  SELECT DISTINCT ON (pool)
    pool,
    SUM(volume_usd) OVER (PARTITION BY pool)                                      AS volume,
    FIRST_VALUE(open)  OVER (PARTITION BY pool ORDER BY minute_id ASC)            AS open,
    FIRST_VALUE(close) OVER (PARTITION BY pool ORDER BY minute_id DESC)           AS close
  FROM base
  ORDER BY pool, minute_id ASC
) AS ohlc
  JOIN token t ON t.pool = ohlc.pool
  JOIN pool p ON p.address = ohlc.pool
  ORDER BY $1 ASC
  OFFSET $3
  LIMIT $4`);
});
test("parseSQLQuery() handles large query", async () => {
  await expect(parseSQLQuery(query)).rejects.toThrow();
});
test("validateAllowableSQLQuery() relational query", async () => {
  const account = onchainTable("account", (p) => ({
    id: p.hex().primaryKey(),
    balance: p.bigint(),
  }));
  const position = onchainTable("position", (p) => ({
    id: p.hex().primaryKey(),
    accountId: p.hex().notNull(),
  }));
  const comment = onchainTable("comment", (p) => ({
    id: p.hex().primaryKey(),
    positionId: p.hex().notNull(),
  }));
  const positionRelations = relations(position, ({ one }) => ({
    profile: one(account, {
      fields: [position.accountId],
      references: [account.id],
    }),
  }));
  const accountProfileRelations = relations(account, ({ many }) => ({
    positionList: many(position),
  }));
  const commentRelations = relations(comment, ({ one }) => ({
    position: one(position, {
      fields: [comment.positionId],
      references: [position.id],
    }),
  }));
  const positionCommentRelations = relations(position, ({ many }) => ({
    commentList: many(comment),
  }));
  const client = createClient("http://.../sql", {
    schema: {
      account,
      position,
      comment,
      positionRelations,
      accountProfileRelations,
      commentRelations,
      positionCommentRelations,
    },
  });
  let query = client.db.query.account
    .findMany({
      with: {
        positionList: true,
      },
    })
    .toSQL();
  await validateAllowableSQLQuery(query.sql);
  query = client.db.query.account
    .findFirst({
      with: {
        positionList: true,
      },
    })
    .toSQL();
  await validateAllowableSQLQuery(query.sql);
  query = client.db.query.account.findMany().toSQL();
  await validateAllowableSQLQuery(query.sql);
  query = client.db.query.account.findFirst().toSQL();
  await validateAllowableSQLQuery(query.sql);
  // nested relational query
  query = client.db.query.account
    .findMany({
      with: {
        positionList: {
          with: {
            commentList: true,
          },
        },
      },
    })
    .toSQL();
  await validateAllowableSQLQuery(query.sql);
  // nested relational query with where
  query = client.db.query.account
    .findMany({
      with: {
        positionList: {
          where: eq(position.id, "0x0"),
          with: {
            commentList: true,
          },
        },
      },
    })
    .toSQL();
  await validateAllowableSQLQuery(query.sql);
  // nested relational query with limit
  query = client.db.query.account
    .findMany({
      with: {
        positionList: {
          limit: 10,
          with: {
            commentList: true,
          },
        },
      },
    })
    .toSQL();
  await validateAllowableSQLQuery(query.sql);
  // relational query with offset
  query = client.db.query.account
    .findMany({
      offset: 10,
      with: {
        positionList: true,
      },
    })
    .toSQL();
  await validateAllowableSQLQuery(query.sql);
});
test("validateAllowableSQLQuery() race condition", async () => {
  const promise1 = validateAllowableSQLQuery("SELECT u.name FROM users as u;");
  const promise2 = validateAllowableSQLQuery("SELECT u.name FROM users as u;");
  const promise3 = validateAllowableSQLQuery("SELECT u.name FROM users as u;");
  await Promise.all([promise1, promise2, promise3]);
});
const query = `insert into "more"."AccountDailySnapshot" ("id", "chain_id", "account_address", "total_borrows_usd", "total_supplies_usd", "total_collateral_usd", "timestamp") values ($1, $2, $3, $4, $5, $6, $7), ($8, $9, $10, $11, $12, $13, $14), ($15, $16, $17, $18, $19, $20, $21), ($22, $23, $24, $25, $26, $27, $28), ($29, $30, $31, $32, $33, $34, $35), ($36, $37, $38, $39, $40, $41, $42), ($43, $44, $45, $46, $47, $48, $49), ($50, $51, $52, $53, $54, $55, $56), ($57, $58, $59, $60, $61, $62, $63), ($64, $65, $66, $67, $68, $69, $70), ($71, $72, $73, $74, $75, $76, $77), ($78, $79, $80, $81, $82, $83, $84), ($85, $86, $87, $88, $89, $90, $91), ($92, $93, $94, $95, $96, $97, $98), ($99, $100, $101, $102, $103, $104, $105), ($106, $107, $108, $109, $110, $111, $112), ($113, $114, $115, $116, $117, $118, $119), ($120, $121, $122, $123, $124, $125, $126), ($127, $128, $129, $130, $131, $132, $133), ($134, $135, $136, $137, $138, $139, $140), ($141, $142, $143, $144, $145, $146, $147), ($148, $149, $150, $151, $152, $153, $154), ($155, $156, $157, $158, $159, $160, $161), ($162, $163, $164, $165, $166, $167, $168), ($169, $170, $171, $172, $173, $174, $175), ($176, $177, $178, $179, $180, $181, $182), ($183, $184, $185, $186, $187, $188, $189), ($190, $191, $192, $193, $194, $195, $196), ($197, $198, $199, $200, $201, $202, $203), ($204, $205, $206, $207, $208, $209, $210), ($211, $212, $213, $214, $215, $216, $217), ($218, $219, $220, $221, $222, $223, $224), ($225, $226, $227, $228, $229, $230, $231), ($232, $233, $234, $235, $236, $237, $238), ($239, $240, $241, $242, $243, $244, $245), ($246, $247, $248, $249, $250, $251, $252), ($253, $254, $255, $256, $257, $258, $259), ($260, $261, $262, $263, $264, $265, $266), ($267, $268, $269, $270, $271, $272, $273), ($274, $275, $276, $277, $278, $279, $280), ($281, $282, $283, $284, $285, $286, $287), ($288, $289, $290, $291, $292, $293, $294), ($295, $296, $297, $298, $299, $300, $301), ($302, $303, $304, $305, $306, $307, $308), ($309, $310, $311, $312, $313, $314, $315), ($316, $317, $318, $319, $320, $321, $322), ($323, $324, $325, $326, $327, $328, $329), ($330, $331, $332, $333, $334, $335, $336), ($337, $338, $339, $340, $341, $342, $343), ($344, $345, $346, $347, $348, $349, $350), ($351, $352, $353, $354, $355, $356, $357), ($358, $359, $360, $361, $362, $363, $364), ($365, $366, $367, $368, $369, $370, $371), ($372, $373, $374, $375, $376, $377, $378), ($379, $380, $381, $382, $383, $384, $385), ($386, $387, $388, $389, $390, $391, $392), ($393, $394, $395, $396, $397, $398, $399), ($400, $401, $402, $403, $404, $405, $406), ($407, $408, $409, $410, $411, $412, $413), ($414, $415, $416, $417, $418, $419, $420), ($421, $422, $423, $424, $425, $426, $427), ($428, $429, $430, $431, $432, $433, $434), ($435, $436, $437, $438, $439, $440, $441), ($442, $443, $444, $445, $446, $447, $448), ($449, $450, $451, $452, $453, $454, $455), ($456, $457, $458, $459, $460, $461, $462), ($463, $464, $465, $466, $467, $468, $469), ($470, $471, $472, $473, $474, $475, $476), ($477, $478, $479, $480, $481, $482, $483), ($484, $485, $486, $487, $488, $489, $490), ($491, $492, $493, $494, $495, $496, $497), ($498, $499, $500, $501, $502, $503, $504), ($505, $506, $507, $508, $509, $510, $511), ($512, $513, $514, $515, $516, $517, $518), ($519, $520, $521, $522, $523, $524, $525), ($526, $527, $528, $529, $530, $531, $532), ($533, $534, $535, $536, $537, $538, $539), ($540, $541, $542, $543, $544, $545, $546), ($547, $548, $549, $550, $551, $552, $553), ($554, $555, $556, $557, $558, $559, $560), ($561, $562, $563, $564, $565, $566, $567), ($568, $569, $570, $571, $572, $573, $574), ($575, $576, $577, $578, $579, $580, $581), ($582, $583, $584, $585, $586, $587, $588), ($589, $590, $591, $592, $593, $594, $595), ($596, $597, $598, $599, $600, $601, $602), ($603, $604, $605, $606, $607, $608, $609), ($610, $611, $612, $613, $614, $615, $616), ($617, $618, $619, $620, $621, $622, $623), ($624, $625, $626, $627, $628, $629, $630), ($631, $632, $633, $634, $635, $636, $637), ($638, $639, $640, $641, $642, $643, $644), ($645, $646, $647, $648, $649, $650, $651), ($652, $653, $654, $655, $656, $657, $658), ($659, $660, $661, $662, $663, $664, $665), ($666, $667, $668, $669, $670, $671, $672), ($673, $674, $675, $676, $677, $678, $679), ($680, $681, $682, $683, $684, $685, $686), ($687, $688, $689, $690, $691, $692, $693), ($694, $695, $696, $697, $698, $699, $700), ($701, $702, $703, $704, $705, $706, $707), ($708, $709, $710, $711, $712, $713, $714), ($715, $716, $717, $718, $719, $720, $721), ($722, $723, $724, $725, $726, $727, $728), ($729, $730, $731, $732, $733, $734, $735), ($736, $737, $738, $739, $740, $741, $742), ($743, $744, $745, $746, $747, $748, $749), ($750, $751, $752, $753, $754, $755, $756), ($757, $758, $759, $760, $761, $762, $763), ($764, $765, $766, $767, $768, $769, $770), ($771, $772, $773, $774, $775, $776, $777), ($778, $779, $780, $781, $782, $783, $784), ($785, $786, $787, $788, $789, $790, $791), ($792, $793, $794, $795, $796, $797, $798), ($799, $800, $801, $802, $803, $804, $805), ($806, $807, $808, $809, $810, $811, $812), ($813, $814, $815, $816, $817, $818, $819), ($820, $821, $822, $823, $824, $825, $826), ($827, $828, $829, $830, $831, $832, $833), ($834, $835, $836, $837, $838, $839, $840), ($841, $842, $843, $844, $845, $846, $847), ($848, $849, $850, $851, $852, $853, $854), ($855, $856, $857, $858, $859, $860, $861), ($862, $863, $864, $865, $866, $867, $868), ($869, $870, $871, $872, $873, $874, $875), ($876, $877, $878, $879, $880, $881, $882), ($883, $884, $885, $886, $887, $888, $889), ($890, $891, $892, $893, $894, $895, $896), ($897, $898, $899, $900, $901, $902, $903), ($904, $905, $906, $907, $908, $909, $910), ($911, $912, $913, $914, $915, $916, $917), ($918, $919, $920, $921, $922, $923, $924), ($925, $926, $927, $928, $929, $930, $931), ($932, $933, $934, $935, $936, $937, $938), ($939, $940, $941, $942, $943, $944, $945), ($946, $947, $948, $949, $950, $951, $952), ($953, $954, $955, $956, $957, $958, $959), ($960, $961, $962, $963, $964, $965, $966), ($967, $968, $969, $970, $971, $972, $973), ($974, $975, $976, $977, $978, $979, $980), ($981, $982, $983, $984, $985, $986, $987), ($988, $989, $990, $991, $992, $993, $994), ($995, $996, $997, $998, $999, $1000, $1001), ($1002, $1003, $1004, $1005, $1006, $1007, $1008), ($1009, $1010, $1011, $1012, $1013, $1014, $1015), ($1016, $1017, $1018, $1019, $1020, $1021, $1022), ($1023, $1024, $1025, $1026, $1027, $1028, $1029), ($1030, $1031, $1032, $1033, $1034, $1035, $1036), ($1037, $1038, $1039, $1040, $1041, $1042, $1043), ($1044, $1045, $1046, $1047, $1048, $1049, $1050), ($1051, $1052, $1053, $1054, $1055, $1056, $1057), ($1058, $1059, $1060, $1061, $1062, $1063, $1064), ($1065, $1066, $1067, $1068, $1069, $1070, $1071), ($1072, $1073, $1074, $1075, $1076, $1077, $1078), ($1079, $1080, $1081, $1082, $1083, $1084, $1085), ($1086, $1087, $1088, $1089, $1090, $1091, $1092), ($1093, $1094, $1095, $1096, $1097, $1098, $1099), ($1100, $1101, $1102, $1103, $1104, $1105, $1106), ($1107, $1108, $1109, $1110, $1111, $1112, $1113), ($1114, $1115, $1116, $1117, $1118, $1119, $1120), ($1121, $1122, $1123, $1124, $1125, $1126, $1127), ($1128, $1129, $1130, $1131, $1132, $1133, $1134), ($1135, $1136, $1137, $1138, $1139, $1140, $1141), ($1142, $1143, $1144, $1145, $1146, $1147, $1148), ($1149, $1150, $1151, $1152, $1153, $1154, $1155), ($1156, $1157, $1158, $1159, $1160, $1161, $1162), ($1163, $1164, $1165, $1166, $1167, $1168, $1169), ($1170, $1171, $1172, $1173, $1174, $1175, $1176), ($1177, $1178, $1179, $1180, $1181, $1182, $1183), ($1184, $1185, $1186, $1187, $1188, $1189, $1190), ($1191, $1192, $1193, $1194, $1195, $1196, $1197), ($1198, $1199, $1200, $1201, $1202, $1203, $1204), ($1205, $1206, $1207, $1208, $1209, $1210, $1211), ($1212, $1213, $1214, $1215, $1216, $1217, $1218), ($1219, $1220, $1221, $1222, $1223, $1224, $1225), ($1226, $1227, $1228, $1229, $1230, $1231, $1232), ($1233, $1234, $1235, $1236, $1237, $1238, $1239), ($1240, $1241, $1242, $1243, $1244, $1245, $1246), ($1247, $1248, $1249, $1250, $1251, $1252, $1253), ($1254, $1255, $1256, $1257, $1258, $1259, $1260), ($1261, $1262, $1263, $1264, $1265, $1266, $1267), ($1268, $1269, $1270, $1271, $1272, $1273, $1274), ($1275, $1276, $1277, $1278, $1279, $1280, $1281), ($1282, $1283, $1284, $1285, $1286, $1287, $1288), ($1289, $1290, $1291, $1292, $1293, $1294, $1295), ($1296, $1297, $1298, $1299, $1300, $1301, $1302), ($1303, $1304, $1305, $1306, $1307, $1308, $1309), ($1310, $1311, $1312, $1313, $1314, $1315, $1316), ($1317, $1318, $1319, $1320, $1321, $1322, $1323), ($1324, $1325, $1326, $1327, $1328, $1329, $1330), ($1331, $1332, $1333, $1334, $1335, $1336, $1337), ($1338, $1339, $1340, $1341, $1342, $1343, $1344), ($1345, $1346, $1347, $1348, $1349, $1350, $1351), ($1352, $1353, $1354, $1355, $1356, $1357, $1358), ($1359, $1360, $1361, $1362, $1363, $1364, $1365), ($1366, $1367, $1368, $1369, $1370, $1371, $1372), ($1373, $1374, $1375, $1376, $1377, $1378, $1379), ($1380, $1381, $1382, $1383, $1384, $1385, $1386), ($1387, $1388, $1389, $1390, $1391, $1392, $1393), ($1394, $1395, $1396, $1397, $1398, $1399, $1400), ($1401, $1402, $1403, $1404, $1405, $1406, $1407), ($1408, $1409, $1410, $1411, $1412, $1413, $1414), ($1415, $1416, $1417, $1418, $1419, $1420, $1421), ($1422, $1423, $1424, $1425, $1426, $1427, $1428), ($1429, $1430, $1431, $1432, $1433, $1434, $1435), ($1436, $1437, $1438, $1439, $1440, $1441, $1442), ($1443, $1444, $1445, $1446, $1447, $1448, $1449), ($1450, $1451, $1452, $1453, $1454, $1455, $1456), ($1457, $1458, $1459, $1460, $1461, $1462, $1463), ($1464, $1465, $1466, $1467, $1468, $1469, $1470), ($1471, $1472, $1473, $1474, $1475, $1476, $1477), ($1478, $1479, $1480, $1481, $1482, $1483, $1484), ($1485, $1486, $1487, $1488, $1489, $1490, $1491), ($1492, $1493, $1494, $1495, $1496, $1497, $1498), ($1499, $1500, $1501, $1502, $1503, $1504, $1505), ($1506, $1507, $1508, $1509, $1510, $1511, $1512), ($1513, $1514, $1515, $1516, $1517, $1518, $1519), ($1520, $1521, $1522, $1523, $1524, $1525, $1526), ($1527, $1528, $1529, $1530, $1531, $1532, $1533), ($1534, $1535, $1536, $1537, $1538, $1539, $1540), ($1541, $1542, $1543, $1544, $1545, $1546, $1547), ($1548, $1549, $1550, $1551, $1552, $1553, $1554), ($1555, $1556, $1557, $1558, $1559, $1560, $1561), ($1562, $1563, $1564, $1565, $1566, $1567, $1568), ($1569, $1570, $1571, $1572, $1573, $1574, $1575), ($1576, $1577, $1578, $1579, $1580, $1581, $1582), ($1583, $1584, $1585, $1586, $1587, $1588, $1589), ($1590, $1591, $1592, $1593, $1594, $1595, $1596), ($1597, $1598, $1599, $1600, $1601, $1602, $1603), ($1604, $1605, $1606, $1607, $1608, $1609, $1610), ($1611, $1612, $1613, $1614, $1615, $1616, $1617), ($1618, $1619, $1620, $1621, $1622, $1623, $1624), ($1625, $1626, $1627, $1628, $1629, $1630, $1631), ($1632, $1633, $1634, $1635, $1636, $1637, $1638), ($1639, $1640, $1641, $1642, $1643, $1644, $1645), ($1646, $1647, $1648, $1649, $1650, $1651, $1652), ($1653, $1654, $1655, $1656, $1657, $1658, $1659), ($1660, $1661, $1662, $1663, $1664, $1665, $1666), ($1667, $1668, $1669, $1670, $1671, $1672, $1673), ($1674, $1675, $1676, $1677, $1678, $1679, $1680), ($1681, $1682, $1683, $1684, $1685, $1686, $1687), ($1688, $1689, $1690, $1691, $1692, $1693, $1694), ($1695, $1696, $1697, $1698, $1699, $1700, $1701), ($1702, $1703, $1704, $1705, $1706, $1707, $1708), ($1709, $1710, $1711, $1712, $1713, $1714, $1715), ($1716, $1717, $1718, $1719, $1720, $1721, $1722), ($1723, $1724, $1725, $1726, $1727, $1728, $1729), ($1730, $1731, $1732, $1733, $1734, $1735, $1736), ($1737, $1738, $1739, $1740, $1741, $1742, $1743), ($1744, $1745, $1746, $1747, $1748, $1749, $1750), ($1751, $1752, $1753, $1754, $1755, $1756, $1757), ($1758, $1759, $1760, $1761, $1762, $1763, $1764), ($1765, $1766, $1767, $1768, $1769, $1770, $1771), ($1772, $1773, $1774, $1775, $1776, $1777, $1778), ($1779, $1780, $1781, $1782, $1783, $1784, $1785), ($1786, $1787, $1788, $1789, $1790, $1791, $1792), ($1793, $1794, $1795, $1796, $1797, $1798, $1799), ($1800, $1801, $1802, $1803, $1804, $1805, $1806), ($1807, $1808, $1809, $1810, $1811, $1812, $1813), ($1814, $1815, $1816, $1817, $1818, $1819, $1820), ($1821, $1822, $1823, $1824, $1825, $1826, $1827), ($1828, $1829, $1830, $1831, $1832, $1833, $1834), ($1835, $1836, $1837, $1838, $1839, $1840, $1841), ($1842, $1843, $1844, $1845, $1846, $1847, $1848), ($1849, $1850, $1851, $1852, $1853, $1854, $1855), ($1856, $1857, $1858, $1859, $1860, $1861, $1862), ($1863, $1864, $1865, $1866, $1867, $1868, $1869), ($1870, $1871, $1872, $1873, $1874, $1875, $1876), ($1877, $1878, $1879, $1880, $1881, $1882, $1883), ($1884, $1885, $1886, $1887, $1888, $1889, $1890), ($1891, $1892, $1893, $1894, $1895, $1896, $1897), ($1898, $1899, $1900, $1901, $1902, $1903, $1904), ($1905, $1906, $1907, $1908, $1909, $1910, $1911), ($1912, $1913, $1914, $1915, $1916, $1917, $1918), ($1919, $1920, $1921, $1922, $1923, $1924, $1925), ($1926, $1927, $1928, $1929, $1930, $1931, $1932), ($1933, $1934, $1935, $1936, $1937, $1938, $1939), ($1940, $1941, $1942, $1943, $1944, $1945, $1946), ($1947, $1948, $1949, $1950, $1951, $1952, $1953), ($1954, $1955, $1956, $1957, $1958, $1959, $1960), ($1961, $1962, $1963, $1964, $1965, $1966, $1967), ($1968, $1969, $1970, $1971, $1972, $1973, $1974), ($1975, $1976, $1977, $1978, $1979, $1980, $1981), ($1982, $1983, $1984, $1985, $1986, $1987, $1988), ($1989, $1990, $1991, $1992, $1993, $1994, $1995), ($1996, $1997, $1998, $1999, $2000, $2001, $2002), ($2003, $2004, $2005, $2006, $2007, $2008, $2009), ($2010, $2011, $2012, $2013, $2014, $2015, $2016), ($2017, $2018, $2019, $2020, $2021, $2022, $2023), ($2024, $2025, $2026, $2027, $2028, $2029, $2030), ($2031, $2032, $2033, $2034, $2035, $2036, $2037), ($2038, $2039, $2040, $2041, $2042, $2043, $2044), ($2045, $2046, $2047, $2048, $2049, $2050, $2051), ($2052, $2053, $2054, $2055, $2056, $2057, $2058), ($2059, $2060, $2061, $2062, $2063, $2064, $2065), ($2066, $2067, $2068, $2069, $2070, $2071, $2072), ($2073, $2074, $2075, $2076, $2077, $2078, $2079), ($2080, $2081, $2082, $2083, $2084, $2085, $2086), ($2087, $2088, $2089, $2090, $2091, $2092, $2093), ($2094, $2095, $2096, $2097, $2098, $2099, $2100), ($2101, $2102, $2103, $2104, $2105, $2106, $2107), ($2108, $2109, $2110, $2111, $2112, $2113, $2114), ($2115, $2116, $2117, $2118, $2119, $2120, $2121), ($2122, $2123, $2124, $2125, $2126, $2127, $2128), ($2129, $2130, $2131, $2132, $2133, $2134, $2135), ($2136, $2137, $2138, $2139, $2140, $2141, $2142), ($2143, $2144, $2145, $2146, $2147, $2148, $2149), ($2150, $2151, $2152, $2153, $2154, $2155, $2156), ($2157, $2158, $2159, $2160, $2161, $2162, $2163), ($2164, $2165, $2166, $2167, $2168, $2169, $2170), ($2171, $2172, $2173, $2174, $2175, $2176, $2177), ($2178, $2179, $2180, $2181, $2182, $2183, $2184), ($2185, $2186, $2187, $2188, $2189, $2190, $2191), ($2192, $2193, $2194, $2195, $2196, $2197, $2198), ($2199, $2200, $2201, $2202, $2203, $2204, $2205), ($2206, $2207, $2208, $2209, $2210, $2211, $2212), ($2213, $2214, $2215, $2216, $2217, $2218, $2219), ($2220, $2221, $2222, $2223, $2224, $2225, $2226), ($2227, $2228, $2229, $2230, $2231, $2232, $2233), ($2234, $2235, $2236, $2237, $2238, $2239, $2240), ($2241, $2242, $2243, $2244, $2245, $2246, $2247), ($2248, $2249, $2250, $2251, $2252, $2253, $2254), ($2255, $2256, $2257, $2258, $2259, $2260, $2261), ($2262, $2263, $2264, $2265, $2266, $2267, $2268), ($2269, $2270, $2271, $2272, $2273, $2274, $2275), ($2276, $2277, $2278, $2279, $2280, $2281, $2282), ($2283, $2284, $2285, $2286, $2287, $2288, $2289), ($2290, $2291, $2292, $2293, $2294, $2295, $2296), ($2297, $2298, $2299, $2300, $2301, $2302, $2303), ($2304, $2305, $2306, $2307, $2308, $2309, $2310), ($2311, $2312, $2313, $2314, $2315, $2316, $2317), ($2318, $2319, $2320, $2321, $2322, $2323, $2324), ($2325, $2326, $2327, $2328, $2329, $2330, $2331), ($2332, $2333, $2334, $2335, $2336, $2337, $2338), ($2339, $2340, $2341, $2342, $2343, $2344, $2345), ($2346, $2347, $2348, $2349, $2350, $2351, $2352), ($2353, $2354, $2355, $2356, $2357, $2358, $2359), ($2360, $2361, $2362, $2363, $2364, $2365, $2366), ($2367, $2368, $2369, $2370, $2371, $2372, $2373), ($2374, $2375, $2376, $2377, $2378, $2379, $2380), ($2381, $2382, $2383, $2384, $2385, $2386, $2387), ($2388, $2389, $2390, $2391, $2392, $2393, $2394), ($2395, $2396, $2397, $2398, $2399, $2400, $2401), ($2402, $2403, $2404, $2405, $2406, $2407, $2408), ($2409, $2410, $2411, $2412, $2413, $2414, $2415), ($2416, $2417, $2418, $2419, $2420, $2421, $2422), ($2423, $2424, $2425, $2426, $2427, $2428, $2429), ($2430, $2431, $2432, $2433, $2434, $2435, $2436), ($2437, $2438, $2439, $2440, $2441, $2442, $2443), ($2444, $2445, $2446, $2447, $2448, $2449, $2450), ($2451, $2452, $2453, $2454, $2455, $2456, $2457), ($2458, $2459, $2460, $2461, $2462, $2463, $2464), ($2465, $2466, $2467, $2468, $2469, $2470, $2471), ($2472, $2473, $2474, $2475, $2476, $2477, $2478), ($2479, $2480, $2481, $2482, $2483, $2484, $2485), ($2486, $2487, $2488, $2489, $2490, $2491, $2492), ($2493, $2494, $2495, $2496, $2497, $2498, $2499), ($2500, $2501, $2502, $2503, $2504, $2505, $2506), ($2507, $2508, $2509, $2510, $2511, $2512, $2513), ($2514, $2515, $2516, $2517, $2518, $2519, $2520), ($2521, $2522, $2523, $2524, $2525, $2526, $2527), ($2528, $2529, $2530, $2531, $2532, $2533, $2534), ($2535, $2536, $2537, $2538, $2539, $2540, $2541), ($2542, $2543, $2544, $2545, $2546, $2547, $2548), ($2549, $2550, $2551, $2552, $2553, $2554, $2555), ($2556, $2557, $2558, $2559, $2560, $2561, $2562), ($2563, $2564, $2565, $2566, $2567, $2568, $2569), ($2570, $2571, $2572, $2573, $2574, $2575, $2576), ($2577, $2578, $2579, $2580, $2581, $2582, $2583), ($2584, $2585, $2586, $2587, $2588, $2589, $2590), ($2591, $2592, $2593, $2594, $2595, $2596, $2597), ($2598, $2599, $2600, $2601, $2602, $2603, $2604), ($2605, $2606, $2607, $2608, $2609, $2610, $2611), ($2612, $2613, $2614, $2615, $2616, $2617, $2618), ($2619, $2620, $2621, $2622, $2623, $2624, $2625), ($2626, $2627, $2628, $2629, $2630, $2631, $2632), ($2633, $2634, $2635, $2636, $2637, $2638, $2639), ($2640, $2641, $2642, $2643, $2644, $2645, $2646), ($2647, $2648, $2649, $2650, $2651, $2652, $2653), ($2654, $2655, $2656, $2657, $2658, $2659, $2660), ($2661, $2662, $2663, $2664, $2665, $2666, $2667), ($2668, $2669, $2670, $2671, $2672, $2673, $2674), ($2675, $2676, $2677, $2678, $2679, $2680, $2681), ($2682, $2683, $2684, $2685, $2686, $2687, $2688), ($2689, $2690, $2691, $2692, $2693, $2694, $2695), ($2696, $2697, $2698, $2699, $2700, $2701, $2702), ($2703, $2704, $2705, $2706, $2707, $2708, $2709), ($2710, $2711, $2712, $2713, $2714, $2715, $2716), ($2717, $2718, $2719, $2720, $2721, $2722, $2723), ($2724, $2725, $2726, $2727, $2728, $2729, $2730), ($2731, $2732, $2733, $2734, $2735, $2736, $2737), ($2738, $2739, $2740, $2741, $2742, $2743, $2744), ($2745, $2746, $2747, $2748, $2749, $2750, $2751), ($2752, $2753, $2754, $2755, $2756, $2757, $2758), ($2759, $2760, $2761, $2762, $2763, $2764, $2765), ($2766, $2767, $2768, $2769, $2770, $2771, $2772), ($2773, $2774, $2775, $2776, $2777, $2778, $2779), ($2780, $2781, $2782, $2783, $2784, $2785, $2786), ($2787, $2788, $2789, $2790, $2791, $2792, $2793), ($2794, $2795, $2796, $2797, $2798, $2799, $2800), ($2801, $2802, $2803, $2804, $2805, $2806, $2807), ($2808, $2809, $2810, $2811, $2812, $2813, $2814), ($2815, $2816, $2817, $2818, $2819, $2820, $2821), ($2822, $2823, $2824, $2825, $2826, $2827, $2828), ($2829, $2830, $2831, $2832, $2833, $2834, $2835), ($2836, $2837, $2838, $2839, $2840, $2841, $2842), ($2843, $2844, $2845, $2846, $2847, $2848, $2849), ($2850, $2851, $2852, $2853, $2854, $2855, $2856), ($2857, $2858, $2859, $2860, $2861, $2862, $2863), ($2864, $2865, $2866, $2867, $2868, $2869, $2870), ($2871, $2872, $2873, $2874, $2875, $2876, $2877), ($2878, $2879, $2880, $2881, $2882, $2883, $2884), ($2885, $2886, $2887, $2888, $2889, $2890, $2891), ($2892, $2893, $2894, $2895, $2896, $2897, $2898), ($2899, $2900, $2901, $2902, $2903, $2904, $2905), ($2906, $2907, $2908, $2909, $2910, $2911, $2912), ($2913, $2914, $2915, $2916, $2917, $2918, $2919), ($2920, $2921, $2922, $2923, $2924, $2925, $2926), ($2927, $2928, $2929, $2930, $2931, $2932, $2933), ($2934, $2935, $2936, $2937, $2938, $2939, $2940), ($2941, $2942, $2943, $2944, $2945, $2946, $2947), ($2948, $2949, $2950, $2951, $2952, $2953, $2954), ($2955, $2956, $2957, $2958, $2959, $2960, $2961), ($2962, $2963, $2964, $2965, $2966, $2967, $2968), ($2969, $2970, $2971, $2972, $2973, $2974, $2975), ($2976, $2977, $2978, $2979, $2980, $2981, $2982), ($2983, $2984, $2985, $2986, $2987, $2988, $2989), ($2990, $2991, $2992, $2993, $2994, $2995, $2996), ($2997, $2998, $2999, $3000, $3001, $3002, $3003), ($3004, $3005, $3006, $3007, $3008, $3009, $3010), ($3011, $3012, $3013, $3014, $3015, $3016, $3017), ($3018, $3019, $3020, $3021, $3022, $3023, $3024), ($3025, $3026, $3027, $3028, $3029, $3030, $3031), ($3032, $3033, $3034, $3035, $3036, $3037, $3038), ($3039, $3040, $3041, $3042, $3043, $3044, $3045), ($3046, $3047, $3048, $3049, $3050, $3051, $3052), ($3053, $3054, $3055, $3056, $3057, $3058, $3059), ($3060, $3061, $3062, $3063, $3064, $3065, $3066), ($3067, $3068, $3069, $3070, $3071, $3072, $3073), ($3074, $3075, $3076, $3077, $3078, $3079, $3080), ($3081, $3082, $3083, $3084, $3085, $3086, $3087), ($3088, $3089, $3090, $3091, $3092, $3093, $3094), ($3095, $3096, $3097, $3098, $3099, $3100, $3101), ($3102, $3103, $3104, $3105, $3106, $3107, $3108), ($3109, $3110, $3111, $3112, $3113, $3114, $3115), ($3116, $3117, $3118, $3119, $3120, $3121, $3122), ($3123, $3124, $3125, $3126, $3127, $3128, $3129), ($3130, $3131, $3132, $3133, $3134, $3135, $3136), ($3137, $3138, $3139, $3140, $3141, $3142, $3143), ($3144, $3145, $3146, $3147, $3148, $3149, $3150), ($3151, $3152, $3153, $3154, $3155, $3156, $3157), ($3158, $3159, $3160, $3161, $3162, $3163, $3164), ($3165, $3166, $3167, $3168, $3169, $3170, $3171), ($3172, $3173, $3174, $3175, $3176, $3177, $3178), ($3179, $3180, $3181, $3182, $3183, $3184, $3185), ($3186, $3187, $3188, $3189, $3190, $3191, $3192), ($3193, $3194, $3195, $3196, $3197, $3198, $3199), ($3200, $3201, $3202, $3203, $3204, $3205, $3206), ($3207, $3208, $3209, $3210, $3211, $3212, $3213), ($3214, $3215, $3216, $3217, $3218, $3219, $3220), ($3221, $3222, $3223, $3224, $3225, $3226, $3227), ($3228, $3229, $3230, $3231, $3232, $3233, $3234), ($3235, $3236, $3237, $3238, $3239, $3240, $3241), ($3242, $3243, $3244, $3245, $3246, $3247, $3248), ($3249, $3250, $3251, $3252, $3253, $3254, $3255), ($3256, $3257, $3258, $3259, $3260, $3261, $3262), ($3263, $3264, $3265, $3266, $3267, $3268, $3269), ($3270, $3271, $3272, $3273, $3274, $3275, $3276), ($3277, $3278, $3279, $3280, $3281, $3282, $3283), ($3284, $3285, $3286, $3287, $3288, $3289, $3290), ($3291, $3292, $3293, $3294, $3295, $3296, $3297), ($3298, $3299, $3300, $3301, $3302, $3303, $3304), ($3305, $3306, $3307, $3308, $3309, $3310, $3311), ($3312, $3313, $3314, $3315, $3316, $3317, $3318), ($3319, $3320, $3321, $3322, $3323, $3324, $3325), ($3326, $3327, $3328, $3329, $3330, $3331, $3332), ($3333, $3334, $3335, $3336, $3337, $3338, $3339), ($3340, $3341, $3342, $3343, $3344, $3345, $3346), ($3347, $3348, $3349, $3350, $3351, $3352, $3353), ($3354, $3355, $3356, $3357, $3358, $3359, $3360), ($3361, $3362, $3363, $3364, $3365, $3366, $3367), ($3368, $3369, $3370, $3371, $3372, $3373, $3374), ($3375, $3376, $3377, $3378, $3379, $3380, $3381), ($3382, $3383, $3384, $3385, $3386, $3387, $3388), ($3389, $3390, $3391, $3392, $3393, $3394, $3395), ($3396, $3397, $3398, $3399, $3400, $3401, $3402), ($3403, $3404, $3405, $3406, $3407, $3408, $3409), ($3410, $3411, $3412, $3413, $3414, $3415, $3416), ($3417, $3418, $3419, $3420, $3421, $3422, $3423), ($3424, $3425, $3426, $3427, $3428, $3429, $3430), ($3431, $3432, $3433, $3434, $3435, $3436, $3437), ($3438, $3439, $3440, $3441, $3442, $3443, $3444), ($3445, $3446, $3447, $3448, $3449, $3450, $3451), ($3452, $3453, $3454, $3455, $3456, $3457, $3458), ($3459, $3460, $3461, $3462, $3463, $3464, $3465), ($3466, $3467, $3468, $3469, $3470, $3471, $3472), ($3473, $3474, $3475, $3476, $3477, $3478, $3479), ($3480, $3481, $3482, $3483, $3484, $3485, $3486), ($3487, $3488, $3489, $3490, $3491, $3492, $3493), ($3494, $3495, $3496, $3497, $3498, $3499, $3500), ($3501, $3502, $3503, $3504, $3505, $3506, $3507), ($3508, $3509, $3510, $3511, $3512, $3513, $3514), ($3515, $3516, $3517, $3518, $3519, $3520, $3521), ($3522, $3523, $3524, $3525, $3526, $3527, $3528), ($3529, $3530, $3531, $3532, $3533, $3534, $3535), ($3536, $3537, $3538, $3539, $3540, $3541, $3542), ($3543, $3544, $3545, $3546, $3547, $3548, $3549), ($3550, $3551, $3552, $3553, $3554, $3555, $3556), ($3557, $3558, $3559, $3560, $3561, $3562, $3563), ($3564, $3565, $3566, $3567, $3568, $3569, $3570), ($3571, $3572, $3573, $3574, $3575, $3576, $3577), ($3578, $3579, $3580, $3581, $3582, $3583, $3584), ($3585, $3586, $3587, $3588, $3589, $3590, $3591), ($3592, $3593, $3594, $3595, $3596, $3597, $3598), ($3599, $3600, $3601, $3602, $3603, $3604, $3605), ($3606, $3607, $3608, $3609, $3610, $3611, $3612), ($3613, $3614, $3615, $3616, $3617, $3618, $3619), ($3620, $3621, $3622, $3623, $3624, $3625, $3626), ($3627, $3628, $3629, $3630, $3631, $3632, $3633), ($3634, $3635, $3636, $3637, $3638, $3639, $3640), ($3641, $3642, $3643, $3644, $3645, $3646, $3647), ($3648, $3649, $3650, $3651, $3652, $3653, $3654), ($3655, $3656, $3657, $3658, $3659, $3660, $3661), ($3662, $3663, $3664, $3665, $3666, $3667, $3668), ($3669, $3670, $3671, $3672, $3673, $3674, $3675), ($3676, $3677, $3678, $3679, $3680, $3681, $3682), ($3683, $3684, $3685, $3686, $3687, $3688, $3689), ($3690, $3691, $3692, $3693, $3694, $3695, $3696), ($3697, $3698, $3699, $3700, $3701, $3702, $3703), ($3704, $3705, $3706, $3707, $3708, $3709, $3710), ($3711, $3712, $3713, $3714, $3715, $3716, $3717), ($3718, $3719, $3720, $3721, $3722, $3723, $3724), ($3725, $3726, $3727, $3728, $3729, $3730, $3731), ($3732, $3733, $3734, $3735, $3736, $3737, $3738), ($3739, $3740, $3741, $3742, $3743, $3744, $3745), ($3746, $3747, $3748, $3749, $3750, $3751, $3752), ($3753, $3754, $3755, $3756, $3757, $3758, $3759), ($3760, $3761, $3762, $3763, $3764, $3765, $3766), ($3767, $3768, $3769, $3770, $3771, $3772, $3773), ($3774, $3775, $3776, $3777, $3778, $3779, $3780), ($3781, $3782, $3783, $3784, $3785, $3786, $3787), ($3788, $3789, $3790, $3791, $3792, $3793, $3794), ($3795, $3796, $3797, $3798, $3799, $3800, $3801), ($3802, $3803, $3804, $3805, $3806, $3807, $3808), ($3809, $3810, $3811, $3812, $3813, $3814, $3815), ($3816, $3817, $3818, $3819, $3820, $3821, $3822), ($3823, $3824, $3825, $3826, $3827, $3828, $3829), ($3830, $3831, $3832, $3833, $3834, $3835, $3836), ($3837, $3838, $3839, $3840, $3841, $3842, $3843), ($3844, $3845, $3846, $3847, $3848, $3849, $3850), ($3851, $3852, $3853, $3854, $3855, $3856, $3857), ($3858, $3859, $3860, $3861, $3862, $3863, $3864), ($3865, $3866, $3867, $3868, $3869, $3870, $3871), ($3872, $3873, $3874, $3875, $3876, $3877, $3878), ($3879, $3880, $3881, $3882, $3883, $3884, $3885), ($3886, $3887, $3888, $3889, $3890, $3891, $3892), ($3893, $3894, $3895, $3896, $3897, $3898, $3899), ($3900, $3901, $3902, $3903, $3904, $3905, $3906), ($3907, $3908, $3909, $3910, $3911, $3912, $3913), ($3914, $3915, $3916, $3917, $3918, $3919, $3920), ($3921, $3922, $3923, $3924, $3925, $3926, $3927), ($3928, $3929, $3930, $3931, $3932, $3933, $3934), ($3935, $3936, $3937, $3938, $3939, $3940, $3941), ($3942, $3943, $3944, $3945, $3946, $3947, $3948), ($3949, $3950, $3951, $3952, $3953, $3954, $3955), ($3956, $3957, $3958, $3959, $3960, $3961, $3962), ($3963, $3964, $3965, $3966, $3967, $3968, $3969), ($3970, $3971, $3972, $3973, $3974, $3975, $3976), ($3977, $3978, $3979, $3980, $3981, $3982, $3983), ($3984, $3985, $3986, $3987, $3988, $3989, $3990), ($3991, $3992, $3993, $3994, $3995, $3996, $3997), ($3998, $3999, $4000, $4001, $4002, $4003, $4004), ($4005, $4006, $4007, $4008, $4009, $4010, $4011), ($4012, $4013, $4014, $4015, $4016, $4017, $4018), ($4019, $4020, $4021, $4022, $4023, $4024, $4025), ($4026, $4027, $4028, $4029, $4030, $4031, $4032), ($4033, $4034, $4035, $4036, $4037, $4038, $4039), ($4040, $4041, $4042, $4043, $4044, $4045, $4046), ($4047, $4048, $4049, $4050, $4051, $4052, $4053), ($4054, $4055, $4056, $4057, $4058, $4059, $4060), ($4061, $4062, $4063, $4064, $4065, $4066, $4067), ($4068, $4069, $4070, $4071, $4072, $4073, $4074), ($4075, $4076, $4077, $4078, $4079, $4080, $4081), ($4082, $4083, $4084, $4085, $4086, $4087, $4088), ($4089, $4090, $4091, $4092, $4093, $4094, $4095), ($4096, $4097, $4098, $4099, $4100, $4101, $4102), ($4103, $4104, $4105, $4106, $4107, $4108, $4109), ($4110, $4111, $4112, $4113, $4114, $4115, $4116), ($4117, $4118, $4119, $4120, $4121, $4122, $4123), ($4124, $4125, $4126, $4127, $4128, $4129, $4130), ($4131, $4132, $4133, $4134, $4135, $4136, $4137), ($4138, $4139, $4140, $4141, $4142, $4143, $4144), ($4145, $4146, $4147, $4148, $4149, $4150, $4151), ($4152, $4153, $4154, $4155, $4156, $4157, $4158), ($4159, $4160, $4161, $4162, $4163, $4164, $4165), ($4166, $4167, $4168, $4169, $4170, $4171, $4172), ($4173, $4174, $4175, $4176, $4177, $4178, $4179), ($4180, $4181, $4182, $4183, $4184, $4185, $4186), ($4187, $4188, $4189, $4190, $4191, $4192, $4193), ($4194, $4195, $4196, $4197, $4198, $4199, $4200), ($4201, $4202, $4203, $4204, $4205, $4206, $4207), ($4208, $4209, $4210, $4211, $4212, $4213, $4214), ($4215, $4216, $4217, $4218, $4219, $4220, $4221), ($4222, $4223, $4224, $4225, $4226, $4227, $4228), ($4229, $4230, $4231, $4232, $4233, $4234, $4235), ($4236, $4237, $4238, $4239, $4240, $4241, $4242), ($4243, $4244, $4245, $4246, $4247, $4248, $4249), ($4250, $4251, $4252, $4253, $4254, $4255, $4256), ($4257, $4258, $4259, $4260, $4261, $4262, $4263), ($4264, $4265, $4266, $4267, $4268, $4269, $4270), ($4271, $4272, $4273, $4274, $4275, $4276, $4277), ($4278, $4279, $4280, $4281, $4282, $4283, $4284), ($4285, $4286, $4287, $4288, $4289, $4290, $4291), ($4292, $4293, $4294, $4295, $4296, $4297, $4298), ($4299, $4300, $4301, $4302, $4303, $4304, $4305), ($4306, $4307, $4308, $4309, $4310, $4311, $4312), ($4313, $4314, $4315, $4316, $4317, $4318, $4319), ($4320, $4321, $4322, $4323, $4324, $4325, $4326), ($4327, $4328, $4329, $4330, $4331, $4332, $4333), ($4334, $4335, $4336, $4337, $4338, $4339, $4340), ($4341, $4342, $4343, $4344, $4345, $4346, $4347), ($4348, $4349, $4350, $4351, $4352, $4353, $4354), ($4355, $4356, $4357, $4358, $4359, $4360, $4361), ($4362, $4363, $4364, $4365, $4366, $4367, $4368), ($4369, $4370, $4371, $4372, $4373, $4374, $4375), ($4376, $4377, $4378, $4379, $4380, $4381, $4382), ($4383, $4384, $4385, $4386, $4387, $4388, $4389), ($4390, $4391, $4392, $4393, $4394, $4395, $4396), ($4397, $4398, $4399, $4400, $4401, $4402, $4403), ($4404, $4405, $4406, $4407, $4408, $4409, $4410), ($4411, $4412, $4413, $4414, $4415, $4416, $4417), ($4418, $4419, $4420, $4421, $4422, $4423, $4424), ($4425, $4426, $4427, $4428, $4429, $4430, $4431), ($4432, $4433, $4434, $4435, $4436, $4437, $4438), ($4439, $4440, $4441, $4442, $4443, $4444, $4445), ($4446, $4447, $4448, $4449, $4450, $4451, $4452), ($4453, $4454, $4455, $4456, $4457, $4458, $4459), ($4460, $4461, $4462, $4463, $4464, $4465, $4466), ($4467, $4468, $4469, $4470, $4471, $4472, $4473), ($4474, $4475, $4476, $4477, $4478, $4479, $4480), ($4481, $4482, $4483, $4484, $4485, $4486, $4487), ($4488, $4489, $4490, $4491, $4492, $4493, $4494), ($4495, $4496, $4497, $4498, $4499, $4500, $4501), ($4502, $4503, $4504, $4505, $4506, $4507, $4508), ($4509, $4510, $4511, $4512, $4513, $4514, $4515), ($4516, $4517, $4518, $4519, $4520, $4521, $4522), ($4523, $4524, $4525, $4526, $4527, $4528, $4529), ($4530, $4531, $4532, $4533, $4534, $4535, $4536), ($4537, $4538, $4539, $4540, $4541, $4542, $4543), ($4544, $4545, $4546, $4547, $4548, $4549, $4550), ($4551, $4552, $4553, $4554, $4555, $4556, $4557), ($4558, $4559, $4560, $4561, $4562, $4563, $4564), ($4565, $4566, $4567, $4568, $4569, $4570, $4571), ($4572, $4573, $4574, $4575, $4576, $4577, $4578), ($4579, $4580, $4581, $4582, $4583, $4584, $4585), ($4586, $4587, $4588, $4589, $4590, $4591, $4592), ($4593, $4594, $4595, $4596, $4597, $4598, $4599), ($4600, $4601, $4602, $4603, $4604, $4605, $4606), ($4607, $4608, $4609, $4610, $4611, $4612, $4613), ($4614, $4615, $4616, $4617, $4618, $4619, $4620), ($4621, $4622, $4623, $4624, $4625, $4626, $4627), ($4628, $4629, $4630, $4631, $4632, $4633, $4634), ($4635, $4636, $4637, $4638, $4639, $4640, $4641), ($4642, $4643, $4644, $4645, $4646, $4647, $4648), ($4649, $4650, $4651, $4652, $4653, $4654, $4655), ($4656, $4657, $4658, $4659, $4660, $4661, $4662), ($4663, $4664, $4665, $4666, $4667, $4668, $4669), ($4670, $4671, $4672, $4673, $4674, $4675, $4676), ($4677, $4678, $4679, $4680, $4681, $4682, $4683), ($4684, $4685, $4686, $4687, $4688, $4689, $4690), ($4691, $4692, $4693, $4694, $4695, $4696, $4697), ($4698, $4699, $4700, $4701, $4702, $4703, $4704), ($4705, $4706, $4707, $4708, $4709, $4710, $4711), ($4712, $4713, $4714, $4715, $4716, $4717, $4718), ($4719, $4720, $4721, $4722, $4723, $4724, $4725), ($4726, $4727, $4728, $4729, $4730, $4731, $4732), ($4733, $4734, $4735, $4736, $4737, $4738, $4739), ($4740, $4741, $4742, $4743, $4744, $4745, $4746), ($4747, $4748, $4749, $4750, $4751, $4752, $4753), ($4754, $4755, $4756, $4757, $4758, $4759, $4760), ($4761, $4762, $4763, $4764, $4765, $4766, $4767), ($4768, $4769, $4770, $4771, $4772, $4773, $4774), ($4775, $4776, $4777, $4778, $4779, $4780, $4781), ($4782, $4783, $4784, $4785, $4786, $4787, $4788), ($4789, $4790, $4791, $4792, $4793, $4794, $4795), ($4796, $4797, $4798, $4799, $4800, $4801, $4802), ($4803, $4804, $4805, $4806, $4807, $4808, $4809), ($4810, $4811, $4812, $4813, $4814, $4815, $4816), ($4817, $4818, $4819, $4820, $4821, $4822, $4823), ($4824, $4825, $4826, $4827, $4828, $4829, $4830), ($4831, $4832, $4833, $4834, $4835, $4836, $4837), ($4838, $4839, $4840, $4841, $4842, $4843, $4844), ($4845, $4846, $4847, $4848, $4849, $4850, $4851), ($4852, $4853, $4854, $4855, $4856, $4857, $4858), ($4859, $4860, $4861, $4862, $4863, $4864, $4865), ($4866, $4867, $4868, $4869, $4870, $4871, $4872), ($4873, $4874, $4875, $4876, $4877, $4878, $4879), ($4880, $4881, $4882, $4883, $4884, $4885, $4886), ($4887, $4888, $4889, $4890, $4891, $4892, $4893), ($4894, $4895, $4896, $4897, $4898, $4899, $4900), ($4901, $4902, $4903, $4904, $4905, $4906, $4907), ($4908, $4909, $4910, $4911, $4912, $4913, $4914), ($4915, $4916, $4917, $4918, $4919, $4920, $4921), ($4922, $4923, $4924, $4925, $4926, $4927, $4928), ($4929, $4930, $4931, $4932, $4933, $4934, $4935), ($4936, $4937, $4938, $4939, $4940, $4941, $4942), ($4943, $4944, $4945, $4946, $4947, $4948, $4949), ($4950, $4951, $4952, $4953, $4954, $4955, $4956), ($4957, $4958, $4959, $4960, $4961, $4962, $4963), ($4964, $4965, $4966, $4967, $4968, $4969, $4970), ($4971, $4972, $4973, $4974, $4975, $4976, $4977), ($4978, $4979, $4980, $4981, $4982, $4983, $4984), ($4985, $4986, $4987, $4988, $4989, $4990, $4991), ($4992, $4993, $4994, $4995, $4996, $4997, $4998), ($4999, $5000, $5001, $5002, $5003, $5004, $5005), ($5006, $5007, $5008, $5009, $5010, $5011, $5012), ($5013, $5014, $5015, $5016, $5017, $5018, $5019), ($5020, $5021, $5022, $5023, $5024, $5025, $5026), ($5027, $5028, $5029, $5030, $5031, $5032, $5033), ($5034, $5035, $5036, $5037, $5038, $5039, $5040), ($5041, $5042, $5043, $5044, $5045, $5046, $5047), ($5048, $5049, $5050, $5051, $5052, $5053, $5054), ($5055, $5056, $5057, $5058, $5059, $5060, $5061), ($5062, $5063, $5064, $5065, $5066, $5067, $5068), ($5069, $5070, $5071, $5072, $5073, $5074, $5075), ($5076, $5077, $5078, $5079, $5080, $5081, $5082), ($5083, $5084, $5085, $5086, $5087, $5088, $5089), ($5090, $5091, $5092, $5093, $5094, $5095, $5096), ($5097, $5098, $5099, $5100, $5101, $5102, $5103), ($5104, $5105, $5106, $5107, $5108, $5109, $5110), ($5111, $5112, $5113, $5114, $5115, $5116, $5117), ($5118, $5119, $5120, $5121, $5122, $5123, $5124), ($5125, $5126, $5127, $5128, $5129, $5130, $5131), ($5132, $5133, $5134, $5135, $5136, $5137, $5138), ($5139, $5140, $5141, $5142, $5143, $5144, $5145), ($5146, $5147, $5148, $5149, $5150, $5151, $5152), ($5153, $5154, $5155, $5156, $5157, $5158, $5159), ($5160, $5161, $5162, $5163, $5164, $5165, $5166), ($5167, $5168, $5169, $5170, $5171, $5172, $5173), ($5174, $5175, $5176, $5177, $5178, $5179, $5180), ($5181, $5182, $5183, $5184, $5185, $5186, $5187), ($5188, $5189, $5190, $5191, $5192, $5193, $5194), ($5195, $5196, $5197, $5198, $5199, $5200, $5201), ($5202, $5203, $5204, $5205, $5206, $5207, $5208), ($5209, $5210, $5211, $5212, $5213, $5214, $5215), ($5216, $5217, $5218, $5219, $5220, $5221, $5222), ($5223, $5224, $5225, $5226, $5227, $5228, $5229), ($5230, $5231, $5232, $5233, $5234, $5235, $5236), ($5237, $5238, $5239, $5240, $5241, $5242, $5243), ($5244, $5245, $5246, $5247, $5248, $5249, $5250), ($5251, $5252, $5253, $5254, $5255, $5256, $5257), ($5258, $5259, $5260, $5261, $5262, $5263, $5264), ($5265, $5266, $5267, $5268, $5269, $5270, $5271), ($5272, $5273, $5274, $5275, $5276, $5277, $5278), ($5279, $5280, $5281, $5282, $5283, $5284, $5285), ($5286, $5287, $5288, $5289, $5290, $5291, $5292), ($5293, $5294, $5295, $5296, $5297, $5298, $5299), ($5300, $5301, $5302, $5303, $5304, $5305, $5306), ($5307, $5308, $5309, $5310, $5311, $5312, $5313), ($5314, $5315, $5316, $5317, $5318, $5319, $5320), ($5321, $5322, $5323, $5324, $5325, $5326, $5327), ($5328, $5329, $5330, $5331, $5332, $5333, $5334), ($5335, $5336, $5337, $5338, $5339, $5340, $5341), ($5342, $5343, $5344, $5345, $5346, $5347, $5348), ($5349, $5350, $5351, $5352, $5353, $5354, $5355), ($5356, $5357, $5358, $5359, $5360, $5361, $5362), ($5363, $5364, $5365, $5366, $5367, $5368, $5369), ($5370, $5371, $5372, $5373, $5374, $5375, $5376), ($5377, $5378, $5379, $5380, $5381, $5382, $5383), ($5384, $5385, $5386, $5387, $5388, $5389, $5390), ($5391, $5392, $5393, $5394, $5395, $5396, $5397), ($5398, $5399, $5400, $5401, $5402, $5403, $5404), ($5405, $5406, $5407, $5408, $5409, $5410, $5411), ($5412, $5413, $5414, $5415, $5416, $5417, $5418), ($5419, $5420, $5421, $5422, $5423, $5424, $5425), ($5426, $5427, $5428, $5429, $5430, $5431, $5432), ($5433, $5434, $5435, $5436, $5437, $5438, $5439), ($5440, $5441, $5442, $5443, $5444, $5445, $5446), ($5447, $5448, $5449, $5450, $5451, $5452, $5453), ($5454, $5455, $5456, $5457, $5458, $5459, $5460), ($5461, $5462, $5463, $5464, $5465, $5466, $5467), ($5468, $5469, $5470, $5471, $5472, $5473, $5474), ($5475, $5476, $5477, $5478, $5479, $5480, $5481), ($5482, $5483, $5484, $5485, $5486, $5487, $5488), ($5489, $5490, $5491, $5492, $5493, $5494, $5495), ($5496, $5497, $5498, $5499, $5500, $5501, $5502), ($5503, $5504, $5505, $5506, $5507, $5508, $5509), ($5510, $5511, $5512, $5513, $5514, $5515, $5516), ($5517, $5518, $5519, $5520, $5521, $5522, $5523), ($5524, $5525, $5526, $5527, $5528, $5529, $5530), ($5531, $5532, $5533, $5534, $5535, $5536, $5537), ($5538, $5539, $5540, $5541, $5542, $5543, $5544), ($5545, $5546, $5547, $5548, $5549, $5550, $5551), ($5552, $5553, $5554, $5555, $5556, $5557, $5558), ($5559, $5560, $5561, $5562, $5563, $5564, $5565), ($5566, $5567, $5568, $5569, $5570, $5571, $5572), ($5573, $5574, $5575, $5576, $5577, $5578, $5579), ($5580, $5581, $5582, $5583, $5584, $5585, $5586), ($5587, $5588, $5589, $5590, $5591, $5592, $5593), ($5594, $5595, $5596, $5597, $5598, $5599, $5600), ($5601, $5602, $5603, $5604, $5605, $5606, $5607), ($5608, $5609, $5610, $5611, $5612, $5613, $5614), ($5615, $5616, $5617, $5618, $5619, $5620, $5621), ($5622, $5623, $5624, $5625, $5626, $5627, $5628), ($5629, $5630, $5631, $5632, $5633, $5634, $5635), ($5636, $5637, $5638, $5639, $5640, $5641, $5642), ($5643, $5644, $5645, $5646, $5647, $5648, $5649), ($5650, $5651, $5652, $5653, $5654, $5655, $5656), ($5657, $5658, $5659, $5660, $5661, $5662, $5663), ($5664, $5665, $5666, $5667, $5668, $5669, $5670), ($5671, $5672, $5673, $5674, $5675, $5676, $5677), ($5678, $5679, $5680, $5681, $5682, $5683, $5684), ($5685, $5686, $5687, $5688, $5689, $5690, $5691), ($5692, $5693, $5694, $5695, $5696, $5697, $5698), ($5699, $5700, $5701, $5702, $5703, $5704, $5705), ($5706, $5707, $5708, $5709, $5710, $5711, $5712), ($5713, $5714, $5715, $5716, $5717, $5718, $5719), ($5720, $5721, $5722, $5723, $5724, $5725, $5726), ($5727, $5728, $5729, $5730, $5731, $5732, $5733), ($5734, $5735, $5736, $5737, $5738, $5739, $5740), ($5741, $5742, $5743, $5744, $5745, $5746, $5747), ($5748, $5749, $5750, $5751, $5752, $5753, $5754), ($5755, $5756, $5757, $5758, $5759, $5760, $5761), ($5762, $5763, $5764, $5765, $5766, $5767, $5768), ($5769, $5770, $5771, $5772, $5773, $5774, $5775), ($5776, $5777, $5778, $5779, $5780, $5781, $5782), ($5783, $5784, $5785, $5786, $5787, $5788, $5789), ($5790, $5791, $5792, $5793, $5794, $5795, $5796), ($5797, $5798, $5799, $5800, $5801, $5802, $5803), ($5804, $5805, $5806, $5807, $5808, $5809, $5810), ($5811, $5812, $5813, $5814, $5815, $5816, $5817), ($5818, $5819, $5820, $5821, $5822, $5823, $5824), ($5825, $5826, $5827, $5828, $5829, $5830, $5831), ($5832, $5833, $5834, $5835, $5836, $5837, $5838), ($5839, $5840, $5841, $5842, $5843, $5844, $5845), ($5846, $5847, $5848, $5849, $5850, $5851, $5852), ($5853, $5854, $5855, $5856, $5857, $5858, $5859), ($5860, $5861, $5862, $5863, $5864, $5865, $5866), ($5867, $5868, $5869, $5870, $5871, $5872, $5873), ($5874, $5875, $5876, $5877, $5878, $5879, $5880), ($5881, $5882, $5883, $5884, $5885, $5886, $5887), ($5888, $5889, $5890, $5891, $5892, $5893, $5894), ($5895, $5896, $5897, $5898, $5899, $5900, $5901), ($5902, $5903, $5904, $5905, $5906, $5907, $5908), ($5909, $5910, $5911, $5912, $5913, $5914, $5915), ($5916, $5917, $5918, $5919, $5920, $5921, $5922), ($5923, $5924, $5925, $5926, $5927, $5928, $5929), ($5930, $5931, $5932, $5933, $5934, $5935, $5936), ($5937, $5938, $5939, $5940, $5941, $5942, $5943), ($5944, $5945, $5946, $5947, $5948, $5949, $5950), ($5951, $5952, $5953, $5954, $5955, $5956, $5957), ($5958, $5959, $5960, $5961, $5962, $5963, $5964), ($5965, $5966, $5967, $5968, $5969, $5970, $5971), ($5972, $5973, $5974, $5975, $5976, $5977, $5978), ($5979, $5980, $5981, $5982, $5983, $5984, $5985), ($5986, $5987, $5988, $5989, $5990, $5991, $5992), ($5993, $5994, $5995, $5996, $5997, $5998, $5999), ($6000, $6001, $6002, $6003, $6004, $6005, $6006), ($6007, $6008, $6009, $6010, $6011, $6012, $6013), ($6014, $6015, $6016, $6017, $6018, $6019, $6020), ($6021, $6022, $6023, $6024, $6025, $6026, $6027), ($6028, $6029, $6030, $6031, $6032, $6033, $6034), ($6035, $6036, $6037, $6038, $6039, $6040, $6041), ($6042, $6043, $6044, $6045, $6046, $6047, $6048), ($6049, $6050, $6051, $6052, $6053, $6054, $6055), ($6056, $6057, $6058, $6059, $6060, $6061, $6062), ($6063, $6064, $6065, $6066, $6067, $6068, $6069), ($6070, $6071, $6072, $6073, $6074, $6075, $6076), ($6077, $6078, $6079, $6080, $6081, $6082, $6083), ($6084, $6085, $6086, $6087, $6088, $6089, $6090), ($6091, $6092, $6093, $6094, $6095, $6096, $6097), ($6098, $6099, $6100, $6101, $6102, $6103, $6104), ($6105, $6106, $6107, $6108, $6109, $6110, $6111), ($6112, $6113, $6114, $6115, $6116, $6117, $6118), ($6119, $6120, $6121, $6122, $6123, $6124, $6125), ($6126, $6127, $6128, $6129, $6130, $6131, $6132), ($6133, $6134, $6135, $6136, $6137, $6138, $6139), ($6140, $6141, $6142, $6143, $6144, $6145, $6146), ($6147, $6148, $6149, $6150, $6151, $6152, $6153), ($6154, $6155, $6156, $6157, $6158, $6159, $6160), ($6161, $6162, $6163, $6164, $6165, $6166, $6167), ($6168, $6169, $6170, $6171, $6172, $6173, $6174), ($6175, $6176, $6177, $6178, $6179, $6180, $6181), ($6182, $6183, $6184, $6185, $6186, $6187, $6188), ($6189, $6190, $6191, $6192, $6193, $6194, $6195), ($6196, $6197, $6198, $6199, $6200, $6201, $6202), ($6203, $6204, $6205, $6206, $6207, $6208, $6209), ($6210, $6211, $6212, $6213, $6214, $6215, $6216), ($6217, $6218, $6219, $6220, $6221, $6222, $6223), ($6224, $6225, $6226, $6227, $6228, $6229, $6230), ($6231, $6232, $6233, $6234, $6235, $6236, $6237), ($6238, $6239, $6240, $6241, $6242, $6243, $6244), ($6245, $6246, $6247, $6248, $6249, $6250, $6251), ($6252, $6253, $6254, $6255, $6256, $6257, $6258), ($6259, $6260, $6261, $6262, $6263, $6264, $6265), ($6266, $6267, $6268, $6269, $6270, $6271, $6272), ($6273, $6274, $6275, $6276, $6277, $6278, $6279), ($6280, $6281, $6282, $6283, $6284, $6285, $6286), ($6287, $6288, $6289, $6290, $6291, $6292, $6293), ($6294, $6295, $6296, $6297, $6298, $6299, $6300), ($6301, $6302, $6303, $6304, $6305, $6306, $6307), ($6308, $6309, $6310, $6311, $6312, $6313, $6314), ($6315, $6316, $6317, $6318, $6319, $6320, $6321), ($6322, $6323, $6324, $6325, $6326, $6327, $6328), ($6329, $6330, $6331, $6332, $6333, $6334, $6335), ($6336, $6337, $6338, $6339, $6340, $6341, $6342), ($6343, $6344, $6345, $6346, $6347, $6348, $6349), ($6350, $6351, $6352, $6353, $6354, $6355, $6356), ($6357, $6358, $6359, $6360, $6361, $6362, $6363), ($6364, $6365, $6366, $6367, $6368, $6369, $6370), ($6371, $6372, $6373, $6374, $6375, $6376, $6377), ($6378, $6379, $6380, $6381, $6382, $6383, $6384), ($6385, $6386, $6387, $6388, $6389, $6390, $6391), ($6392, $6393, $6394, $6395, $6396, $6397, $6398), ($6399, $6400, $6401, $6402, $6403, $6404, $6405), ($6406, $6407, $6408, $6409, $6410, $6411, $6412), ($6413, $6414, $6415, $6416, $6417, $6418, $6419), ($6420, $6421, $6422, $6423, $6424, $6425, $6426), ($6427, $6428, $6429, $6430, $6431, $6432, $6433), ($6434, $6435, $6436, $6437, $6438, $6439, $6440), ($6441, $6442, $6443, $6444, $6445, $6446, $6447), ($6448, $6449, $6450, $6451, $6452, $6453, $6454), ($6455, $6456, $6457, $6458, $6459, $6460, $6461), ($6462, $6463, $6464, $6465, $6466, $6467, $6468), ($6469, $6470, $6471, $6472, $6473, $6474, $6475), ($6476, $6477, $6478, $6479, $6480, $6481, $6482), ($6483, $6484, $6485, $6486, $6487, $6488, $6489), ($6490, $6491, $6492, $6493, $6494, $6495, $6496), ($6497, $6498, $6499, $6500, $6501, $6502, $6503), ($6504, $6505, $6506, $6507, $6508, $6509, $6510), ($6511, $6512, $6513, $6514, $6515, $6516, $6517), ($6518, $6519, $6520, $6521, $6522, $6523, $6524), ($6525, $6526, $6527, $6528, $6529, $6530, $6531), ($6532, $6533, $6534, $6535, $6536, $6537, $6538), ($6539, $6540, $6541, $6542, $6543, $6544, $6545), ($6546, $6547, $6548, $6549, $6550, $6551, $6552), ($6553, $6554, $6555, $6556, $6557, $6558, $6559), ($6560, $6561, $6562, $6563, $6564, $6565, $6566), ($6567, $6568, $6569, $6570, $6571, $6572, $6573), ($6574, $6575, $6576, $6577, $6578, $6579, $6580), ($6581, $6582, $6583, $6584, $6585, $6586, $6587), ($6588, $6589, $6590, $6591, $6592, $6593, $6594), ($6595, $6596, $6597, $6598, $6599, $6600, $6601), ($6602, $6603, $6604, $6605, $6606, $6607, $6608), ($6609, $6610, $6611, $6612, $6613, $6614, $6615), ($6616, $6617, $6618, $6619, $6620, $6621, $6622), ($6623, $6624, $6625, $6626, $6627, $6628, $6629), ($6630, $6631, $6632, $6633, $6634, $6635, $6636), ($6637, $6638, $6639, $6640, $6641, $6642, $6643), ($6644, $6645, $6646, $6647, $6648, $6649, $6650), ($6651, $6652, $6653, $6654, $6655, $6656, $6657), ($6658, $6659, $6660, $6661, $6662, $6663, $6664), ($6665, $6666, $6667, $6668, $6669, $6670, $6671), ($6672, $6673, $6674, $6675, $6676, $6677, $6678), ($6679, $6680, $6681, $6682, $6683, $6684, $6685), ($6686, $6687, $6688, $6689, $6690, $6691, $6692), ($6693, $6694, $6695, $6696, $6697, $6698, $6699), ($6700, $6701, $6702, $6703, $6704, $6705, $6706), ($6707, $6708, $6709, $6710, $6711, $6712, $6713), ($6714, $6715, $6716, $6717, $6718, $6719, $6720), ($6721, $6722, $6723, $6724, $6725, $6726, $6727), ($6728, $6729, $6730, $6731, $6732, $6733, $6734), ($6735, $6736, $6737, $6738, $6739, $6740, $6741), ($6742, $6743, $6744, $6745, $6746, $6747, $6748), ($6749, $6750, $6751, $6752, $6753, $6754, $6755), ($6756, $6757, $6758, $6759, $6760, $6761, $6762), ($6763, $6764, $6765, $6766, $6767, $6768, $6769), ($6770, $6771, $6772, $6773, $6774, $6775, $6776), ($6777, $6778, $6779, $6780, $6781, $6782, $6783), ($6784, $6785, $6786, $6787, $6788, $6789, $6790), ($6791, $6792, $6793, $6794, $6795, $6796, $6797), ($6798, $6799, $6800, $6801, $6802, $6803, $6804), ($6805, $6806, $6807, $6808, $6809, $6810, $6811), ($6812, $6813, $6814, $6815, $6816, $6817, $6818), ($6819, $6820, $6821, $6822, $6823, $6824, $6825), ($6826, $6827, $6828, $6829, $6830, $6831, $6832), ($6833, $6834, $6835, $6836, $6837, $6838, $6839), ($6840, $6841, $6842, $6843, $6844, $6845, $6846), ($6847, $6848, $6849, $6850, $6851, $6852, $6853), ($6854, $6855, $6856, $6857, $6858, $6859, $6860), ($6861, $6862, $6863, $6864, $6865, $6866, $6867), ($6868, $6869, $6870, $6871, $6872, $6873, $6874), ($6875, $6876, $6877, $6878, $6879, $6880, $6881), ($6882, $6883, $6884, $6885, $6886, $6887, $6888), ($6889, $6890, $6891, $6892, $6893, $6894, $6895), ($6896, $6897, $6898, $6899, $6900, $6901, $6902), ($6903, $6904, $6905, $6906, $6907, $6908, $6909), ($6910, $6911, $6912, $6913, $6914, $6915, $6916), ($6917, $6918, $6919, $6920, $6921, $6922, $6923), ($6924, $6925, $6926, $6927, $6928, $6929, $6930), ($6931, $6932, $6933, $6934, $6935, $6936, $6937), ($6938, $6939, $6940, $6941, $6942, $6943, $6944), ($6945, $6946, $6947, $6948, $6949, $6950, $6951), ($6952, $6953, $6954, $6955, $6956, $6957, $6958), ($6959, $6960, $6961, $6962, $6963, $6964, $6965), ($6966, $6967, $6968, $6969, $6970, $6971, $6972), ($6973, $6974, $6975, $6976, $6977, $6978, $6979), ($6980, $6981, $6982, $6983, $6984, $6985, $6986), ($6987, $6988, $6989, $6990, $6991, $6992, $6993), ($6994, $6995, $6996, $6997, $6998, $6999, $7000), ($7001, $7002, $7003, $7004, $7005, $7006, $7007), ($7008, $7009, $7010, $7011, $7012, $7013, $7014), ($7015, $7016, $7017, $7018, $7019, $7020, $7021), ($7022, $7023, $7024, $7025, $7026, $7027, $7028), ($7029, $7030, $7031, $7032, $7033, $7034, $7035), ($7036, $7037, $7038, $7039, $7040, $7041, $7042), ($7043, $7044, $7045, $7046, $7047, $7048, $7049), ($7050, $7051, $7052, $7053, $7054, $7055, $7056), ($7057, $7058, $7059, $7060, $7061, $7062, $7063), ($7064, $7065, $7066, $7067, $7068, $7069, $7070), ($7071, $7072, $7073, $7074, $7075, $7076, $7077), ($7078, $7079, $7080, $7081, $7082, $7083, $7084), ($7085, $7086, $7087, $7088, $7089, $7090, $7091), ($7092, $7093, $7094, $7095, $7096, $7097, $7098), ($7099, $7100, $7101, $7102, $7103, $7104, $7105), ($7106, $7107, $7108, $7109, $7110, $7111, $7112), ($7113, $7114, $7115, $7116, $7117, $7118, $7119), ($7120, $7121, $7122, $7123, $7124, $7125, $7126), ($7127, $7128, $7129, $7130, $7131, $7132, $7133), ($7134, $7135, $7136, $7137, $7138, $7139, $7140), ($7141, $7142, $7143, $7144, $7145, $7146, $7147), ($7148, $7149, $7150, $7151, $7152, $7153, $7154), ($7155, $7156, $7157, $7158, $7159, $7160, $7161), ($7162, $7163, $7164, $7165, $7166, $7167, $7168), ($7169, $7170, $7171, $7172, $7173, $7174, $7175), ($7176, $7177, $7178, $7179, $7180, $7181, $7182), ($7183, $7184, $7185, $7186, $7187, $7188, $7189), ($7190, $7191, $7192, $7193, $7194, $7195, $7196), ($7197, $7198, $7199, $7200, $7201, $7202, $7203), ($7204, $7205, $7206, $7207, $7208, $7209, $7210), ($7211, $7212, $7213, $7214, $7215, $7216, $7217), ($7218, $7219, $7220, $7221, $7222, $7223, $7224), ($7225, $7226, $7227, $7228, $7229, $7230, $7231), ($7232, $7233, $7234, $7235, $7236, $7237, $7238), ($7239, $7240, $7241, $7242, $7243, $7244, $7245), ($7246, $7247, $7248, $7249, $7250, $7251, $7252), ($7253, $7254, $7255, $7256, $7257, $7258, $7259), ($7260, $7261, $7262, $7263, $7264, $7265, $7266), ($7267, $7268, $7269, $7270, $7271, $7272, $7273), ($7274, $7275, $7276, $7277, $7278, $7279, $7280), ($7281, $7282, $7283, $7284, $7285, $7286, $7287), ($7288, $7289, $7290, $7291, $7292, $7293, $7294), ($7295, $7296, $7297, $7298, $7299, $7300, $7301), ($7302, $7303, $7304, $7305, $7306, $7307, $7308), ($7309, $7310, $7311, $7312, $7313, $7314, $7315), ($7316, $7317, $7318, $7319, $7320, $7321, $7322), ($7323, $7324, $7325, $7326, $7327, $7328, $7329), ($7330, $7331, $7332, $7333, $7334, $7335, $7336), ($7337, $7338, $7339, $7340, $7341, $7342, $7343), ($7344, $7345, $7346, $7347, $7348, $7349, $7350), ($7351, $7352, $7353, $7354, $7355, $7356, $7357), ($7358, $7359, $7360, $7361, $7362, $7363, $7364), ($7365, $7366, $7367, $7368, $7369, $7370, $7371), ($7372, $7373, $7374, $7375, $7376, $7377, $7378), ($7379, $7380, $7381, $7382, $7383, $7384, $7385), ($7386, $7387, $7388, $7389, $7390, $7391, $7392), ($7393, $7394, $7395, $7396, $7397, $7398, $7399), ($7400, $7401, $7402, $7403, $7404, $7405, $7406), ($7407, $7408, $7409, $7410, $7411, $7412, $7413), ($7414, $7415, $7416, $7417, $7418, $7419, $7420), ($7421, $7422, $7423, $7424, $7425, $7426, $7427), ($7428, $7429, $7430, $7431, $7432, $7433, $7434), ($7435, $7436, $7437, $7438, $7439, $7440, $7441), ($7442, $7443, $7444, $7445, $7446, $7447, $7448), ($7449, $7450, $7451, $7452, $7453, $7454, $7455), ($7456, $7457, $7458, $7459, $7460, $7461, $7462), ($7463, $7464, $7465, $7466, $7467, $7468, $7469), ($7470, $7471, $7472, $7473, $7474, $7475, $7476), ($7477, $7478, $7479, $7480, $7481, $7482, $7483), ($7484, $7485, $7486, $7487, $7488, $7489, $7490), ($7491, $7492, $7493, $7494, $7495, $7496, $7497), ($7498, $7499, $7500, $7501, $7502, $7503, $7504), ($7505, $7506, $7507, $7508, $7509, $7510, $7511), ($7512, $7513, $7514, $7515, $7516, $7517, $7518), ($7519, $7520, $7521, $7522, $7523, $7524, $7525), ($7526, $7527, $7528, $7529, $7530, $7531, $7532), ($7533, $7534, $7535, $7536, $7537, $7538, $7539), ($7540, $7541, $7542, $7543, $7544, $7545, $7546), ($7547, $7548, $7549, $7550, $7551, $7552, $7553), ($7554, $7555, $7556, $7557, $7558, $7559, $7560), ($7561, $7562, $7563, $7564, $7565, $7566, $7567), ($7568, $7569, $7570, $7571, $7572, $7573, $7574), ($7575, $7576, $7577, $7578, $7579, $7580, $7581), ($7582, $7583, $7584, $7585, $7586, $7587, $7588), ($7589, $7590, $7591, $7592, $7593, $7594, $7595), ($7596, $7597, $7598, $7599, $7600, $7601, $7602), ($7603, $7604, $7605, $7606, $7607, $7608, $7609), ($7610, $7611, $7612, $7613, $7614, $7615, $7616), ($7617, $7618, $7619, $7620, $7621, $7622, $7623), ($7624, $7625, $7626, $7627, $7628, $7629, $7630), ($7631, $7632, $7633, $7634, $7635, $7636, $7637), ($7638, $7639, $7640, $7641, $7642, $7643, $7644), ($7645, $7646, $7647, $7648, $7649, $7650, $7651), ($7652, $7653, $7654, $7655, $7656, $7657, $7658), ($7659, $7660, $7661, $7662, $7663, $7664, $7665), ($7666, $7667, $7668, $7669, $7670, $7671, $7672), ($7673, $7674, $7675, $7676, $7677, $7678, $7679), ($7680, $7681, $7682, $7683, $7684, $7685, $7686), ($7687, $7688, $7689, $7690, $7691, $7692, $7693), ($7694, $7695, $7696, $7697, $7698, $7699, $7700), ($7701, $7702, $7703, $7704, $7705, $7706, $7707), ($7708, $7709, $7710, $7711, $7712, $7713, $7714), ($7715, $7716, $7717, $7718, $7719, $7720, $7721), ($7722, $7723, $7724, $7725, $7726, $7727, $7728), ($7729, $7730, $7731, $7732, $7733, $7734, $7735), ($7736, $7737, $7738, $7739, $7740, $7741, $7742), ($7743, $7744, $7745, $7746, $7747, $7748, $7749), ($7750, $7751, $7752, $7753, $7754, $7755, $7756), ($7757, $7758, $7759, $7760, $7761, $7762, $7763), ($7764, $7765, $7766, $7767, $7768, $7769, $7770), ($7771, $7772, $7773, $7774, $7775, $7776, $7777), ($7778, $7779, $7780, $7781, $7782, $7783, $7784), ($7785, $7786, $7787, $7788, $7789, $7790, $7791), ($7792, $7793, $7794, $7795, $7796, $7797, $7798), ($7799, $7800, $7801, $7802, $7803, $7804, $7805), ($7806, $7807, $7808, $7809, $7810, $7811, $7812), ($7813, $7814, $7815, $7816, $7817, $7818, $7819), ($7820, $7821, $7822, $7823, $7824, $7825, $7826), ($7827, $7828, $7829, $7830, $7831, $7832, $7833), ($7834, $7835, $7836, $7837, $7838, $7839, $7840), ($7841, $7842, $7843, $7844, $7845, $7846, $7847), ($7848, $7849, $7850, $7851, $7852, $7853, $7854), ($7855, $7856, $7857, $7858, $7859, $7860, $7861), ($7862, $7863, $7864, $7865, $7866, $7867, $7868), ($7869, $7870, $7871, $7872, $7873, $7874, $7875), ($7876, $7877, $7878, $7879, $7880, $7881, $7882), ($7883, $7884, $7885, $7886, $7887, $7888, $7889), ($7890, $7891, $7892, $7893, $7894, $7895, $7896), ($7897, $7898, $7899, $7900, $7901, $7902, $7903), ($7904, $7905, $7906, $7907, $7908, $7909, $7910), ($7911, $7912, $7913, $7914, $7915, $7916, $7917), ($7918, $7919, $7920, $7921, $7922, $7923, $7924), ($7925, $7926, $7927, $7928, $7929, $7930, $7931), ($7932, $7933, $7934, $7935, $7936, $7937, $7938), ($7939, $7940, $7941, $7942, $7943, $7944, $7945), ($7946, $7947, $7948, $7949, $7950, $7951, $7952), ($7953, $7954, $7955, $7956, $7957, $7958, $7959), ($7960, $7961, $7962, $7963, $7964, $7965, $7966), ($7967, $7968, $7969, $7970, $7971, $7972, $7973), ($7974, $7975, $7976, $7977, $7978, $7979, $7980), ($7981, $7982, $7983, $7984, $7985, $7986, $7987), ($7988, $7989, $7990, $7991, $7992, $7993, $7994), ($7995, $7996, $7997, $7998, $7999, $8000, $8001), ($8002, $8003, $8004, $8005, $8006, $8007, $8008), ($8009, $8010, $8011, $8012, $8013, $8014, $8015), ($8016, $8017, $8018, $8019, $8020, $8021, $8022), ($8023, $8024, $8025, $8026, $8027, $8028, $8029), ($8030, $8031, $8032, $8033, $8034, $8035, $8036), ($8037, $8038, $8039, $8040, $8041, $8042, $8043), ($8044, $8045, $8046, $8047, $8048, $8049, $8050), ($8051, $8052, $8053, $8054, $8055, $8056, $8057), ($8058, $8059, $8060, $8061, $8062, $8063, $8064), ($8065, $8066, $8067, $8068, $8069, $8070, $8071), ($8072, $8073, $8074, $8075, $8076, $8077, $8078), ($8079, $8080, $8081, $8082, $8083, $8084, $8085), ($8086, $8087, $8088, $8089, $8090, $8091, $8092), ($8093, $8094, $8095, $8096, $8097, $8098, $8099), ($8100, $8101, $8102, $8103, $8104, $8105, $8106), ($8107, $8108, $8109, $8110, $8111, $8112, $8113), ($8114, $8115, $8116, $8117, $8118, $8119, $8120), ($8121, $8122, $8123, $8124, $8125, $8126, $8127), ($8128, $8129, $8130, $8131, $8132, $8133, $8134), ($8135, $8136, $8137, $8138, $8139, $8140, $8141), ($8142, $8143, $8144, $8145, $8146, $8147, $8148), ($8149, $8150, $8151, $8152, $8153, $8154, $8155), ($8156, $8157, $8158, $8159, $8160, $8161, $8162), ($8163, $8164, $8165, $8166, $8167, $8168, $8169), ($8170, $8171, $8172, $8173, $8174, $8175, $8176), ($8177, $8178, $8179, $8180, $8181, $8182, $8183), ($8184, $8185, $8186, $8187, $8188, $8189, $8190), ($8191, $8192, $8193, $8194, $8195, $8196, $8197), ($8198, $8199, $8200, $8201, $8202, $8203, $8204), ($8205, $8206, $8207, $8208, $8209, $8210, $8211), ($8212, $8213, $8214, $8215, $8216, $8217, $8218), ($8219, $8220, $8221, $8222, $8223, $8224, $8225), ($8226, $8227, $8228, $8229, $8230, $8231, $8232), ($8233, $8234, $8235, $8236, $8237, $8238, $8239), ($8240, $8241, $8242, $8243, $8244, $8245, $8246), ($8247, $8248, $8249, $8250, $8251, $8252, $8253), ($8254, $8255, $8256, $8257, $8258, $8259, $8260), ($8261, $8262, $8263, $8264, $8265, $8266, $8267), ($8268, $8269, $8270, $8271, $8272, $8273, $8274), ($8275, $8276, $8277, $8278, $8279, $8280, $8281), ($8282, $8283, $8284, $8285, $8286, $8287, $8288), ($8289, $8290, $8291, $8292, $8293, $8294, $8295), ($8296, $8297, $8298, $8299, $8300, $8301, $8302), ($8303, $8304, $8305, $8306, $8307, $8308, $8309), ($8310, $8311, $8312, $8313, $8314, $8315, $8316), ($8317, $8318, $8319, $8320, $8321, $8322, $8323), ($8324, $8325, $8326, $8327, $8328, $8329, $8330), ($8331, $8332, $8333, $8334, $8335, $8336, $8337), ($8338, $8339, $8340, $8341, $8342, $8343, $8344), ($8345, $8346, $8347, $8348, $8349, $8350, $8351), ($8352, $8353, $8354, $8355, $8356, $8357, $8358), ($8359, $8360, $8361, $8362, $8363, $8364, $8365), ($8366, $8367, $8368, $8369, $8370, $8371, $8372), ($8373, $8374, $8375, $8376, $8377, $8378, $8379), ($8380, $8381, $8382, $8383, $8384, $8385, $8386), ($8387, $8388, $8389, $8390, $8391, $8392, $8393), ($8394, $8395, $8396, $8397, $8398, $8399, $8400), ($8401, $8402, $8403, $8404, $8405, $8406, $8407), ($8408, $8409, $8410, $8411, $8412, $8413, $8414), ($8415, $8416, $8417, $8418, $8419, $8420, $8421), ($8422, $8423, $8424, $8425, $8426, $8427, $8428), ($8429, $8430, $8431, $8432, $8433, $8434, $8435), ($8436, $8437, $8438, $8439, $8440, $8441, $8442), ($8443, $8444, $8445, $8446, $8447, $8448, $8449), ($8450, $8451, $8452, $8453, $8454, $8455, $8456), ($8457, $8458, $8459, $8460, $8461, $8462, $8463), ($8464, $8465, $8466, $8467, $8468, $8469, $8470), ($8471, $8472, $8473, $8474, $8475, $8476, $8477), ($8478, $8479, $8480, $8481, $8482, $8483, $8484), ($8485, $8486, $8487, $8488, $8489, $8490, $8491), ($8492, $8493, $8494, $8495, $8496, $8497, $8498), ($8499, $8500, $8501, $8502, $8503, $8504, $8505), ($8506, $8507, $8508, $8509, $8510, $8511, $8512), ($8513, $8514, $8515, $8516, $8517, $8518, $8519), ($8520, $8521, $8522, $8523, $8524, $8525, $8526), ($8527, $8528, $8529, $8530, $8531, $8532, $8533), ($8534, $8535, $8536, $8537, $8538, $8539, $8540), ($8541, $8542, $8543, $8544, $8545, $8546, $8547), ($8548, $8549, $8550, $8551, $8552, $8553, $8554), ($8555, $8556, $8557, $8558, $8559, $8560, $8561), ($8562, $8563, $8564, $8565, $8566, $8567, $8568), ($8569, $8570, $8571, $8572, $8573, $8574, $8575), ($8576, $8577, $8578, $8579, $8580, $8581, $8582), ($8583, $8584, $8585, $8586, $8587, $8588, $8589), ($8590, $8591, $8592, $8593, $8594, $8595, $8596)`;
</file>

<file path="packages/core/src/utils/sql-parse.ts">
import type { Node, RangeVar, RawStmt } from "@pgsql/types";
type ValidatorNode<
  node extends Node extends infer T ? (T extends T ? keyof T : never) : never,
> = {
  node: node | (string & {});
  children: (node: Extract<Node, { [key in node]: unknown }>[node]) => Node[];
  validate?: (node: Extract<Node, { [key in node]: unknown }>[node]) => void;
};
const getNodeType = (node: Node) => Object.keys(node)[0]!;
const ALLOW_CACHE = new Map<string, boolean>();
const RELATIONS_CACHE = new Map<string, Set<string>>();
export const parseSQLQuery = async (sql: string): Promise<Node> => {
  // @ts-ignore
  const Parser = await import(/* webpackIgnore: true */ "pg-query-emscripten");
  if (sql.length > 5_000) {
    throw new Error("Invalid query");
  }
  const { parse } = await Parser.default();
  const parseResult = parse(sql) as {
    parse_tree: { stmts: RawStmt[] };
    error: string | null;
  };
  if (parseResult.error !== null) {
    throw new Error(parseResult.error);
  }
  if (parseResult.parse_tree.stmts.length === 0) {
    throw new Error("Invalid query");
  }
  if (parseResult.parse_tree.stmts.length > 1) {
    throw new Error("Multiple statements not supported");
  }
  const stmt = parseResult.parse_tree.stmts[0]!;
  if (stmt.stmt === undefined) {
    throw new Error("Invalid query");
  }
  return stmt.stmt;
};
/**
 * Validate a SQL query.
 *
 * @param sql - SQL query
 */
export const validateAllowableSQLQuery = async (sql: string) => {
  const crypto = await import(/* webpackIgnore: true */ "node:crypto");
  const hash = crypto
    .createHash("sha256")
    .update(sql)
    .digest("hex")
    .slice(0, 10);
  if (ALLOW_CACHE.has(hash)) {
    const result = ALLOW_CACHE.get(hash)!;
    ALLOW_CACHE.delete(hash);
    ALLOW_CACHE.set(hash, result);
    if (result) return;
    throw new Error("Invalid query");
  }
  let root: Node;
  try {
    root = await parseSQLQuery(sql);
  } catch (error) {
    ALLOW_CACHE.set(hash, false);
    throw error;
  }
  const validate = (node: Node) => {
    if (ALLOW_LIST.has(getNodeType(node)) === false) {
      throw new Error(`${getNodeType(node)} not supported`);
    }
    // @ts-ignore
    ALLOW_LIST.get(getNodeType(node))!.validate?.(node[getNodeType(node)]);
    for (const child of ALLOW_LIST.get(getNodeType(node))!.children(
      // @ts-ignore
      node[getNodeType(node)],
    )) {
      validate(child);
    }
  };
  try {
    validate(root);
  } catch (error) {
    ALLOW_CACHE.set(hash, false);
    throw error;
  }
  ALLOW_CACHE.set(hash, true);
  if (ALLOW_CACHE.size > 1_000_000) {
    const firstKey = ALLOW_CACHE.keys().next().value;
    if (firstKey) ALLOW_CACHE.delete(firstKey);
  }
};
/**
 * Return `true` if the SQL query is readonly, else `false`.
 *
 * @param sql - SQL query
 */
export const isReadonlySQLQuery = async (sql: string): Promise<boolean> => {
  let root: Node;
  try {
    root = await parseSQLQuery(sql);
  } catch {
    return false;
  }
  const validate = (node: Node) => {
    if (ALLOW_LIST.has(getNodeType(node)) === false) {
      throw new Error(`${getNodeType(node)} not supported`);
    }
    for (const child of ALLOW_LIST.get(getNodeType(node))!.children(
      // @ts-ignore
      node[getNodeType(node)],
    )) {
      validate(child);
    }
  };
  try {
    validate(root);
    return true;
  } catch {
    return false;
  }
};
/**
 * Find all referenced relations in a SQL query.
 *
 * @param sql - SQL query
 */
export const getSQLQueryRelations = async (
  sql: string,
): Promise<Set<string>> => {
  const crypto = await import(/* webpackIgnore: true */ "node:crypto");
  if (sql.length > 5_000) {
    throw new Error("Invalid query");
  }
  const hash = crypto
    .createHash("sha256")
    .update(sql)
    .digest("hex")
    .slice(0, 10);
  if (RELATIONS_CACHE.has(hash)) {
    const result = RELATIONS_CACHE.get(hash)!;
    RELATIONS_CACHE.delete(hash);
    RELATIONS_CACHE.set(hash, result);
    return result;
  }
  const root = await parseSQLQuery(sql);
  const find = (node: Node) => {
    if (FIND_LIST.has(getNodeType(node)) === false) {
      throw new Error(`${getNodeType(node)} not supported`);
    }
    if (getNodeType(node) === "RangeVar") {
      const relation = (node as { RangeVar: RangeVar }).RangeVar.relname;
      if (relation) relations.add(relation);
    }
    for (const child of FIND_LIST.get(getNodeType(node))!.children(
      // @ts-ignore
      node[getNodeType(node)],
    )) {
      find(child);
    }
  };
  const relations = new Set<string>();
  find(root);
  RELATIONS_CACHE.set(hash, relations);
  if (RELATIONS_CACHE.size > 1_000_000) {
    const firstKey = RELATIONS_CACHE.keys().next().value;
    if (firstKey) RELATIONS_CACHE.delete(firstKey);
  }
  return relations;
};
// https://github.com/launchql/pgsql-parser/blob/f1df82ed4358e47c682e....ts#L38
const INTEGER_VALIDATOR: ValidatorNode<"Integer"> = {
  node: "Integer",
  children: () => [],
};
const FLOAT_VALIDATOR: ValidatorNode<"Float"> = {
  node: "Float",
  children: () => [],
};
const BOOLEAN_VALIDATOR: ValidatorNode<"Boolean"> = {
  node: "Boolean",
  children: () => [],
};
const STRING_VALIDATOR: ValidatorNode<"String"> = {
  node: "String",
  children: () => [],
};
const BIT_STRING_VALIDATOR: ValidatorNode<"BitString"> = {
  node: "BitString",
  children: () => [],
};
const LIST_VALIDATOR: ValidatorNode<"List"> = {
  node: "List",
  children: (node) => [...(node.items ?? [])],
};
const OID_LIST_VALIDATOR: ValidatorNode<"OidList"> = {
  node: "OidList",
  children: (node) => [...(node.items ?? [])],
};
const INT_LIST_VALIDATOR: ValidatorNode<"IntList"> = {
  node: "IntList",
  children: (node) => [...(node.items ?? [])],
};
const A_CONST_VALIDATOR: ValidatorNode<"A_Const"> = {
  node: "A_Const",
  children: () => [],
};
const ALIAS_VALIDATOR: ValidatorNode<"Alias"> = {
  node: "Alias",
  children: (node) => [...(node.colnames ?? [])],
};
const RANGE_VAR_VALIDATOR: ValidatorNode<"RangeVar"> = {
  node: "RangeVar",
  children: (node) => [...(node.alias ? [{ Alias: node.alias }] : [])],
  validate: (node) => {
    if (node.schemaname) {
      throw new Error("Schema name not supported");
    }
    if (node.relname && SYSTEM_TABLES.has(node.relname)) {
      throw new Error("System tables not supported");
    }
  },
};
const VAR_VALIDATOR: ValidatorNode<"Var"> = {
  node: "Var",
  children: (node) => [...(node.xpr ? [node.xpr] : [])],
};
const PARAM_VALIDATOR: ValidatorNode<"Param"> = {
  node: "Param",
  children: (node) => [...(node.xpr ? [node.xpr] : [])],
};
const AGGREF_VALIDATOR: ValidatorNode<"Aggref"> = {
  node: "Aggref",
  children: (node) => [
    ...(node.aggargtypes ?? []),
    ...(node.aggdirectargs ?? []),
    ...(node.args ?? []),
    ...(node.aggorder ?? []),
    ...(node.aggdistinct ?? []),
    ...(node.aggfilter ? [node.aggfilter] : []),
  ],
};
const GROUPING_FUNC_VALIDATOR: ValidatorNode<"GroupingFunc"> = {
  node: "GroupingFunc",
  children: (node) => [...(node.args ?? []), ...(node.refs ?? [])],
};
const WINDOW_FUNC_VALIDATOR: ValidatorNode<"WindowFunc"> = {
  node: "WindowFunc",
  children: (node) => [
    ...(node.args ?? []),
    ...(node.aggfilter ? [node.aggfilter] : []),
  ],
};
const NAMED_ARG_EXPR_VALIDATOR: ValidatorNode<"NamedArgExpr"> = {
  node: "NamedArgExpr",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.arg ? [node.arg] : []),
  ],
};
const OP_EXPR_VALIDATOR: ValidatorNode<"OpExpr"> = {
  node: "OpExpr",
  children: (node) => [...(node.xpr ? [node.xpr] : []), ...(node.args ?? [])],
};
const DISTINCT_EXPR_VALIDATOR: ValidatorNode<"DistinctExpr"> = {
  node: "DistinctExpr",
  children: (node) => [...(node.xpr ? [node.xpr] : []), ...(node.args ?? [])],
};
const NULL_IF_EXPR_VALIDATOR: ValidatorNode<"NullIfExpr"> = {
  node: "NullIfExpr",
  children: (node) => [...(node.xpr ? [node.xpr] : []), ...(node.args ?? [])],
};
const SCALAR_ARRAY_OP_EXPR_VALIDATOR: ValidatorNode<"ScalarArrayOpExpr"> = {
  node: "ScalarArrayOpExpr",
  children: (node) => [...(node.xpr ? [node.xpr] : []), ...(node.args ?? [])],
};
const BOOL_EXPR_VALIDATOR: ValidatorNode<"BoolExpr"> = {
  node: "BoolExpr",
  children: (node) => [...(node.xpr ? [node.xpr] : []), ...(node.args ?? [])],
};
const FIELD_SELECT_VALIDATOR: ValidatorNode<"FieldSelect"> = {
  node: "FieldSelect",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.arg ? [node.arg] : []),
  ],
};
const RELABEL_TYPE_VALIDATOR: ValidatorNode<"RelabelType"> = {
  node: "RelabelType",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.arg ? [node.arg] : []),
  ],
};
const ARRAY_COERCE_EXPR_VALIDATOR: ValidatorNode<"ArrayCoerceExpr"> = {
  node: "ArrayCoerceExpr",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.arg ? [node.arg] : []),
    ...(node.elemexpr ? [node.elemexpr] : []),
  ],
};
const CONVERT_ROWTYPE_EXPR_VALIDATOR: ValidatorNode<"ConvertRowtypeExpr"> = {
  node: "ConvertRowtypeExpr",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.arg ? [node.arg] : []),
  ],
};
const COLLATE_EXPR_VALIDATOR: ValidatorNode<"CollateExpr"> = {
  node: "CollateExpr",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.arg ? [node.arg] : []),
  ],
};
const CASE_EXPR_VALIDATOR: ValidatorNode<"CaseExpr"> = {
  node: "CaseExpr",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.arg ? [node.arg] : []),
    ...(node.args ?? []),
    ...(node.defresult ? [node.defresult] : []),
  ],
};
const CASE_WHEN_VALIDATOR: ValidatorNode<"CaseWhen"> = {
  node: "CaseWhen",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.expr ? [node.expr] : []),
    ...(node.result ? [node.result] : []),
  ],
};
const CASE_TEST_EXPR_VALIDATOR: ValidatorNode<"CaseTestExpr"> = {
  node: "CaseTestExpr",
  children: (node) => [...(node.xpr ? [node.xpr] : [])],
};
const ARRAY_EXPR_VALIDATOR: ValidatorNode<"ArrayExpr"> = {
  node: "ArrayExpr",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.elements ?? []),
  ],
};
const ROW_EXPR_VALIDATOR: ValidatorNode<"RowExpr"> = {
  node: "RowExpr",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.args ?? []),
    ...(node.colnames ?? []),
  ],
};
const ROW_COMPARE_EXPR_VALIDATOR: ValidatorNode<"RowCompareExpr"> = {
  node: "RowCompareExpr",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.opnos ?? []),
    ...(node.opfamilies ?? []),
    ...(node.inputcollids ?? []),
    ...(node.largs ?? []),
    ...(node.rargs ?? []),
  ],
};
const COALESC_EXPR_VALIDATOR: ValidatorNode<"CoalesceExpr"> = {
  node: "CoalesceExpr",
  children: (node) => [...(node.xpr ? [node.xpr] : []), ...(node.args ?? [])],
};
const MIN_MAX_EXPR_VALIDATOR: ValidatorNode<"MinMaxExpr"> = {
  node: "MinMaxExpr",
  children: (node) => [...(node.xpr ? [node.xpr] : []), ...(node.args ?? [])],
};
const SQL_VALUE_FUNCTION_VALIDATOR: ValidatorNode<"SQLValueFunction"> = {
  node: "SQLValueFunction",
  children: (node) => [...(node.xpr ? [node.xpr] : [])],
};
const JSON_FORMAT_VALIDATOR: ValidatorNode<"JsonFormat"> = {
  node: "JsonFormat",
  children: () => [],
};
const JSON_RETURNING_VALIDATOR: ValidatorNode<"JsonReturning"> = {
  node: "JsonReturning",
  children: (node) => [...(node.format ? [{ JsonFormat: node.format }] : [])],
};
const JSON_VALUE_EXPR_VALIDATOR: ValidatorNode<"JsonValueExpr"> = {
  node: "JsonValueExpr",
  children: (node) => [
    ...(node.raw_expr ? [node.raw_expr] : []),
    ...(node.formatted_expr ? [node.formatted_expr] : []),
    ...(node.format ? [{ JsonFormat: node.format }] : []),
  ],
};
const JSON_CONSTRUCTOR_EXPR_VALIDATOR: ValidatorNode<"JsonConstructorExpr"> = {
  node: "JsonConstructorExpr",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.args ?? []),
    ...(node.func ? [node.func] : []),
    ...(node.coercion ? [node.coercion] : []),
    ...(node.returning ? [{ JsonReturning: node.returning }] : []),
  ],
};
const JSON_IS_PREDICATE_VALIDATOR: ValidatorNode<"JsonIsPredicate"> = {
  node: "JsonIsPredicate",
  children: (node) => [
    ...(node.expr ? [node.expr] : []),
    ...(node.format ? [{ JsonFormat: node.format }] : []),
  ],
};
const NULL_TEST_VALIDATOR: ValidatorNode<"NullTest"> = {
  node: "NullTest",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.arg ? [node.arg] : []),
  ],
};
const BOOLEAN_TEST_VALIDATOR: ValidatorNode<"BooleanTest"> = {
  node: "BooleanTest",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.arg ? [node.arg] : []),
  ],
};
const COERCE_TO_DOMAIN_VALIDATOR: ValidatorNode<"CoerceToDomain"> = {
  node: "CoerceToDomain",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.arg ? [node.arg] : []),
  ],
};
const COERCE_TO_DOMAIN_VALUE_VALIDATOR: ValidatorNode<"CoerceToDomainValue"> = {
  node: "CoerceToDomainValue",
  children: (node) => [...(node.xpr ? [node.xpr] : [])],
};
const CURRENT_OF_EXPR_VALIDATOR: ValidatorNode<"CurrentOfExpr"> = {
  node: "CurrentOfExpr",
  children: (node) => [...(node.xpr ? [node.xpr] : [])],
};
const NEXT_VALUE_EXPR_VALIDATOR: ValidatorNode<"NextValueExpr"> = {
  node: "NextValueExpr",
  children: (node) => [...(node.xpr ? [node.xpr] : [])],
};
const INFERENCE_ELEM_VALIDATOR: ValidatorNode<"InferenceElem"> = {
  node: "InferenceElem",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.expr ? [node.expr] : []),
  ],
};
const TARGET_ENTRY_VALIDATOR: ValidatorNode<"TargetEntry"> = {
  node: "TargetEntry",
  children: (node) => [
    ...(node.xpr ? [node.xpr] : []),
    ...(node.expr ? [node.expr] : []),
  ],
};
const RANGE_TBL_REF_VALIDATOR: ValidatorNode<"RangeTblRef"> = {
  node: "RangeTblRef",
  children: () => [],
};
const JOIN_EXPR_VALIDATOR: ValidatorNode<"JoinExpr"> = {
  node: "JoinExpr",
  children: (node) => [
    ...(node.larg ? [node.larg] : []),
    ...(node.rarg ? [node.rarg] : []),
    ...(node.usingClause ?? []),
    ...(node.join_using_alias ? [{ Alias: node.join_using_alias }] : []),
    ...(node.quals ? [node.quals] : []),
    ...(node.alias ? [{ Alias: node.alias }] : []),
  ],
};
const FROM_EXPR_VALIDATOR: ValidatorNode<"FromExpr"> = {
  node: "FromExpr",
  children: (node) => [
    ...(node.fromlist ?? []),
    ...(node.quals ? [node.quals] : []),
  ],
};
const ON_CONFLICT_EXPR_VALIDATOR: ValidatorNode<"OnConflictExpr"> = {
  node: "OnConflictExpr",
  children: (node) => [
    ...(node.arbiterElems ?? []),
    ...(node.arbiterWhere ? [node.arbiterWhere] : []),
    ...(node.onConflictSet ?? []),
    ...(node.onConflictWhere ? [node.onConflictWhere] : []),
    ...(node.exclRelTlist ?? []),
  ],
};
const TYPE_NAME_VALIDATOR: ValidatorNode<"TypeName"> = {
  node: "TypeName",
  children: (node) => [
    ...(node.names ?? []),
    ...(node.typmods ?? []),
    ...(node.arrayBounds ?? []),
  ],
};
const COLUMN_REF_VALIDATOR: ValidatorNode<"ColumnRef"> = {
  node: "ColumnRef",
  children: (node) => [...(node.fields ?? [])],
};
const PARAM_REF_VALIDATOR: ValidatorNode<"ParamRef"> = {
  node: "ParamRef",
  children: () => [],
};
const A_EXPR_VALIDATOR: ValidatorNode<"A_Expr"> = {
  node: "A_Expr",
  children: (node) => [
    ...(node.name ?? []),
    ...(node.lexpr ? [node.lexpr] : []),
    ...(node.rexpr ? [node.rexpr] : []),
  ],
};
const TYPE_CAST_VALIDATOR: ValidatorNode<"TypeCast"> = {
  node: "TypeCast",
  children: (node) => [
    ...(node.arg ? [node.arg] : []),
    ...(node.typeName ? [{ TypeName: node.typeName }] : []),
  ],
};
const COLLATE_CLAUSE_VALIDATOR: ValidatorNode<"CollateClause"> = {
  node: "CollateClause",
  children: (node) => [
    ...(node.arg ? [node.arg] : []),
    ...(node.collname ?? []),
  ],
};
const ALLOWED_FUNCTIONS = new Set([
  "count",
  "sum",
  "avg",
  "min",
  "max",
  "lower",
  "upper",
  "length",
  "trim",
  "replace",
  "substring",
  "cast",
  "concat",
  "now",
  "current_timestamp",
  "current_date",
  "current_time",
  "coalesce",
  "json_agg",
  "json_object",
  "json_array",
  "json_object_agg",
  "json_array_agg",
  "json_build_array",
  "row_number",
  "rank",
  "dense_rank",
  "percent_rank",
  "cume_dist",
  "ntile",
  "lag",
  "lead",
  "first_value",
  "last_value",
  "nth_value",
  "abs",
  "cbrt",
  "ceil",
  "ceiling",
  "degrees",
  "div",
  "exp",
  "floor",
  "ln",
  "log",
  "mod",
  "pi",
  "power",
  "radians",
  "random",
  "round",
  "sign",
  "sqrt",
  "width_bucket",
  "acos",
  "asin",
  "atan",
  "atan2",
  "cos",
  "cot",
  "sin",
  "tan",
]);
const FUNC_CALL_VALIDATOR: ValidatorNode<"FuncCall"> = {
  node: "FuncCall",
  children: (node) => [
    ...(node.funcname ?? []),
    ...(node.args ?? []),
    ...(node.agg_order ?? []),
    ...(node.agg_filter ? [node.agg_filter] : []),
    ...(node.over ? [{ WindowDef: node.over }] : []),
  ],
  validate: (node) => {
    if (
      node.funcname?.every(
        (name) =>
          getNodeType(name) === "String" &&
          // @ts-ignore
          ALLOWED_FUNCTIONS.has(name.String.sval),
      )
    ) {
      return;
    }
    throw new Error("Function call not supported");
  },
};
const A_STAR_VALIDATOR: ValidatorNode<"A_Star"> = {
  node: "A_Star",
  children: () => [],
};
const A_INDICES_VALIDATOR: ValidatorNode<"A_Indices"> = {
  node: "A_Indices",
  children: (node) => [
    ...(node.lidx ? [node.lidx] : []),
    ...(node.uidx ? [node.uidx] : []),
  ],
};
const A_INDIRECTION_VALIDATOR: ValidatorNode<"A_Indirection"> = {
  node: "A_Indirection",
  children: (node) => [
    ...(node.arg ? [node.arg] : []),
    ...(node.indirection ?? []),
  ],
};
const A_ARRAY_EXPR_VALIDATOR: ValidatorNode<"A_ArrayExpr"> = {
  node: "A_ArrayExpr",
  children: (node) => [...(node.elements ?? [])],
};
const RES_TARGET_VALIDATOR: ValidatorNode<"ResTarget"> = {
  node: "ResTarget",
  children: (node) => [
    ...(node.indirection ?? []),
    ...(node.val ? [node.val] : []),
  ],
};
const MULTI_ASSIGN_REF_VALIDATOR: ValidatorNode<"MultiAssignRef"> = {
  node: "MultiAssignRef",
  children: (node) => [...(node.source ? [node.source] : [])],
};
const SORT_BY_VALIDATOR: ValidatorNode<"SortBy"> = {
  node: "SortBy",
  children: (node) => [
    ...(node.node ? [node.node] : []),
    ...(node.useOp ?? []),
  ],
};
const WINDOW_DEF_VALIDATOR: ValidatorNode<"WindowDef"> = {
  node: "WindowDef",
  children: (node) => [
    ...(node.partitionClause ?? []),
    ...(node.orderClause ?? []),
    ...(node.startOffset ? [node.startOffset] : []),
    ...(node.endOffset ? [node.endOffset] : []),
  ],
};
const RANGE_SUBSELECT_VALIDATOR: ValidatorNode<"RangeSubselect"> = {
  node: "RangeSubselect",
  children: (node) => [
    ...(node.subquery ? [node.subquery] : []),
    ...(node.alias ? [{ Alias: node.alias }] : []),
  ],
};
const SORT_GROUP_CLAUSE_VALIDATOR: ValidatorNode<"SortGroupClause"> = {
  node: "SortGroupClause",
  children: () => [],
};
const GROUPING_SET_VALIDATOR: ValidatorNode<"GroupingSet"> = {
  node: "GroupingSet",
  children: (node) => [...(node.content ?? [])],
};
const WITH_CLAUSE_VALIDATOR: ValidatorNode<"WithClause"> = {
  node: "WithClause",
  children: (node) => [...(node.ctes ?? [])],
  validate: (node) => {
    if (node.recursive) {
      throw new Error("Recursive CTEs not supported");
    }
  },
};
const COMMON_TABLE_EXPR_VALIDATOR: ValidatorNode<"CommonTableExpr"> = {
  node: "CommonTableExpr",
  children: (node) => [
    ...(node.aliascolnames ?? []),
    ...(node.ctequery ? [node.ctequery] : []),
    ...(node.search_clause ? [{ CTESearchClause: node.search_clause }] : []),
    ...(node.cycle_clause ? [{ CTECycleClause: node.cycle_clause }] : []),
    ...(node.ctecolnames ?? []),
    ...(node.ctecoltypes ?? []),
    ...(node.ctecoltypmods ?? []),
    ...(node.ctecolcollations ?? []),
  ],
  validate: (node) => {
    if (node.ctematerialized === "CTEMaterializeAlways" || node.cterecursive) {
      throw new Error("Invalid CTE");
    }
  },
};
const JSON_OUTPUT_VALIDATOR: ValidatorNode<"JsonOutput"> = {
  node: "JsonOutput",
  children: (node) => [
    ...(node.returning ? [{ JsonReturning: node.returning }] : []),
  ],
};
const JSON_KEY_VALUE_VALIDATOR: ValidatorNode<"JsonKeyValue"> = {
  node: "JsonKeyValue",
  children: (node) => [
    ...(node.key ? [node.key] : []),
    ...(node.value ? [{ JsonValueExpr: node.value }] : []),
  ],
};
const JSON_OBJECT_CONSTRUCTOR_VALIDATOR: ValidatorNode<"JsonObjectConstructor"> =
  {
    node: "JsonObjectConstructor",
    children: (node) => [
      ...(node.exprs ?? []),
      ...(node.output ? [{ JsonOutput: node.output }] : []),
    ],
  };
const JSON_ARRAY_CONSTRUCTOR_VALIDATOR: ValidatorNode<"JsonArrayConstructor"> =
  {
    node: "JsonArrayConstructor",
    children: (node) => [
      ...(node.exprs ?? []),
      ...(node.output ? [{ JsonOutput: node.output }] : []),
    ],
  };
const JSON_ARRAY_QUERY_CONSTRUCTOR_VALIDATOR: ValidatorNode<"JsonArrayQueryConstructor"> =
  {
    node: "JsonArrayQueryConstructor",
    children: (node) => [
      ...(node.query ? [node.query] : []),
      ...(node.output ? [{ JsonOutput: node.output }] : []),
      ...(node.format ? [{ JsonFormat: node.format }] : []),
    ],
  };
const JSON_AGG_CONSTRUCTOR_VALIDATOR: ValidatorNode<"JsonAggConstructor"> = {
  node: "JsonAggConstructor",
  children: (node) => [
    ...(node.output ? [{ JsonOutput: node.output }] : []),
    ...(node.agg_filter ? [node.agg_filter] : []),
    ...(node.agg_order ?? []),
    ...(node.over ? [{ WindowDef: node.over }] : []),
  ],
};
const JSON_OBJECT_AGG_VALIDATOR: ValidatorNode<"JsonObjectAgg"> = {
  node: "JsonObjectAgg",
  children: (node) => [
    ...(node.constructor ? [{ JsonAggConstructor: node.constructor }] : []),
    ...(node.arg ? [{ JsonKeyValue: node.arg }] : []),
  ],
};
const JSON_ARRAY_AGG_VALIDATOR: ValidatorNode<"JsonArrayAgg"> = {
  node: "JsonArrayAgg",
  children: (node) => [
    ...(node.constructor ? [{ JsonAggConstructor: node.constructor }] : []),
    ...(node.arg ? [{ JsonValueExpr: node.arg }] : []),
  ],
};
const SELECT_STMT_VALIDATOR: ValidatorNode<"SelectStmt"> = {
  node: "SelectStmt",
  children: (node) => [
    ...(node.distinctClause ?? []),
    ...(node.intoClause ? [{ IntoClause: node.intoClause }] : []),
    ...(node.targetList ?? []),
    ...(node.fromClause ?? []),
    ...(node.whereClause ? [node.whereClause] : []),
    ...(node.groupClause ?? []),
    ...(node.havingClause ? [node.havingClause] : []),
    ...(node.windowClause ?? []),
    ...(node.valuesLists ?? []),
    ...(node.sortClause ?? []),
    ...(node.limitOffset ? [node.limitOffset] : []),
    ...(node.limitCount ? [node.limitCount] : []),
    ...(node.lockingClause ?? []),
    ...(node.withClause ? [{ WithClause: node.withClause }] : []),
    ...(node.larg ? [{ SelectStmt: node.larg }] : []),
    ...(node.rarg ? [{ SelectStmt: node.rarg }] : []),
  ],
  validate: (node) => {
    if (node.lockingClause) {
      throw new Error("Invalid query");
    }
  },
};
const COMMENT_STMT_VALIDATOR: ValidatorNode<"CommentStmt"> = {
  node: "CommentStmt",
  children: (node) => [...(node.object ? [node.object] : [])],
};
/** Validation rules for allowed Postgres SQL AST nodes. */
const ALLOW_LIST = new Map(
  [
    INTEGER_VALIDATOR,
    FLOAT_VALIDATOR,
    BOOLEAN_VALIDATOR,
    STRING_VALIDATOR,
    BIT_STRING_VALIDATOR,
    LIST_VALIDATOR,
    OID_LIST_VALIDATOR,
    INT_LIST_VALIDATOR,
    A_CONST_VALIDATOR,
    ALIAS_VALIDATOR,
    RANGE_VAR_VALIDATOR,
    VAR_VALIDATOR,
    PARAM_VALIDATOR,
    AGGREF_VALIDATOR,
    GROUPING_FUNC_VALIDATOR,
    WINDOW_FUNC_VALIDATOR,
    NAMED_ARG_EXPR_VALIDATOR,
    OP_EXPR_VALIDATOR,
    DISTINCT_EXPR_VALIDATOR,
    NULL_IF_EXPR_VALIDATOR,
    SCALAR_ARRAY_OP_EXPR_VALIDATOR,
    BOOL_EXPR_VALIDATOR,
    FIELD_SELECT_VALIDATOR,
    RELABEL_TYPE_VALIDATOR,
    ARRAY_COERCE_EXPR_VALIDATOR,
    CONVERT_ROWTYPE_EXPR_VALIDATOR,
    COLLATE_EXPR_VALIDATOR,
    CASE_EXPR_VALIDATOR,
    CASE_WHEN_VALIDATOR,
    CASE_TEST_EXPR_VALIDATOR,
    ARRAY_EXPR_VALIDATOR,
    ROW_EXPR_VALIDATOR,
    ROW_COMPARE_EXPR_VALIDATOR,
    COALESC_EXPR_VALIDATOR,
    MIN_MAX_EXPR_VALIDATOR,
    SQL_VALUE_FUNCTION_VALIDATOR,
    JSON_FORMAT_VALIDATOR,
    JSON_RETURNING_VALIDATOR,
    JSON_VALUE_EXPR_VALIDATOR,
    JSON_CONSTRUCTOR_EXPR_VALIDATOR,
    JSON_IS_PREDICATE_VALIDATOR,
    NULL_TEST_VALIDATOR,
    BOOLEAN_TEST_VALIDATOR,
    COERCE_TO_DOMAIN_VALIDATOR,
    COERCE_TO_DOMAIN_VALUE_VALIDATOR,
    CURRENT_OF_EXPR_VALIDATOR,
    NEXT_VALUE_EXPR_VALIDATOR,
    INFERENCE_ELEM_VALIDATOR,
    TARGET_ENTRY_VALIDATOR,
    RANGE_TBL_REF_VALIDATOR,
    JOIN_EXPR_VALIDATOR,
    FROM_EXPR_VALIDATOR,
    ON_CONFLICT_EXPR_VALIDATOR,
    TYPE_NAME_VALIDATOR,
    COLUMN_REF_VALIDATOR,
    PARAM_REF_VALIDATOR,
    A_EXPR_VALIDATOR,
    TYPE_CAST_VALIDATOR,
    COLLATE_CLAUSE_VALIDATOR,
    FUNC_CALL_VALIDATOR,
    A_STAR_VALIDATOR,
    A_INDICES_VALIDATOR,
    A_INDIRECTION_VALIDATOR,
    A_ARRAY_EXPR_VALIDATOR,
    RES_TARGET_VALIDATOR,
    MULTI_ASSIGN_REF_VALIDATOR,
    SORT_BY_VALIDATOR,
    WINDOW_DEF_VALIDATOR,
    RANGE_SUBSELECT_VALIDATOR,
    SORT_GROUP_CLAUSE_VALIDATOR,
    GROUPING_SET_VALIDATOR,
    WITH_CLAUSE_VALIDATOR,
    COMMON_TABLE_EXPR_VALIDATOR,
    JSON_OUTPUT_VALIDATOR,
    JSON_KEY_VALUE_VALIDATOR,
    JSON_OBJECT_CONSTRUCTOR_VALIDATOR,
    JSON_ARRAY_CONSTRUCTOR_VALIDATOR,
    JSON_ARRAY_QUERY_CONSTRUCTOR_VALIDATOR,
    JSON_AGG_CONSTRUCTOR_VALIDATOR,
    JSON_OBJECT_AGG_VALIDATOR,
    JSON_ARRAY_AGG_VALIDATOR,
    SELECT_STMT_VALIDATOR,
    COMMENT_STMT_VALIDATOR,
  ].map((node) => [node.node, node]),
);
const UPDATE_STMT_VALIDATOR: ValidatorNode<"UpdateStmt"> = {
  node: "UpdateStmt",
  children: (node) => [
    ...(node.relation ? [{ RangeVar: node.relation }] : []),
    ...(node.targetList ?? []),
    ...(node.whereClause ? [node.whereClause] : []),
    ...(node.fromClause ?? []),
    ...(node.returningList ?? []),
    ...(node.withClause ? [{ WithClause: node.withClause }] : []),
  ],
};
const DELETE_STMT_VALIDATOR: ValidatorNode<"DeleteStmt"> = {
  node: "DeleteStmt",
  children: (node) => [
    ...(node.relation ? [{ RangeVar: node.relation }] : []),
    ...(node.usingClause ?? []),
    ...(node.whereClause ? [node.whereClause] : []),
    ...(node.returningList ?? []),
    ...(node.withClause ? [{ WithClause: node.withClause }] : []),
  ],
};
const FIND_LIST = new Map(
  [
    ...Array.from(ALLOW_LIST.values()),
    UPDATE_STMT_VALIDATOR,
    DELETE_STMT_VALIDATOR,
  ].map((node) => [node.node, node]),
);
// NOT_ALLOW_LIST
// ParseResult
// ScanResult
// TableFunc
// IntoClause
// SubscriptingRef
// FuncExpr
// SubLink
// SubPlan
// AlternativeSubPlan
// FieldStore
// CoerceViaIO
// XmlExpr
// SetToDefault
// Query
// RoleSpec
// RangeFunction
// RangeTableFunc
// RangeTableFuncCol
// RangeTableSample
// ColumnDef
// TableLikeClause
// IndexElem
// DefElem
// LockingClause
// XmlSerialize
// PartitionElem
// PartitionSpec
// PartitionBounSpec
// PartitionRangeDatum
// PartitionCmd
// RangeTableEntry
// RTEPermissionInfo
// RangeTblFunction
// TableSampleClause
// WithCheckOption
// WindowClause
// RowMarkClause
// InferClausej
// OnConflictClause
// CTESearchClause
// CTECycleClause
// MergeWhenClause
// MergeAction
// TriggerTransition
// RawStmt
// InsertStmt
// DeleteStmt
// UpdateStmt
// MergeStmt
// SetOperationStmt
// ReturnStmt
// PLAssignStmt
// CreateSchemaStmt
// AlterTableStmt
// ReplicaIdentityStmt
// AlterTableCmd
// AlterCollationStmt
// AlterDomainStmt
// GrantStmt
// ObjectWithArgs
// AccessPriv
// GrantRoleStmt
// AlterDefaultPrivilegesStmt
// CopyStmt
// VariableSetStmt
// VariableShowStmt
// CreateStmt
// Constraint
// CreateTableSpaceStmt
// DropTableSpaceStmt
// AlterTableSpaceOptionsStmt
// AlterTableMoveAllStmt
// CreateExtensionStmt
// AlterExtensionStmt
// AlterExtensionContentsStmt
// CreateFdwStmt
// AlterFdwStmt
// CreateForeignServerStmt
// AlterForeignServerStmt
// CreateForeignTableStmt
// CreateUserMappingStmt
// AlterUserMappingStmt
// DropUserMappingStmt
// ImportForeignSchemaStmt
// CreatePolicyStmt
// AlterPolicyStmt
// CreateAmStmt
// CreateTrigStmt
// CreateEventTrigStmt
// AlterEventTrigStmt
// CreatePLangStmt
// CreateRoleStmt
// AlterRoleStmt
// AlterRoleSetStmt
// DropRoleStmt
// CreateSeqStmt
// AlterSeqStmt
// DefineStmt
// CreateDomainStmt
// CreateOpClassStmt
// CreateOpClassItem
// CreateOpFamilyStmt
// AlterOpFamilyStmt
// DropStmt
// TruncateStmt
// SecLabelStmt
// DeclareCursorStmt
// ClosePortalStmt
// FetchStmt
// IndexStmt
// CreateStatsStmt
// StatsElem
// AlterStatsStmt
// CreateFunctionStmt
// FunctionParameter
// AlterFunctionStmt
// DoStmt
// InlineCodeBlock
// CallStmt
// CallContext
// RenameStmt
// AlterObjectDependsStmt
// AlterObjectSchemaStmt
// AlterOwnerStmt
// AlterOperatorStmt
// AlterTypeStmt
// RuleStmt
// NotifyStmt
// ListenStmt
// UnlistenStmt
// TransactionStmt
// CompositeTypeStmt
// CreateEnumStmt
// CreateRangeStmt
// AlterEnumStmt
// ViewStmt
// LoadStmt
// CreatedbStmt
// AlterDatabaseStmt
// AlterDatabaseRefreshCollStmt
// AlterDatabaseSetStmt
// DropdbStmt
// AlterSystemStmt
// ClusterStmt
// VacuumStmt
// VacuumRelation
// ExplainStmt
// CreateTableAsStmt
// RefreshMatViewStmt
// CheckPointStmt
// DiscardStmt
// LockStmt
// ConstraintsSetStmt
// ReindexStmt
// CreateConversionStmt
// CreateCastStmt
// CreateTransformStmt
// PrepareStmt
// ExecuteStmt
// DeallocateStmt
// DropOwnedStmt
// ReassignOwnedStmt
// AlterTSDictionaryStmt
// AlterTSConfigurationStmt
// PublicationTable
// PublicationObjSpec
// CreatePublicationStmt
// AlterPublicationStmt
// CreateSubscriptionStmt
// AlterSubscriptionStmt
// DropSubscriptionStmt
// ScanToken
const SYSTEM_TABLES = new Set([
  "pg_statistic",
  "pg_type",
  "pg_foreign_table",
  "pg_proc_oid_index",
  "pg_proc_proname_args_nsp_index",
  "pg_type_oid_index",
  "pg_type_typname_nsp_index",
  "pg_attribute_relid_attnam_index",
  "pg_attribute_relid_attnum_index",
  "pg_class_oid_index",
  "pg_class_relname_nsp_index",
  "pg_class_tblspc_relfilenode_index",
  "pg_attrdef_adrelid_adnum_index",
  "pg_attrdef_oid_index",
  "pg_constraint_conname_nsp_index",
  "pg_constraint_conrelid_contypid_conname_index",
  "pg_constraint_contypid_index",
  "pg_constraint_oid_index",
  "pg_constraint_conparentid_index",
  "pg_inherits_relid_seqno_index",
  "pg_inherits_parent_index",
  "pg_index_indrelid_index",
  "pg_index_indexrelid_index",
  "pg_operator_oid_index",
  "pg_operator_oprname_l_r_n_index",
  "pg_opfamily_am_name_nsp_index",
  "pg_opfamily_oid_index",
  "pg_opclass_am_name_nsp_index",
  "pg_opclass_oid_index",
  "pg_am_name_index",
  "pg_am_oid_index",
  "pg_amop_fam_strat_index",
  "pg_amop_opr_fam_index",
  "pg_amop_oid_index",
  "pg_amproc_fam_proc_index",
  "pg_amproc_oid_index",
  "pg_language_name_index",
  "pg_language_oid_index",
  "pg_largeobject_metadata_oid_index",
  "pg_largeobject_loid_pn_index",
  "pg_aggregate_fnoid_index",
  "pg_statistic_relid_att_inh_index",
  "pg_statistic_ext_oid_index",
  "pg_statistic_ext_name_index",
  "pg_statistic_ext_relid_index",
  "pg_statistic_ext_data_stxoid_inh_index",
  "pg_rewrite_oid_index",
  "pg_rewrite_rel_rulename_index",
  "pg_trigger_tgconstraint_index",
  "pg_trigger_tgrelid_tgname_index",
  "pg_trigger_oid_index",
  "pg_event_trigger_evtname_index",
  "pg_event_trigger_oid_index",
  "pg_description_o_c_o_index",
  "pg_cast_oid_index",
  "pg_cast_source_target_index",
  "pg_enum_oid_index",
  "pg_enum_typid_label_index",
  "pg_enum_typid_sortorder_index",
  "pg_namespace_nspname_index",
  "pg_namespace_oid_index",
  "pg_conversion_default_index",
  "pg_conversion_name_nsp_index",
  "pg_conversion_oid_index",
  "pg_depend_depender_index",
  "pg_depend_reference_index",
  "pg_database_datname_index",
  "pg_database_oid_index",
  "pg_db_role_setting_databaseid_rol_index",
  "pg_tablespace_oid_index",
  "pg_tablespace_spcname_index",
  "pg_authid_rolname_index",
  "pg_authid_oid_index",
  "pg_auth_members_oid_index",
  "pg_auth_members_role_member_index",
  "pg_auth_members_member_role_index",
  "pg_auth_members_grantor_index",
  "pg_shdepend_depender_index",
  "pg_shdepend_reference_index",
  "pg_shdescription_o_c_index",
  "pg_ts_config_cfgname_index",
  "pg_ts_config_oid_index",
  "pg_ts_config_map_index",
  "pg_ts_dict_dictname_index",
  "pg_ts_dict_oid_index",
  "pg_ts_parser_prsname_index",
  "pg_ts_parser_oid_index",
  "pg_ts_template_tmplname_index",
  "pg_ts_template_oid_index",
  "pg_extension_oid_index",
  "pg_extension_name_index",
  "pg_foreign_data_wrapper_oid_index",
  "pg_foreign_data_wrapper_name_index",
  "pg_foreign_server_oid_index",
  "pg_foreign_server_name_index",
  "pg_user_mapping_oid_index",
  "pg_user_mapping_user_server_index",
  "pg_foreign_table_relid_index",
  "pg_policy_oid_index",
  "pg_policy_polrelid_polname_index",
  "pg_replication_origin_roiident_index",
  "pg_replication_origin_roname_index",
  "pg_default_acl_role_nsp_obj_index",
  "pg_default_acl_oid_index",
  "pg_init_privs_o_c_o_index",
  "pg_seclabel_object_index",
  "pg_shseclabel_object_index",
  "pg_collation_name_enc_nsp_index",
  "pg_collation_oid_index",
  "pg_parameter_acl_parname_index",
  "pg_parameter_acl_oid_index",
  "pg_partitioned_table_partrelid_index",
  "pg_range_rngtypid_index",
  "pg_range_rngmultitypid_index",
  "pg_transform_oid_index",
  "pg_transform_type_lang_index",
  "pg_sequence_seqrelid_index",
  "pg_publication_oid_index",
  "pg_publication_pubname_index",
  "pg_publication_namespace_oid_index",
  "pg_publication_namespace_pnnspid_pnpubid_index",
  "pg_publication_rel_oid_index",
  "pg_publication_rel_prrelid_prpubid_index",
  "pg_publication_rel_prpubid_index",
  "pg_subscription_oid_index",
  "pg_subscription_subname_index",
  "pg_subscription_rel_srrelid_srsubid_index",
  "pg_authid",
  "pg_shadow",
  "pg_roles",
  "pg_statistic_ext_data",
  "pg_hba_file_rules",
  "pg_settings",
  "pg_file_settings",
  "pg_backend_memory_contexts",
  "pg_ident_file_mappings",
  "pg_config",
  "pg_shmem_allocations",
  "pg_tables",
  "pg_user_mapping",
  "pg_replication_origin_status",
  "pg_subscription",
  "pg_attribute",
  "pg_proc",
  "pg_class",
  "pg_attrdef",
  "pg_constraint",
  "pg_inherits",
  "pg_index",
  "pg_operator",
  "pg_statio_all_sequences",
  "pg_opfamily",
  "pg_opclass",
  "pg_am",
  "pg_amop",
  "pg_amproc",
  "pg_language",
  "pg_largeobject_metadata",
  "pg_aggregate",
  "pg_statistic_ext",
  "pg_rewrite",
  "pg_trigger",
  "pg_event_trigger",
  "pg_description",
  "pg_cast",
  "pg_enum",
  "pg_namespace",
  "pg_conversion",
  "pg_depend",
  "pg_database",
  "pg_db_role_setting",
  "pg_tablespace",
  "pg_auth_members",
  "pg_shdepend",
  "pg_shdescription",
  "pg_ts_config",
  "pg_ts_config_map",
  "pg_ts_dict",
  "pg_ts_parser",
  "pg_ts_template",
  "pg_extension",
  "pg_foreign_data_wrapper",
  "pg_foreign_server",
  "pg_policy",
  "pg_replication_origin",
  "pg_default_acl",
  "pg_init_privs",
  "pg_seclabel",
  "pg_shseclabel",
  "pg_collation",
  "pg_parameter_acl",
  "pg_partitioned_table",
  "pg_range",
  "pg_transform",
  "pg_sequence",
  "pg_publication",
  "pg_publication_namespace",
  "pg_publication_rel",
  "pg_subscription_rel",
  "pg_group",
  "pg_user",
  "pg_policies",
  "pg_rules",
  "pg_views",
  "pg_matviews",
  "pg_indexes",
  "pg_sequences",
  "pg_stats",
  "pg_stats_ext",
  "pg_stats_ext_exprs",
  "pg_publication_tables",
  "pg_locks",
  "pg_cursors",
  "pg_available_extensions",
  "pg_available_extension_versions",
  "pg_prepared_xacts",
  "pg_prepared_statements",
  "pg_seclabels",
  "pg_timezone_abbrevs",
  "pg_timezone_names",
  "pg_stat_all_tables",
  "pg_stat_xact_all_tables",
  "pg_stat_xact_user_tables",
  "pg_stat_sys_tables",
  "pg_stat_xact_sys_tables",
  "pg_stat_user_tables",
  "pg_statio_all_tables",
  "pg_statio_sys_tables",
  "pg_statio_user_tables",
  "pg_stat_all_indexes",
  "pg_stat_sys_indexes",
  "pg_stat_user_indexes",
  "pg_statio_all_indexes",
  "pg_statio_sys_indexes",
  "pg_statio_user_indexes",
  "pg_statio_sys_sequences",
  "pg_statio_user_sequences",
  "pg_stat_activity",
  "pg_stat_replication",
  "pg_stat_slru",
  "pg_stat_wal_receiver",
  "pg_stat_recovery_prefetch",
  "pg_stat_subscription",
  "pg_stat_ssl",
  "pg_stat_gssapi",
  "pg_replication_slots",
  "pg_stat_replication_slots",
  "pg_stat_database",
  "pg_stat_database_conflicts",
  "pg_stat_user_functions",
  "pg_stat_xact_user_functions",
  "pg_stat_archiver",
  "pg_stat_bgwriter",
  "pg_stat_io",
  "pg_stat_wal",
  "pg_stat_progress_analyze",
  "pg_stat_progress_vacuum",
  "pg_stat_progress_cluster",
  "pg_stat_progress_create_index",
  "pg_stat_progress_basebackup",
  "pg_stat_progress_copy",
  "pg_user_mappings",
  "pg_stat_subscription_stats",
  "pg_largeobject",
]);
</file>

<file path="packages/core/src/utils/timer.bench.ts">
import { bench, run } from "mitata";
import { startClock } from "./timer.js";
// 21.62 ns/iter
bench("startClock", () => {
  const clock = startClock();
  clock();
}).gc("inner");
run();
</file>

<file path="packages/core/src/utils/timer.ts">
/**
 * Measures the elapsed wall clock time in milliseconds (ms) between two points.
 * @returns A function returning the elapsed time in milliseconds (ms).
 */
export function startClock() {
  const start = performance.now();
  return () => performance.now() - start;
}
</file>

<file path="packages/core/src/utils/truncate.test.ts">
import { expect } from "vitest";
import { test } from "vitest";
import { truncate } from "./truncate.js";
test("truncates a long string correctly", () => {
  expect(truncate("EnergyMarketOrderBookV1Contract", 24)).toBe(
    "EnergyMark...V1Contract",
  );
});
test("returns the same string if it's within maxLength", () => {
  expect(truncate("EnergyMarketOrderBookV1Contract", 31)).toBe(
    "EnergyMarketOrderBookV1Contract",
  );
});
test("truncates a long string correctly with default maxLength", () => {
  expect(truncate("EnergyMarketOrderBookV1Contract:Action")).toBe(
    "EnergyMarketOrde...1Contract:Action",
  );
});
</file>

<file path="packages/core/src/utils/truncate.ts">
/**
 * Truncates a given string to a specified maximum length, adding ellipsis in the middle if necessary.
 *
 * @param string - The string to be truncated.
 * @param maxLength - The maximum length of the truncated string, including the ellipsis. Defaults to 36.
 * @returns The truncated string with ellipsis in the middle if it exceeds the maximum length.
 */
export const truncate = (string: string, maxLength = 36): string => {
  if (string.length <= maxLength) return string;
  const prefixLength = Math.floor(maxLength / 2) - 2; // Keep half the start
  const suffixLength = Math.ceil(maxLength / 2) - 2; // Keep half the end
  return `${string.slice(0, prefixLength)}...${string.slice(-suffixLength)}`;
};
</file>

<file path="packages/core/src/utils/wait.ts">
/** Waits at least a specified amount of time.
 *
 * @param milliseconds Minimum number of milliseconds to wait.
 */
export async function wait(milliseconds: number) {
  if (process.env.NODE_ENV === "ci") return;
  return new Promise<void>((res) => setTimeout(res, milliseconds));
}
</file>

<file path="packages/core/src/utils/zipper.test.ts">
import { zipper, zipperMany } from "@/utils/zipper.js";
import { expect, test } from "vitest";
test("zipper", () => {
  const result = zipper([1, 3, 5], [2, 4, 6]);
  expect(result).toStrictEqual([1, 2, 3, 4, 5, 6]);
});
test("zipperMany", () => {
  const result = zipperMany([
    [1, 3, 5],
    [2, 4, 6],
    [7, 8, 9],
  ]);
  expect(result).toStrictEqual([1, 2, 3, 4, 5, 6, 7, 8, 9]);
});
</file>

<file path="packages/core/src/utils/zipper.ts">
/**
 * Merges two sorted arrays into a single sorted array.
 *
 * @param array1 - The first array to merge.
 * @param array2 - The second array to merge.
 * @param compare - The comparison function to use.
 *
 * @returns The merged array.
 *
 * @example
 * ```ts
 * const result = zipper([1, 3, 5], [2, 4, 6]);
 * // result = [1, 2, 3, 4, 5, 6]
 * ```
 */
export const zipper = <T>(
  array1: T[],
  array2: T[],
  compare?: (a: T, b: T) => number,
): T[] => {
  const result: T[] = [];
  let i = 0;
  let j = 0;
  while (i < array1.length && j < array2.length) {
    if (
      compare ? compare(array1[i]!, array2[j]!) < 0 : array1[i]! < array2[j]!
    ) {
      result.push(array1[i]!);
      i++;
    } else {
      result.push(array2[j]!);
      j++;
    }
  }
  if (i < array1.length) {
    result.push(...array1.slice(i));
  }
  if (j < array2.length) {
    result.push(...array2.slice(j));
  }
  return result;
};
/**
 * Merges many sorted arrays into a single sorted array.
 *
 * @param arrays - The arrays to merge.
 * @param compare - The comparison function to use.
 *
 * @returns The merged array.
 *
 * @example
 * ```ts
 * const result = zipperMany([
 *   [1, 3, 5],
 *   [2, 4, 6],
 *   [7, 8, 9],
 * ]);
 * // result = [1, 2, 3, 4, 5, 6, 7, 8, 9]
 * ```
 */
export const zipperMany = <T>(
  arrays: T[][],
  compare?: (a: T, b: T) => number,
): T[] => {
  if (arrays.length === 0) return [];
  if (arrays.length === 1) return arrays[0]!;
  let result: T[] = arrays[0]!;
  for (let i = 1; i < arrays.length; i++) {
    result = zipper(result, arrays[i]!, compare);
  }
  return result;
};
</file>

<file path="packages/core/src/index.test.ts">
import { readFileSync } from "node:fs";
import path from "node:path";
import { fileURLToPath } from "node:url";
import { build } from "vite";
import { expect, test } from "vitest";
let __dirname = fileURLToPath(new URL(".", import.meta.url));
__dirname = path.resolve(__dirname, "..");
const packageJson = JSON.parse(
  readFileSync(path.resolve(__dirname, "./package.json"), "utf-8"),
);
const dependencies = Object.keys(packageJson.dependencies).filter(
  (dep) => !["@ponder/client", "@ponder/utils"].includes(dep),
);
test("should bundle the entry file for the browser without throwing", async () => {
  await expect(
    build({
      resolve: {
        alias: {
          "@": path.resolve(__dirname, "./src"),
          "@ponder/client": path.resolve(__dirname, "../client/src"),
          "@ponder/utils": path.resolve(__dirname, "../utils/src"),
        },
      },
      // Mock build settings
      logLevel: "error",
      build: {
        lib: {
          entry: path.resolve(__dirname, "./src/index.ts"),
          name: "ponder",
          formats: ["es"],
        },
        // Speed up the build
        write: false,
        minify: false,
        reportCompressedSize: false,
        sourcemap: false,
        // Exclude all dependencies
        rollupOptions: { external: dependencies },
      },
    }),
  ).resolves.toBeDefined();
});
</file>

<file path="packages/core/src/index.ts">
export { createConfig } from "@/config/index.js";
export { factory } from "@/config/address.js";
export type {
  Block,
  Log,
  Transaction,
  TransactionReceipt,
} from "@/types/eth.js";
export type { Virtual } from "@/types/virtual.js";
export {
  type MergeAbis,
  type ReplaceBigInts,
  mergeAbis,
  loadBalance,
  rateLimit,
  replaceBigInts,
} from "@ponder/utils";
import type { Config } from "@/config/index.js";
import type { Prettify } from "./types/utils.js";
export type ContractConfig = Prettify<Config["contracts"][string]>;
export type ChainConfig = Prettify<Config["chains"][string]>;
export type BlockConfig = Prettify<Config["blocks"][string]>;
export type DatabaseConfig = Prettify<Config["database"]>;
export type { CreateConfigReturnType } from "@/config/index.js";
export type { GetEventFilter } from "@/config/eventFilter.js";
export type { AddressConfig } from "@/config/address.js";
export {
  onchainTable,
  onchainView,
  onchainEnum,
  primaryKey,
  hex,
  bigint,
} from "@/drizzle/onchain.js";
export type {
  AnyPgColumn,
  PgColumn,
  ExtraConfigColumn,
  PgColumnBuilder,
  PgColumnBuilderBase,
  PgEnumColumnBuilder,
  PgEnumColumnBuilderInitial,
  PgTable,
  PgTableExtraConfig,
  PgTableWithColumns,
  PgTextConfig,
  TableConfig,
} from "drizzle-orm/pg-core";
export type {
  OnchainTable,
  BuildExtraConfigColumns,
  PgColumnsBuilders,
  PrimaryKeyBuilder,
  OnchainEnum,
} from "@/drizzle/onchain.js";
export type { ReadonlyDrizzle } from "@/types/db.js";
export { client } from "@/client/index.js";
export { graphql } from "@/graphql/middleware.js";
export {
  sql,
  eq,
  gt,
  gte,
  lt,
  lte,
  ne,
  isNull,
  isNotNull,
  inArray,
  notInArray,
  exists,
  notExists,
  between,
  notBetween,
  like,
  notLike,
  ilike,
  notIlike,
  not,
  asc,
  desc,
  and,
  or,
  count,
  countDistinct,
  avg,
  avgDistinct,
  sum,
  sumDistinct,
  max,
  min,
  relations,
} from "drizzle-orm";
export {
  bigint as int8,
  boolean,
  char,
  cidr,
  date,
  doublePrecision,
  inet,
  integer,
  interval,
  json,
  jsonb,
  line,
  macaddr,
  macaddr8,
  numeric,
  point,
  real,
  smallint,
  text,
  time,
  timestamp,
  uuid,
  varchar,
  index,
  uniqueIndex,
  alias,
  foreignKey,
  union,
  unionAll,
  intersect,
  intersectAll,
  except,
  exceptAll,
} from "drizzle-orm/pg-core";
</file>

<file path="packages/core/src/types.d.ts">
declare module "ponder:registry" {
  import type { Virtual } from "ponder";
  type config = typeof import("ponder:internal").config;
  type schema = typeof import("ponder:internal").schema;
  export const ponder: Virtual.Registry<config["default"], schema>;
  export type EventNames = Virtual.EventNames<config["default"]>;
  export type Event<name extends EventNames = EventNames> = Virtual.Event<
    config["default"],
    name
  >;
  export type Context<name extends EventNames = EventNames> = Virtual.Context<
    config["default"],
    schema,
    name
  >;
  export type IndexingFunctionArgs<name extends EventNames = EventNames> =
    Virtual.IndexingFunctionArgs<config["default"], schema, name>;
}
declare module "ponder:schema" {
  const schema: typeof import("ponder:internal").schema;
  export { schema as default };
}
declare module "ponder:api" {
  import type { ReadonlyDrizzle } from "ponder";
  import type { PublicClient } from "viem";
  type schema = typeof import("ponder:internal").schema;
  type config = typeof import("ponder:internal").config;
  export const db: ReadonlyDrizzle<schema>;
  export const publicClients: {
    [chainName in keyof config["default"]["chains"]]: PublicClient;
  };
}
</file>

<file path="packages/core/.env.example">
# If DATABASE_URL is set, tests will run against the specified Postgres database.
# Leave this commented out by default (tests will run against an in-memory SQLite database).
# DATABASE_URL=postgres://{username}@localhost:5432/{username}
</file>

<file path="packages/core/.gitignore">
dist/
.ponder/
ponder-env.d.ts
generated/
.env.local
**/*.node
</file>

<file path="packages/core/build.ts">
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath } from "node:url";
import { watch } from "chokidar";
import { execa } from "execa";
import { glob } from "glob";
import pc from "picocolors";
import { rimraf } from "rimraf";
const PACKAGE_NAME = "@PONDER/CORE";
const TSCONFIG = "tsconfig.build.json";
const WATCH_DIRECTORY = "src";
const prefix = pc.gray(`[${PACKAGE_NAME}]`);
const log = {
  cli: (msg: string) => console.log(`${prefix} ${pc.magenta("CLI")} ${msg}`),
  error: (msg: string) => console.error(`${prefix} ${pc.red("ERROR")} ${msg}`),
  success: (msg: string) => console.log(`${prefix} ${pc.green("CLI")} ${msg}`),
  tsc: (msg: string) => console.log(`${prefix} ${pc.blue("TSC")} ${msg}`),
};
const isWatchMode =
  process.argv.includes("--watch") || process.argv.includes("-w");
if (isWatchMode) {
  watchMode().catch((error) => {
    log.error(`Watch mode failed: ${error}`);
    process.exit(1);
  });
} else {
  build().then((success) => {
    process.exit(success ? 0 : 1);
  });
}
async function build() {
  try {
    log.cli("Build start");
    const startTime = Date.now();
    await rimraf("dist");
    log.cli("Cleaned output folder");
    const tscResult = await execa("tsc", ["--project", TSCONFIG], {
      reject: false,
      stderr: "pipe",
      stdout: "pipe",
    });
    `${tscResult.stdout}\n${tscResult.stderr}`
      .trim()
      .split("\n")
      .filter(Boolean)
      .forEach((line) => log.tsc(line));
    if (tscResult.exitCode !== 0) {
      log.error("Build failed");
      return false;
    } else {
      log.cli("Completed tsc without error");
    }
    const replacePathsResult = replaceAliasedPaths();
    if (replacePathsResult.error) {
      log.error(`Failed to replace import paths: ${replacePathsResult.error}`);
      return false;
    }
    const duration = Date.now() - startTime;
    log.success(` Build success in ${duration}ms`);
    return true;
  } catch (error) {
    log.error(`Build failed: ${error}`);
    return false;
  }
}
async function watchMode() {
  await build();
  const watcher = watch(WATCH_DIRECTORY, {
    cwd: path.dirname(fileURLToPath(import.meta.url)),
    persistent: true,
  });
  let isBuilding = false;
  let isBuildQueued = false;
  async function enqueueBuild() {
    if (!isBuilding) {
      try {
        isBuilding = true;
        await build();
      } finally {
        isBuilding = false;
      }
      if (isBuildQueued) {
        isBuildQueued = false;
        await enqueueBuild();
      }
    } else {
      isBuildQueued = true;
    }
  }
  watcher.on("change", async (path) => {
    log.cli(`Change detected: ${path}`);
    await enqueueBuild();
  });
  watcher.on("error", (error) => {
    log.error(`Watch error: ${error}`);
  });
  watcher.on("ready", () => {
    log.cli(`Watching for changes in "${WATCH_DIRECTORY}"`);
  });
  process.on("SIGINT", () => {
    watcher.close().then(() => {
      log.cli("Watch mode terminated");
      process.exit(0);
    });
  });
}
const importRegex = /from ['"](@\/[^'"]*)['"]/g;
function replaceAliasedPaths() {
  try {
    const directories = ["dist/esm", "dist/types"];
    for (const dir of directories) {
      let replacementCount = 0;
      // Normalize the directory path for the current OS
      const normalizedDir = path.normalize(dir);
      const filePaths = glob.sync(`${normalizedDir}/**/*`, { nodir: true });
      for (const filePath of filePaths) {
        const content = fs.readFileSync(filePath, "utf8");
        // Get the corresponding source path relative to src directory
        // Use posix-style paths for consistency in the relative path calculation
        const relativePath = path
          .relative(normalizedDir, filePath)
          // Convert Windows backslashes to forward slashes for consistency
          .split(path.sep)
          .join("/");
        const sourceFilePath = path.join("src", relativePath);
        // Ensure we use forward slashes for the source directory path
        const sourceFileDir = path
          .dirname(sourceFilePath)
          .split(path.sep)
          .join("/");
        // Replace all @/ imports with relative paths
        const newContent = content.replace(
          importRegex,
          (_match, importPath) => {
            // Remove @ prefix
            const importWithoutAlias = importPath.replace("@/", "");
            // Use forward slashes for the target path
            const targetPath = path
              .join("src", importWithoutAlias)
              .split(path.sep)
              .join("/");
            // Create relative path from the source file to the target
            // and ensure it uses forward slashes
            let relativePath = path
              .relative(sourceFileDir, targetPath)
              .split(path.sep)
              .join("/");
            // Ensure the path starts with ./ or ../
            if (!relativePath.startsWith(".")) {
              relativePath = `./${relativePath}`;
            }
            const replacementText = `from '${relativePath}'`;
            // Useful for debugging.
            // console.log({ file: filePath, old: _match, new: replacementText });
            replacementCount++;
            return replacementText;
          },
        );
        if (newContent !== content) {
          fs.writeFileSync(filePath, newContent);
        }
      }
      log.cli(
        `Replaced import paths in '${normalizedDir}' (${filePaths.length} files, ${replacementCount} replacements)`,
      );
    }
    return { error: null };
  } catch (e) {
    const error = e as Error;
    return { error };
  }
}
</file>

<file path="packages/core/bunfig.toml">
[test]
preload = ["./src/_test/globalSetup.ts"]
timeout = 15000
</file>

<file path="packages/core/drizzle.config.ts">
import { defineConfig } from "drizzle-kit";
export default defineConfig({
  dialect: "postgresql",
  schema: "./src/sync-store/schema.ts",
  out: "./migrations",
  casing: "snake_case",
});
</file>

<file path="packages/core/package.json">
{
  "name": "ponder",
  "version": "0.16.1",
  "description": "An open-source framework for crypto application backends",
  "license": "MIT",
  "type": "module",
  "repository": {
    "type": "git",
    "url": "https://github.com/ponder-sh/ponder",
    "directory": "packages/core"
  },
  "files": [
    "dist",
    "src/**/*.ts",
    "!src/_test/**/*",
    "!**/*.test.ts",
    "!**/*.bench.ts",
    "!**/*.test-d.ts",
    "!**/*.tsbuildinfo",
    "CHANGELOG.md"
  ],
  "bin": {
    "ponder": "./dist/esm/bin/ponder.js"
  },
  "module": "./dist/esm/index.js",
  "types": "./dist/types/index.d.ts",
  "typings": "./dist/types/index.d.ts",
  "exports": {
    ".": {
      "types": "./dist/types/index.d.ts",
      "import": "./dist/esm/index.js"
    },
    "./virtual": {
      "types": "./src/types.d.ts"
    }
  },
  "scripts": {
    "build": "tsx build.ts",
    "test": "vitest",
    "test:bun": "bun test",
    "test:typecheck": "vitest --typecheck.only",
    "typecheck": "tsc --noEmit"
  },
  "peerDependencies": {
    "hono": ">=4.5",
    "typescript": ">=5.0.4",
    "viem": ">=2"
  },
  "peerDependenciesMeta": {
    "typescript": {
      "optional": true
    }
  },
  "dependencies": {
    "@babel/code-frame": "^7.23.4",
    "@commander-js/extra-typings": "^12.0.1",
    "@electric-sql/pglite": "0.2.13",
    "@escape.tech/graphql-armor-max-aliases": "^2.3.0",
    "@escape.tech/graphql-armor-max-depth": "^2.2.0",
    "@escape.tech/graphql-armor-max-tokens": "^2.3.0",
    "@hono/node-server": "1.19.5",
    "@ponder/utils": "workspace:*",
    "abitype": "^0.10.2",
    "ansi-escapes": "^7.0.0",
    "commander": "^12.0.0",
    "conf": "^12.0.0",
    "dataloader": "^2.2.2",
    "detect-package-manager": "^3.0.2",
    "dotenv": "^16.3.1",
    "drizzle-orm": "0.41.0",
    "glob": "10.5.0",
    "graphql": "16.8.2",
    "graphql-yoga": "5.17.1",
    "http-terminator": "^3.2.0",
    "kysely": "^0.26.3",
    "pg": "^8.11.3",
    "pg-connection-string": "^2.6.2",
    "pg-copy-streams": "^6.0.6",
    "pg-query-emscripten": "5.1.0",
    "picocolors": "^1.1.1",
    "pino": "^8.16.2",
    "prom-client": "^15.0.0",
    "semver": "^7.7.1",
    "stacktrace-parser": "^0.1.10",
    "superjson": "^2.2.2",
    "terminal-size": "^4.0.0",
    "vite": "5.4.21",
    "vite-node": "1.0.2",
    "vite-tsconfig-paths": "4.3.1",
    "ws": "^8.18.3"
  },
  "devDependencies": {
    "@pgsql/types": "16.0.0",
    "@ponder/client": "workspace:*",
    "@types/babel__code-frame": "^7.0.6",
    "@types/glob": "^8.1.0",
    "@types/node": "^20.10.0",
    "@types/pg": "^8.10.9",
    "@types/pg-copy-streams": "^1.2.5",
    "@types/semver": "^7.5.8",
    "@types/ws": "^8.18.1",
    "@viem/anvil": "^0.0.6",
    "@wagmi/cli": "^1.5.2",
    "chokidar": "^4.0.3",
    "execa": "^8.0.1",
    "mitata": "^1.0.34",
    "rimraf": "^5.0.5",
    "tsx": "^4.19.2",
    "vitest": "1.6.1"
  },
  "engines": {
    "node": ">=18.14"
  }
}
</file>

<file path="packages/core/README.md">
# Ponder

[![CI status][ci-badge]][ci-url]
[![Version][version-badge]][version-url]
[![Telegram chat][tg-badge]][tg-url]
[![License][license-badge]][license-url]

Ponder is an open-source framework for blockchain application backends.

## Documentation

Visit [ponder.sh](https://ponder.sh) for documentation, guides, and the API reference.

## Support

Join [Ponder's telegram chat](https://t.me/pondersh) for support, feedback, and general chatter.

## Features

 &nbsp;Local development server with hot reloading<br/>
 &nbsp;`create-ponder` CLI tool to get started from an Etherscan link or Graph Protocol subgraph<br/>
 &nbsp;End-to-end type safety using [viem](https://viem.sh) and [ABIType](https://github.com/wagmi-dev/abitype)<br/>
 &nbsp;Autogenerated GraphQL API<br/>
 &nbsp;Easy to deploy anywhere using Node.js/Docker<br/>
 &nbsp;Supports all Ethereum-based blockchains, including test nodes like [Anvil](https://book.getfoundry.sh/anvil)<br/>
 &nbsp;Index events from multiple chains in the same app<br/>
 &nbsp;Reconciles chain reorganization<br/>
 &nbsp;Factory contracts<br/>
 &nbsp;Process transactions calls (in addition to logs)<br/>
 &nbsp;Run effects (e.g. send an API request) in indexing code<br/>

## Quickstart

### 1. Run `create-ponder`

You will be asked for a project name, and if you are using a [template](https://ponder.sh/docs/api-reference/create-ponder#templates) (recommended). Then, the CLI will create a project directory, install dependencies, and initialize a git repository.

```bash
npm init ponder@latest
# or
pnpm create ponder
# or
yarn create ponder
```

### 2. Start the development server

Just like Next.js and Vite, Ponder has a development server that automatically reloads when you save changes in any project file. It also prints `console.log` statements and errors encountered while running your code. First, `cd` into your project directory, then start the server.

```bash
npm run dev
# or
pnpm dev
# or
yarn dev
```

### 3. Add contracts & chains

Ponder fetches event logs for the contracts added to `ponder.config.ts`, and passes those events to the indexing functions you write.

```ts
// ponder.config.ts

import { createConfig } from "ponder";
import { BaseRegistrarAbi } from "./abis/BaseRegistrar";
 
export default createConfig({
  chains: {
    mainnet: { 
      id: 1,
      rpc: "https://eth-mainnet.g.alchemy.com/v2/...",
    },
  },
  contracts: {
    BaseRegistrar: {
      abi: BaseRegistrarAbi,
      chain: "mainnet",
      address: "0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85",
      startBlock: 9380410,
    },
  },
});
```

### 4. Define your schema

The `ponder.schema.ts` file contains the database schema, and defines the shape data that the GraphQL API serves.

```ts
// ponder.schema.ts

import { onchainTable } from "ponder";

export const ensName = onchainTable("ens_name", (t) => ({
  name: p.text().primaryKey(),
  owner: p.text().notNull(),
  registeredAt: p.integer().notNull(),
}));
```

### 5. Write indexing functions

Files in the `src/` directory contain **indexing functions**, which are TypeScript functions that process a contract event. The purpose of these functions is to insert data into the entity store.

```ts
// src/BaseRegistrar.ts

import { ponder } from "ponder:registry";
import schema from "ponder:schema";

ponder.on("BaseRegistrar:NameRegistered", async ({ event, context }) => {
  const { name, owner } = event.params;

  await context.db.insert(schema.ensName).values({
    name: name,
    owner: owner,
    registeredAt: event.block.timestamp,
  });
});
```

See the [create & update records](https://ponder.sh/docs/indexing/write) docs for a detailed guide on writing indexing functions.

### 6. Query the GraphQL API

Ponder automatically generates a frontend-ready GraphQL API based on your `ponder.schema.ts` file. The API serves data that you inserted in your indexing functions.

```ts
{
  ensNames(limit: 2) {
    items {
      name
      owner
      registeredAt
    }
  }
}
```

```json
{
  "ensNames": {
    "items": [
      {
        "name": "vitalik.eth",
        "owner": "0x0904Dac3347eA47d208F3Fd67402D039a3b99859",
        "registeredAt": 1580345271
      },
      {
        "name": "joe.eth",
        "owner": "0x6109DD117AA5486605FC85e040ab00163a75c662",
        "registeredAt": 1580754710
      }
    ]
  }
}
```

That's it! Visit [ponder.sh](https://ponder.sh) for documentation, guides for deploying to production, and the API reference.

## Contributing

If you're interested in contributing to Ponder, please read the [contribution guide](/.github/CONTRIBUTING.md).

## Packages

- `ponder`
- `@ponder/client`
- `@ponder/react`
- `@ponder/utils`
- `create-ponder`
- `eslint-config-ponder`

## About

Ponder is MIT-licensed open-source software.

[ci-badge]: https://github.com/ponder-sh/ponder/actions/workflows/main.yml/badge.svg
[ci-url]: https://github.com/ponder-sh/ponder/actions/workflows/main.yml
[tg-badge]: https://img.shields.io/endpoint?color=neon&logo=telegram&label=chat&url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Fpondersh
[tg-url]: https://t.me/pondersh
[license-badge]: https://img.shields.io/npm/l/ponder?label=License
[license-url]: https://github.com/ponder-sh/ponder/blob/main/LICENSE
[version-badge]: https://img.shields.io/npm/v/ponder
[version-url]: https://github.com/ponder-sh/ponder/releases
</file>

<file path="packages/core/tsconfig.build.json">
{
  "extends": "../../tsconfig.base.json",
  "include": ["src/**/*.ts"],
  "exclude": [
    "src/**/*.test.ts",
    "src/**/*.bench.ts",
    "src/**/*.test-d.ts",
    "src/_test"
  ],
  "compilerOptions": {
    "outDir": "./dist/esm",
    "declarationDir": "./dist/types",
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true,

    "rootDir": "src",
    "baseUrl": ".",
    "paths": {
      "@/*": ["src/*"]
    }
  }
}
</file>

<file path="packages/core/tsconfig.json">
{
  "extends": "./tsconfig.build.json",
  "include": ["src/**/*.ts", "src/**/*.tsx"],
  "exclude": [],
  "compilerOptions": {
    "paths": {
      "@/*": ["./src/*"],
      "@ponder/client": ["../client/src"],
      "@ponder/utils": ["../utils/src"]
    }
  }
}
</file>

<file path="packages/core/vite.config.ts">
import { createRequire } from "node:module";
import path from "node:path";
import { defineConfig } from "vitest/config";
// Fixes `Duplicate "graphql" modules cannot be used at the same time` issue
const graphqlPath = createRequire(import.meta.url).resolve("graphql");
export default defineConfig({
  resolve: {
    alias: {
      "@": path.resolve(__dirname, "./src"),
      "@ponder/client": path.resolve(__dirname, "../client/src"),
      "@ponder/utils": path.resolve(__dirname, "../utils/src"),
      graphql: graphqlPath,
    },
  },
  test: {
    globalSetup: ["src/_test/globalSetup.ts"],
    setupFiles: ["src/_test/setup.ts"],
    poolOptions: {
      threads: {
        maxThreads: 4,
        minThreads: 1,
      },
    },
    sequence: { hooks: "stack" },
    testTimeout: 15000,
  },
});
</file>

<file path="packages/core/wagmi.config.ts">
import { defineConfig } from "@wagmi/cli";
import { foundry } from "@wagmi/cli/plugins";
export default defineConfig({
  out: "./src/_test/generated.ts",
  plugins: [
    foundry({
      project: "./src/_test/contracts/",
    }),
  ],
});
</file>

<file path="packages/create-ponder/src/_test/cli.test.ts">
import { randomUUID } from "node:crypto";
import { mkdirSync, readdirSync } from "node:fs";
import os from "node:os";
import path from "node:path";
import { run } from "@/index.js";
import { rimrafSync } from "rimraf";
import { beforeEach, expect, test } from "vitest";
const tempDir = path.join(os.tmpdir(), randomUUID());
beforeEach(() => {
  mkdirSync(tempDir, { recursive: true });
  return () => rimrafSync(tempDir);
});
test("create empty", async () => {
  const rootDir = path.join(tempDir, "empty");
  await run({
    args: [rootDir],
    options: { template: "empty", skipGit: true, skipInstall: true },
  });
  const files = readdirSync(rootDir, { recursive: true, encoding: "utf8" });
  expect(files).toEqual(
    expect.arrayContaining([
      ".env.local",
      ".eslintrc.json",
      ".gitignore",
      "package.json",
      "ponder-env.d.ts",
      "ponder.config.ts",
      "ponder.schema.ts",
      "tsconfig.json",
      path.join("abis", "ExampleContractAbi.ts"),
      path.join("src", "index.ts"),
      path.join("src", "api", "index.ts"),
    ]),
  );
});
</file>

<file path="packages/create-ponder/src/_test/globalSetup.ts">
import dotenv from "dotenv";
async function globalSetup() {
  dotenv.config({ path: ".env.local" });
}
if ("bun" in process.versions) {
  require("bun:test").beforeAll(async () => {
    await globalSetup();
  });
}
export default globalSetup;
</file>

<file path="packages/create-ponder/src/helpers/getPackageManager.ts">
import pico from "picocolors";
import type { CLIOptions } from "../index.js";
export const getPackageManager = ({
  options,
}: {
  options?: CLIOptions;
}): "bun" | "pnpm" | "npm" | "yarn" => {
  if (options) {
    if (options.bun) return "bun";
    if (options.pnpm) return "pnpm";
    if (options.npm) return "npm";
    if (options.yarn) return "yarn";
  }
  const userAgent = process.env.npm_config_user_agent;
  if (userAgent) {
    if (userAgent.includes("bun")) return "bun";
    if (userAgent.includes("pnpm")) return "pnpm";
    if (userAgent.includes("npm")) return "npm";
    if (userAgent.includes("yarn")) return "yarn";
  }
  throw new Error(pico.red("Undetectable package manager"));
};
</file>

<file path="packages/create-ponder/src/helpers/mergeAbis.ts">
import type { Abi } from "abitype";
import type { AbiItem } from "viem";
import { formatAbiItem } from "viem/utils";
export type MergeAbi<
  TBase extends Abi,
  TInsert extends Abi,
> = TInsert extends readonly [
  infer First extends AbiItem,
  ...infer Rest extends Abi,
]
  ? Extract<TBase[number], First> extends never
    ? First["type"] extends "constructor" | "receive" | "fallback"
      ? MergeAbi<TBase, Rest>
      : MergeAbi<readonly [...TBase, First], Rest>
    : MergeAbi<TBase, Rest>
  : TBase;
type MergeAbis<
  TAbis extends readonly Abi[],
  TMerged extends Abi = [],
> = TAbis extends readonly [
  infer First extends Abi,
  ...infer Rest extends readonly Abi[],
]
  ? MergeAbis<Rest, MergeAbi<TMerged, First>>
  : TMerged;
const isAbiItemEqual = (a: AbiItem, b: AbiItem): boolean =>
  formatAbiItem(a) === formatAbiItem(b);
/**
 * Combine multiple ABIs into one, removing duplicates if necessary.
 */
export const mergeAbis = <const TAbis extends readonly Abi[]>(abis: TAbis) => {
  let merged: Abi = [];
  for (const abi of abis) {
    for (const item of abi) {
      // Don't add a duplicate items
      // if item is constructor or already in merged, don't add it
      if (
        item.type !== "constructor" &&
        item.type !== "receive" &&
        item.type !== "fallback" &&
        !merged.some((m) => isAbiItemEqual(m, item))
      ) {
        merged = [...merged, item];
      }
    }
  }
  return merged as MergeAbis<TAbis>;
};
</file>

<file path="packages/create-ponder/src/helpers/notifyUpdate.ts">
import pico from "picocolors";
import checkForUpdate from "update-check";
import packageJson from "../../package.json" assert { type: "json" };
import type { CLIOptions } from "../index.js";
import { getPackageManager } from "./getPackageManager.js";
const log = console.log;
export async function notifyUpdate({ options }: { options: CLIOptions }) {
  try {
    const res = await checkForUpdate.default(packageJson);
    if (res?.latest) {
      const packageManager = getPackageManager({ options });
      const updateMessage =
        packageManager === "bun"
          ? "bun install --global create-ponder"
          : packageManager === "pnpm"
            ? "pnpm add -g create-ponder"
            : packageManager === "npm"
              ? "npm install -g create-ponder"
              : "yarn global add create-ponder";
      log(
        pico.bold(
          `${pico.yellow(
            "A new version of `create-ponder` is available!",
          )}\nYou can update by running: ${pico.cyan(updateMessage)}\n`,
        ),
      );
    }
    process.exit();
  } catch {
    // ignore error
  }
}
</file>

<file path="packages/create-ponder/src/helpers/validate.ts">
import path from "node:path";
import { pathExists } from "fs-extra";
import pico from "picocolors";
import validatePackageName from "validate-npm-package-name";
import type { Template } from "../index.js";
type ValidationResult =
  | {
      valid: false;
      message: string;
      problems: string;
    }
  | {
      valid: true;
      message?: never;
      problems?: never;
    };
export async function validateProjectName({
  projectName,
}: {
  projectName: string;
}): Promise<ValidationResult> {
  // Validate project name
  const nameValidation = validatePackageName(projectName);
  if (!nameValidation.validForNewPackages) {
    const problems = [
      ...(nameValidation.errors ?? []),
      ...(nameValidation.warnings ?? []),
    ];
    return {
      valid: false,
      message: ` "${projectName}" is not a valid project name.`,
      problems: problems.map((problem) => ` ${problem}`).join("\n"),
    };
  }
  return {
    valid: true,
  };
}
export async function validateProjectPath({
  projectPath,
}: { projectPath: string }): Promise<ValidationResult> {
  // Validate project target path
  if (await pathExists(projectPath))
    return {
      valid: false,
      message: ` the directory "${path.relative(process.cwd(), projectPath)}" already exists.`,
      problems: " choose another name or delete the directory.",
    };
  return {
    valid: true,
  };
}
export async function validateTemplateName({
  isNameRequired = true,
  templateId,
  templates,
}: {
  isNameRequired?: boolean;
  templateId: string;
  templates: readonly Template[];
}): Promise<ValidationResult> {
  if (isNameRequired && !templateId)
    return {
      valid: false,
      message: " no template provided.",
      problems: " select a template or provide one using --template.",
    };
  if (templateId && !templates.find((template) => template.id === templateId))
    return {
      valid: false,
      message: ` the template "${templateId}" does not exist.`,
      problems: `Choose a valid name. Available: ${templates.map(({ id }) => id).join(", ")}`,
    };
  return { valid: true };
}
export class ValidationError extends Error {
  override name = "ValidationError";
  constructor(validation: ValidationResult) {
    super([pico.red(validation.message), validation.problems].join("\n"));
  }
}
</file>

<file path="packages/create-ponder/src/helpers/wait.ts">
export const wait = (ms: number) =>
  new Promise((resolve) => setTimeout(resolve, ms));
</file>

<file path="packages/create-ponder/src/index.ts">
#!/usr/bin/env node
import { mkdirSync, writeFileSync } from "node:fs";
import path from "node:path";
import { fileURLToPath } from "node:url";
import type { Abi, AbiEvent } from "abitype";
import { cac } from "cac";
import cpy from "cpy";
import { execa } from "execa";
import fs from "fs-extra";
import { oraPromise } from "ora";
import pico from "picocolors";
import prettier from "prettier";
import { default as prompts } from "prompts";
// NOTE: This is a workaround for tsconfig `rootDir` nonsense.
// @ts-ignore
import rootPackageJson from "../package.json" assert { type: "json" };
import { getPackageManager } from "./helpers/getPackageManager.js";
import { mergeAbis } from "./helpers/mergeAbis.js";
import { notifyUpdate } from "./helpers/notifyUpdate.js";
import {
  ValidationError,
  validateProjectName,
  validateProjectPath,
  validateTemplateName,
} from "./helpers/validate.js";
const log = console.log;
export type SerializableChain = {
  id: number;
  rpc: string;
};
export type SerializableContract = {
  abi:
    | { abi: Abi; name: string; dir: string }
    | { abi: Abi; name: string; dir: string }[];
  address: string;
  chain: Record<string, any> | string;
  startBlock?: number;
};
export type SerializableConfig = {
  chains: Record<string, SerializableChain>;
  contracts: Record<string, SerializableContract>;
};
export type Template = {
  title: string;
  description: string;
  id: string;
};
export type CLIArgs = readonly string[];
export type CLIOptions = {
  [k: string]: any;
};
const templates = [
  { id: "empty", title: "Default", description: "A blank-slate Ponder app" },
  {
    id: "feature-factory",
    title: "Feature - Factory contract",
    description: "A Ponder app using a factory contract",
  },
  {
    id: "feature-accounts",
    title: "Feature - Accounts",
    description: "A Ponder app using accounts",
  },
  {
    id: "feature-filter",
    title: "Feature - Custom event filter",
    description: "A Ponder app using an event filter",
  },
  {
    id: "feature-api-functions",
    title: "Feature - Custom api functions",
    description: "A Ponder app using a custom api functions",
  },
  {
    id: "feature-blocks",
    title: "Feature - Block filter",
    description: "A Ponder app using a block filter",
  },
  {
    id: "feature-call-traces",
    title: "Feature - Call traces",
    description: "A Ponder app using a call traces",
  },
  {
    id: "feature-multichain",
    title: "Feature - Multichain contract",
    description: "A Ponder app using multiple chains",
  },
  {
    id: "feature-proxy",
    title: "Feature - Proxy contract",
    description: "A Ponder app that uses a proxy contract",
  },
  {
    id: "feature-read-contract",
    title: "Feature - Read from a contract",
    description: "A Ponder app that uses contract calls",
  },
  {
    id: "project-friendtech",
    title: "project-friendtech",
    description: "A Ponder app for Friendtech",
  },
  {
    id: "project-uniswap-v3-flash",
    title: "Project - Uniswap V3 flash loans",
    description: "A Ponder app for Uniswap V3 flash loans",
  },
  {
    id: "reference-erc20",
    title: "Reference - ERC20 token",
    description: "A Ponder app for an ERC20 token",
  },
  {
    id: "reference-erc721",
    title: "Reference - ERC721",
    description: "A Ponder app for an ERC721 token",
  },
  {
    id: "reference-erc1155",
    title: "Reference - ERC1155",
    description: "A Ponder app for an ERC1155 token",
  },
  {
    id: "reference-erc4626",
    title: "Reference - ERC4626",
    description: "A Ponder app for an ERC4626 token",
  },
] as const satisfies readonly Template[];
export async function run({
  args,
  options,
}: {
  args: CLIArgs;
  options: CLIOptions;
}) {
  if (options.help) return;
  const warnings: string[] = [];
  log();
  log(
    `Welcome to ${pico.bold(
      pico.blue("create-ponder"),
    )}  the quickest way to get started with Ponder!`,
  );
  log();
  const __dirname = fileURLToPath(new URL(".", import.meta.url));
  const templatesPath = path.join(__dirname, "..", "templates");
  let templateId = options.template || options.t;
  // Validate template if provided
  let templateValidation = await validateTemplateName({
    isNameRequired: false,
    templateId,
    templates,
  });
  if (!templateValidation.valid) throw new ValidationError(templateValidation);
  // Project name.
  let projectName: string;
  // Absolute path to project directory.
  let projectPath: string;
  if (args[0]) {
    projectPath = args[0].trim();
    // If the provided path is not absolute, make it absolute.
    if (!path.isAbsolute(projectPath)) projectPath = path.resolve(projectPath);
    const splitPath = projectPath.split(path.sep);
    // Use the last segment of the provided path as the project name.
    projectName = splitPath[splitPath.length - 1]?.trim() || "";
    const nameValidation = await validateProjectName({ projectName });
    if (!nameValidation.valid) throw new ValidationError(nameValidation);
    log(pico.green(""), pico.bold("Using project name:"), projectName);
  } else {
    const res = await prompts({
      initial: "my-app",
      name: "projectName",
      message: "What's the name of your project?",
      type: "text",
      async validate(projectName) {
        const validation = await validateProjectName({ projectName });
        if (!validation.valid) return validation.message;
        return true;
      },
    });
    projectName = res.projectName?.trim();
    projectPath = path.resolve(projectName);
  }
  // Validate project path
  const pathValidation = await validateProjectPath({ projectPath });
  if (!pathValidation.valid) throw new ValidationError(pathValidation);
  // After validating that the directory does not already exist, create it.
  mkdirSync(projectPath, { recursive: true });
  // Automatically set template if using shortcut.
  if (options.etherscan && !templateId) templateId = "etherscan";
  if (options.subgraph && !templateId) templateId = "subgraph";
  // Extract template ID from CLI or prompt
  if (!templateId) {
    templateId = (
      await prompts({
        name: "templateId",
        message: "Which template would you like to use?",
        type: "select",
        choices: templates.map(({ id, ...t }) => ({
          ...t,
          value: id,
        })),
      })
    ).templateId;
  }
  // Get template meta
  const templateMeta = templates.find(({ id }) => id === templateId);
  if (!templateMeta) throw new ValidationError(templateValidation);
  // Validate template name
  templateValidation = await validateTemplateName({
    templateId,
    templates,
  });
  if (!templateValidation.valid) throw new ValidationError(templateValidation);
  let config: SerializableConfig | undefined;
  log();
  // Copy template contents into the target path
  const templatePath = path.join(templatesPath, templateMeta.id);
  await cpy(path.join(templatePath, "**", "*"), projectPath, {
    rename: (name) => name.replace(/^_dot_/, "."),
  });
  if (config) {
    // Write the config file.
    const configContent = `
      import { createConfig${
        Object.values(config.contracts).some((c) => Array.isArray(c.abi))
          ? ", mergeAbis"
          : ""
      } } from "ponder";
      ${Object.values(config.contracts)
        .flatMap((c) => c.abi)
        .filter(
          (tag, index, array) =>
            array.findIndex((t) => t.dir === tag.dir) === index,
        )
        .map(
          (abi) =>
            `import {${abi.name}} from "${abi.dir.slice(0, abi.dir.length - 3)}"`,
        )
        .join("\n")}
      export default createConfig({
        chains: ${JSON.stringify(config.chains).replaceAll(
          /"process.env.PONDER_RPC_URL_(.*?)"/g,
          "process.env.PONDER_RPC_URL_$1",
        )},
        contracts: ${JSON.stringify(
          Object.entries(config.contracts).reduce<Record<string, any>>(
            (acc, [name, c]) => {
              acc[name] = {
                ...c,
                abi: Array.isArray(c.abi)
                  ? `mergeAbis([${c.abi.map((a) => a.name).join(",")}])`
                  : c.abi.name,
              };
              return acc;
            },
            {},
          ),
        ).replaceAll(/"abi":"(.*?)"/g, "abi:$1")},
      });
    `;
    writeFileSync(
      path.join(projectPath, "ponder.config.ts"),
      await prettier.format(configContent, { parser: "typescript" }),
    );
    // Write the indexing function files.
    for (const [name, contract] of Object.entries(config.contracts)) {
      // If it's an array of ABIs, use the 2nd one (the implementation ABI).
      const abi = Array.isArray(contract.abi)
        ? mergeAbis(contract.abi.map((a) => a.abi))
        : contract.abi.abi;
      const abiEvents = abi.filter(
        (item): item is AbiEvent => item.type === "event" && !item.anonymous,
      );
      const eventNamesToWrite = abiEvents
        .map((event) => event.name)
        .slice(0, 4);
      const indexingFunctionFileContents = `
      import { ponder } from 'ponder:registry'
      ${eventNamesToWrite
        .map(
          (eventName) => `
          ponder.on("${name}:${eventName}", async ({ event, context }) => {
            console.log(event.args)
          })`,
        )
        .join("\n")}
    `;
      writeFileSync(
        path.join(projectPath, "src", `${name}.ts`),
        await prettier.format(indexingFunctionFileContents, {
          parser: "typescript",
        }),
      );
    }
  }
  // Create package.json for project
  const packageJson = await fs.readJSON(path.join(projectPath, "package.json"));
  packageJson.name = projectName;
  packageJson.dependencies.ponder = `^${rootPackageJson.version}`;
  packageJson.devDependencies["eslint-config-ponder"] =
    `^${rootPackageJson.version}`;
  if ("bun" in process.versions) {
    packageJson.scripts = addBunFlagToScripts(packageJson.scripts ?? {});
  }
  await fs.writeFile(
    path.join(projectPath, "package.json"),
    JSON.stringify(packageJson, null, 2),
  );
  const packageManager = getPackageManager({ options });
  // Install in background to not clutter screen
  const installArgs = [
    "install",
    packageManager === "npm" ? "--quiet" : "--silent",
  ];
  if (!options.skipInstall) {
    await oraPromise(
      execa(packageManager, installArgs, {
        cwd: projectPath,
        env: {
          ...process.env,
          ADBLOCK: "1",
          DISABLE_OPENCOLLECTIVE: "1",
          // we set NODE_ENV to development as pnpm skips dev
          // dependencies when production
          NODE_ENV: "development",
        },
      }),
      {
        text: `Installing packages with ${pico.bold(packageManager)}. This may take a few seconds.`,
        failText: "Failed to install packages.",
        successText: `Installed packages with ${pico.bold(packageManager)}.`,
      },
    );
  }
  // Create git repository
  if (!options.skipGit) {
    await oraPromise(
      async () => {
        await execa("git", ["init"], { cwd: projectPath });
        await execa("git", ["add", "."], { cwd: projectPath });
        await execa(
          "git",
          [
            "commit",
            "--no-verify",
            "--message",
            "chore: initial commit from create-ponder",
          ],
          { cwd: projectPath },
        );
      },
      {
        text: "Initializing git repository.",
        failText: "Failed to initialize git repository.",
        successText: "Initialized git repository.",
      },
    );
  }
  log();
  for (const warning of warnings) {
    log(`${pico.yellow("")} ${warning}`);
  }
  log();
  log("");
  log();
  log(
    `${pico.green("Success!")} Created ${pico.bold(projectName)} at ${pico.green(
      path.resolve(projectPath),
    )}`,
  );
  log();
  log(
    `To start your app, run ${pico.bold(
      pico.cyan(`cd ${path.relative(process.cwd(), projectPath)}`),
    )} and then ${pico.bold(
      pico.cyan(
        `${packageManager}${
          packageManager === "npm" || packageManager === "bun" ? " run" : ""
        } dev`,
      ),
    )}`,
  );
  log();
  log("");
  log();
}
function addBunFlagToScripts(scripts: Record<string, string>) {
  const ret: Record<string, string> = {};
  for (const [k, v] of Object.entries(scripts)) {
    if (v.startsWith("ponder")) ret[k] = `bun --bun ${v}`;
    else ret[k] = v;
  }
  return ret;
}
(async () => {
  const cli = cac(rootPackageJson.name)
    .version(rootPackageJson.version)
    .usage(`${pico.green("<directory>")} [options]`)
    .option(
      "-t, --template [id]",
      `Use a template. Options: ${templates.map(({ id }) => id).join(", ")}`,
    )
    .option("--npm", "Use npm as your package manager")
    .option("--pnpm", "Use pnpm as your package manager")
    .option("--yarn", "Use yarn as your package manager")
    .option("--skip-git", "Skip initializing a git repository")
    .option("--skip-install", "Skip installing packages")
    .help();
  // Check Nodejs version
  const _nodeVersion = process.version.split(".");
  const nodeVersion = [
    Number(_nodeVersion[0]!.slice(1)),
    Number(_nodeVersion[1]),
    Number(_nodeVersion[2]),
  ];
  if (nodeVersion[0]! < 18 || (nodeVersion[0] === 18 && nodeVersion[1]! < 14))
    throw new Error(
      pico.red(
        `Node version:${process.version} does not meet the >=18.14 requirement`,
      ),
    );
  const { args, options } = cli.parse(process.argv);
  try {
    await run({ args, options });
    log();
    await notifyUpdate({ options });
  } catch (error) {
    log(
      error instanceof ValidationError
        ? error.message
        : pico.red((<Error>error).message),
    );
    log();
    await notifyUpdate({ options });
    process.exit(1);
  }
})();
</file>

<file path="packages/create-ponder/templates/empty/abis/ExampleContractAbi.ts">
export const ExampleContractAbi = [] as const;
</file>

<file path="packages/create-ponder/templates/empty/src/api/index.ts">
import { db } from "ponder:api";
import schema from "ponder:schema";
import { Hono } from "hono";
import { client, graphql } from "ponder";
const app = new Hono();
app.use("/sql/*", client({ db, schema }));
app.use("/", graphql({ db, schema }));
app.use("/graphql", graphql({ db, schema }));
export default app;
</file>

<file path="packages/create-ponder/templates/empty/src/index.ts">
import { ponder } from "ponder:registry";
</file>

<file path="packages/create-ponder/templates/empty/_dot_env.local">
# Mainnet RPC URL used for fetching blockchain data. Alchemy is recommended.
PONDER_RPC_URL_1=

# (Optional) Postgres database URL. If not provided, SQLite will be used. 
DATABASE_URL=
</file>

<file path="packages/create-ponder/templates/empty/_dot_eslintrc.json">
{
  "extends": "ponder"
}
</file>

<file path="packages/create-ponder/templates/empty/_dot_gitignore">
# Dependencies
/node_modules

# Debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# Misc
.DS_Store

# Env files
.env*.local

# Ponder
/generated/
/.ponder/
</file>

<file path="packages/create-ponder/templates/empty/package.json">
{
  "name": "ponder-template",
  "version": "0.0.1",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "ponder dev",
    "start": "ponder start",
    "db": "ponder db",
    "codegen": "ponder codegen",
    "lint": "eslint .",
    "typecheck": "tsc"
  },
  "dependencies": {
    "ponder": "^0.0.95",
    "hono": "^4.5.0",
    "viem": "^2.21.3"
  },
  "devDependencies": {
    "@types/node": "^20.9.0",
    "eslint": "^8.53.0",
    "eslint-config-ponder": "^0.0.95",
    "typescript": "^5.2.2"
  },
  "engines": {
    "node": ">=18.14"
  }
}
</file>

<file path="packages/create-ponder/templates/empty/ponder-env.d.ts">
/// <reference types="ponder/virtual" />
declare module "ponder:internal" {
  const config: typeof import("./ponder.config.ts");
  const schema: typeof import("./ponder.schema.ts");
}
declare module "ponder:schema" {
  export * from "./ponder.schema.ts";
}
// This file enables type checking and editor autocomplete for this Ponder project.
// After upgrading, you may find that changes have been made to this file.
// If this happens, please commit the changes. Do not manually edit this file.
// See https://ponder.sh/docs/requirements#typescript for more information.
</file>

<file path="packages/create-ponder/templates/empty/ponder.config.ts">
import { createConfig } from "ponder";
import { ExampleContractAbi } from "./abis/ExampleContractAbi";
export default createConfig({
  chains: {
    mainnet: {
      id: 1,
      rpc: process.env.PONDER_RPC_URL_1!,
    },
  },
  contracts: {
    ExampleContract: {
      chain: "mainnet",
      abi: ExampleContractAbi,
      address: "0x0000000000000000000000000000000000000000",
      startBlock: 1234567,
    },
  },
});
</file>

<file path="packages/create-ponder/templates/empty/ponder.schema.ts">
import { onchainTable } from "ponder";
export const example = onchainTable("example", (t) => ({
  id: t.text().primaryKey(),
  name: t.text(),
}));
</file>

<file path="packages/create-ponder/templates/empty/tsconfig.json">
{
  "compilerOptions": {
    // Type checking
    "strict": true,
    "noUncheckedIndexedAccess": true,

    // Interop constraints
    "verbatimModuleSyntax": false,
    "esModuleInterop": true,
    "isolatedModules": true,
    "allowSyntheticDefaultImports": true,
    "resolveJsonModule": true,

    // Language and environment
    "moduleResolution": "bundler",
    "module": "ESNext",
    "noEmit": true,
    "lib": ["ES2022"],
    "target": "ES2022",

    // Skip type checking for node modules
    "skipLibCheck": true
  },
  "include": ["./**/*.ts"],
  "exclude": ["node_modules"]
}
</file>

<file path="packages/create-ponder/.env.example">
ETHERSCAN_API_KEY=
</file>

<file path="packages/create-ponder/.gitignore">
dist/
.env.local
templates/*
!templates/empty
!templates/etherscan
!templates/subgraph
</file>

<file path="packages/create-ponder/bunfig.toml">
[test]
preload = ["./src/_test/globalSetup.ts"]
timeout = 15000
</file>

<file path="packages/create-ponder/package.json">
{
  "name": "create-ponder",
  "version": "0.16.1",
  "type": "module",
  "description": "A CLI tool to create Ponder apps",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/ponder-sh/ponder",
    "directory": "packages/create-ponder"
  },
  "files": ["dist", "templates"],
  "bin": {
    "create-ponder": "./dist/index.js"
  },
  "scripts": {
    "build": "tsup",
    "test": "vitest run",
    "test:bun": "bun test",
    "typecheck": "tsc --noEmit"
  },
  "dependencies": {
    "cac": "^6.7.14",
    "cpy": "^11.0.0",
    "execa": "^8.0.1",
    "fs-extra": "^11.1.1",
    "ora": "^7.0.1",
    "picocolors": "^1.0.0",
    "prettier": "^3.1.0",
    "prompts": "^2.4.2",
    "update-check": "^1.5.4",
    "validate-npm-package-name": "^5.0.0",
    "viem": "^2.21.3",
    "yaml": "^2.3.4"
  },
  "devDependencies": {
    "@types/fs-extra": "^11.0.4",
    "@types/node": "^20.10.0",
    "@types/prompts": "^2.4.9",
    "@types/validate-npm-package-name": "^4.0.2",
    "abitype": "^0.10.2",
    "dotenv": "^16.3.1",
    "rimraf": "^5.0.5",
    "tsup": "^8.0.1",
    "vitest": "1.6.1"
  },
  "engines": {
    "node": ">=18.14"
  }
}
</file>

<file path="packages/create-ponder/README.md">
# Create Ponder

[![CI status][ci-badge]][ci-url]
[![Version][version-badge]][version-url]
[![Telegram chat][tg-badge]][tg-url]
[![License][license-badge]][license-url]

Ponder is an open-source framework for blockchain application backends.

## Documentation

Visit [ponder.sh](https://ponder.sh) for documentation, guides, and the API reference.

## Support

Join [Ponder's telegram chat](https://t.me/pondersh) for support, feedback, and general chatter.

## Features

 &nbsp;Local development server with hot reloading<br/>
 &nbsp;`create-ponder` CLI tool to get started from an Etherscan link or Graph Protocol subgraph<br/>
 &nbsp;End-to-end type safety using [viem](https://viem.sh) and [ABIType](https://github.com/wagmi-dev/abitype)<br/>
 &nbsp;Autogenerated GraphQL API<br/>
 &nbsp;Easy to deploy anywhere using Node.js/Docker<br/>
 &nbsp;Supports all Ethereum-based blockchains, including test nodes like [Anvil](https://book.getfoundry.sh/anvil)<br/>
 &nbsp;Index events from multiple chains in the same app<br/>
 &nbsp;Reconciles chain reorganization<br/>
 &nbsp;Factory contracts<br/>
 &nbsp;Process transactions calls (in addition to logs)<br/>
 &nbsp;Run effects (e.g. send an API request) in indexing code<br/>

## Quickstart

### 1. Run `create-ponder`

You will be asked for a project name, and if you are using a [template](https://ponder.sh/docs/api-reference/create-ponder#templates) (recommended). Then, the CLI will create a project directory, install dependencies, and initialize a git repository.

```bash
npm init ponder@latest
# or
pnpm create ponder
# or
yarn create ponder
```

### 2. Start the development server

Just like Next.js and Vite, Ponder has a development server that automatically reloads when you save changes in any project file. It also prints `console.log` statements and errors encountered while running your code. First, `cd` into your project directory, then start the server.

```bash
npm run dev
# or
pnpm dev
# or
yarn dev
```

### 3. Add contracts & chains

Ponder fetches event logs for the contracts added to `ponder.config.ts`, and passes those events to the indexing functions you write.

```ts
// ponder.config.ts

import { createConfig } from "ponder";
import { BaseRegistrarAbi } from "./abis/BaseRegistrar";
 
export default createConfig({
  chains: {
    mainnet: { 
      id: 1,
      rpc: "https://eth-mainnet.g.alchemy.com/v2/...",
    },
  },
  contracts: {
    BaseRegistrar: {
      abi: BaseRegistrarAbi,
      chain: "mainnet",
      address: "0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85",
      startBlock: 9380410,
    },
  },
});
```

### 4. Define your schema

The `ponder.schema.ts` file contains the database schema, and defines the shape data that the GraphQL API serves.

```ts
// ponder.schema.ts

import { onchainTable } from "ponder";

export const ensName = onchainTable("ens_name", (t) => ({
  name: p.text().primaryKey(),
  owner: p.text().notNull(),
  registeredAt: p.integer().notNull(),
}));
```

### 5. Write indexing functions

Files in the `src/` directory contain **indexing functions**, which are TypeScript functions that process a contract event. The purpose of these functions is to insert data into the entity store.

```ts
// src/BaseRegistrar.ts

import { ponder } from "ponder:registry";
import schema from "ponder:schema";

ponder.on("BaseRegistrar:NameRegistered", async ({ event, context }) => {
  const { name, owner } = event.params;

  await context.db.insert(schema.ensName).values({
    name: name,
    owner: owner,
    registeredAt: event.block.timestamp,
  });
});
```

See the [create & update records](https://ponder.sh/docs/indexing/write) docs for a detailed guide on writing indexing functions.

### 6. Query the GraphQL API

Ponder automatically generates a frontend-ready GraphQL API based on your `ponder.schema.ts` file. The API serves data that you inserted in your indexing functions.

```ts
{
  ensNames(limit: 2) {
    items {
      name
      owner
      registeredAt
    }
  }
}
```

```json
{
  "ensNames": {
    "items": [
      {
        "name": "vitalik.eth",
        "owner": "0x0904Dac3347eA47d208F3Fd67402D039a3b99859",
        "registeredAt": 1580345271
      },
      {
        "name": "joe.eth",
        "owner": "0x6109DD117AA5486605FC85e040ab00163a75c662",
        "registeredAt": 1580754710
      }
    ]
  }
}
```

That's it! Visit [ponder.sh](https://ponder.sh) for documentation, guides for deploying to production, and the API reference.

## Contributing

If you're interested in contributing to Ponder, please read the [contribution guide](/.github/CONTRIBUTING.md).

## Packages

- `ponder`
- `@ponder/client`
- `@ponder/react`
- `@ponder/utils`
- `create-ponder`
- `eslint-config-ponder`

## About

Ponder is MIT-licensed open-source software.

[ci-badge]: https://github.com/ponder-sh/ponder/actions/workflows/main.yml/badge.svg
[ci-url]: https://github.com/ponder-sh/ponder/actions/workflows/main.yml
[tg-badge]: https://img.shields.io/endpoint?color=neon&logo=telegram&label=chat&url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Fpondersh
[tg-url]: https://t.me/pondersh
[license-badge]: https://img.shields.io/npm/l/ponder?label=License
[license-url]: https://github.com/ponder-sh/ponder/blob/main/LICENSE
[version-badge]: https://img.shields.io/npm/v/ponder
[version-url]: https://github.com/ponder-sh/ponder/releases
</file>

<file path="packages/create-ponder/tsconfig.json">
{
  // Adapted from viem (https://github.com/wagmi-dev/viem/blob/ed779e9d5667704fd7....base.json).
  "include": ["src"],
  "compilerOptions": {
    // Type checking
    "strict": true,
    "useDefineForClassFields": true,
    "noFallthroughCasesInSwitch": true,
    "noImplicitReturns": true,
    "useUnknownInCatchVariables": true,
    "noImplicitOverride": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noUncheckedIndexedAccess": true,

    // JavaScript support
    "allowJs": false,
    "checkJs": false,

    // Interop constraints
    "verbatimModuleSyntax": false,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "resolveJsonModule": true,

    // Language and environment
    "moduleResolution": "NodeNext",
    "module": "NodeNext",
    "target": "ESNext",
    "lib": [
      "ES2022" // By using ES2022 we get access to the `.cause` property on `Error` instances.
    ],

    // Skip type checking for node modules
    "skipLibCheck": true,

    // File path alias support
    "baseUrl": ".",
    "paths": { "@/*": ["src/*"] }
  }
}
</file>

<file path="packages/create-ponder/tsup.config.ts">
import { readFileSync, readdirSync, writeFileSync } from "node:fs";
import path from "node:path";
import { fileURLToPath } from "node:url";
import cpy from "cpy";
import { defineConfig } from "tsup";
import { dependencies } from "./package.json";
export default defineConfig({
  name: "create-ponder",
  bundle: true,
  clean: true,
  entry: ["src/index.ts"],
  external: Object.keys(dependencies),
  format: ["esm"],
  platform: "node",
  async onSuccess() {
    const __dirname = fileURLToPath(new URL(".", import.meta.url));
    const examplesPath = path.join(__dirname, "../..", "examples");
    const targetPath = path.join(__dirname, "templates");
    // Copy examples contents into the templates path
    await cpy(
      [
        path.join(examplesPath, "**", "*"),
        "!**/with-nextjs/**",
        "!**/with-foundry/**",
        "!**/with-trpc/**",
        "!**/with-client/**",
        "!**/with-offchain/**",
        "!**/node_modules/**",
        "!**/generated/**",
        "!**/.ponder/**",
      ],
      targetPath,
      {
        filter: (file) => file.name !== ".env.local",
        rename: (name) =>
          name === ".env.example"
            ? "_dot_env.local"
            : name.replace(/^\./, "_dot_"),
      },
    );
    readdirSync(targetPath)
      .filter((d) => d !== "default" && d !== "etherscan")
      .map((d) => {
        const contents = readFileSync(
          path.join(targetPath, d, "_dot_env.local"),
          "utf-8",
        );
        writeFileSync(
          path.join(targetPath, d, "_dot_env.local"),
          contents.replace(/PONDER_RPC_URL_(\d+)=.*/, "PONDER_RPC_URL_$1="),
        );
      });
  },
});
</file>

<file path="packages/create-ponder/vite.config.ts">
import path from "node:path";
import { defineConfig } from "vitest/config";
export default defineConfig({
  resolve: {
    alias: {
      "@": path.resolve(__dirname, "./src"),
    },
  },
  test: {
    globalSetup: ["src/_test/globalSetup.ts"],
    testTimeout: 60_000,
  },
});
</file>

<file path="packages/eslint-config-ponder/index.js">
module.exports = {
  parser: "@typescript-eslint/parser",
  parserOptions: {
    project: true,
    tsconfigRootDir: "./",
  },
  plugins: ["@typescript-eslint"],
  rules: {
    "@typescript-eslint/no-floating-promises": "error",
  },
};
</file>

<file path="packages/eslint-config-ponder/package.json">
{
  "name": "eslint-config-ponder",
  "version": "0.16.1",
  "description": "ESLint config for Ponder apps",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/ponder-sh/ponder",
    "directory": "packages/eslint-config-ponder"
  },
  "main": "./index.js",
  "peerDependencies": {
    "@typescript-eslint/eslint-plugin": "^6.3.0",
    "@typescript-eslint/parser": "^6.3.0",
    "eslint": ">= 3"
  }
}
</file>

<file path="packages/eslint-config-ponder/README.md">
# eslint-config-ponder

[![CI status][ci-badge]][ci-url]
[![Version][version-badge]][version-url]
[![Telegram chat][tg-badge]][tg-url]
[![License][license-badge]][license-url]

Ponder is an open-source framework for blockchain application backends.

## Documentation

Visit [ponder.sh](https://ponder.sh) for documentation, guides, and the API reference.

## Support

Join [Ponder's telegram chat](https://t.me/pondersh) for support, feedback, and general chatter.

## Features

 &nbsp;Local development server with hot reloading<br/>
 &nbsp;`create-ponder` CLI tool to get started from an Etherscan link or Graph Protocol subgraph<br/>
 &nbsp;End-to-end type safety using [viem](https://viem.sh) and [ABIType](https://github.com/wagmi-dev/abitype)<br/>
 &nbsp;Autogenerated GraphQL API<br/>
 &nbsp;Easy to deploy anywhere using Node.js/Docker<br/>
 &nbsp;Supports all Ethereum-based blockchains, including test nodes like [Anvil](https://book.getfoundry.sh/anvil)<br/>
 &nbsp;Index events from multiple chains in the same app<br/>
 &nbsp;Reconciles chain reorganization<br/>
 &nbsp;Factory contracts<br/>
 &nbsp;Process transactions calls (in addition to logs)<br/>
 &nbsp;Run effects (e.g. send an API request) in indexing code<br/>

## Quickstart

### 1. Run `create-ponder`

You will be asked for a project name, and if you are using a [template](https://ponder.sh/docs/api-reference/create-ponder#templates) (recommended). Then, the CLI will create a project directory, install dependencies, and initialize a git repository.

```bash
npm init ponder@latest
# or
pnpm create ponder
# or
yarn create ponder
```

### 2. Start the development server

Just like Next.js and Vite, Ponder has a development server that automatically reloads when you save changes in any project file. It also prints `console.log` statements and errors encountered while running your code. First, `cd` into your project directory, then start the server.

```bash
npm run dev
# or
pnpm dev
# or
yarn dev
```

### 3. Add contracts & chains

Ponder fetches event logs for the contracts added to `ponder.config.ts`, and passes those events to the indexing functions you write.

```ts
// ponder.config.ts

import { createConfig } from "ponder";
import { http } from "viem";
 
import { BaseRegistrarAbi } from "./abis/BaseRegistrar";
 
export default createConfig({
  chains: {
    mainnet: { 
      id: 1,
      rpc: "https://eth-mainnet.g.alchemy.com/v2/...",
    },
  },
  contracts: {
    BaseRegistrar: {
      abi: BaseRegistrarAbi,
      chain: "mainnet",
      address: "0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85",
      startBlock: 9380410,
    },
  },
});
```

### 4. Define your schema

The `ponder.schema.ts` file contains the database schema, and defines the shape data that the GraphQL API serves.

```ts
// ponder.schema.ts

import { onchainTable } from "ponder";

export const ensName = onchainTable("ens_name", (t) => ({
  name: p.text().primaryKey(),
  owner: p.text().notNull(),
  registeredAt: p.integer().notNull(),
}));
```

### 5. Write indexing functions

Files in the `src/` directory contain **indexing functions**, which are TypeScript functions that process a contract event. The purpose of these functions is to insert data into the entity store.

```ts
// src/BaseRegistrar.ts

import { ponder } from "ponder:registry";
import schema from "ponder:schema";

ponder.on("BaseRegistrar:NameRegistered", async ({ event, context }) => {
  const { name, owner } = event.params;

  await context.db.insert(schema.ensName).values({
    name: name,
    owner: owner,
    registeredAt: event.block.timestamp,
  });
});
```

See the [create & update records](https://ponder.sh/docs/indexing/write#insert) docs for a detailed guide on writing indexing functions.

### 6. Query the GraphQL API

Ponder automatically generates a frontend-ready GraphQL API based on your `ponder.schema.ts` file. The API serves data that you inserted in your indexing functions.

```ts
{
  ensNames(limit: 2) {
    items {
      name
      owner
      registeredAt
    }
  }
}
```

```json
{
  "ensNames": {
    "items": [
      {
        "name": "vitalik.eth",
        "owner": "0x0904Dac3347eA47d208F3Fd67402D039a3b99859",
        "registeredAt": 1580345271
      },
      {
        "name": "joe.eth",
        "owner": "0x6109DD117AA5486605FC85e040ab00163a75c662",
        "registeredAt": 1580754710
      }
    ]
  }
}
```

That's it! Visit [ponder.sh](https://ponder.sh) for documentation, guides for deploying to production, and the API reference.

## Contributing

If you're interested in contributing to Ponder, please read the [contribution guide](/.github/CONTRIBUTING.md).

## Packages

- `ponder`
- `@ponder/client`
- `@ponder/react`
- `@ponder/utils`
- `create-ponder`
- `eslint-config-ponder`

## About

Ponder is MIT-licensed open-source software.

[ci-badge]: https://github.com/ponder-sh/ponder/actions/workflows/main.yml/badge.svg
[ci-url]: https://github.com/ponder-sh/ponder/actions/workflows/main.yml
[tg-badge]: https://img.shields.io/endpoint?color=neon&logo=telegram&label=chat&url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Fpondersh
[tg-url]: https://t.me/pondersh
[license-badge]: https://img.shields.io/npm/l/ponder?label=License
[license-url]: https://github.com/ponder-sh/ponder/blob/main/LICENSE
[version-badge]: https://img.shields.io/npm/v/ponder
[version-url]: https://github.com/ponder-sh/ponder/releases
</file>

<file path="packages/react/src/checkpoint.ts">
export type Checkpoint = {
  blockTimestamp: bigint;
  chainId: bigint;
  blockNumber: bigint;
  transactionIndex: bigint;
  eventType: number;
  eventIndex: bigint;
};
// 10 digits for unix timestamp gets us to the year 2277.
const BLOCK_TIMESTAMP_DIGITS = 10;
// Chain IDs are uint256. As of writing the largest Chain ID on https://chainlist.org
// is 13 digits. 16 digits should be enough (JavaScript's max safe integer).
const CHAIN_ID_DIGITS = 16;
// Same logic as chain ID.
const BLOCK_NUMBER_DIGITS = 16;
// Same logic as chain ID.
const TRANSACTION_INDEX_DIGITS = 16;
// At time of writing, we only have 2 event types planned, so one digit (10 types) is enough.
const EVENT_TYPE_DIGITS = 1;
// This could contain log index, trace index, etc. 16 digits should be enough.
const EVENT_INDEX_DIGITS = 16;
export const decodeCheckpoint = (checkpoint: string): Checkpoint => {
  let offset = 0;
  const blockTimestamp = BigInt(
    checkpoint.slice(offset, offset + BLOCK_TIMESTAMP_DIGITS),
  );
  offset += BLOCK_TIMESTAMP_DIGITS;
  const chainId = BigInt(checkpoint.slice(offset, offset + CHAIN_ID_DIGITS));
  offset += CHAIN_ID_DIGITS;
  const blockNumber = BigInt(
    checkpoint.slice(offset, offset + BLOCK_NUMBER_DIGITS),
  );
  offset += BLOCK_NUMBER_DIGITS;
  const transactionIndex = BigInt(
    checkpoint.slice(offset, offset + TRANSACTION_INDEX_DIGITS),
  );
  offset += TRANSACTION_INDEX_DIGITS;
  const eventType = +checkpoint.slice(offset, offset + EVENT_TYPE_DIGITS);
  offset += EVENT_TYPE_DIGITS;
  const eventIndex = BigInt(
    checkpoint.slice(offset, offset + EVENT_INDEX_DIGITS),
  );
  offset += EVENT_INDEX_DIGITS;
  return {
    blockTimestamp,
    chainId,
    blockNumber,
    transactionIndex,
    eventType,
    eventIndex,
  };
};
</file>

<file path="packages/react/src/context.ts">
"use client";
import type { Client } from "@ponder/client";
import { createContext, createElement } from "react";
import type { ResolvedSchema } from "./index.js";
export const PonderContext = createContext<Client<ResolvedSchema> | undefined>(
  undefined,
);
type PonderProviderProps = {
  client: Client<ResolvedSchema>;
};
export function PonderProvider(
  parameters: React.PropsWithChildren<PonderProviderProps>,
) {
  const { children, client } = parameters;
  const props = { value: client };
  return createElement(PonderContext.Provider, props, children);
}
</file>

<file path="packages/react/src/hook.test-d.ts">
import { sql } from "@ponder/client";
import { test } from "vitest";
import { usePonderQuery } from "./hook.js";
test("usePonderQuery", () => {
  const use = usePonderQuery({
    queryFn: (db) => db.execute<{ a: number; b: string }>(sql``),
    select: (data) => data.map((row) => row.a),
  });
  if (use.isSuccess) {
    // @ts-ignore
    type _ = typeof use.data;
    //   ^?
  }
});
</file>

<file path="packages/react/src/hook.ts">
"use client";
import type { Client, Status } from "@ponder/client";
import {
  type DefaultError,
  type QueryKey,
  type UseQueryOptions,
  type UseQueryResult,
  useQuery,
  useQueryClient,
} from "@tanstack/react-query";
import { useContext, useEffect, useMemo } from "react";
import { decodeCheckpoint } from "./checkpoint.js";
import { PonderContext } from "./context.js";
import type { ResolvedSchema } from "./index.js";
import { getPonderQueryOptions } from "./utils.js";
export function usePonderQuery<
  queryFnData = unknown,
  error = DefaultError,
  data = queryFnData,
>(
  params: {
    queryFn: (db: Client<ResolvedSchema>["db"]) => Promise<queryFnData>;
    live?: boolean;
  } & Omit<UseQueryOptions<queryFnData, error, data>, "queryFn" | "queryKey">,
): UseQueryResult<data, error> {
  const live = params.live ?? true;
  const queryClient = useQueryClient();
  const client = usePonderClient();
  // biome-ignore lint/correctness/useExhaustiveDependencies: <explanation>
  const queryOptions = useMemo(
    () => getPonderQueryOptions(client, params.queryFn),
    [params.queryFn],
  );
  useEffect(() => {
    if (live === false || params.enabled === false) return;
    const { unsubscribe } = client.live(queryOptions.queryFn, (data) => {
      queryClient.setQueryData(queryOptions.queryKey, data);
    });
    return unsubscribe;
  }, [
    live,
    params.enabled,
    client,
    queryOptions.queryFn,
    queryOptions.queryKey,
    queryClient,
  ]);
  return useQuery({
    ...params,
    queryKey: queryOptions.queryKey,
    queryFn: queryOptions.queryFn,
    staleTime: live
      ? (params.staleTime ?? Number.POSITIVE_INFINITY)
      : params.staleTime,
  });
}
export function usePonderClient(): Client<ResolvedSchema> {
  const client = useContext(PonderContext);
  if (client === undefined) {
    throw new Error("PonderProvider not found");
  }
  return client;
}
export function usePonderQueryOptions<T>(
  queryFn: (db: Client<ResolvedSchema>["db"]) => T,
): {
  queryKey: QueryKey;
  queryFn: () => T;
} {
  const client = usePonderClient();
  return getPonderQueryOptions(client, queryFn);
}
export function usePonderStatus<error = DefaultError>(
  params?: { live?: boolean } & Omit<
    UseQueryOptions<
      { chain_name: string; chain_id: number; latest_checkpoint: string }[],
      error,
      Status
    >,
    "queryFn" | "queryKey" | "select"
  >,
): UseQueryResult<Status, error> {
  return usePonderQuery<
    { chain_name: string; chain_id: number; latest_checkpoint: string }[],
    error,
    Status
  >({
    ...params,
    queryFn: (db) => db.execute("SELECT * FROM _ponder_checkpoint"),
    select(checkpoints) {
      const status: Status = {};
      for (const {
        chain_name,
        chain_id,
        latest_checkpoint,
      } of checkpoints.sort((a, b) => (a.chain_id > b.chain_id ? 1 : -1))) {
        status[chain_name] = {
          id: chain_id,
          block: {
            number: Number(decodeCheckpoint(latest_checkpoint).blockNumber),
            timestamp: Number(
              decodeCheckpoint(latest_checkpoint).blockTimestamp,
            ),
          },
        };
      }
      return status;
    },
  });
}
</file>

<file path="packages/react/src/index.ts">
export { PonderProvider, PonderContext } from "./context.js";
export {
  usePonderQuery,
  usePonderStatus,
  usePonderClient,
  usePonderQueryOptions,
} from "./hook.js";
export { getPonderQueryOptions } from "./utils.js";
// biome-ignore lint/suspicious/noEmptyInterface: <explanation>
export interface Register {}
export type ResolvedSchema = Register extends { schema: infer schema }
  ? schema
  : {
      [name: string]: unknown;
    };
</file>

<file path="packages/react/src/utils.ts">
import { type Client, compileQuery } from "@ponder/client";
import type { QueryKey } from "@tanstack/react-query";
import { stringify } from "superjson";
import type { ResolvedSchema } from "./index.js";
export type SQLWrapper = Exclude<Parameters<typeof compileQuery>[0], string>;
export function getPonderQueryOptions<T>(
  client: Client<ResolvedSchema>,
  queryFn: (db: Client<ResolvedSchema>["db"]) => T,
): {
  queryKey: QueryKey;
  queryFn: () => T;
} {
  const queryPromise = queryFn(client.db);
  // @ts-expect-error
  if ("getSQL" in queryPromise === false) {
    throw new Error(
      '"queryFn" must return SQL. You may have to remove `.execute()` from your query.',
    );
  }
  const query = compileQuery(queryPromise as unknown as SQLWrapper);
  const queryKey = ["__ponder_react", query.sql, stringify(query.params)];
  return {
    queryKey,
    queryFn: () => queryPromise,
  };
}
</file>

<file path="packages/react/.gitignore">
dist/
</file>

<file path="packages/react/build.ts">
import path from "node:path";
import { fileURLToPath } from "node:url";
import { watch } from "chokidar";
import { execa } from "execa";
import pc from "picocolors";
import { rimraf } from "rimraf";
const PACKAGE_NAME = "@PONDER/REACT";
const TSCONFIG = "tsconfig.build.json";
const WATCH_DIRECTORY = "src";
const prefix = pc.gray(`[${PACKAGE_NAME}]`);
const log = {
  cli: (msg: string) => console.log(`${prefix} ${pc.magenta("CLI")} ${msg}`),
  error: (msg: string) => console.error(`${prefix} ${pc.red("ERROR")} ${msg}`),
  success: (msg: string) => console.log(`${prefix} ${pc.green("CLI")} ${msg}`),
  tsc: (msg: string) => console.log(`${prefix} ${pc.blue("TSC")} ${msg}`),
};
const isWatchMode =
  process.argv.includes("--watch") || process.argv.includes("-w");
if (isWatchMode) {
  watchMode().catch((error) => {
    log.error(`Watch mode failed: ${error}`);
    process.exit(1);
  });
} else {
  build().then((success) => {
    process.exit(success ? 0 : 1);
  });
}
async function build() {
  try {
    log.cli("Build start");
    const startTime = Date.now();
    await rimraf("dist");
    log.cli("Cleaned output folder");
    const tscResult = await execa("tsc", ["--project", TSCONFIG], {
      reject: false,
      stderr: "pipe",
      stdout: "pipe",
    });
    `${tscResult.stdout}\n${tscResult.stderr}`
      .trim()
      .split("\n")
      .filter(Boolean)
      .forEach((line) => log.tsc(line));
    if (tscResult.exitCode !== 0) {
      log.error("Build failed");
      return false;
    } else {
      log.cli("Completed tsc without error");
    }
    const duration = Date.now() - startTime;
    log.success(` Build success in ${duration}ms`);
    return true;
  } catch (error) {
    log.error(`Build failed: ${error}`);
    return false;
  }
}
async function watchMode() {
  await build();
  const watcher = watch(WATCH_DIRECTORY, {
    cwd: path.dirname(fileURLToPath(import.meta.url)),
    persistent: true,
  });
  let isBuilding = false;
  let isBuildQueued = false;
  async function enqueueBuild() {
    if (!isBuilding) {
      try {
        isBuilding = true;
        await build();
      } finally {
        isBuilding = false;
      }
      if (isBuildQueued) {
        isBuildQueued = false;
        await enqueueBuild();
      }
    } else {
      isBuildQueued = true;
    }
  }
  watcher.on("change", async (path) => {
    log.cli(`Change detected: ${path}`);
    await enqueueBuild();
  });
  watcher.on("error", (error) => {
    log.error(`Watch error: ${error}`);
  });
  watcher.on("ready", () => {
    log.cli(`Watching for changes in "${WATCH_DIRECTORY}"`);
  });
  process.on("SIGINT", () => {
    watcher.close().then(() => {
      log.cli("Watch mode terminated");
      process.exit(0);
    });
  });
}
</file>

<file path="packages/react/package.json">
{
  "name": "@ponder/react",
  "version": "0.16.1",
  "description": "React hooks for Ponder",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/ponder-sh/ponder",
    "directory": "packages/react"
  },
  "scripts": {
    "build": "tsx build.ts",
    "test": "vitest",
    "test:typecheck": "vitest --typecheck.only",
    "typecheck": "tsc --noEmit"
  },
  "files": [
    "dist/**",
    "!dist/**/*.tsbuildinfo",
    "src/**/*.ts",
    "!src/**/*.test.ts",
    "!src/**/*.test-d.ts",
    "!src/_test/**/*"
  ],
  "sideEffects": false,
  "type": "module",
  "main": "./dist/esm/index.js",
  "types": "./dist/types/index.d.ts",
  "typings": "./dist/types/index.d.ts",
  "exports": {
    ".": {
      "types": "./dist/types/index.d.ts",
      "default": "./dist/esm/index.js"
    }
  },
  "dependencies": {
    "superjson": "^2.2.2"
  },
  "peerDependencies": {
    "@ponder/client": ">=0.16.1",
    "@tanstack/react-query": ">=5.0.0",
    "react": ">=18",
    "typescript": ">=5.0.4"
  },
  "peerDependenciesMeta": {
    "typescript": {
      "optional": true
    }
  },
  "devDependencies": {
    "@ponder/client": "workspace:*",
    "@tanstack/react-query": "^5.12.2",
    "@types/node": "^22.10.6",
    "@types/react": "^18.2.0",
    "chokidar": "^4.0.3",
    "execa": "^8.0.1",
    "picocolors": "^1.0.0",
    "react": "^18.2.0",
    "rimraf": "^5.0.5",
    "tsx": "^4.19.2",
    "vitest": "1.6.1"
  }
}
</file>

<file path="packages/react/tsconfig.build.json">
{
  "extends": "../../tsconfig.base.json",
  "include": ["src/**/*.ts"],
  "exclude": ["src/**/*.test.ts", "src/**/*.test-d.ts"],
  "compilerOptions": {
    "outDir": "./dist/esm",
    "declarationDir": "./dist/types",
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true,

    "jsx": "preserve"
  }
}
</file>

<file path="packages/react/tsconfig.json">
{
  "extends": "./tsconfig.build.json",
  "include": ["src/**/*.ts", "src/**/*.tsx"],
  "exclude": []
}
</file>

<file path="packages/utils/src/_test/1rpc.test.ts">
import { InvalidInputRpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, fromBlock, getRequest } from "./utils.js";
const request = getRequest("https://1rpc.io/eth");
const maxBlockRange = 1_000n;
test(
  "1rpc success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: UNI,
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
      ],
    });
    expect(logs).toHaveLength(9);
  },
  { timeout: 30_000 },
);
test("1rpc block range", async () => {
  const params: Params = [
    {
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(InvalidInputRpcError);
  expect(JSON.stringify(error)).includes(
    "eth_getLogs is limited to a 1000 blocks range",
  );
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry).toStrictEqual({
    shouldRetry: true,
    isSuggestedRange: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/_test/alchemy.test.ts">
import { RpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, WETH, fromBlock, getRequest } from "./utils.js";
const request = getRequest(process.env.RPC_URL_ALCHEMY_1!);
const maxBlockRange = 2000n;
test(
  "alchemy success response size",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: WETH,
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
      ],
    });
    expect(logs).toHaveLength(140192);
  },
  { timeout: 15_000 },
);
test("alchemy success block range", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: UNI,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + 1_000_000n),
      },
    ],
  });
  expect(logs).toHaveLength(3773);
});
test("alchemy block range", async () => {
  const params: Params = [
    {
      address: WETH,
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(RpcError);
  expect(JSON.stringify(error)).includes("this block range should work");
  const retry = getLogsRetryHelper({
    params,
    error,
  });
  expect(retry.shouldRetry).toBe(true);
  expect(retry.ranges).toHaveLength(30);
  expect(retry.ranges![0]).toStrictEqual({
    fromBlock: "0x112a880",
    toBlock: "0x112a8c2",
  });
});
</file>

<file path="packages/utils/src/_test/altitude.test.ts">
import { HttpRequestError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest(process.env.RPC_URL_ALTITUDE_999!);
const fromBlock = 9419400;
const maxBlockRange = 29999;
test("hyperliquid success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: "0xfD739d4e423301CE9385c1fb8850539D657C296D",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(4217);
});
test("hyperliquid block range", async () => {
  const params: Params = [
    {
      address: "0xfD739d4e423301CE9385c1fb8850539D657C296D",
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(HttpRequestError);
  expect(JSON.stringify(error)).includes(
    "allowed block range threshold exceeded",
  );
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry.shouldRetry).toBe(true);
  expect(retry.ranges).toHaveLength(2);
});
test("hyperliquid response size", async () => {
  const params: Params = [
    {
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(HttpRequestError);
  expect(JSON.stringify(error)).includes("query exceeds max results 20000");
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry.shouldRetry).toBe(true);
  expect(retry.ranges).toHaveLength(2);
});
</file>

<file path="packages/utils/src/_test/ankr-tac.test.ts">
import { RpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest(process.env.RPC_URL_ANKR_239!);
const fromBlock = 3105972n;
const maxBlockRange = 10000n;
test("ankr success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: "0xB63B9f0eb4A6E6f191529D71d4D88cc8900Df2C9",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(185);
});
test("ankr request range", async () => {
  const params: Params = [
    {
      address: "0xB63B9f0eb4A6E6f191529D71d4D88cc8900Df2C9",
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(RpcError);
  expect(JSON.stringify(error)).includes(
    "maximum [from, to] blocks distance: 10000",
  );
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry).toStrictEqual({
    shouldRetry: true,
    isSuggestedRange: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/_test/ankr.test.ts">
import { RpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, fromBlock, getRequest } from "./utils.js";
const request = getRequest(process.env.RPC_URL_ANKR_1!);
const maxBlockRange = 3000n;
test("ankr success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: UNI,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(13);
});
test("ankr response size", async () => {
  const params: Params = [
    {
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(RpcError);
  expect(JSON.stringify(error)).includes("query exceeds max results");
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry.shouldRetry).toBe(true);
  expect(retry.ranges).toHaveLength(2);
  expect(retry.ranges![0]).toStrictEqual({
    fromBlock: numberToHex(fromBlock),
    toBlock: numberToHex(fromBlock + maxBlockRange / 2n),
  });
});
</file>

<file path="packages/utils/src/_test/arbitrum.test.ts">
import { InvalidInputRpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://arb1.arbitrum.io/rpc");
const fromBlock = 1_000_000n;
test(
  "arbitrum success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: "0x82aF49447D8a07e3bd95BD0d56f35241523fBab1",
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + 1_000n),
        },
      ],
    });
    expect(logs).toHaveLength(714);
  },
  { timeout: 15_000 },
);
test(
  "arbitrum response size",
  async () => {
    const params: Params = [
      {
        address: "0x82aF49447D8a07e3bd95BD0d56f35241523fBab1",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + 20_000n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(error).toBeInstanceOf(InvalidInputRpcError);
    expect(JSON.stringify(error)).includes(
      "logs matched by query exceeds limit of 10000",
    );
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry.shouldRetry).toBe(true);
    expect(retry.ranges).toHaveLength(2);
    expect(retry.ranges![0]).toStrictEqual({
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + 10_000n),
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/aurora.test.ts">
import { LimitExceededRpcError, numberToHex } from "viem";
import { aurora } from "viem/chains";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://mainnet.aurora.dev", aurora);
const fromBlock = 57_600_000n;
const maxBlockRange = 2000n;
test(
  "aurora success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
      ],
    });
    expect(logs).toBeDefined();
  },
  { timeout: 15_000 },
);
test(
  "aurora block range",
  async () => {
    const params: Params = [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(error).toBeInstanceOf(LimitExceededRpcError);
    expect(JSON.stringify(error)).includes("up to a 2000 block range");
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry).toStrictEqual({
      shouldRetry: true,
      isSuggestedRange: true,
      ranges: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
        {
          fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
          toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        },
      ],
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/avalanche.test.ts">
import { InvalidInputRpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://api.avax.network/ext/bc/C/rpc");
const maxBlockRange = 2047n;
const fromBlock = 53_164_500n;
test(
  "avalanche success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: "0xB31f66AA3C1e785363F0875A1B74E27b85FD66c7",
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
      ],
    });
    expect(logs).toHaveLength(3135);
  },
  { timeout: 15_000 },
);
test("avalanche block range", async () => {
  const params: Params = [
    {
      address: "0xB31f66AA3C1e785363F0875A1B74E27b85FD66c7",
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({ method: "eth_getLogs", params }).catch(
    (error) => error,
  );
  expect(error).toBeInstanceOf(InvalidInputRpcError);
  expect(JSON.stringify(error?.details)).includes("maximum is set to 2048");
  const retry = getLogsRetryHelper({ params, error });
  expect(retry).toStrictEqual({
    shouldRetry: true,
    isSuggestedRange: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/_test/base.test.ts">
import { numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://mainnet.base.org");
const fromBlock = 10_000_000n;
test(
  "base success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: "0x2Ae3F1Ec7F1F5012CFEab0185bfc7aa3cf0DEc22",
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + 1_000n),
        },
      ],
    });
    expect(logs).toHaveLength(77);
  },
  { timeout: 15_000 },
);
test(
  "base block range",
  async () => {
    const params: Params = [
      {
        address: "0x2Ae3F1Ec7F1F5012CFEab0185bfc7aa3cf0DEc22",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + 20_000n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(JSON.stringify(error)).includes(
      "no backend is currently healthy to serve traffic",
    );
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry.shouldRetry).toBe(true);
    expect(retry.ranges).toHaveLength(2);
    expect(retry.ranges![0]).toStrictEqual({
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + 10_000n),
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/blast.test.ts">
import { HttpRequestError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://rpc.blast.io");
const fromBlock = 1_000_000n;
const maxBlockRange = 10_000n;
test(
  "blast success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: "0xb9dfCd4CF589bB8090569cb52FaC1b88Dbe4981F",
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
      ],
    });
    expect(logs).toHaveLength(109);
  },
  { timeout: 15_000 },
);
test(
  "blast block range",
  async () => {
    const params: Params = [
      {
        address: "0xb9dfCd4CF589bB8090569cb52FaC1b88Dbe4981F",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(error).toBeInstanceOf(HttpRequestError);
    expect(JSON.stringify(error)).includes(
      "eth_getLogs is limited to a 10,000 range",
    );
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry).toStrictEqual({
      shouldRetry: true,
      isSuggestedRange: true,
      ranges: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
        {
          fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
          toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        },
      ],
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/blockpi.test.ts">
import { InvalidParamsRpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, fromBlock, getRequest } from "./utils.js";
const request = getRequest("https://ethereum.blockpi.network/v1/rpc/public");
const maxBlockRange = 1024n;
test(
  "blockpi success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: UNI,
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
      ],
    });
    expect(logs).toHaveLength(9);
  },
  { timeout: 15_000 },
);
test(
  "blockpi block range",
  async () => {
    const params: Params = [
      {
        address: UNI,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(error).toBeInstanceOf(InvalidParamsRpcError);
    expect(JSON.stringify(error)).includes(
      "eth_getLogs is limited to 1024 block range",
    );
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry).toStrictEqual({
      shouldRetry: true,
      isSuggestedRange: true,
      ranges: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
        {
          fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
          toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        },
      ],
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/celo.test.ts">
import { numberToHex } from "viem";
import { expect, test } from "vitest";
import { getRequest } from "./utils.js";
const request = getRequest("https://forno.celo.org");
const fromBlock = 10_000_000n;
test("celo success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: "0x471EcE3750Da237f93B8E339c536989b8978a438",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + 1_000n),
      },
    ],
  });
  expect(logs).toHaveLength(3649);
});
</file>

<file path="packages/utils/src/_test/chainstack.test.ts">
import { InvalidParamsRpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, fromBlock, getRequest } from "./utils.js";
const request = getRequest(process.env.RPC_URL_CHAINSTACK_1!);
const maxBlockRange = 110n;
test(
  "chainstack success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: UNI,
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
      ],
    });
    expect(logs).toHaveLength(1);
  },
  { timeout: 15_000 },
);
test(
  "chainstack block range",
  async () => {
    const params: Params = [
      {
        address: UNI,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(error).toBeInstanceOf(InvalidParamsRpcError);
    expect(JSON.stringify(error)).includes("Block range limit exceeded.");
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry).toStrictEqual({
      shouldRetry: true,
      isSuggestedRange: true,
      ranges: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange - 10n),
        },
        {
          fromBlock: numberToHex(fromBlock + maxBlockRange - 10n + 1n),
          toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        },
      ],
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/cloudflare.test.ts">
import { RpcRequestError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, fromBlock, getRequest } from "./utils.js";
const request = getRequest("https://cloudflare-eth.com");
const maxBlockRange = 799n;
test.skip("cloudflare success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: UNI,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(7);
});
test("cloudflare block range", async () => {
  const params: Params = [
    {
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(RpcRequestError);
  expect(JSON.stringify(error)).includes("Max range: 800");
  const retry = getLogsRetryHelper({
    params,
    error,
  });
  expect(retry).toStrictEqual({
    shouldRetry: true,
    isSuggestedRange: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/_test/coinbase.test.ts">
import { LimitExceededRpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest(process.env.RPC_URL_COINBASE_8453!);
const fromBlock = 10_000_000n;
const maxBlockRange = 999n;
test("coinbase success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: "0x2Ae3F1Ec7F1F5012CFEab0185bfc7aa3cf0DEc22",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(77);
});
test(
  "coinbase block range",
  async () => {
    const params: Params = [
      {
        address: "0x2Ae3F1Ec7F1F5012CFEab0185bfc7aa3cf0DEc22",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(error).toBeInstanceOf(LimitExceededRpcError);
    expect(JSON.stringify(error)).includes(
      "please limit the query to at most 1000 blocks",
    );
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry).toStrictEqual({
      shouldRetry: true,
      isSuggestedRange: true,
      ranges: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
        {
          fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
          toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        },
      ],
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/globalSetup.ts">
import dotenv from "dotenv";
const globalSetup = async () => {
  dotenv.config({ path: ".env.local" });
};
if ("bun" in process.versions) {
  require("bun:test").beforeAll(async () => {
    await globalSetup();
  });
}
export default globalSetup;
</file>

<file path="packages/utils/src/_test/harmony.test.ts">
import { InvalidInputRpcError, numberToHex } from "viem";
import { harmonyOne } from "viem/chains";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://api.harmony.one", harmonyOne);
const fromBlock = 70_000_000n;
const maxBlockRange = 1024n;
test(
  "harmony success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
      ],
    });
    expect(logs).toHaveLength(703);
  },
  { timeout: 15_000 },
);
test(
  "harmony block range",
  async () => {
    const params: Params = [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(error).toBeInstanceOf(InvalidInputRpcError);
    expect(JSON.stringify(error)).includes(
      "query must be smaller than size 1024",
    );
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry).toStrictEqual({
      shouldRetry: true,
      isSuggestedRange: true,
      ranges: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
        {
          fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
          toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        },
      ],
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/hyperliquid.test.ts">
import { RpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://rpc.hyperliquid.xyz/evm");
const fromBlock = 9419400;
const maxBlockRange = 50;
test.skip("hyperliquid success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(5);
});
test.skip("hyperliquid block range", async () => {
  const params: Params = [
    {
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(RpcError);
  expect(JSON.stringify(error)).includes("query exceeds max block range 50");
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry).toStrictEqual({
    shouldRetry: true,
    isSuggestedRange: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange + 1),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/_test/infura.test.ts">
import { LimitExceededRpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, WETH, fromBlock, getRequest } from "./utils.js";
const request = getRequest(process.env.RPC_URL_INFURA_1!);
test(
  "infura success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: UNI,
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + 1_000n),
        },
      ],
    });
    expect(logs).toHaveLength(9);
  },
  { timeout: 15_000 },
);
test("infura block range", async () => {
  const params: Params = [
    {
      address: WETH,
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + 1_000n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(LimitExceededRpcError);
  expect(JSON.stringify(error)).includes("Try with this block range ");
  const retry = getLogsRetryHelper({
    params,
    error,
  });
  expect(retry.shouldRetry).toBe(true);
  expect(retry.ranges).toHaveLength(8);
  expect(retry.ranges![0]).toStrictEqual({
    fromBlock: "0x112a880",
    toBlock: "0x112a908",
  });
});
</file>

<file path="packages/utils/src/_test/llamarpc.test.ts">
import { InvalidParamsRpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, WETH, fromBlock, getRequest } from "./utils.js";
const request = getRequest("https://eth.llamarpc.com");
test(
  "llamarpc success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: UNI,
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + 1_000n),
        },
      ],
    });
    expect(logs).toHaveLength(9);
  },
  { timeout: 15_000 },
);
test("llamarpc response size", async () => {
  const params: Params = [
    {
      address: WETH,
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + 1_000n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(InvalidParamsRpcError);
  expect(JSON.stringify(error)).includes("query exceeds max results 20000");
  const retry = getLogsRetryHelper({
    params,
    error,
  });
  expect(retry.shouldRetry).toBe(true);
  expect(retry.ranges).toHaveLength(2);
  expect(retry.ranges![0]).toStrictEqual({
    fromBlock: numberToHex(fromBlock),
    toBlock: numberToHex(fromBlock + 500n),
  });
});
</file>

<file path="packages/utils/src/_test/merkle.test.ts">
import { RpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, WETH, fromBlock, getRequest } from "./utils.js";
const request = getRequest("https://eth.merkle.io");
const maxBlockRange10k = 10_000n;
const maxBlockRange1k = 1_000n;
test("merkle success 10k", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: UNI,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange10k),
      },
    ],
  });
  expect(logs).toHaveLength(49);
});
test("merkle block range 10k", async () => {
  const params: Params = [
    {
      address: WETH,
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange10k + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(RpcError);
  expect(JSON.stringify(error)).includes(
    "eth_getLogs range is too large, max is 10k blocks",
  );
  const retry = getLogsRetryHelper({
    params,
    error,
  });
  expect(retry.shouldRetry).toBe(true);
  expect(retry).toStrictEqual({
    isSuggestedRange: true,
    shouldRetry: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange10k),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange10k + 1n),
        toBlock: numberToHex(fromBlock + maxBlockRange10k + 1n),
      },
    ],
  });
});
test("merkle success 1k", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: UNI,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange1k),
      },
    ],
  });
  expect(logs).toHaveLength(9);
});
test("merkle block range 1k", async () => {
  const params: Params = [
    {
      address: WETH,
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange1k + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(RpcError);
  expect(JSON.stringify(error)).includes(
    "eth_getLogs range is too large, max is 1k blocks",
  );
  const retry = getLogsRetryHelper({
    params,
    error,
  });
  expect(retry.shouldRetry).toBe(true);
  expect(retry).toStrictEqual({
    isSuggestedRange: true,
    shouldRetry: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange1k),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange1k + 1n),
        toBlock: numberToHex(fromBlock + maxBlockRange1k + 1n),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/_test/moonriver.test.ts">
import { InternalRpcError, numberToHex } from "viem";
import { moonriver } from "viem/chains";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest(
  "https://rpc.api.moonriver.moonbeam.network",
  moonriver,
);
const fromBlock = 12_000_000n;
const maxBlockRange = 1024n;
test(
  "moonriver success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
      ],
    });
    expect(logs).toBeDefined();
  },
  { timeout: 15_000 },
);
test(
  "moonriver block range",
  async () => {
    const params: Params = [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(error).toBeInstanceOf(InternalRpcError);
    expect(JSON.stringify(error)).includes(
      "block range is too wide (maximum 1024)",
    );
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry).toStrictEqual({
      shouldRetry: true,
      isSuggestedRange: true,
      ranges: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
        {
          fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
          toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        },
      ],
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/optimism.test.ts">
import { RpcRequestError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://mainnet.optimism.io");
const fromBlock = 100_000_000n;
test(
  "optimism success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          address: "0x871f2F2ff935FD1eD867842FF2a7bfD051A5E527",
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + 1_000n),
        },
      ],
    });
    expect(logs).toHaveLength(0);
  },
  { timeout: 15_000 },
);
// Reported as block range but behaves inconsistently
test(
  "optimism block range",
  async () => {
    const params: Params = [
      {
        address: "0x4200000000000000000000000000000000000006",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + 10_000n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(error).toBeInstanceOf(RpcRequestError);
    expect(JSON.stringify(error)).includes("Block range is too large");
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry.shouldRetry).toBe(true);
    expect(retry.ranges).toHaveLength(2);
    expect(retry.ranges![0]).toStrictEqual({
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + 5_000n),
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/publicnode.test.ts">
import { numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, fromBlock, getRequest } from "./utils.js";
const request = getRequest("https://ethereum-rpc.publicnode.com");
const maxBlockRange = 50_000n;
test("publicnode success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: UNI,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(203);
});
test("publicnode block range", async () => {
  const params: Params = [
    {
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(JSON.stringify(error)).includes("exceed maximum block range: 50000");
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry).toStrictEqual({
    isSuggestedRange: true,
    shouldRetry: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/_test/quicknode.test.ts">
import { HttpRequestError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, fromBlock, getRequest } from "./utils.js";
const request = getRequest(process.env.RPC_URL_QUICKNODE_1!);
const maxBlockRange = 10000n;
test("quicknode success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: UNI,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(49);
});
test("quicknode block range", async () => {
  const params: Params = [
    {
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(HttpRequestError);
  expect(JSON.stringify(error)).includes(
    "eth_getLogs is limited to a 10,000 range",
  );
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry).toStrictEqual({
    shouldRetry: true,
    isSuggestedRange: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/_test/somnia.test.ts">
import { RpcRequestError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://dream-rpc.somnia.network");
const fromBlock = 53_580_000n;
const maxBlockRange = 1_000n;
test(
  "somnia success",
  async () => {
    const logs = await request({
      method: "eth_getLogs",
      params: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
      ],
    });
    expect(logs).toHaveLength(5616);
  },
  { timeout: 15_000 },
);
test(
  "somnia block range",
  async () => {
    const params: Params = [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ];
    const error = await request({
      method: "eth_getLogs",
      params,
    }).catch((error) => error);
    expect(error).toBeInstanceOf(RpcRequestError);
    expect(JSON.stringify(error)).includes("block range exceeds 1000");
    const retry = getLogsRetryHelper({
      params,
      error,
    });
    expect(retry).toStrictEqual({
      shouldRetry: true,
      isSuggestedRange: true,
      ranges: [
        {
          fromBlock: numberToHex(fromBlock),
          toBlock: numberToHex(fromBlock + maxBlockRange),
        },
        {
          fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
          toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        },
      ],
    });
  },
  { timeout: 15_000 },
);
</file>

<file path="packages/utils/src/_test/swell.test.ts">
import { numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://swell-mainnet.alt.technology");
const fromBlock = 6002000n;
const maxBlockRange = 1000n;
test("swell success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        // address: "0x259813b665c8f6074391028ef782e27b65840d89",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(58);
});
test("swell block range", async () => {
  const params: Params = [
    {
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(JSON.stringify(error)).includes("block range greater than 1000 max");
  const retry = getLogsRetryHelper({
    params,
    error,
  });
  expect(retry).toStrictEqual({
    shouldRetry: true,
    isSuggestedRange: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/_test/thirdweb.test.ts">
import { LimitExceededRpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, UNI, WETH, fromBlock, getRequest } from "./utils.js";
const request = getRequest("https://1.rpc.thirdweb.com");
const maxBlockRange = 1_000n;
test("thirdweb success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: UNI,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(9);
});
test("thirdweb response size", async () => {
  const params: Params = [
    {
      address: WETH,
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(LimitExceededRpcError);
  expect(JSON.stringify(error)).includes(
    "Maximum allowed number of requested blocks is 1000",
  );
  const retry = getLogsRetryHelper({
    params,
    error,
  });
  expect(retry).toStrictEqual({
    shouldRetry: true,
    isSuggestedRange: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
      {
        fromBlock: numberToHex(fromBlock + maxBlockRange + 1n),
        toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/_test/tron.test.ts">
import { RpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://api.trongrid.io/jsonrpc");
const fromBlock = 71583814n;
const maxBlockRange = 5000n;
// THePheuMzpeYaEscvEPJhmvzjvQAq1ptqe from https://tronscan.org/#/tools/code-converter/tron-ethereum-address
const TEST_ADDRESS = "0x543208eB34ecad8f91a4D83e597d3c39D67ca47B";
test("tron success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: TEST_ADDRESS,
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + maxBlockRange),
      },
    ],
  });
  expect(logs).toHaveLength(4);
});
test("tron response size", async () => {
  const params: Params = [
    {
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + maxBlockRange + 1n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(RpcError);
  expect(JSON.stringify(error)).includes("exceed max block range");
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry.shouldRetry).toBe(true);
  expect(retry.ranges).toHaveLength(2);
  expect(retry.ranges![0]).toStrictEqual({
    fromBlock: numberToHex(fromBlock),
    toBlock: numberToHex(fromBlock + maxBlockRange - 1n),
  });
});
</file>

<file path="packages/utils/src/_test/utils.ts">
import {
  http,
  type Chain,
  type EIP1193RequestFn,
  type PublicRpcSchema,
} from "viem";
import { mainnet } from "viem/chains";
import type { GetLogsRetryHelperParameters } from "../getLogsRetryHelper.js";
export type Params = GetLogsRetryHelperParameters["params"];
export const WETH = "0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2";
export const UNI = "0x1F98431c8aD98523631AE4a59f267346ea31F984";
export const getRequest = (url: string, chain: Chain = mainnet) => {
  const request = http(url)({
    chain,
  }).request as EIP1193RequestFn<PublicRpcSchema>;
  return request;
};
export const fromBlock = 18_000_000n;
</file>

<file path="packages/utils/src/_test/zkevm.test.ts">
import { RpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://zkevm-rpc.com");
const fromBlock = 950_000n;
test("zkevm success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + 1_000n),
      },
    ],
  });
  expect(logs).toHaveLength(2979);
});
test.skip("zkevm response size", async () => {
  const params: Params = [
    {
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + 10_000n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(RpcError);
  expect(JSON.stringify(error)).includes(
    "query returned more than 10000 results",
  );
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry.shouldRetry).toBe(true);
  expect(retry.ranges).toHaveLength(2);
});
</file>

<file path="packages/utils/src/_test/zksync.test.ts">
import { RpcError, numberToHex } from "viem";
import { expect, test } from "vitest";
import { getLogsRetryHelper } from "../getLogsRetryHelper.js";
import { type Params, getRequest } from "./utils.js";
const request = getRequest("https://mainnet.era.zksync.io");
const fromBlock = 18406545n;
test("zksync success", async () => {
  const logs = await request({
    method: "eth_getLogs",
    params: [
      {
        address: "0xfc00dac251711508d4dd7b0c310e913575988838",
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + 10_000n),
      },
    ],
  });
  expect(logs).toHaveLength(726);
});
test("zksync block range", async () => {
  const params: Params = [
    {
      address: "0xfc00dac251711508d4dd7b0c310e913575988838",
      fromBlock: numberToHex(fromBlock),
      toBlock: numberToHex(fromBlock + 50_000n),
    },
  ];
  const error = await request({
    method: "eth_getLogs",
    params,
  }).catch((error) => error);
  expect(error).toBeInstanceOf(RpcError);
  expect(JSON.stringify(error)).includes("Try with this block range ");
  const retry = getLogsRetryHelper({
    params,
    error: error,
  });
  expect(retry).toStrictEqual({
    shouldRetry: true,
    isSuggestedRange: true,
    ranges: [
      {
        fromBlock: numberToHex(fromBlock),
        toBlock: numberToHex(fromBlock + 45106n),
      },
      {
        fromBlock: numberToHex(fromBlock + 45106n + 1n),
        toBlock: numberToHex(fromBlock + 50_000n),
      },
    ],
  });
});
</file>

<file path="packages/utils/src/utils/promiseWithResolvers.test-d.ts">
import { assertType, test } from "vitest";
import { promiseWithResolvers } from "./promiseWithResolvers.js";
test("resolve type", () => {
  const { resolve } = promiseWithResolvers<number>();
  assertType<(arg: number) => void>(resolve);
});
test("promise type", () => {
  const { promise } = promiseWithResolvers<number>();
  assertType<Promise<number>>(promise);
});
</file>

<file path="packages/utils/src/utils/promiseWithResolvers.test.ts">
import { expect, test } from "vitest";
import { promiseWithResolvers } from "./promiseWithResolvers.js";
test("resolves", async () => {
  const { promise, resolve } = promiseWithResolvers<number>();
  resolve(1);
  const value = await promise;
  expect(value).toBe(1);
});
test("rejects", async () => {
  let rejected = false;
  const { promise, reject } = promiseWithResolvers();
  promise.catch(() => {
    rejected = true;
  });
  await Promise.reject().catch(reject);
  expect(rejected).toBe(true);
});
</file>

<file path="packages/utils/src/utils/promiseWithResolvers.ts">
export type PromiseWithResolvers<TPromise> = {
  resolve: (arg: TPromise) => void;
  reject: (error: Error) => void;
  promise: Promise<TPromise>;
};
/**
 * @description Application level polyfill.
 */
export const promiseWithResolvers = <
  TPromise,
>(): PromiseWithResolvers<TPromise> => {
  let resolve: (arg: TPromise) => void;
  let reject: (error: Error) => void;
  const promise = new Promise<TPromise>((_resolve, _reject) => {
    resolve = _resolve;
    reject = _reject;
  });
  return { resolve: resolve!, reject: reject!, promise };
};
</file>

<file path="packages/utils/src/utils/queue.test-d.ts">
import { assertType, test } from "vitest";
import { createQueue } from "./queue.js";
test("add type", () => {
  const queue = createQueue({
    concurrency: 1,
    worker: (_arg: "a" | "b" | "c") => Promise.resolve(),
  });
  assertType<(task: "a" | "b" | "c") => Promise<void>>(queue.add);
});
</file>

<file path="packages/utils/src/utils/queue.test.ts">
import { expect, test, vi } from "vitest";
import { promiseWithResolvers } from "./promiseWithResolvers.js";
import { createQueue } from "./queue.js";
test("add resolves", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    initialStart: true,
    browser: false,
    worker: () => Promise.resolve(1),
  });
  const promise = queue.add();
  expect(await promise).toBe(1);
});
test("add rejects", async () => {
  let rejected = false;
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.reject(),
  });
  const promise = queue.add();
  await queue.start();
  await promise.catch(() => {
    rejected = true;
  });
  expect(rejected).toBe(true);
});
test("size", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  expect(queue.size()).toBe(1);
  await queue.start();
  expect(queue.size()).toBe(0);
});
test("pending", async () => {
  const { promise, resolve } = promiseWithResolvers<void>();
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    initialStart: true,
    browser: false,
    worker: () => promise,
  });
  queue.add();
  expect(await queue.pending()).toBe(1);
  resolve();
  expect(await queue.pending()).toBe(0);
});
test("clear", () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  queue.add();
  queue.add();
  queue.clear();
  expect(queue.size()).toBe(0);
});
test("clear timer", async () => {
  const queue = createQueue({
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  queue.add();
  queue.add();
  await queue.start();
  queue.clear();
  await queue.onIdle();
  expect(queue.size()).toBe(0);
  expect(await queue.pending()).toBe(0);
});
test("isStarted", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  expect(queue.isStarted()).toBe(false);
  await queue.start();
  expect(queue.isStarted()).toBe(true);
});
test("initial start", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    initialStart: true,
    worker: () => Promise.resolve(),
  });
  expect(queue.isStarted()).toBe(true);
  await queue.add();
  expect(queue.size()).toBe(0);
});
test("start", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  const promise = queue.add();
  expect(queue.size()).toBe(1);
  await queue.start();
  expect(queue.isStarted()).toBe(true);
  await promise;
  expect(queue.size()).toBe(0);
});
test("pause", () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    initialStart: true,
    worker: () => Promise.resolve(),
  });
  queue.pause();
  queue.add();
  expect(queue.size()).toBe(1);
});
test.todo("restart");
test("onIdle short loop", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  await queue.onIdle();
});
test("onIdle", async () => {
  const queue = createQueue({
    concurrency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  const promise = queue.onIdle();
  await queue.start();
  await promise;
});
test("onIdle twice", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  queue.onIdle();
  await queue.start();
  queue.pause();
  queue.add();
  const promise = queue.onIdle();
  await queue.start();
  await promise;
});
test("onEmpty short loop", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  await queue.onEmpty();
});
test("onEmpty", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  const promise = queue.onEmpty();
  await queue.start();
  await promise;
});
test("onEmpty twice", async () => {
  const queue = createQueue({
    concurrency: 1,
    frequency: 1,
    browser: false,
    worker: () => Promise.resolve(),
  });
  queue.add();
  queue.onEmpty();
  await queue.start();
  queue.pause();
  queue.add();
  const promise = queue.onEmpty();
  await queue.start();
  await promise;
});
test("concurrency", async () => {
  const func = vi.fn(() => Promise.resolve());
  const queue = createQueue({
    concurrency: 2,
    frequency: 5,
    browser: false,
    worker: func,
  });
  queue.add();
  queue.add();
  queue.add();
  queue.add();
  await queue.start();
  queue.pause();
  expect(queue.size()).toBe(2);
  expect(func).toHaveBeenCalledTimes(2);
});
test("frequency", async () => {
  const func = vi.fn(() => Promise.resolve());
  const queue = createQueue({
    frequency: 2,
    concurrency: 5,
    browser: false,
    worker: func,
  });
  queue.add();
  queue.add();
  queue.add();
  queue.add();
  await queue.start();
  expect(queue.size()).toBe(2);
  expect(func).toHaveBeenCalledTimes(2);
  await new Promise((resolve) => setTimeout(resolve, 1_010));
  expect(queue.size()).toBe(0);
  expect(func).toHaveBeenCalledTimes(4);
});
/**
 * Two queues running at the same time should alternate between events.
 * One queue running all its event in a row would mean the event loop
 * is being "starved".
 */
test("event loop", async () => {
  const out: number[] = [];
  const queue1 = createQueue({
    concurrency: 1,
    browser: false,
    worker: () => {
      out.push(1);
      return Promise.resolve();
    },
  });
  const queue2 = createQueue({
    concurrency: 1,
    browser: false,
    worker: () => {
      out.push(2);
      return Promise.resolve();
    },
  });
  for (let i = 0; i < 10; i++) {
    queue1.add();
    queue2.add();
  }
  await Promise.all([queue1.start(), queue2.start()]);
  await Promise.all([queue1.onIdle(), queue2.onIdle()]);
  const expectedOut: number[] = [];
  for (let i = 0; i < 10; i++) {
    expectedOut.push(1, 2);
  }
  expect(out).toStrictEqual(expectedOut);
});
test("update parameters", async () => {
  const func = vi.fn(() => Promise.resolve());
  const queue = createQueue({
    concurrency: 2,
    frequency: 5,
    browser: false,
    worker: func,
  });
  queue.add();
  queue.add();
  queue.add();
  queue.add();
  queue.add();
  queue.add();
  await queue.start();
  queue.pause();
  expect(queue.size()).toBe(4);
  expect(func).toHaveBeenCalledTimes(2);
  queue.setParameters({ concurrency: undefined, frequency: 8 });
  await queue.start();
  queue.pause();
  expect(queue.size()).toBe(0);
  expect(func).toHaveBeenCalledTimes(6);
});
</file>

<file path="packages/utils/src/utils/queue.ts">
import {
  type PromiseWithResolvers,
  promiseWithResolvers,
} from "./promiseWithResolvers.js";
export type InnerQueue<returnType, taskType> = {
  task: taskType;
  resolve: (arg: returnType) => void;
  reject: (error: Error) => void;
}[];
export type Queue<returnType, taskType> = {
  size: () => number;
  pending: () => Promise<number>;
  add: (task: taskType) => Promise<returnType>;
  clear: () => void;
  isStarted: () => boolean;
  start: () => Promise<void>;
  pause: () => void;
  onIdle: () => Promise<void>;
  onEmpty: () => Promise<void>;
  setParameters: (
    parameters: Pick<
      CreateQueueParameters<unknown, unknown>,
      "frequency" | "concurrency"
    >,
  ) => void;
};
export type CreateQueueParameters<returnType, taskType> = {
  worker: (task: taskType) => Promise<returnType>;
  initialStart?: boolean;
  browser?: boolean;
} & (
  | {
      concurrency: number;
      frequency: number;
    }
  | { concurrency: number; frequency?: undefined }
  | { concurrency?: undefined; frequency: number }
);
const validateParameters = ({
  concurrency,
  frequency,
}: Pick<
  CreateQueueParameters<unknown, unknown>,
  "frequency" | "concurrency"
>) => {
  if (concurrency === undefined && frequency === undefined) {
    throw new Error(
      "Invalid queue configuration, must specify either 'concurrency' or 'frequency'.",
    );
  }
  if (concurrency !== undefined && concurrency <= 0) {
    throw new Error(
      `Invalid value for queue 'concurrency' option. Got ${concurrency}, expected a number greater than zero.`,
    );
  }
  if (frequency !== undefined && frequency <= 0) {
    throw new Error(
      `Invalid value for queue 'frequency' option. Got ${frequency}, expected a number greater than zero.`,
    );
  }
};
export const createQueue = <returnType, taskType = void>({
  worker,
  initialStart = false,
  browser = true,
  ..._parameters
}: CreateQueueParameters<returnType, taskType>): Queue<
  returnType,
  taskType
> => {
  validateParameters(_parameters);
  const parameters: Pick<
    CreateQueueParameters<unknown, unknown>,
    "frequency" | "concurrency"
  > = _parameters;
  let queue = new Array<InnerQueue<returnType, taskType>[number]>();
  let pending = 0;
  let timestamp = 0;
  let requests = 0;
  let isStarted = initialStart;
  let timer: NodeJS.Timeout | undefined;
  let emptyPromiseWithResolvers:
    | (PromiseWithResolvers<void> & { completed: boolean })
    | undefined = undefined;
  let idlePromiseWithResolvers:
    | (PromiseWithResolvers<void> & { completed: boolean })
    | undefined = undefined;
  const next = () => {
    if (!isStarted) return;
    const _timestamp = Date.now();
    if (Math.floor(_timestamp / 1_000) !== timestamp) {
      requests = 0;
      timestamp = Math.floor(_timestamp / 1_000);
    }
    if (timer) return;
    while (
      (parameters.frequency !== undefined
        ? requests < parameters.frequency
        : true) &&
      (parameters.concurrency !== undefined
        ? pending < parameters.concurrency
        : true) &&
      queue.length > 0
    ) {
      const { task, resolve, reject } = queue.shift()!;
      requests++;
      pending++;
      worker(task)
        .then(resolve)
        .catch(reject)
        .finally(() => {
          pending--;
          if (
            idlePromiseWithResolvers !== undefined &&
            queue.length === 0 &&
            pending === 0
          ) {
            idlePromiseWithResolvers.resolve();
            idlePromiseWithResolvers.completed = true;
          }
          browser ? next() : process.nextTick(next);
        });
      if (emptyPromiseWithResolvers !== undefined && queue.length === 0) {
        emptyPromiseWithResolvers.resolve();
        emptyPromiseWithResolvers.completed = true;
      }
    }
    if (
      parameters.frequency !== undefined &&
      requests >= parameters.frequency
    ) {
      timer = setTimeout(
        () => {
          timer = undefined;
          next();
        },
        1_000 - (_timestamp % 1_000),
      );
      return;
    }
  };
  return {
    size: () => queue.length,
    pending: () => {
      if (browser) {
        return new Promise<number>((resolve) =>
          setTimeout(() => resolve(pending)),
        );
      } else {
        return new Promise<number>((resolve) =>
          setImmediate(() => resolve(pending)),
        );
      }
    },
    add: (task: taskType) => {
      const { promise, resolve, reject } = promiseWithResolvers<returnType>();
      queue.push({ task, resolve, reject });
      next();
      return promise.catch((error) => {
        Error.captureStackTrace(error);
        throw error;
      });
    },
    clear: () => {
      queue = new Array<InnerQueue<returnType, taskType>[number]>();
      clearTimeout(timer);
      timer = undefined;
    },
    isStarted: () => isStarted,
    start: () => {
      if (browser) {
        return new Promise<number>((resolve) =>
          setTimeout(() => resolve(pending)),
        ).then(() => {
          isStarted = true;
          next();
        });
      } else {
        return new Promise<number>((resolve) =>
          process.nextTick(() => resolve(pending)),
        ).then(() => {
          isStarted = true;
          next();
        });
      }
    },
    pause: () => {
      isStarted = false;
    },
    onIdle: () => {
      if (
        idlePromiseWithResolvers === undefined ||
        idlePromiseWithResolvers.completed
      ) {
        if (queue.length === 0 && pending === 0) return Promise.resolve();
        idlePromiseWithResolvers = {
          ...promiseWithResolvers<void>(),
          completed: false,
        };
      }
      return idlePromiseWithResolvers.promise;
    },
    onEmpty: () => {
      if (
        emptyPromiseWithResolvers === undefined ||
        emptyPromiseWithResolvers.completed
      ) {
        if (queue.length === 0) return Promise.resolve();
        emptyPromiseWithResolvers = {
          ...promiseWithResolvers<void>(),
          completed: false,
        };
      }
      return emptyPromiseWithResolvers.promise;
    },
    setParameters: (_parameters) => {
      validateParameters(_parameters);
      if ("frequency" in _parameters) {
        parameters.frequency = _parameters.frequency;
      }
      if ("concurrency" in _parameters) {
        parameters.concurrency = _parameters.concurrency;
      }
    },
  } as Queue<returnType, taskType>;
};
</file>

<file path="packages/utils/src/getLogsRetryHelper.ts">
import {
  type Address,
  type Hex,
  type LogTopic,
  type RpcError,
  hexToBigInt,
  numberToHex,
} from "viem";
export type GetLogsRetryHelperParameters = {
  error: RpcError;
  params: [
    {
      address?: Address | Address[];
      topics?: LogTopic[];
      fromBlock: Hex;
      toBlock: Hex;
    },
  ];
};
export type GetLogsRetryHelperReturnType =
  | {
      shouldRetry: true;
      /** `true` if the error message suggested to use this range on retry. */
      isSuggestedRange: boolean;
      /** Suggested values to use for (fromBlock, toBlock) in follow-up eth_getLogs requests. */
      ranges: { fromBlock: Hex; toBlock: Hex }[];
    }
  | {
      shouldRetry: false;
      /** Suggested values to use for (fromBlock, toBlock) in follow-up eth_getLogs requests. */
      ranges?: never;
    };
export const getLogsRetryHelper = ({
  params,
  error,
}: GetLogsRetryHelperParameters): GetLogsRetryHelperReturnType => {
  const sError = JSON.stringify(error);
  let match: RegExpMatchArray | null;
  // avalanche
  match = sError.match(
    /requested too many blocks from (\d+) to (\d+), maximum is set to (\d+)/,
  );
  if (match !== null) {
    const ranges = chunk({ params, range: BigInt(match[3]!) - 1n });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // Cloudflare
  match = sError.match(/Max range: (\d+)/);
  if (match !== null) {
    const ranges = chunk({ params, range: BigInt(match[1]!) - 1n });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // thirdweb
  match = sError.match(/Maximum allowed number of requested blocks is ([\d]+)/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // infura, zksync
  match = sError.match(
    /Try with this block range \[0x([0-9a-fA-F]+),\s*0x([0-9a-fA-F]+)\]/,
  )!;
  if (match !== null) {
    const start = hexToBigInt(`0x${match[1]}`);
    const end = hexToBigInt(`0x${match[2]}`);
    const range = end - start;
    const ranges = chunk({ params, range });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // ankr
  match = sError.match("block range is too wide");
  if (match !== null && error.code === -32600) {
    const ranges = chunk({ params, range: 3000n });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // alchemy
  match = sError.match(
    /this block range should work: \[0x([0-9a-fA-F]+),\s*0x([0-9a-fA-F]+)\]/,
  );
  if (match !== null) {
    const start = hexToBigInt(`0x${match[1]}`);
    const end = hexToBigInt(`0x${match[2]}`);
    const range = end - start;
    const ranges = chunk({ params, range });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // quicknode, 1rpc, blast
  match = sError.match(/limited to a ([\d,.]+)/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!.replace(/[,.]/g, "")),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // blockpi
  match = sError.match(/limited to ([\d,.]+) block/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!.replace(/[,.]/g, "")),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // blast (paid)
  match = sError.match(
    /exceeds the range allowed for your plan \(\d+ > (\d+)\)/,
  );
  if (match !== null) {
    const ranges = chunk({ params, range: BigInt(match[1]!) });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // chainstack
  match = sError.match(/Block range limit exceeded./);
  if (match !== null) {
    const prevRange =
      hexToBigInt(params[0].toBlock) - hexToBigInt(params[0].fromBlock);
    // chainstack has different limits for free and paid plans.
    const ranges =
      prevRange < 10_000n
        ? chunk({ params, range: 100n })
        : chunk({ params, range: 10_000n });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // coinbase
  match = sError.match(/please limit the query to at most ([\d,.]+) blocks/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!.replace(/[,.]/g, "")) - 1n,
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // publicnode
  match = sError.match(/maximum block range: ([\d,.]+)/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!.replace(/[,.]/g, "")),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // hyperliquid
  match = sError.match(/query exceeds max block range ([\d,.]+)/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!.replace(/[,.]/g, "")),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // swell
  match = sError.match(/block range greater than ([\d,.]+) max/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!.replace(/[,.]/g, "")),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // somnia
  match = sError.match(/block range exceeds ([\d,.]+)/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!.replace(/[,.]/g, "")),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // merkle 10k
  match = sError.match(/eth_getLogs range is too large, max is 10k blocks/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: 10_000n,
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // merkle 1k
  match = sError.match(/eth_getLogs range is too large, max is 1k blocks/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: 1_000n,
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // harmony
  match = sError.match(/query must be smaller than size ([\d,.]+)/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!.replace(/[,.]/g, "")),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // moonriver
  match = sError.match(/block range is too wide \(maximum (\d+)\)/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!.replace(/[,.]/g, "")),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // aurora
  match = sError.match(/up to a ([\d,.]+) block range/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!.replace(/[,.]/g, "")),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // ankr (tac)
  match = sError.match(/maximum \[from, to\] blocks distance: (\d+)/);
  if (match !== null) {
    const ranges = chunk({
      params,
      range: BigInt(match[1]!),
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      } as const;
    }
  }
  // tron
  match = sError.match(/exceed max block range: (\d+)/);
  if (match !== null) {
    const ranges = chunk({ params, range: BigInt(match[1]!) - 1n });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        shouldRetry: true,
        ranges,
        isSuggestedRange: true,
      };
    }
  }
  // catch-all
  if (
    // valtitude
    sError.includes("allowed block range threshold exceeded") ||
    // erpc
    sError.includes("exceeded max allowed") ||
    // erpc
    sError.includes("range threshold exceeded") ||
    // base
    sError.includes("no backend is currently healthy to serve traffic") ||
    // base, monad
    sError.includes("block range too large") ||
    // optimism
    sError.includes("Block range is too large") ||
    // optimism
    sError.includes("backend response too large") ||
    // llamarpc, ankr, altitude
    sError.includes("query exceeds max results") ||
    // arbitrum
    /logs matched by query exceeds limit of \d+/.test(sError) ||
    // zkevm
    /query returned more than \d+ results/.test(sError) ||
    // 1rpc
    /response size should not greater than \d+ bytes/.test(sError) ||
    // drpc
    /ranges over \d+ blocks are not supported on freetier/.test(sError)
  ) {
    const ranges = chunk({
      params,
      range:
        (hexToBigInt(params[0].toBlock) - hexToBigInt(params[0].fromBlock)) /
        2n,
    });
    if (isRangeUnchanged(params, ranges) === false) {
      return {
        ranges,
        shouldRetry: true,
        isSuggestedRange: false,
      } as const;
    }
  }
  // No match found
  return { shouldRetry: false } as const;
};
const isRangeUnchanged = (
  params: GetLogsRetryHelperParameters["params"],
  ranges: Extract<
    GetLogsRetryHelperReturnType,
    { shouldRetry: true }
  >["ranges"],
) => {
  return (
    ranges.length === 0 ||
    (ranges.length === 1 &&
      ranges[0]!.fromBlock === params[0].fromBlock &&
      ranges[0]!.toBlock === params[0].toBlock)
  );
};
const chunk = ({
  params,
  range,
}: { params: GetLogsRetryHelperParameters["params"]; range: bigint }) => {
  const ranges: { fromBlock: Hex; toBlock: Hex }[] = [];
  const fromBlock = hexToBigInt(params[0].fromBlock);
  const toBlock = hexToBigInt(params[0].toBlock);
  for (let start = fromBlock; start <= toBlock; start += range + 1n) {
    const end = start + range > toBlock ? toBlock : start + range;
    ranges.push({
      fromBlock: numberToHex(start),
      toBlock: numberToHex(end),
    });
  }
  return ranges;
};
</file>

<file path="packages/utils/src/index.ts">
export {
  type GetLogsRetryHelperParameters,
  type GetLogsRetryHelperReturnType,
  getLogsRetryHelper,
} from "./getLogsRetryHelper.js";
export { type MergeAbis, mergeAbis } from "./mergeAbis.js";
export { loadBalance } from "./loadBalance.js";
export { rateLimit } from "./rateLimit.js";
export { type ReplaceBigInts, replaceBigInts } from "./replaceBigInts.js";
</file>

<file path="packages/utils/src/loadBalance.test.ts">
import type { Transport } from "viem";
import { expect, test, vi } from "vitest";
import { loadBalance } from "./loadBalance.js";
const createMockTransport = () => {
  const request = vi.fn(() => Promise.resolve("hi"));
  const mockTransport = (() => ({
    request,
  })) as unknown as Transport;
  return { request, mockTransport };
};
test("sends a request", async () => {
  const { request, mockTransport } = createMockTransport();
  const transport = loadBalance([mockTransport])({});
  await transport.request({ method: "eth_chainId" });
  expect(request).toHaveBeenCalledTimes(1);
});
test("splits requests between transports", async () => {
  const mock1 = createMockTransport();
  const mock2 = createMockTransport();
  const transport = loadBalance([mock1.mockTransport, mock2.mockTransport])({});
  await transport.request({ method: "eth_chainId" });
  await transport.request({ method: "eth_chainId" });
  expect(mock1.request).toHaveBeenCalledTimes(1);
  expect(mock2.request).toHaveBeenCalledTimes(1);
});
</file>

<file path="packages/utils/src/loadBalance.ts">
import { type Transport, type TransportConfig, createTransport } from "viem";
/**
 * @description Creates a load balanced transport that spreads requests between child transports using a round robin algorithm.
 */
export const loadBalance = (_transports: Transport[]): Transport => {
  return ({ chain, retryCount, timeout }) => {
    const transports = _transports.map((t) =>
      chain === undefined
        ? t({ retryCount: 0, timeout })
        : t({ chain, retryCount: 0, timeout }),
    );
    let index = 0;
    return createTransport({
      key: "loadBalance",
      name: "Load Balance",
      request: (body) => {
        const response = transports[index++]!.request(body);
        if (index === transports.length) index = 0;
        return response;
      },
      retryCount,
      timeout,
      type: "loadBalance",
    } as TransportConfig);
  };
};
</file>

<file path="packages/utils/src/mergeAbis.test.ts">
import { parseAbi, parseAbiItem } from "abitype";
import { expect, expectTypeOf, test } from "vitest";
import { mergeAbis } from "./mergeAbis.js";
test("mergeAbis() removes constructors, receive, fallback", () => {
  const abi = parseAbi([
    "constructor()",
    "fallback() external",
    "receive() external payable",
  ]);
  const merged = mergeAbis([abi]);
  //    ^?
  const out = [] as const;
  expect(merged.length).toBe(0);
  expect(merged).toMatchObject(out);
  expectTypeOf<typeof out>(merged);
});
test("mergeAbis() duplicate items", () => {
  const abi = parseAbiItem("function a()");
  const merged = mergeAbis([[abi], [abi]]);
  //    ^?
  const out = [abi] as const;
  expect(merged.length).toBe(1);
  expect(merged).toMatchObject(out);
  expectTypeOf<typeof out>(merged);
});
test("mergeAbis() overloaded items", () => {
  const one = parseAbiItem("function a()");
  const two = parseAbiItem("function a(bytes32)");
  const merged = mergeAbis([[one], [two]]);
  //    ^?
  const out = [one, two] as const;
  expect(merged.length).toBe(2);
  expect(merged).toMatchObject(out);
  expectTypeOf<typeof out>(merged);
});
test("mergeAbis() empty abi", () => {
  const abi = parseAbiItem("function a()");
  const a = mergeAbis([[abi], []]);
  expect(a).toMatchObject([abi] as const);
  expectTypeOf<readonly [typeof abi]>(a);
});
</file>

<file path="packages/utils/src/mergeAbis.ts">
import type { Abi, AbiItem } from "viem";
import { formatAbiItem } from "viem/utils";
type MergeAbi<
  TBase extends Abi,
  TInsert extends Abi,
> = TInsert extends readonly [
  infer First extends AbiItem,
  ...infer Rest extends Abi,
]
  ? Extract<TBase[number], First> extends never
    ? First["type"] extends "constructor" | "receive" | "fallback"
      ? MergeAbi<TBase, Rest>
      : MergeAbi<readonly [...TBase, First], Rest>
    : MergeAbi<TBase, Rest>
  : TBase;
export type MergeAbis<
  TAbis extends readonly Abi[],
  TMerged extends Abi = [],
> = TAbis extends readonly [
  infer First extends Abi,
  ...infer Rest extends readonly Abi[],
]
  ? MergeAbis<Rest, MergeAbi<TMerged, First>>
  : TMerged;
const isAbiItemEqual = (a: AbiItem, b: AbiItem): boolean =>
  formatAbiItem(a) === formatAbiItem(b);
/**
 * Combine multiple ABIs into one, removing duplicates if necessary.
 */
export const mergeAbis = <const TAbis extends readonly Abi[]>(abis: TAbis) => {
  let merged: Abi = [];
  for (const abi of abis) {
    for (const item of abi) {
      // Don't add a duplicate items
      // if item is constructor, receive, fallback, or already in merged, don't add it
      if (
        item.type !== "constructor" &&
        item.type !== "receive" &&
        item.type !== "fallback" &&
        !merged.some((m) => isAbiItemEqual(m, item))
      ) {
        merged = [...merged, item];
      }
    }
  }
  return merged as MergeAbis<TAbis>;
};
</file>

<file path="packages/utils/src/rateLimit.test.ts">
import type { Transport } from "viem";
import { expect, test, vi } from "vitest";
import { rateLimit } from "./rateLimit.js";
const createMockTransport = () => {
  const request = vi.fn(() => Promise.resolve("hi"));
  const mockTransport = (() => ({
    request,
  })) as unknown as Transport;
  return { request, mockTransport };
};
test("sends a request", async () => {
  const { request, mockTransport } = createMockTransport();
  const transport = rateLimit(mockTransport, {
    requestsPerSecond: 1,
    browser: false,
  })({});
  const response = transport.request({ method: "eth_chainId" });
  expect(request).toHaveBeenCalledTimes(1);
  await response;
});
test("limits request rate", async () => {
  const mock = createMockTransport();
  const transport = rateLimit(mock.mockTransport, {
    requestsPerSecond: 1,
    browser: false,
  })({});
  const response1 = transport.request({ method: "eth_chainId" });
  const response2 = transport.request({ method: "eth_chainId" });
  expect(mock.request).toHaveBeenCalledTimes(1);
  await response1;
  expect(mock.request).toHaveBeenCalledTimes(1);
  await response2;
  expect(mock.request).toHaveBeenCalledTimes(2);
});
</file>

<file path="packages/utils/src/rateLimit.ts">
import { type Transport, type TransportConfig, createTransport } from "viem";
import { createQueue } from "./utils/queue.js";
/**
 * @description Creates a rate limited transport that throttles request throughput.
 */
export const rateLimit = (
  _transport: Transport,
  {
    requestsPerSecond,
    browser = true,
  }: { requestsPerSecond: number; browser?: boolean },
): Transport => {
  return ({ chain, retryCount, timeout }) => {
    const transport =
      chain === undefined
        ? _transport({ retryCount: 0, timeout })
        : _transport({ chain, retryCount: 0, timeout });
    const queue = createQueue({
      frequency: requestsPerSecond,
      concurrency: Math.ceil(requestsPerSecond / 4),
      initialStart: true,
      browser,
      worker: (body: {
        method: string;
        params?: unknown;
      }) => {
        return transport.request(body);
      },
    });
    return createTransport({
      key: "rateLimit",
      name: "Rate Limit",
      request: (body) => {
        return queue.add(body);
      },
      retryCount,
      type: "rateLimit",
    } as TransportConfig);
  };
};
</file>

<file path="packages/utils/src/replaceBigInts.test.ts">
import { type Hex, numberToHex } from "viem";
import { expect, expectTypeOf, test } from "vitest";
import { replaceBigInts } from "./replaceBigInts.js";
test("scalar", () => {
  const out = replaceBigInts(5n, numberToHex);
  expect(out).toBe("0x5");
  expectTypeOf<Hex>(out);
});
test("array", () => {
  const out = replaceBigInts([5n], numberToHex);
  expect(out).toStrictEqual(["0x5"]);
  expectTypeOf<readonly [Hex]>(out);
});
test("readonly array", () => {
  const out = replaceBigInts([5n] as const, numberToHex);
  expect(out).toStrictEqual(["0x5"]);
  expectTypeOf<readonly [Hex]>(out);
});
test("object", () => {
  const out = replaceBigInts({ kevin: { kevin: 5n } }, numberToHex);
  expect(out).toStrictEqual({ kevin: { kevin: "0x5" } });
  expectTypeOf<{ kevin: { kevin: Hex } }>(out);
});
</file>

<file path="packages/utils/src/replaceBigInts.ts">
type _ReplaceBigInts<
  arr extends readonly unknown[],
  type,
  result extends readonly unknown[] = [],
> = arr extends [infer first, ...infer rest]
  ? _ReplaceBigInts<
      rest,
      type,
      readonly [...result, first extends bigint ? type : first]
    >
  : result;
export type ReplaceBigInts<obj, type> = obj extends bigint
  ? type
  : obj extends unknown[]
    ? _ReplaceBigInts<Readonly<obj>, type>
    : obj extends readonly []
      ? _ReplaceBigInts<obj, type>
      : obj extends object
        ? { [key in keyof obj]: ReplaceBigInts<obj[key], type> }
        : obj;
export const replaceBigInts = <const T, const type>(
  obj: T,
  replacer: (x: bigint) => type,
): ReplaceBigInts<T, type> => {
  if (typeof obj === "bigint") return replacer(obj) as ReplaceBigInts<T, type>;
  if (Array.isArray(obj))
    return obj.map((x) => replaceBigInts(x, replacer)) as ReplaceBigInts<
      T,
      type
    >;
  if (obj && typeof obj === "object")
    return Object.fromEntries(
      Object.entries(obj).map(([k, v]) => [k, replaceBigInts(v, replacer)]),
    ) as ReplaceBigInts<T, type>;
  return obj as ReplaceBigInts<T, type>;
};
</file>

<file path="packages/utils/.env.example">
# Mainnet alchemy url
RPC_URL_ALCHEMY_1=...

# Mainnet infura url
RPC_URL_INFURA_1=...

# Mainnet quicknode url
RPC_URL_QUICKNODE_1=...

# Mainnet chainstack url
RPC_URL_CHAINSTACK_1=...

# Coinbase url
RPC_URL_COINBASE_8453=...

# Altitude url
RPC_URL_ALTITUDE_999=...
</file>

<file path="packages/utils/.gitignore">
node_modules
dist
.env.local
</file>

<file path="packages/utils/bunfig.toml">
[test]
preload = ["./src/_test/globalSetup.ts"]
timeout = 15000
</file>

<file path="packages/utils/package.json">
{
  "name": "@ponder/utils",
  "version": "0.2.17",
  "description": "",
  "license": "MIT",
  "type": "module",
  "sideEffects": false,
  "main": "./dist/index.cjs",
  "module": "./dist/index.js",
  "types": "./dist/index.d.ts",
  "typings": "./dist/index.d.ts",
  "files": [
    "dist",
    "!dist/**/*.tsbuildinfo",
    "src/**/*.ts",
    "!src/**/*.test.ts",
    "!src/**/*.bench.ts",
    "!src/**/_test/**/*"
  ],
  "repository": {
    "type": "git",
    "url": "https://github.com/ponder-sh/ponder",
    "directory": "packages/utils"
  },
  "exports": {
    ".": {
      "types": "./dist/index.d.ts",
      "import": "./dist/index.js",
      "default": "./dist/index.cjs"
    }
  },
  "scripts": {
    "build": "tsup",
    "test": "vitest",
    "test:bun": "bun test",
    "typecheck": "tsc --noEmit"
  },
  "peerDependencies": {
    "typescript": ">=5.0.4",
    "viem": ">=2"
  },
  "peerDependenciesMeta": {
    "typescript": {
      "optional": true
    }
  },
  "devDependencies": {
    "@types/node": "^20.11.24",
    "abitype": "^1.0.1",
    "dotenv": "^16.3.1",
    "tsup": "^8.0.1",
    "vitest": "1.6.1"
  }
}
</file>

<file path="packages/utils/README.md">
# @ponder/utils

A collection of utilities helpful for building Ponder apps.

Please see the [documentation](https://ponder.sh/docs/api-reference/ponder-utils) for more information.
</file>

<file path="packages/utils/tsconfig.json">
{
  // Adapted from viem (https://github.com/wagmi-dev/viem/blob/ed779e9d5667704fd7....base.json).
  "include": ["src", "src/getLogsRetryHelper.ts"],
  "compilerOptions": {
    // Type checking
    "strict": true,
    "useDefineForClassFields": true,
    "noFallthroughCasesInSwitch": true,
    "noImplicitReturns": true,
    "useUnknownInCatchVariables": true,
    "noImplicitOverride": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noUncheckedIndexedAccess": true,

    // JavaScript support
    "allowJs": false,
    "checkJs": false,
    "jsx": "react",

    // Interop constraints
    "verbatimModuleSyntax": true,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "resolveJsonModule": true,

    // Language and environment
    "moduleResolution": "NodeNext",
    "module": "NodeNext",
    "target": "ESNext",
    "lib": [
      "ES2022" // By using ES2022 we get access to the `.cause` property on `Error` instances.
    ],

    // Skip type checking for node modules
    "skipLibCheck": true
  }
}
</file>

<file path="packages/utils/tsup.config.ts">
import { defineConfig } from "tsup";
export default defineConfig({
  name: "@ponder/utils",
  entry: ["src/index.ts"],
  outDir: "dist",
  format: ["esm", "cjs"],
  sourcemap: true,
  dts: true,
  clean: true,
  splitting: true,
});
</file>

<file path="packages/utils/vite.config.ts">
import path from "node:path";
import { defineConfig } from "vitest/config";
export default defineConfig({
  test: {
    globalSetup: ["src/_test/globalSetup.ts"],
  },
});
</file>

<file path=".gitignore">
node_modules
*debug.log

.vercel
.DS_Store
tmp

# IDE
.idea
</file>

<file path=".npmrc">
auto-install-peers=false
enable-pre-post-scripts=true
link-workspace-packages=deep
strict-peer-dependencies=false
provenance=true
</file>

<file path="biome.json">
{
  "$schema": "https://biomejs.dev/schemas/1.4.1/schema.json",
  "organizeImports": {
    "enabled": true
  },
  "files": {
    "ignore": [
      "**/node_modules",
      "CHANGELOG.md",
      "generated.ts",
      "**/dist",
      "**/.next",
      "**/generated",
      "**/cache",
      "**/out",
      "**/.ponder",
      "docs/",
      "benchmarks/apps/subgraph-*",
      "pnpm-lock.yaml",
      "**/*.sol",
      "tsconfig.base.json"
    ]
  },
  "formatter": {
    "enabled": true,
    "formatWithErrors": true,
    "indentStyle": "space",
    "indentWidth": 2,
    "lineWidth": 80
  },
  "linter": {
    "enabled": true,
    "rules": {
      "recommended": true,
      "style": {
        "noNonNullAssertion": "off",
        "noUselessElse": "off",
        "noParameterAssign": "off"
      },
      "suspicious": {
        "noExplicitAny": "off",
        "noAssignInExpressions": "off",
        "noConfusingVoidType": "off"
      },
      "complexity": {
        "noForEach": "off",
        "noBannedTypes": "off"
      },
      "performance": {
        "noAccumulatingSpread": "off"
      }
    }
  }
}
</file>

<file path="funding.json">
{
  "opRetro": {
    "projectId": "0x351967474a454b494260a488f3ceb77d993580a4fe79fb6b6d132c70634bc516"
  }
}
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2023 Cantrip, Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="package.json">
{
  "private": true,
  "scripts": {
    "bench:ponder:ci": "pnpm --filter \"./benchmarks\" bench:ponder:ci",
    "bench:subgraph:ci": "pnpm --filter \"./benchmarks\" bench:subgraph:ci",
    "build": "pnpm --filter \"./packages/**\" --recursive build",
    "changeset:version": "changeset version && pnpm install --lockfile-only",
    "changeset:release": "pnpm build && changeset publish",
    "format": "biome format . --write",
    "lint": "biome check .",
    "lint:fix": "pnpm lint --write",
    "prepare": "npx simple-git-hooks",
    "test": "pnpm --parallel --no-bail test",
    "test:bun": "pnpm --parallel --no-bail test:bun",
    "typecheck": "pnpm --filter \"./packages/**\" --recursive typecheck"
  },
  "devDependencies": {
    "@biomejs/biome": "^1.9.4",
    "@changesets/changelog-github": "^0.5.1",
    "@changesets/cli": "^2.29.4",
    "drizzle-kit": "^0.30.6",
    "hono": "4.5.0",
    "lint-staged": "^15.1.0",
    "simple-git-hooks": "^2.9.0",
    "typescript": "5.0.4",
    "viem": "2.30.1"
  },
  "lint-staged": {
    "*.ts": [
      "biome format --no-errors-on-unmatched --write",
      "biome check --no-errors-on-unmatched"
    ],
    "!(*.ts)": ["biome format --no-errors-on-unmatched --write"]
  },
  "simple-git-hooks": {
    "pre-commit": "npx lint-staged"
  },
  "packageManager": "pnpm@9.10.0",
  "pnpm": {
    "peerDependencyRules": {
      "ignoreMissing": ["node-fetch"]
    }
  },
  "engines": {
    "node": ">=18.14"
  }
}
</file>

<file path="pnpm-workspace.yaml">
packages:
  - benchmark
  - docs
  - simulation-test
  - examples/**
  - packages/*
  - tmp/*
</file>

<file path="README.md">
# Ponder

[![CI status][ci-badge]][ci-url]
[![Version][version-badge]][version-url]
[![Telegram chat][tg-badge]][tg-url]
[![License][license-badge]][license-url]

Ponder is an open-source framework for blockchain application backends.

## Documentation

Visit [ponder.sh](https://ponder.sh) for documentation, guides, and the API reference.

## Support

Join [Ponder's telegram chat](https://t.me/pondersh) for support, feedback, and general chatter.

## Features

 &nbsp;Local development server with hot reloading<br/>
 &nbsp;`create-ponder` CLI tool to get started from an Etherscan link or Graph Protocol subgraph<br/>
 &nbsp;End-to-end type safety using [viem](https://viem.sh) and [ABIType](https://github.com/wagmi-dev/abitype)<br/>
 &nbsp;Autogenerated GraphQL API<br/>
 &nbsp;Easy to deploy anywhere using Node.js/Docker<br/>
 &nbsp;Supports all Ethereum-based blockchains, including test nodes like [Anvil](https://book.getfoundry.sh/anvil)<br/>
 &nbsp;Index events from multiple chains in the same app<br/>
 &nbsp;Reconciles chain reorganization<br/>
 &nbsp;Factory contracts<br/>
 &nbsp;Process transactions calls (in addition to logs)<br/>
 &nbsp;Run effects (e.g. send an API request) in indexing code<br/>

## Quickstart

### 1. Run `create-ponder`

You will be asked for a project name, and if you are using a [template](https://ponder.sh/docs/api-reference/create-ponder#templates) (recommended). Then, the CLI will create a project directory, install dependencies, and initialize a git repository.

```bash
npm init ponder@latest
# or
pnpm create ponder
# or
yarn create ponder
```

### 2. Start the development server

Just like Next.js and Vite, Ponder has a development server that automatically reloads when you save changes in any project file. It also prints `console.log` statements and errors encountered while running your code. First, `cd` into your project directory, then start the server.

```bash
npm run dev
# or
pnpm dev
# or
yarn dev
```

### 3. Add contracts & chains

Ponder fetches event logs for the contracts added to `ponder.config.ts`, and passes those events to the indexing functions you write.

```ts
// ponder.config.ts

import { createConfig } from "ponder";
import { BaseRegistrarAbi } from "./abis/BaseRegistrar";
 
export default createConfig({
  chains: {
    mainnet: { 
      id: 1,
      rpc: "https://eth-mainnet.g.alchemy.com/v2/...",
    },
  },
  contracts: {
    BaseRegistrar: {
      abi: BaseRegistrarAbi,
      chain: "mainnet",
      address: "0x57f1887a8BF19b14fC0dF6Fd9B2acc9Af147eA85",
      startBlock: 9380410,
    },
  },
});
```

### 4. Define your schema

The `ponder.schema.ts` file contains the database schema, and defines the shape data that the GraphQL API serves.

```ts
// ponder.schema.ts

import { onchainTable } from "ponder";

export const ensName = onchainTable("ens_name", (t) => ({
  name: p.text().primaryKey(),
  owner: p.text().notNull(),
  registeredAt: p.integer().notNull(),
}));
```

### 5. Write indexing functions

Files in the `src/` directory contain **indexing functions**, which are TypeScript functions that process a contract event. The purpose of these functions is to insert data into the entity store.

```ts
// src/BaseRegistrar.ts

import { ponder } from "ponder:registry";
import schema from "ponder:schema";

ponder.on("BaseRegistrar:NameRegistered", async ({ event, context }) => {
  const { name, owner } = event.params;

  await context.db.insert(schema.ensName).values({
    name: name,
    owner: owner,
    registeredAt: event.block.timestamp,
  });
});
```

See the [create & update records](https://ponder.sh/docs/indexing/write) docs for a detailed guide on writing indexing functions.

### 6. Query the GraphQL API

Ponder automatically generates a frontend-ready GraphQL API based on your `ponder.schema.ts` file. The API serves data that you inserted in your indexing functions.

```ts
{
  ensNames(limit: 2) {
    items {
      name
      owner
      registeredAt
    }
  }
}
```

```json
{
  "ensNames": {
    "items": [
      {
        "name": "vitalik.eth",
        "owner": "0x0904Dac3347eA47d208F3Fd67402D039a3b99859",
        "registeredAt": 1580345271
      },
      {
        "name": "joe.eth",
        "owner": "0x6109DD117AA5486605FC85e040ab00163a75c662",
        "registeredAt": 1580754710
      }
    ]
  }
}
```

That's it! Visit [ponder.sh](https://ponder.sh) for documentation, guides for deploying to production, and the API reference.

## Contributing

If you're interested in contributing to Ponder, please read the [contribution guide](/.github/CONTRIBUTING.md).

## Packages

- `ponder`
- `@ponder/client`
- `@ponder/react`
- `@ponder/utils`
- `create-ponder`
- `eslint-config-ponder`

## About

Ponder is MIT-licensed open-source software.

[ci-badge]: https://github.com/ponder-sh/ponder/actions/workflows/main.yml/badge.svg
[ci-url]: https://github.com/ponder-sh/ponder/actions/workflows/main.yml
[tg-badge]: https://img.shields.io/endpoint?color=neon&logo=telegram&label=chat&url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Fpondersh
[tg-url]: https://t.me/pondersh
[license-badge]: https://img.shields.io/npm/l/ponder?label=License
[license-url]: https://github.com/ponder-sh/ponder/blob/main/LICENSE
[version-badge]: https://img.shields.io/npm/v/ponder
[version-url]: https://github.com/ponder-sh/ponder/releases
</file>

<file path="tsconfig.base.json">
{
  // This tsconfig file contains the shared config for the build (tsconfig.build.json) and type checking (tsconfig.json) config.
  // Adapted from viem (https://github.com/wagmi-dev/viem/blob/ed779e9d5667704fd7....base.json).
  "include": [],
  "compilerOptions": {
    // Incremental builds
    // NOTE: Enabling incremental builds speeds up `tsc`. Keep in mind though that it does not reliably bust the cache when the `tsconfig.json` file changes.
    "incremental": true,

    // Type checking
    "strict": true,
    // "exactOptionalPropertyTypes": true,
    "noFallthroughCasesInSwitch": true, // Not enabled by default in `strict` mode.
    "noImplicitOverride": true, // Not enabled by default in `strict` mode.
    "noImplicitReturns": true, // Not enabled by default in `strict` mode.
    "noUncheckedIndexedAccess": true,
    "noUnusedLocals": true, // Not enabled by default in `strict` mode.
    "noUnusedParameters": true, // Not enabled by default in `strict` mode.
    "useDefineForClassFields": true, // Not enabled by default in `strict` mode unless we bump `target` to ES2022.
    "useUnknownInCatchVariables": true,
    "forceConsistentCasingInFileNames": true,

    // JavaScript support
    "allowJs": false,
    "checkJs": false,

    // Interop constraints
    "verbatimModuleSyntax": true,
    "resolveJsonModule": true,

    // Language and environment
    "moduleResolution": "NodeNext",
    "module": "NodeNext",
    "target": "ES2021", // Setting this to `ES2021` enables native support for `Node v16+`: https://github.com/microsoft/TypeScript/wiki/Node-Target-Mapping.
    "lib": [
      "ES2022", // By using ES2022 we get access to the `.cause` property on `Error` instances.
      "DOM" // We are adding `DOM` here to get the `fetch`, etc. types. This should be removed once these types are available via DefinitelyTyped.
    ],

    // Skip type checking for node modules
    "skipLibCheck": true,
    "noErrorTruncation": true
  }
}
</file>

</files>
