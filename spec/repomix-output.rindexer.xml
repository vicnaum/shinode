This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/node_modules/**, **/dist/**, **/build/**, **/target/**, **/*.png, **/*.jpg, **/*.svg, **/*.min.js, **/*.map
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Long base64 data strings (e.g., data:image/png;base64,...) have been truncated to reduce token count
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cargo/
  config.toml
.github/
  workflows/
    ci-cd.yml
    docker.yml
    edit-releases.yml
    migrate-releases.yml
cli/
  src/
    commands/
      add.rs
      codegen.rs
      delete.rs
      mod.rs
      new.rs
      phantom.rs
      start.rs
    cli_interface.rs
    console.rs
    main.rs
    rindexer_yaml.rs
  Cargo.toml
  Makefile
  README.md
core/
  resources/
    blockclock/
      1.blockclock
      10.blockclock
      137.blockclock
      324.blockclock
      42161.blockclock
      43114.blockclock
      534352.blockclock
      56.blockclock
  src/
    api/
      generate_operations.rs
      generate_schema.rs
      graphql.rs
      mod.rs
    blockclock/
      fetcher.rs
      fixed.rs
      mod.rs
      runlencoder.rs
    chat/
      clients.rs
      discord.rs
      mod.rs
      slack.rs
      telegram.rs
      template.rs
    database/
      clickhouse/
        client.rs
        create_batch_db_operation.rs
        generate.rs
        mod.rs
        setup.rs
      postgres/
        client.rs
        create_batch_db_operation.rs
        generate.rs
        indexes.rs
        migrations.rs
        mod.rs
        relationship.rs
        setup.rs
      batch_operations.rs
      generate.rs
      mod.rs
      sql_type_wrapper.rs
    event/
      filter/
        ast.rs
        evaluation.rs
        helpers.rs
        mod.rs
        parsing.rs
      callback_registry.rs
      config.rs
      contract_setup.rs
      factory_event_filter_sync.rs
      message.rs
      mod.rs
      rindexer_event_filter.rs
    generator/
      build.rs
      context_bindings.rs
      database_bindings.rs
      docker.rs
      events_bindings.rs
      mod.rs
      networks_bindings.rs
      trace_bindings.rs
    helpers/
      array.rs
      duration.rs
      evm_log.rs
      file.rs
      mod.rs
      solidity.rs
      thread.rs
    indexer/
      dependency.rs
      fetch_logs.rs
      last_synced.rs
      mod.rs
      native_transfer.rs
      no_code.rs
      process.rs
      progress.rs
      reorg.rs
      start.rs
      task_tracker.rs
    manifest/
      chat.rs
      config.rs
      contract.rs
      core.rs
      global.rs
      graphql.rs
      mod.rs
      native_transfer.rs
      network.rs
      phantom.rs
      reth.rs
      storage.rs
      stream.rs
      yaml.rs
    phantom/
      common.rs
      dyrpc.rs
      mod.rs
      shadow.rs
    reth/
      exex.rs
      mod.rs
      node.rs
      utils.rs
    simple_file_formatters/
      csv.rs
      mod.rs
    streams/
      clients.rs
      cloudflare_queues.rs
      kafka.rs
      mod.rs
      rabbitmq.rs
      redis.rs
      sns.rs
      webhook.rs
    types/
      aws_config.rs
      code.rs
      core.rs
      mod.rs
      single_or_array.rs
    abi.rs
    events.rs
    health.rs
    layer_extensions.rs
    lib.rs
    logger.rs
    notifications.rs
    provider.rs
    start.rs
    system_state.rs
  build.rs
  Cargo.toml
  README.md
documentation/
  docs/
    pages/
      docs/
        accessing-data/
          direct-sql.mdx
          graphql.mdx
        advanced/
          using-reth-exex.md
        deploying/
          aws.mdx
          gcp.mdx
          railway.mdx
        introduction/
          installation.mdx
          other-indexing-tools.mdx
          what-is-rindexer.mdx
          why-rindexer.mdx
        references/
          cli.mdx
          rpc-node-providers.mdx
        start-building/
          chatbots/
            discord.mdx
            index.mdx
            slack.mdx
            telegram.mdx
          create-new-project/
            index.mdx
            reth-mode.mdx
            standard.mdx
          project-types/
            index.mdx
            no-code-project.mdx
            rust-project.mdx
          rust-project-deep-dive/
            ethers-alloy-migration.mdx
            index.mdx
            indexers.mdx
            typings.mdx
          streams/
            cloudflare-queues.mdx
            index.mdx
            kafka.mdx
            rabbitmq.mdx
            redis.mdx
            sns.mdx
            webhooks.mdx
          yaml-config/
            config.mdx
            contracts.mdx
            global.mdx
            graphql.mdx
            index.mdx
            native-transfers.mdx
            networks.mdx
            storage.mdx
            top-level-fields.mdx
          add.mdx
          codegen.mdx
          delete.mdx
          health-monitoring.mdx
          live-indexing-and-historic.mdx
          phantom.mdx
          running.mdx
          timestamps.mdx
        changelog.mdx
        shoutout.mdx
      index.mdx
    public/
      releases/
        resources.zip
      install.sh
  package.json
  README.md
  tsconfig.json
  vocs.config.tsx
examples/
  clickhouse_factory_indexing/
    abis/
      erc20-abi.json
      uniswap-v3-factory-abi.json
      uniswap-v3-pool-abi.json
    src/
      rindexer_lib/
        indexers/
          rindexer_factory_contract/
            mod.rs
            uniswap_v3_factory_pool_created_pool.rs
            uniswap_v3_factory_pool_created_token_0_token_1.rs
            uniswap_v3_factory.rs
            uniswap_v3_pool_token.rs
            uniswap_v3_pool.rs
          all_handlers.rs
          mod.rs
        typings/
          rindexer_factory_contract/
            events/
              mod.rs
              uniswap_v3_factory_abi_gen.rs
              uniswap_v3_factory_pool_created_pool_abi_gen.rs
              uniswap_v3_factory_pool_created_pool.rs
              uniswap_v3_factory_pool_created_token_0_token_1_abi_gen.rs
              uniswap_v3_factory_pool_created_token_0_token_1.rs
              uniswap_v3_factory.rs
              uniswap_v3_pool_abi_gen.rs
              uniswap_v3_pool_token_abi_gen.rs
              uniswap_v3_pool_token.rs
              uniswap_v3_pool.rs
            mod.rs
          database.rs
          mod.rs
          networks.rs
        mod.rs
      main.rs
    .env.example
    .gitignore
    Cargo.toml
    docker-compose.yml
    rindexer.yaml
  nocode_clickhouse/
    abis/
      RocketTokenRETH.abi.json
    .env.example
    .gitignore
    docker-compose.yml
    rindexer.yaml
  rindexer_demo_cli/
    abis/
      RocketTokenRETH.abi.json
    .gitignore
    docker-compose.yml
    rindexer.yaml
  rindexer_factory_indexing/
    abis/
      erc20-abi.json
      uniswap-v3-factory-abi.json
      uniswap-v3-pool-abi.json
    src/
      rindexer_lib/
        indexers/
          rindexer_factory_contract/
            mod.rs
            uniswap_v3_factory_pool_created_pool.rs
            uniswap_v3_factory_pool_created_token_0_token_1.rs
            uniswap_v3_factory.rs
            uniswap_v3_pool_token.rs
            uniswap_v3_pool.rs
          all_handlers.rs
          mod.rs
        typings/
          rindexer_factory_contract/
            events/
              mod.rs
              uniswap_v3_factory_abi_gen.rs
              uniswap_v3_factory_pool_created_pool_abi_gen.rs
              uniswap_v3_factory_pool_created_pool.rs
              uniswap_v3_factory_pool_created_token_0_token_1_abi_gen.rs
              uniswap_v3_factory_pool_created_token_0_token_1.rs
              uniswap_v3_factory.rs
              uniswap_v3_pool_abi_gen.rs
              uniswap_v3_pool_token_abi_gen.rs
              uniswap_v3_pool_token.rs
              uniswap_v3_pool.rs
            mod.rs
          database.rs
          mod.rs
          networks.rs
        mod.rs
      main.rs
    .gitignore
    Cargo.toml
    docker-compose.yml
    rindexer.yaml
  rindexer_native_transfers/
    abis/
      ERC20.abi.json
    rindexer.yaml
  rindexer_rust_playground/
    abis/
      erc20-abi.json
      erc721-abi.json
      lens-hub-events-abi.json
      lens-registry-events-abi.json
      playground-types-abi.json
      uniswap-v3-pool-abi.json
      world.abi.json
    src/
      rindexer_lib/
        indexers/
          rindexer_playground/
            erc_20_filter.rs
            mod.rs
            rocket_pool_eth.rs
            uniswap_v3_pool_filter.rs
          all_handlers.rs
          mod.rs
        typings/
          rindexer_playground/
            events/
              erc_20_filter_abi_gen.rs
              erc_20_filter.rs
              mod.rs
              rocket_pool_eth_abi_gen.rs
              rocket_pool_eth.rs
              uniswap_v3_pool_filter_abi_gen.rs
              uniswap_v3_pool_filter.rs
            mod.rs
          database.rs
          global_contracts.rs
          mod.rs
          networks.rs
        mod.rs
      main.rs
      mod.rs
    .gitignore
    Cargo.toml
    docker-compose.yml
    rindexer.yaml
  rust_clickhouse/
    abis/
      RocketTokenRETH.abi.json
    src/
      rindexer_lib/
        indexers/
          clickhouse_indexer/
            mod.rs
            rocket_pool.rs
          all_handlers.rs
          mod.rs
        typings/
          clickhouse_indexer/
            events/
              mod.rs
              rocket_pool_abi_gen.rs
              rocket_pool.rs
            mod.rs
          database.rs
          mod.rs
          networks.rs
        mod.rs
      main.rs
      mod.rs
    .env.example
    .gitignore
    Cargo.toml
    docker-compose.yml
    rindexer.yaml
graphql/
  index.js
  package.json
  README.md
helm/
  rindexer/
    files/
      abis/
        RocketTokenRETH.abi.json
    templates/
      _helpers.tpl
      configmap-abis.yaml
      configmap.yaml
      deployment.yaml
      ingress.yaml
      NOTES.txt
      service.yaml
      serviceaccount.yaml
    .helmignore
    Chart.yaml
    README.md
    values.yaml
providers/
  aws/
    .gitkeep
  railway/
    example-app/
      erc20.abi.json
      rindexer.yaml
    .dockerignore
    Dockerfile
    railway.toml
    README.md
xtask/
  src/
    main.rs
  Cargo.toml
.dockerignore
.gitattributes
.gitignore
Cargo.toml
docker-compose.yml
Dockerfile
funding.json
LICENSE
README.md
rust-toolchain.toml
rustfmt.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cargo/config.toml">
[alias]
xtask = "run --package xtask --"
</file>

<file path=".github/workflows/ci-cd.yml">
name: CI/CD
on:
  workflow_dispatch:
  push:
    branches:
      - "release/**"
      - "master"
  pull_request:
    branches:
      - "master"
      - "**"
    types: [opened, synchronize, reopened]
jobs:
  fmt:
    name: Rustfmt
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
      # Toolchain file bug workaround: https://github.com/dtolnay/rust-toolchain/issues/153
      - run: rustup component add rustfmt
      - name: Run Fmt Check
        run: cargo fmt --all -- --check
  clippy:
    name: Clippy
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
      - run: rustup component add clippy
      - name: Run clippy
        run: cargo clippy -- -D warnings -A clippy::uninlined_format_args
  build:
    if: |
      (github.event_name == 'push' || github.event_name == 'pull_request') &&
      github.actor != 'github-actions[bot]' &&
      (
        !(github.event_name == 'pull_request' &&
          startsWith(github.event.pull_request.head.ref, 'release/') &&
          github.event.pull_request.base.ref == 'master') ||
        (github.event_name == 'pull_request' &&
          (contains(github.event.pull_request.head.ref, '/merge') ||
           startsWith(github.event.pull_request.head.ref, 'merge')))
      ) &&
      !(github.ref == 'refs/heads/master' &&
        github.event_name == 'push' &&
        (contains(github.event.head_commit.message, 'Release v') ||
         contains(github.event.head_commit.message, 'release/')))
    name: ${{ matrix.target }} (${{ matrix.runner }})
    runs-on: ${{ matrix.runner }}
    timeout-minutes: 240
    strategy:
      fail-fast: false
      matrix:
        include:
          - runner: ubuntu-22.04
            target: x86_64-unknown-linux-gnu
            svm_target_platform: linux-amd64
            platform: linux
            arch: amd64
          - runner: macos-15-intel
            target: x86_64-apple-darwin
            svm_target_platform: macosx-amd64
            platform: darwin
            arch: amd64
          - runner: macos-latest
            target: aarch64-apple-darwin
            svm_target_platform: macosx-aarch64
            platform: darwin
            arch: arm64
          - runner: windows-latest
            target: x86_64-pc-windows-msvc
            svm_target_platform: windows-amd64
            platform: win32
            arch: amd64
    env:
      BUILD_TYPE: release
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event_name == 'pull_request' && format('refs/pull/{0}/merge', github.event.pull_request.number) || github.ref_name }}
      - uses: dtolnay/rust-toolchain@stable
        with:
          targets: ${{ matrix.target }}
      - uses: Swatinem/rust-cache@v2
        with:
          key: ${{ matrix.target }}
          cache-on-failure: true
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
      - name: Apple M1 setup
        if: matrix.target == 'aarch64-apple-darwin'
        run: |
          echo "SDKROOT=$(xcrun -sdk macosx --show-sdk-path)" >> $GITHUB_ENV
          echo "MACOSX_DEPLOYMENT_TARGET=$(xcrun -sdk macosx --show-sdk-platform-version)" >> $GITHUB_ENV
      - name: Linux ARM setup
        if: matrix.target == 'aarch64-unknown-linux-gnu'
        run: |
          sudo apt-get update -y
          sudo apt-get install -y gcc-aarch64-linux-gnu
          echo "CARGO_TARGET_AARCH64_UNKNOWN_LINUX_GNU_LINKER=aarch64-linux-gnu-gcc" >> $GITHUB_ENV
      - name: Install MSVC target
        if: matrix.target == 'x86_64-pc-windows-msvc'
        run: rustup target add x86_64-pc-windows-msvc
      - name: Install OpenSSL development libraries
        if: matrix.target == 'aarch64-unknown-linux-gnu' || matrix.target == 'x86_64-unknown-linux-gnu'
        run: |
          sudo apt-get update -y
          sudo apt-get install -y libssl-dev pkg-config
      - name: Setup cross-compilation for pkg-config
        if: matrix.target == 'aarch64-unknown-linux-gnu'
        run: |
          echo "PKG_CONFIG_ALLOW_CROSS=1" >> $GITHUB_ENV
          echo "PKG_CONFIG_SYSROOT_DIR=/usr/aarch64-linux-gnu" >> $GITHUB_ENV
          echo "PKG_CONFIG_PATH=/usr/lib/aarch64-linux-gnu/pkgconfig" >> $GITHUB_ENV
          echo "PKG_CONFIG_LIBDIR=/usr/lib/aarch64-linux-gnu/pkgconfig" >> $GITHUB_ENV
      - name: Extract version name
        id: extract_version
        shell: bash
        run: echo "VERSION_NAME=${GITHUB_REF#refs/heads/release/}" >> $GITHUB_ENV
      - name: Install and setup NASM on Windows
        if: matrix.target == 'x86_64-pc-windows-msvc'
        shell: pwsh
        run: |
          choco install nasm
          $nasmPath = "C:\Program Files\NASM"
          $env:PATH += ";$nasmPath"
          echo "$nasmPath" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          echo "NASM_PATH=$nasmPath" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
          refreshenv
          nasm -v
      - name: Install vcpkg on Windows
        if: matrix.target == 'x86_64-pc-windows-msvc'
        shell: pwsh
        run: |
          git clone https://github.com/Microsoft/vcpkg.git C:\vcpkg
          C:\vcpkg\bootstrap-vcpkg.bat
          C:\vcpkg\vcpkg integrate install
          echo "VCPKG_ROOT=C:\vcpkg" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
          echo "C:\vcpkg" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
      - name: Install CMake on Windows
        if: matrix.target == 'x86_64-pc-windows-msvc'
        shell: pwsh
        run: |
          choco install cmake --version=3.20.0 --installargs 'ADD_CMAKE_TO_PATH=System'
          refreshenv
          cmake --version
          echo "CMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
      - name: Install dependencies with vcpkg on Windows
        if: matrix.target == 'x86_64-pc-windows-msvc'
        shell: pwsh
        run: |
          C:\vcpkg\vcpkg install openssl:x64-windows-static-md zlib:x64-windows-static-md
      - name: Build binaries
        working-directory: cli
        env:
          SVM_TARGET_PLATFORM: ${{ matrix.svm_target_platform }}
          CMAKE_TOOLCHAIN_FILE: C:/vcpkg/scripts/buildsystems/vcpkg.cmake
        shell: bash
        run: |
          set -eo pipefail
          target="${{ matrix.target }}"
          flags=()
          if [[ "$target" != *msvc* && "$target" != "aarch64-unknown-linux-gnu" ]]; then
            flags+=(--features jemalloc,reth)
          else
            flags+=(--features reth)
          fi
          [[ "$target" == *windows* ]] && exe=".exe"
          if [[ "$target" == *windows* ]]; then
            export PATH="$PATH:/c/Program Files/NASM:/c/vcpkg"
            export NASM="$NASM_PATH/nasm.exe"
            echo "NASM location: $NASM"
            "$NASM" -v
            echo "CMAKE_TOOLCHAIN_FILE: $CMAKE_TOOLCHAIN_FILE"
            ls -l "$CMAKE_TOOLCHAIN_FILE" || echo "Toolchain file not found!"
          fi
          if [[ "${{ env.BUILD_TYPE }}" == "release" ]]; then
            RUST_BACKTRACE=1 CMAKE_TOOLCHAIN_FILE="$CMAKE_TOOLCHAIN_FILE" cargo build --release --target "$target" "${flags[@]}" -vv
          else
            RUST_BACKTRACE=1 CMAKE_TOOLCHAIN_FILE="$CMAKE_TOOLCHAIN_FILE" cargo build --target "$target" "${flags[@]}" -vv
          fi
      - name: Smoke Test
        shell: bash
        run: |
          set -eo pipefail
          target="${{ matrix.target }}"
          build_type="${{ env.BUILD_TYPE }}"
          binary_path="${{ github.workspace }}/target/$target/$build_type/rindexer_cli"
          if [[ "$target" == *windows* ]]; then
            binary_path+=".exe"
          fi
          echo "Running smoke test for $binary_path"
          "$binary_path" --version
          "$binary_path" help
      - name: Archive binaries
        id: artifacts
        if: startsWith(github.ref, 'refs/heads/release/')
        env:
          PLATFORM_NAME: ${{ matrix.platform }}
          TARGET: ${{ matrix.target }}
          ARCH: ${{ matrix.arch }}
          VERSION_NAME: ${{ env.VERSION_NAME }}
        shell: bash
        run: |
          set -eo pipefail
          BUILD_DIR="${{ github.workspace }}/target/${TARGET}/${{ env.BUILD_TYPE }}"
          CLI_BINARY_NAME="rindexer_cli"
          [[ "$PLATFORM_NAME" == "win32" ]] && CLI_BINARY_NAME="rindexer_cli.exe"
          # Create a temporary staging directory for creating the archive
          STAGING_DIR="staging"
          mkdir -p "$STAGING_DIR"
          # Copy binaries to the staging directory
          echo "Copying $BUILD_DIR/$CLI_BINARY_NAME to $STAGING_DIR/"
          cp "$BUILD_DIR/$CLI_BINARY_NAME" "$STAGING_DIR/"
          # Create the final archive
          if [ "$PLATFORM_NAME" == "linux" ] || [ "$PLATFORM_NAME" == "darwin" ]; then
            FILE_NAME="rindexer_${PLATFORM_NAME}-${ARCH}.tar.gz"
            tar -czvf "$FILE_NAME" -C "$STAGING_DIR" .
          else
            FILE_NAME="rindexer_${PLATFORM_NAME}-${ARCH}.zip"
            (cd "$STAGING_DIR" && 7z a -tzip "${{ github.workspace }}/$FILE_NAME" .)
          fi
          echo "Created archive: $FILE_NAME"
          echo "file_name=$FILE_NAME" >> $GITHUB_OUTPUT
      - name: Run tests
        shell: bash
        run: |
          set -eo pipefail
          target="${{ matrix.target }}"
          flags=()
          if [[ "$target" != *msvc* && "$target" != "aarch64-unknown-linux-gnu" ]]; then
            flags+=(--features jemalloc)
          fi
          cargo test --exclude rindexer_rust_playground --workspace --release --target "$target" "${flags[@]}"
      - name: Run tests (reth feature)
        shell: bash
        run: |
          set -eo pipefail
          target="${{ matrix.target }}"
          flags=()
          if [[ "$target" != *msvc* && "$target" != "aarch64-unknown-linux-gnu" ]]; then
            flags+=(--features jemalloc,reth)
          else
            flags+=(--features reth)
          fi
          cargo test --exclude rindexer_rust_playground --workspace --release --target "$target" "${flags[@]}"
      - name: Upload artifact
        if: startsWith(github.ref, 'refs/heads/release/')
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.platform }}-${{ matrix.arch }}
          path: ${{ steps.artifacts.outputs.file_name }}
  create_pr:
    name: Create Release PR
    runs-on: ubuntu-22.04
    needs: build
    if: |
      github.actor != 'github-actions[bot]' &&
      startsWith(github.ref, 'refs/heads/release/')
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
      - name: Extract version from branch name
        shell: bash
        run: |
          VERSION=${GITHUB_REF#refs/heads/release/}
          echo "VERSION_NAME=$VERSION" >> $GITHUB_ENV
      - name: Update Cargo.toml versions
        shell: bash
        run: |
          sed -i 's/^version = ".*"/version = "${{ env.VERSION_NAME }}"/' cli/Cargo.toml
          sed -i 's/^version = ".*"/version = "${{ env.VERSION_NAME }}"/' core/Cargo.toml
      # - name: Update Changelog
      #   shell: bash
      #   run: |
      #     CHANGELOG_FILE="documentation/docs/pages/docs/changelog.mdx"
      #     DAY=$(date '+%d' | sed 's/^0*//')
      #     MONTH=$(date '+%B')
      #     YEAR=$(date '+%Y')
      #     # Add ordinal suffix
      #     case $DAY in
      #       1|21|31) SUFFIX="st";;
      #       2|22) SUFFIX="nd";;
      #       3|23) SUFFIX="rd";;
      #       *) SUFFIX="th";;
      #     esac
      #     DATE="$DAY$SUFFIX $MONTH $YEAR"
      #     echo "Updating changelog for version ${{ env.VERSION_NAME }}"
      #     # Create a temporary file to work with
      #     cp "$CHANGELOG_FILE" changelog_temp.mdx
      #     # Extract just the bug fixes line directly from Changes Not Deployed section
      #     BUG_FIXES=$(sed -n '/^## Changes Not Deployed/,/^## Releases/p' changelog_temp.mdx | grep "^- fix:" | head -5)
      #     # Use the simple extraction for now
      #     FEATURES=""
      #     BUG_FIXES="$BUG_FIXES"
      #     BREAKING_CHANGES=""
      #     # Save the extracted content for PR creation
      #     {
      #       echo "### Changes in this release:"
      #       echo "-------------------------------------------------"
      #       echo "### Features"
      #       echo "-------------------------------------------------"
      #       if [[ -n "$FEATURES" ]]; then
      #         echo "$FEATURES"
      #       fi
      #       echo ""
      #       echo "### Bug fixes"
      #       echo "-------------------------------------------------"
      #       if [[ -n "$BUG_FIXES" ]]; then
      #         echo "$BUG_FIXES"
      #       fi
      #       echo ""
      #       echo "### Breaking changes"
      #       echo "-------------------------------------------------"
      #       if [[ -n "$BREAKING_CHANGES" ]]; then
      #         echo "$BREAKING_CHANGES"
      #       fi
      #     } > pr_body_content.txt
      #     # Save to environment variable
      #     echo "PR_BODY_CONTENT<<EOF" >> $GITHUB_ENV
      #     cat pr_body_content.txt >> $GITHUB_ENV
      #     echo "EOF" >> $GITHUB_ENV
      #     # Create new release entry
      #     cat > new_release.txt << EOF
      #     # ${{ env.VERSION_NAME }}-beta - $DATE
      #     github branch - https://github.com/joshstevens19/rindexer/tree/release/${{ env.VERSION_NAME }}
      #     - linux binary - https://github.com/joshstevens19/rindexer/releases/download/v${{ env.VERSION_NAME }}/rindexer_linux-amd64.tar.gz
      #     - mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v${{ env.VERSION_NAME }}/rindexer_darwin-arm64.tar.gz
      #     - mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v${{ env.VERSION_NAME }}/rindexer_darwin-amd64.tar.gz
      #     - windows binary - https://github.com/joshstevens19/rindexer/releases/download/v${{ env.VERSION_NAME }}/rindexer_win32-amd64.zip
      #     EOF
      #     # Add sections if they have content
      #     if [[ -n "$FEATURES" && "$FEATURES" =~ [^[:space:]] ]]; then
      #       echo "" >> new_release.txt
      #       echo "### Features" >> new_release.txt
      #       echo "-------------------------------------------------" >> new_release.txt
      #       echo "$FEATURES" >> new_release.txt
      #     fi
      #     if [[ -n "$BUG_FIXES" && "$BUG_FIXES" =~ [^[:space:]] ]]; then
      #       echo "" >> new_release.txt
      #       echo "### Bug fixes" >> new_release.txt
      #       echo "-------------------------------------------------" >> new_release.txt
      #       echo "$BUG_FIXES" >> new_release.txt
      #     fi
      #     if [[ -n "$BREAKING_CHANGES" && "$BREAKING_CHANGES" =~ [^[:space:]] ]]; then
      #       echo "" >> new_release.txt
      #       echo "### Breaking changes" >> new_release.txt
      #       echo "-------------------------------------------------" >> new_release.txt
      #       echo "$BREAKING_CHANGES" >> new_release.txt
      #     fi
      #     # Get existing releases (everything after "## Releases")
      #     EXISTING_RELEASES=$(awk '/^## Releases/,0' changelog_temp.mdx | tail -n +5)
      #     # Create new changelog
      #     cat > "$CHANGELOG_FILE" << EOF
      #     # Changelog
      #     ## Changes Not Deployed
      #     -------------------------------------------------
      #     ### Features
      #     -------------------------------------------------
      #     ### Bug fixes
      #     -------------------------------------------------
      #     ### Breaking changes
      #     -------------------------------------------------
      #     ## Releases
      #     -------------------------------------------------
      #     all release branches are deployed through \`release/VERSION_NUMBER\` branches
      #     $(cat new_release.txt)
      #     $EXISTING_RELEASES
      #     EOF
      #     # Clean up temporary files
      #     rm -f changelog_temp.mdx new_release.txt pr_body_content.txt
      #     echo "Changelog updated successfully"
      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add cli/Cargo.toml core/Cargo.toml documentation/docs/pages/docs/changelog.mdx
          git commit -m "Release v${{ env.VERSION_NAME }}"
          git push origin release/${{ env.VERSION_NAME }}
      - name: Check if PR already exists
        id: check_pr
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_EXISTS=$(gh pr list --base master --head release/${{ env.VERSION_NAME }} --json number --jq length)
          if [ "$PR_EXISTS" -gt 0 ]; then
            echo "pr_exists=true" >> $GITHUB_OUTPUT
            echo "PR already exists, skipping creation"
          else
            echo "pr_exists=false" >> $GITHUB_OUTPUT
            echo "No PR exists, will create one"
          fi
      - name: Create Pull Request
        if: steps.check_pr.outputs.pr_exists == 'false'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh pr create \
            --title "Release v${{ env.VERSION_NAME }}" \
            --body "## Release v${{ env.VERSION_NAME }}
          This PR contains:
          - ✅ Version bump to ${{ env.VERSION_NAME }}
          - ✅ Changelog updated with release notes
          - ✅ Ready for release
          **Merging this PR will automatically create a GitHub Release with binaries.**
          ${{ env.PR_BODY_CONTENT }}" \
            --base master \
            --head release/${{ env.VERSION_NAME }}
  release_build:
    name: Build Release Binaries
    runs-on: ${{ matrix.runner }}
    if: |
      github.actor != 'github-actions[bot]' &&
      github.ref == 'refs/heads/master' &&
      github.event_name == 'push'
    timeout-minutes: 240
    strategy:
      fail-fast: false
      matrix:
        include:
          - runner: ubuntu-22.04
            target: x86_64-unknown-linux-gnu
            svm_target_platform: linux-amd64
            platform: linux
            arch: amd64
          - runner: macos-15-intel
            target: x86_64-apple-darwin
            svm_target_platform: macosx-amd64
            platform: darwin
            arch: amd64
          - runner: macos-latest
            target: aarch64-apple-darwin
            svm_target_platform: macosx-aarch64
            platform: darwin
            arch: arm64
          - runner: windows-latest
            target: x86_64-pc-windows-msvc
            svm_target_platform: windows-amd64
            platform: win32
            arch: amd64
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Check if this is a release commit and extract version
        id: check_release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        shell: bash
        run: |
          echo "=== DEBUG: Checking recent commits ==="
          git log --oneline -10 --pretty=format:"%H %s"
          echo ""
          echo "=== DEBUG: Checking for release pattern ==="
          # Check the most recent commit (HEAD) for release pattern
          RECENT_COMMIT_MSG=$(git log --oneline -1 --pretty=format:"%s")
          echo "Most recent commit: '$RECENT_COMMIT_MSG'"
          # Try to extract version from Release v pattern (including PR number)
          VERSION_FROM_RELEASE=$(echo "$RECENT_COMMIT_MSG" | grep -o 'Release v[0-9]*\.[0-9]*\.[0-9]*' | sed 's/Release v//' || echo "")
          echo "VERSION_FROM_RELEASE: '$VERSION_FROM_RELEASE'"
          if [[ -n "$VERSION_FROM_RELEASE" ]]; then
            echo "VERSION_NAME=$VERSION_FROM_RELEASE" >> $GITHUB_ENV
            echo "is_release=true" >> $GITHUB_OUTPUT
            echo "Found release from commit title: Release v$VERSION_FROM_RELEASE"
          else
            echo "Not a release commit"
            echo "is_release=false" >> $GITHUB_OUTPUT
          fi
      - name: Skip if not a release
        if: steps.check_release.outputs.is_release != 'true'
        run: |
          echo "Skipping release build - not a release commit"
          exit 0
      - name: Checkout release branch
        if: steps.check_release.outputs.is_release == 'true'
        uses: actions/checkout@v4
        with:
          ref: release/${{ env.VERSION_NAME }}
          fetch-depth: 0
      - uses: dtolnay/rust-toolchain@stable
        if: steps.check_release.outputs.is_release == 'true'
        with:
          targets: ${{ matrix.target }}
      - uses: Swatinem/rust-cache@v2
        if: steps.check_release.outputs.is_release == 'true'
        with:
          key: ${{ matrix.target }}
          cache-on-failure: true
      - name: Setup Node.js
        if: steps.check_release.outputs.is_release == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: "22"
      - name: Apple M1 setup
        if: steps.check_release.outputs.is_release == 'true' && matrix.target == 'aarch64-apple-darwin'
        run: |
          echo "SDKROOT=$(xcrun -sdk macosx --show-sdk-path)" >> $GITHUB_ENV
          echo "MACOSX_DEPLOYMENT_TARGET=$(xcrun -sdk macosx --show-sdk-platform-version)" >> $GITHUB_ENV
      - name: Install MSVC target
        if: steps.check_release.outputs.is_release == 'true' && matrix.target == 'x86_64-pc-windows-msvc'
        run: rustup target add x86_64-pc-windows-msvc
      - name: Install OpenSSL development libraries
        if: steps.check_release.outputs.is_release == 'true' && matrix.target == 'x86_64-unknown-linux-gnu'
        run: |
          sudo apt-get update -y
          sudo apt-get install -y libssl-dev pkg-config
      - name: Install and setup NASM on Windows
        if: steps.check_release.outputs.is_release == 'true' && matrix.target == 'x86_64-pc-windows-msvc'
        shell: pwsh
        run: |
          choco install nasm
          $nasmPath = "C:\Program Files\NASM"
          $env:PATH += ";$nasmPath"
          echo "$nasmPath" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          echo "NASM_PATH=$nasmPath" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
          refreshenv
          nasm -v
      - name: Install vcpkg on Windows
        if: steps.check_release.outputs.is_release == 'true' && matrix.target == 'x86_64-pc-windows-msvc'
        shell: pwsh
        run: |
          git clone https://github.com/Microsoft/vcpkg.git C:\vcpkg
          C:\vcpkg\bootstrap-vcpkg.bat
          C:\vcpkg\vcpkg integrate install
          echo "VCPKG_ROOT=C:\vcpkg" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
          echo "C:\vcpkg" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
      - name: Install CMake on Windows
        if: steps.check_release.outputs.is_release == 'true' && matrix.target == 'x86_64-pc-windows-msvc'
        shell: pwsh
        run: |
          choco install cmake --version=3.20.0 --installargs 'ADD_CMAKE_TO_PATH=System'
          refreshenv
          cmake --version
          echo "CMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
      - name: Install dependencies with vcpkg on Windows
        if: steps.check_release.outputs.is_release == 'true' && matrix.target == 'x86_64-pc-windows-msvc'
        shell: pwsh
        run: |
          C:\vcpkg\vcpkg install openssl:x64-windows-static-md zlib:x64-windows-static-md
      - name: Build binaries
        if: steps.check_release.outputs.is_release == 'true'
        working-directory: cli
        env:
          SVM_TARGET_PLATFORM: ${{ matrix.svm_target_platform }}
          CMAKE_TOOLCHAIN_FILE: C:/vcpkg/scripts/buildsystems/vcpkg.cmake
        shell: bash
        run: |
          set -eo pipefail
          target="${{ matrix.target }}"
          flags=()
          if [[ "$target" != *msvc* && "$target" != "aarch64-unknown-linux-gnu" ]]; then
            flags+=(--features jemalloc,reth)
          else
            flags+=(--features reth)
          fi
          if [[ "$target" == *windows* ]]; then
            export PATH="$PATH:/c/Program Files/NASM:/c/vcpkg"
            export NASM="$NASM_PATH/nasm.exe"
            echo "NASM location: $NASM"
            "$NASM" -v
            echo "CMAKE_TOOLCHAIN_FILE: $CMAKE_TOOLCHAIN_FILE"
            ls -l "$CMAKE_TOOLCHAIN_FILE" || echo "Toolchain file not found!"
          fi
          RUST_BACKTRACE=1 CMAKE_TOOLCHAIN_FILE="$CMAKE_TOOLCHAIN_FILE" cargo build --release --target "$target" "${flags[@]}" -vv
      - name: Smoke Test
        if: steps.check_release.outputs.is_release == 'true'
        shell: bash
        run: |
          set -eo pipefail
          target="${{ matrix.target }}"
          binary_path="${{ github.workspace }}/target/$target/release/rindexer_cli"
          if [[ "$target" == *windows* ]]; then
            binary_path+=".exe"
          fi
          echo "Running smoke test for $binary_path"
          "$binary_path" --version
          "$binary_path" help
      - name: Archive binaries
        if: steps.check_release.outputs.is_release == 'true'
        id: artifacts
        env:
          PLATFORM_NAME: ${{ matrix.platform }}
          TARGET: ${{ matrix.target }}
          ARCH: ${{ matrix.arch }}
          VERSION_NAME: ${{ env.VERSION_NAME }}
        shell: bash
        run: |
          set -eo pipefail
          BUILD_DIR="${{ github.workspace }}/target/${TARGET}/release"
          CLI_BINARY_NAME="rindexer_cli"
          [[ "$PLATFORM_NAME" == "win32" ]] && CLI_BINARY_NAME="rindexer_cli.exe"
          # Create a temporary staging directory for creating the archive
          STAGING_DIR="staging"
          mkdir -p "$STAGING_DIR"
          # Copy binaries to the staging directory
          echo "Copying $BUILD_DIR/$CLI_BINARY_NAME to $STAGING_DIR/"
          cp "$BUILD_DIR/$CLI_BINARY_NAME" "$STAGING_DIR/"
          # Create the final archive
          if [ "$PLATFORM_NAME" == "linux" ] || [ "$PLATFORM_NAME" == "darwin" ]; then
            FILE_NAME="rindexer_${PLATFORM_NAME}-${ARCH}.tar.gz"
            tar -czvf "$FILE_NAME" -C "$STAGING_DIR" .
          else
            FILE_NAME="rindexer_${PLATFORM_NAME}-${ARCH}.zip"
            (cd "$STAGING_DIR" && 7z a -tzip "${{ github.workspace }}/$FILE_NAME" .)
          fi
          echo "Created archive: $FILE_NAME"
          echo "file_name=$FILE_NAME" >> $GITHUB_OUTPUT
      - name: Upload artifact
        if: steps.check_release.outputs.is_release == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: release-${{ matrix.platform }}-${{ matrix.arch }}
          path: ${{ steps.artifacts.outputs.file_name }}
  release:
    name: Create GitHub Release
    runs-on: ubuntu-22.04
    needs: release_build
    if: |
      github.actor != 'github-actions[bot]' &&
      github.ref == 'refs/heads/master' &&
      github.event_name == 'push'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Check if this is a release commit and extract version
        id: check_release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        shell: bash
        run: |
          echo "=== DEBUG: Checking recent commits ==="
          git log --oneline -10 --pretty=format:"%H %s"
          echo ""
          echo "=== DEBUG: Checking for release pattern ==="
          # Check the most recent commit (HEAD) for release pattern
          RECENT_COMMIT_MSG=$(git log --oneline -1 --pretty=format:"%s")
          echo "Most recent commit: '$RECENT_COMMIT_MSG'"
          # Try to extract version from Release v pattern (including PR number)
          VERSION_FROM_RELEASE=$(echo "$RECENT_COMMIT_MSG" | grep -o 'Release v[0-9]*\.[0-9]*\.[0-9]*' | sed 's/Release v//' || echo "")
          echo "VERSION_FROM_RELEASE: '$VERSION_FROM_RELEASE'"
          if [[ -n "$VERSION_FROM_RELEASE" ]]; then
            echo "VERSION_NAME=$VERSION_FROM_RELEASE" >> $GITHUB_ENV
            echo "is_release=true" >> $GITHUB_OUTPUT
            echo "Found release from commit title: Release v$VERSION_FROM_RELEASE"
          else
            echo "Not a release commit"
            echo "is_release=false" >> $GITHUB_OUTPUT
          fi
      - name: Skip if not a release
        if: steps.check_release.outputs.is_release != 'true'
        run: |
          echo "Skipping release creation - not a release commit"
          exit 0
      - name: Download release artifacts
        if: steps.check_release.outputs.is_release == 'true'
        uses: actions/download-artifact@v4
        with:
          path: ./release-artifacts
      - name: Display structure of downloaded files
        if: steps.check_release.outputs.is_release == 'true'
        run: ls -la ./release-artifacts/
      - name: Create GitHub Release
        if: steps.check_release.outputs.is_release == 'true'
        id: create_release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: v${{ env.VERSION_NAME }}
          release_name: Release v${{ env.VERSION_NAME }}
          draft: false
          prerelease: false
          body: |
            Release v${{ env.VERSION_NAME }}
            ## Installation
            ```bash
            # Latest version
            curl -L https://rindexer.xyz/install.sh | bash
            # Specific version
            curl -L https://rindexer.xyz/install.sh | bash -s -- --version ${{ env.VERSION_NAME }}
            ```
        continue-on-error: true
      - name: Upload Release Assets
        if: steps.check_release.outputs.is_release == 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          for platform_dir in ./release-artifacts/*/; do
            for file in "$platform_dir"*; do
              if [[ -f "$file" ]]; then
                filename=$(basename "$file")
                echo "Uploading $filename"
                gh release upload v${{ env.VERSION_NAME }} "$file" --clobber
              fi
            done
          done
</file>

<file path=".github/workflows/docker.yml">
name: Build & publish docker image
on:
  workflow_dispatch:
  release:
    types: [ published ]
env:
  REGISTRY: ghcr.io
jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    strategy:
      matrix:
        include:
          - dockerfile: Dockerfile
            image: ghcr.io/${{ github.repository }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      # Login against a Docker registry
      # https://github.com/docker/login-action
      - name: Log into registry ${{ env.REGISTRY }}
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      # Extract metadata (tags, labels) for Docker
      # https://github.com/docker/metadata-action
      - name: Extract Docker metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ matrix.image }}
          tags: |
            type=sha
            type=sha,format=long
            # set latest tag for default branch
            type=raw,value=latest,enable={{is_default_branch}}
      # Build and push Docker image with Buildx
      # https://github.com/docker/build-push-action
      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ${{ matrix.dockerfile }}
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
</file>

<file path=".github/workflows/edit-releases.yml">
name: Edit Existing GitHub Releases (One-Time)
on:
  workflow_dispatch:
    inputs:
      start_version:
        description: 'Start editing from this version (e.g., 0.1.0)'
        required: false
        default: '0.1.0' # Adjust this to the earliest version you want to edit
      end_version:
        description: 'End editing at this version (e.g., 0.21.0). Leave empty for all versions from start_version.'
        required: false
        default: '' # Leave empty to process all versions from start_version onwards
jobs:
  edit_releases:
    runs-on: ubuntu-latest
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Install GitHub CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y gh
      - name: Get all existing release tags
        id: get_tags
        run: |
          # Get all tags that start with 'v' and look like versions, then sort them.
          # This ensures we only process valid version tags that GitHub created.
          ALL_TAGS=$(gh api --paginate "/repos/${{ github.repository }}/tags" --jq '.[].name' | grep -E '^v[0-9]+\.[0-9]+\.[0-9]+$' | sed 's/^v//' | sort -V | paste -sd ' ' -)
          echo "Found tags: $ALL_TAGS"
          echo "all_versions=$ALL_TAGS" >> $GITHUB_OUTPUT
      - name: Edit Releases
        shell: bash
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          START_VERSION: ${{ github.event.inputs.start_version }}
          END_VERSION: ${{ github.event.inputs.end_version }}
        run: |
          #!/bin/bash
          set -e
          ALL_VERSIONS="${{ steps.get_tags.outputs.all_versions }}"
          CHANGELOG_FILE="documentation/docs/pages/docs/changelog.mdx"
          # Check if changelog file exists
          if [[ ! -f "$CHANGELOG_FILE" ]]; then
            echo "Changelog file not found: $CHANGELOG_FILE"
            exit 1
          fi
          echo "Debug: Changelog file found. First 20 lines:"
          head -20 "$CHANGELOG_FILE"
          echo "Debug: ===================="
          # Filter versions based on optional inputs
          FILTERED_VERSIONS=""
          # Function for version comparison using sort -V (semantic versioning)
          version_ge() {
            local v1="$1"
            local v2="$2"
            [[ "$v1" = "$v2" ]] || [[ "$(printf '%s\n' "$v1" "$v2" | sort -V | head -n1)" = "$v2" ]]
          }
          version_le() {
            local v1="$1"
            local v2="$2"
            [[ "$v1" = "$v2" ]] || [[ "$(printf '%s\n' "$v1" "$v2" | sort -V | tail -n1)" = "$v2" ]]
          }
          # Function to extract changelog content for a specific version
          extract_changelog() {
            local version="$1"
            local changelog_content=""
            echo "Debug: Looking for version $version in changelog" >&2
            # First, let's see if we can find the version at all
            if ! grep -q "$version-beta" "$CHANGELOG_FILE"; then
              echo "Debug: Version $version-beta not found in changelog" >&2
              return
            fi
            echo "Debug: Found version $version-beta in changelog" >&2
            # Find the section for this version and extract until the next version
            # Look for both # and ## patterns to handle different header levels
            changelog_content=$(awk -v version="$version" '
              BEGIN { found=0; in_section=0; debug=0 }
              /^##? [0-9]+\.[0-9]+\.[0-9]+-beta/ {
                if (debug) print "Found version header: " $0 > "/dev/stderr"
                if (found && in_section) {
                  if (debug) print "Exiting - found next version" > "/dev/stderr"
                  exit
                }
                if ($0 ~ version "-beta") {
                  if (debug) print "Found our version: " version > "/dev/stderr"
                  found=1
                  in_section=1
                  next
                }
              }
              found && in_section {
                # Skip certain lines but keep most content
                if ($0 !~ /^github branch -/ && 
                    $0 !~ /^- (linux|mac|windows) binary -/ &&
                    $0 !~ /^all release branches are deployed/ &&
                    $0 != "") {
                  print $0
                }
              }
            ' "$CHANGELOG_FILE")
            echo "Debug: Extracted content length: ${#changelog_content}" >&2
            echo "$changelog_content"
          }
          for ver in $ALL_VERSIONS; do
            if [[ -n "$START_VERSION" ]]; then
              if ! version_ge "$ver" "$START_VERSION"; then
                continue # Skip if less than start_version
              fi
            fi
            if [[ -n "$END_VERSION" ]]; then
              if ! version_le "$ver" "$END_VERSION"; then
                continue # Skip if greater than end_version
              fi
            fi
            FILTERED_VERSIONS+=" $ver"
          done
          if [ -z "$FILTERED_VERSIONS" ]; then
            echo "No versions to edit after applying filters. Exiting."
            exit 0
          fi
          echo "Editing versions: $FILTERED_VERSIONS"
          for VERSION in $FILTERED_VERSIONS; do
              echo "--- Processing version: $VERSION ---"
              TAG="v$VERSION"
              # Extract changelog content for this version
              echo "Extracting changelog for version: $VERSION"
              CHANGELOG_CONTENT=$(extract_changelog "$VERSION")
              echo "Debug: Raw changelog content:"
              echo "[$CHANGELOG_CONTENT]"
              echo "Debug: Content length: ${#CHANGELOG_CONTENT}"
              # Clean up the changelog content - remove empty lines at start and end
              CHANGELOG_CONTENT=$(echo "$CHANGELOG_CONTENT" | sed '/^$/d' | sed '/^-\+$/d')
              echo "Debug: Cleaned changelog content:"
              echo "[$CHANGELOG_CONTENT]"
              # Build the release body with install instructions and changelog
              INSTALL_SECTION="# Install
          \`\`\`bash
          curl -L https://rindexer.xyz/install.sh | bash -s -- --version $VERSION
          \`\`\`"
          if [[ -n "$CHANGELOG_CONTENT" ]]; then
          RELEASE_BODY="$INSTALL_SECTION
          # Changelog
          $CHANGELOG_CONTENT"
          else
          echo "Warning: No changelog content found for version $VERSION"
          RELEASE_BODY="$INSTALL_SECTION"
          fi
          echo "Attempting to edit release $TAG..."
          echo "Release body preview:"
          echo "========================"
          echo "$RELEASE_BODY"
          echo "========================"
          gh release edit "$TAG" \
          --repo ${{ github.repository }} \
          --prerelease=false \
          --notes "$RELEASE_BODY" \
          --target master
          echo "Successfully edited release $TAG."
          done
</file>

<file path=".github/workflows/migrate-releases.yml">
name: Migrate Historical Releases to GitHub Releases
on:
  workflow_dispatch:
    inputs:
      start_version:
        description: 'Start migration from this version (e.g., 0.1.0)'
        required: false
        default: '0.1.0'
      end_version:
        description: 'End migration at this version (e.g., 0.21.0). Leave empty for all versions from start_version.'
        required: false
        default: ''
jobs:
  migrate_releases:
    runs-on: ubuntu-latest
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # Ensure this secret is available
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          # Checkout the specific commit where these old releases were located
          # If your main branch still has these files, you can use `master` or `main`
          ref: 96ead5ee413f83fc58796f1661791122cf1a7f60 # The commit hash you provided
          fetch-depth: 0 # Get full history if needed for changelog parsing
      - name: Install GitHub CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y gh
      - name: Set up Node.js for Changelog Parsing (Optional, but recommended for better parsing)
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      - name: Create release body parser script
        id: create_parser
        run: |
          cat << 'EOF' > parse_changelog.js
          const fs = require('fs');
          const path = require('path');
          const changelogPath = path.join(process.env.GITHUB_WORKSPACE, 'documentation', 'docs', 'pages', 'docs', 'changelog.mdx');
          const version = process.argv[2]; // Version to extract (e.g., "0.1.0-beta")
          console.log(`Attempting to parse changelog for version: ${version}`);
          try {
            const changelogContent = fs.readFileSync(changelogPath, 'utf8');
            const releasesSection = changelogContent.split(/^## Releases/m)[1];
            if (!releasesSection) {
              console.error("Could not find '## Releases' section in changelog.");
              process.exit(1);
            }
            const releasePattern = new RegExp(`^# ${version}(.*?)(\\n# |^## |$)`, 'ms');
            const match = releasesSection.match(releasePattern);
            let releaseNotes = "No release notes found in changelog.";
            if (match && match[1]) {
                releaseNotes = match[1].trim();
                // Clean up specific lines that are just headers or blank lines from your template
                releaseNotes = releaseNotes
                  .replace(/^(github branch - https:\/\/github\.com\/joshstevens19\/rindexer\/tree\/release\/.*)$/gm, '')
                  .replace(/^- (linux|mac|windows) binary - https:\/\/(github|rindexer\.xyz).*$/gm, '')
                  .replace(/^### Features\n-------------------------------------------------$/gm, '### Features')
                  .replace(/^### Bug fixes\n-------------------------------------------------$/gm, '### Bug fixes')
                  .replace(/^### Breaking changes\n-------------------------------------------------$/gm, '### Breaking changes')
                  .replace(/^-+\s*$/gm, '') // Remove horizontal rules
                  .replace(/^\s*[\r\n]+/gm, '') // Remove empty lines
                  .trim();
                 if (releaseNotes === '') {
                   releaseNotes = "No specific features/bug fixes mentioned in changelog.";
                 }
            }
            console.log(`::set-output name=release_body::${releaseNotes}`);
          } catch (error) {
            console.error(`Error reading or parsing changelog: ${error.message}`);
            process.exit(1);
          }
          EOF
          chmod +x parse_changelog.js
      - name: Find all versions and platforms
        id: find_versions
        run: |
          RELEASE_BASE_PATH="documentation/docs/public/releases"
          declare -A versions_map # Associative array to store versions and their assets
          # Loop through each platform directory (e.g., darwin-amd64, linux-amd64)
          for platform_dir in ${RELEASE_BASE_PATH}/*/; do
            PLATFORM=$(basename "$platform_dir")
            # Loop through each version directory within the platform
            for version_dir in "$platform_dir"*/; do
              VERSION=$(basename "$version_dir")
              # Collect all files in this version directory
              for file_path in "$version_dir"*; do
                FILE_NAME=$(basename "$file_path")
                # Append file path to the list for this version
                versions_map["$VERSION"]+="$file_path;"
              done
            done
          done
          # Output sorted unique versions as a single-line, space-separated string
          # This is the primary output for versions_to_migrate
          UNIQUE_VERSIONS_SINGLE_LINE=$(printf "%s " "${!versions_map[@]}" | sort -V | tr '\n' ' ')
          echo "Found versions (single line): $UNIQUE_VERSIONS_SINGLE_LINE"
          echo "versions_to_migrate=$UNIQUE_VERSIONS_SINGLE_LINE" >> $GITHUB_OUTPUT
          # Prepare assets map for subsequent steps as a multi-line string with a delimiter
          # This should be a robust JSON string
          JSON_ASSETS="{"
          for version in $(printf "%s\n" "${!versions_map[@]}" | sort -V); do # Use sorted versions here too
            ASSETS_LIST="${versions_map[$version]%??}" # Remove trailing semicolon
            JSON_ASSETS+="\"$version\":\"$ASSETS_LIST\","
          done
          JSON_ASSETS="${JSON_ASSETS%,}" # Remove trailing comma
          JSON_ASSETS+="}"
          # Use multi-line output for JSON_ASSETS
          # Define a custom delimiter, e.g., EOF_JSON_ASSETS
          echo "assets_map<<EOF_JSON_ASSETS" >> $GITHUB_OUTPUT
          echo "$JSON_ASSETS" >> $GITHUB_OUTPUT
          echo "EOF_JSON_ASSETS" >> $GITHUB_OUTPUT
      - name: Migrate Releases
        shell: bash
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ASSETS_MAP: ${{ steps.find_versions.outputs.assets_map }}
          START_VERSION: ${{ github.event.inputs.start_version }}
          END_VERSION: ${{ github.event.inputs.end_version }}
        run: |
          #!/bin/bash
          set -e
          # Parse JSON assets map
          # Using Python for robust JSON parsing
          ASSETS_DICT=$(python3 -c 'import json; import os; print(json.loads(os.environ["ASSETS_MAP"]))')
          # Use `eval` with caution, but for simple dict from trusted source, it's ok.
          # More robust: parse with jq or python directly for each loop.
          # For simplicity here, let's process the string output from python.
          # Convert the Python dict string to a bash associative array
          declare -A assets_map_bash
          # Example Python output: {'0.1.0': 'path1;path2', '0.2.0': 'path3'}
          # We need to parse this into a bash associative array.
          # A safer way is to use `jq` if available, or simpler string manipulation.
          # Let's simplify the python step to produce an array of versions and then loop.
          # Re-evaluate ASSETS_MAP for direct use
          # `jq` is excellent for this.
          # `sudo apt-get install -y jq` if not already installed on runner.
          # Let's assume the previous step outputs a simple space-separated list of versions
          # and then we iterate over that.
          ALL_VERSIONS="${{ steps.find_versions.outputs.versions_to_migrate }}"
          # Filter versions based on optional inputs
          FILTERED_VERSIONS=""
          version_compare_start=$(printf '%s\n' "$START_VERSION" | sed 's/\.//g') # Remove dots for numeric comparison
          # Iterate over versions and apply filters
          for ver in $ALL_VERSIONS; do
            current_version_numeric=$(printf '%s\n' "$ver" | sed 's/\.//g')
            # Start version filter
            if [[ -n "$START_VERSION" ]]; then
              if (( $(echo "$current_version_numeric >= $version_compare_start" | bc -l) )); then
                # Version is greater than or equal to start_version
                : # Do nothing, proceed
              else
                continue # Skip this version
              fi
            fi
            # End version filter
            if [[ -n "$END_VERSION" ]]; then
              version_compare_end=$(printf '%s\n' "$END_VERSION" | sed 's/\.//g')
              if (( $(echo "$current_version_numeric <= $version_compare_end" | bc -l) )); then
                : # Do nothing, proceed
              else
                continue # Skip this version
              fi
            fi
            FILTERED_VERSIONS+=" $ver"
          done
          if [ -z "$FILTERED_VERSIONS" ]; then
            echo "No versions to migrate after applying filters. Exiting."
            exit 0
          fi
          echo "Migrating versions: $FILTERED_VERSIONS"
          for VERSION in $FILTERED_VERSIONS; do
              echo "--- Processing version: $VERSION ---"
              TAG="v$VERSION"
              RELEASE_NAME="Release $TAG"
              # Extract release notes from changelog.mdx using the Node.js script
              RELEASE_BODY=$(node parse_changelog.js "$VERSION-beta") # Assuming beta suffix for older releases
              if [ -z "$RELEASE_BODY" ]; then
                RELEASE_BODY="No specific release notes found for $VERSION in changelog."
              fi
              echo "Release Body for $TAG: $RELEASE_BODY"
              # Check if release already exists
              if gh release view "$TAG" &> /dev/null; then
                  echo "Release $TAG already exists. Skipping creation."
              else
                  echo "Creating release $TAG..."
                  gh release create "$TAG" \
                      --title "$RELEASE_NAME" \
                      --notes "$RELEASE_BODY" \
                      --prerelease # Assuming all these old releases were beta/prerelease
              fi
              # Get the assets list for this version from the ASSETS_MAP (needs proper JSON parsing or iteration)
              # A simpler way given the previous step's output is to reconstruct the path
              # Construct paths for this version for each platform
              PLATFORMS=("darwin-amd64" "darwin-arm64" "linux-amd64" "win32-amd64")
              ASSETS_TO_UPLOAD=()
              for PLATFORM_ARCH in "${PLATFORMS[@]}"; do
                FILE_EXT="tar.gz"
                if [[ "$PLATFORM_ARCH" == "win32-amd64" ]]; then
                  FILE_EXT="zip"
                fi
                # Check for rindexer_cli (newer) or rindexer (older)
                FILE_BASE_NAME="rindexer_${PLATFORM_ARCH}.${FILE_EXT}"
                FILE_CLI_NAME="rindexer_cli_${PLATFORM_ARCH}.${FILE_EXT}" # If cli was used
                # Full path in the checked out repository
                CURRENT_FILE_PATH="documentation/docs/public/releases/${PLATFORM_ARCH}/${VERSION}/${FILE_BASE_NAME}"
                CURRENT_CLI_FILE_PATH="documentation/docs/public/releases/${PLATFORM_ARCH}/${VERSION}/${FILE_CLI_NAME}"
                if [ -f "$CURRENT_FILE_PATH" ]; then
                    ASSETS_TO_UPLOAD+=("$CURRENT_FILE_PATH")
                elif [ -f "$CURRENT_CLI_FILE_PATH" ]; then # Check for cli version if base not found
                    ASSETS_TO_UPLOAD+=("$CURRENT_CLI_FILE_PATH")
                else
                    echo "Warning: Binary not found for $VERSION, platform $PLATFORM_ARCH at $CURRENT_FILE_PATH or $CURRENT_CLI_FILE_PATH"
                fi
              done
              if [ ${#ASSETS_TO_UPLOAD[@]} -eq 0 ]; then
                  echo "No assets found for version $VERSION. Skipping asset upload for this release."
              else
                  echo "Uploading assets for $TAG: ${ASSETS_TO_UPLOAD[*]}"
                  # Loop and upload each asset
                  for ASSET_PATH in "${ASSETS_TO_UPLOAD[@]}"; do
                      gh release upload "$TAG" "$ASSET_PATH" --clobber # --clobber allows re-uploading if existing asset has same name
                  done
              fi
          done
</file>

<file path="cli/src/commands/add.rs">
use std::{borrow::Cow, fs, path::PathBuf, time::Duration};
use alloy::{primitives::Address, rpc::types::ValueOrArray};
use alloy_chains::Chain;
use foundry_block_explorers::Client;
use rindexer::{
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::{read_manifest_raw, write_manifest, YAML_CONFIG_NAME},
    },
    public_read_env_value, write_file, StringOrArray,
};
use crate::{
    commands::BACKUP_ETHERSCAN_API_KEY,
    console::{
        print_error_message, print_success_message, prompt_for_input, prompt_for_input_list,
    },
    rindexer_yaml::validate_rindexer_yaml_exist,
};
pub async fn handle_add_contract_command(
    project_path: PathBuf,
) -> Result<(), Box<dyn std::error::Error>> {
    validate_rindexer_yaml_exist(&project_path);
    let rindexer_yaml_path = project_path.join(YAML_CONFIG_NAME);
    let mut manifest = read_manifest_raw(&rindexer_yaml_path).inspect_err(|e| {
        print_error_message(&format!("Could not read the rindexer.yaml file: {e}"))
    })?;
    let rindexer_abis_folder = project_path.join("abis");
    if let Err(err) = fs::create_dir_all(&rindexer_abis_folder) {
        print_error_message(&format!("Failed to create directory: {err}"));
        return Err(err.into());
    }
    let networks: Vec<(&str, u64)> =
        manifest.networks.iter().map(|network| (network.name.as_str(), network.chain_id)).collect();
    if networks.is_empty() {
        print_error_message("No networks found in rindexer.yaml. Please add a network first before downloading ABIs.");
        return Err("No networks found in rindexer.yaml.".into());
    }
    let network_choices: Vec<String> = networks.iter().map(|(name, _)| name.to_string()).collect();
    let network = if network_choices.len() > 1 {
        Cow::Owned(prompt_for_input_list("Enter Network Name", &network_choices, None))
    } else {
        Cow::Borrowed(&network_choices[0])
    };
    let chain_id = networks
        .iter()
        .find(|(name, _)| *name == network.as_ref())
        .expect("Unreachable: Network not found in networks")
        .1;
    let chain_network = Chain::from(chain_id);
    let contract_address =
        prompt_for_input(&format!("Enter {network} Contract Address"), None, None, None);
    let etherscan_api_key = manifest.global.etherscan_api_key.as_ref().map_or_else(
        || BACKUP_ETHERSCAN_API_KEY.to_string(),
        |key| public_read_env_value(key).unwrap_or_else(|_| key.to_string()),
    );
    let client = Client::builder()
        .with_api_key(etherscan_api_key)
        .chain(chain_network)
        .map_err(|e| {
            print_error_message(&format!("Invalid chain id {e}"));
            e
        })?
        .build()
        .map_err(|e| {
            print_error_message(&format!("Failed to create etherscan client: {e}"));
            e
        })?;
    let address = contract_address
        .parse::<Address>()
        .inspect_err(|e| print_error_message(&format!("Invalid contract address: {e}")))?;
    let mut abi_lookup_address: Address = address;
    let mut timeout = 1000;
    let mut retry_attempts = 0;
    let max_retries = 3;
    loop {
        let metadata = match client
            .contract_source_code(
                abi_lookup_address.to_string().parse().expect("contract already checked"),
            )
            .await
        {
            Ok(data) => data,
            Err(e) => {
                if retry_attempts >= max_retries {
                    print_error_message(&format!(
                        "Failed to fetch contract metadata: {e}, retries: {retry_attempts}"
                    ));
                    return Err(Box::new(e));
                }
                // Different verifiers have different rate limits which leads to
                // rate limit errors when adding a contract.
                // Etherscan has good rate limits whereas Arbiscan is not as good
                // Sleeping here avoids APIs hitting rate limit
                tokio::time::sleep(Duration::from_millis(timeout)).await;
                retry_attempts += 1;
                timeout *= retry_attempts * 2;
                continue;
            }
        };
        if metadata.items.is_empty() {
            print_error_message(&format!(
                "No contract found on network {network} with address {contract_address}."
            ));
            break;
        }
        let item = &metadata.items[0];
        if item.proxy == 1 && item.implementation.is_some() {
            abi_lookup_address = item.implementation.unwrap().to_string().parse().unwrap();
            println!(
                "This contract is a proxy contract. Loading the implementation contract {abi_lookup_address}"
            );
            tokio::time::sleep(Duration::from_millis(1000)).await;
            continue;
        }
        let contract_name = manifest.contracts.iter().find(|c| c.name == item.contract_name);
        let contract_name = if contract_name.is_some() {
            Cow::Owned(prompt_for_input(
                &format!("Enter a name for the contract as it is clashing with another registered contract name in the yaml: {}", item.contract_name),
                None,
                None,
                None,
            ))
        } else {
            Cow::Borrowed(&item.contract_name)
        };
        let abi_file_name = format!("{contract_name}.abi.json");
        let abi_path = rindexer_abis_folder.join(&abi_file_name);
        write_file(&abi_path, &item.abi).map_err(|e| {
            print_error_message(&format!("Failed to write ABI file: {e}"));
            e
        })?;
        let abi_path_relative = format!("./abis/{abi_file_name}");
        print_success_message(&format!(
            "Downloaded ABI for: {} in {}",
            contract_name, &abi_path_relative
        ));
        let success_message = format!(
            "Updated rindexer.yaml with contract: {contract_name} and ABI path: {abi_path_relative}"
        );
        manifest.contracts.push(Contract {
            name: contract_name.into_owned(),
            details: vec![ContractDetails::new_with_address(
                network.to_string(),
                ValueOrArray::<Address>::Value(address),
                None,
                None,
                None,
            )],
            abi: StringOrArray::Single(abi_path_relative),
            include_events: None,
            index_event_in_order: None,
            dependency_events: None,
            reorg_safe_distance: None,
            generate_csv: None,
            streams: None,
            chat: None,
        });
        write_manifest(&manifest, &rindexer_yaml_path).map_err(|e| {
            print_error_message(&format!("Failed to write rindexer.yaml file: {e}"));
            e
        })?;
        print_success_message(&success_message);
        break;
    }
    Ok(())
}
</file>

<file path="cli/src/commands/codegen.rs">
use std::path::PathBuf;
use crate::{
    cli_interface::CodegenSubcommands,
    console::{print_error_message, print_success_message},
    rindexer_yaml::validate_rindexer_yaml_exist,
};
use rindexer::{
    format_all_files_for_project, generate_graphql_queries,
    generator::build::{generate_rindexer_handlers, generate_rindexer_typings},
    manifest::{
        core::ProjectType,
        graphql::default_graphql_port,
        yaml::{read_manifest, YAML_CONFIG_NAME},
    },
};
pub async fn handle_codegen_command(
    project_path: PathBuf,
    subcommand: &CodegenSubcommands,
) -> Result<(), Box<dyn std::error::Error>> {
    if let CodegenSubcommands::GraphQL { endpoint } = subcommand {
        let default_url = format!("http://localhost:{}/graphql", default_graphql_port());
        let url = endpoint.as_deref().unwrap_or(&default_url);
        generate_graphql_queries(url, &project_path).await.map_err(|e| {
            print_error_message(&format!("Failed to generate graphql queries: {e}"));
            e
        })?;
        print_success_message("Generated graphql queries.");
        return Ok(());
    }
    validate_rindexer_yaml_exist(&project_path);
    let rindexer_yaml_path = project_path.join(YAML_CONFIG_NAME);
    let manifest = read_manifest(&rindexer_yaml_path).map_err(|e| {
        print_error_message(&format!("Could not read the rindexer.yaml file: {e}"));
        e
    })?;
    if manifest.project_type == ProjectType::NoCode {
        let error = "This command is not supported for no-code projects, please migrate to a project to use this.";
        print_error_message(error);
        return Err(error.into());
    }
    match subcommand {
        CodegenSubcommands::Typings => {
            generate_rindexer_typings(&manifest, &rindexer_yaml_path, true).map_err(|e| {
                print_error_message(&format!("Failed to generate rindexer typings: {e}"));
                e
            })?;
            format_all_files_for_project(project_path);
            print_success_message("Generated rindexer typings.");
        }
        CodegenSubcommands::Indexer => {
            generate_rindexer_handlers(manifest, &rindexer_yaml_path, true).map_err(|e| {
                print_error_message(&format!("Failed to generate rindexer indexer handlers: {e}"));
                e
            })?;
            format_all_files_for_project(project_path);
            print_success_message("Generated rindexer indexer handlers.");
        }
        CodegenSubcommands::GraphQL { endpoint: _endpoint } => {
            unreachable!("This should not be reachable");
        }
    }
    Ok(())
}
</file>

<file path="cli/src/commands/delete.rs">
use std::path::PathBuf;
use rindexer::{
    drop_tables_for_indexer_sql,
    manifest::yaml::{read_manifest, YAML_CONFIG_NAME},
    PostgresClient,
};
use tokio::fs::remove_dir_all;
use crate::console::{
    print_error_message, print_success_message, print_warn_message, prompt_for_input_list,
};
pub async fn handle_delete_command(
    project_path: PathBuf,
) -> Result<(), Box<dyn std::error::Error>> {
    print_warn_message(&format!(
        "This will delete all data in the postgres database and csv files for the project at: {}",
        project_path.display()
    ));
    print_warn_message(
        "This operation can not be reverted. Make sure you know what you are doing.",
    );
    let manifest = read_manifest(&project_path.join(YAML_CONFIG_NAME)).map_err(|e| {
        print_error_message(&format!("Could read the rindexer.yaml please make sure you are running the command with rindexer.yaml in root: trace: {e}"));
        e
    })?;
    let postgres_enabled = manifest.storage.postgres_enabled();
    let csv_enabled = manifest.storage.csv_enabled();
    if !postgres_enabled && !csv_enabled {
        print_success_message("No storage enabled. Nothing to delete.");
        return Ok(());
    }
    if postgres_enabled {
        let postgres_delete = prompt_for_input_list(
            "Are you sure you wish to delete the database data (it can not be reverted)?",
            &["yes".to_string(), "no".to_string()],
            None,
        );
        if postgres_delete == "yes" {
            let postgres_client = PostgresClient::new().await.map_err(|e| {
                print_error_message(&format!("Could not connect to Postgres, make sure your connection string is mapping in the .env correctly: trace: {e}"));
                e
            })?;
            let sql = drop_tables_for_indexer_sql(&project_path, &manifest.to_indexer());
            postgres_client.batch_execute(sql.as_str()).await.map_err(|e| {
                print_error_message(&format!("Could not delete tables from Postgres make sure your connection string is mapping in the .env correctly: trace: {e}"));
                e
            })?;
            print_success_message(
                "\n\nSuccessfully deleted all data from the postgres database.\n\n",
            );
        }
    }
    if csv_enabled {
        let csv_delete = prompt_for_input_list(
            "Are you sure you wish to delete the csv data (it can not be reverted)?",
            &["yes".to_string(), "no".to_string()],
            None,
        );
        if csv_delete == "yes" {
            if let Some(csv) = &manifest.storage.csv {
                let path = &project_path.join(&csv.path);
                // if no csv exist we will just look like it cleared it
                if path.exists() {
                    remove_dir_all(&project_path.join(path)).await.map_err(|e| {
                        print_error_message(&format!("Could not delete csv files: trace: {e}"));
                        e
                    })?;
                }
                print_success_message("\n\nSuccessfully deleted all csv files.\n\n");
            } else {
                print_error_message("CSV storage is not enabled so no storage can be deleted. Please enable it in the YAML configuration file.");
            }
        }
    }
    Ok(())
}
</file>

<file path="cli/src/commands/mod.rs">
pub mod add;
pub mod codegen;
pub mod delete;
pub mod new;
pub mod phantom;
pub mod start;
const BACKUP_ETHERSCAN_API_KEY: &str = "DHBPB1EJ84JMSWP7C86387NK7IIRRQJVV1";
</file>

<file path="cli/src/commands/phantom.rs">
use std::{
    env,
    error::Error,
    fs,
    fs::OpenOptions,
    io::Write,
    path::{Path, PathBuf},
    process::Command,
};
use alloy::{
    primitives::{Address, U64},
    rpc::types::ValueOrArray,
};
use rindexer::{
    manifest::{
        network::Network,
        phantom::{Phantom, PhantomDyrpc, PhantomShadow},
        yaml::{read_manifest, read_manifest_raw, write_manifest, YAML_CONFIG_NAME},
    },
    phantom::{
        common::{read_compiled_contract, read_contract_clone_metadata},
        create_dyrpc_api_key, deploy_dyrpc_contract,
        shadow::deploy_shadow_contract,
    },
    public_read_env_value, write_file, StringOrArray,
};
use crate::{
    cli_interface::{PhantomBaseArgs, PhantomSubcommands},
    commands::BACKUP_ETHERSCAN_API_KEY,
    console::{
        print_error_message, print_success_message, print_warn_message, prompt_for_input,
        prompt_for_input_list,
    },
    rindexer_yaml::validate_rindexer_yaml_exist,
};
const RINDEXER_PHANTOM_API_ENV_KEY: &str = "RINDEXER_PHANTOM_API_KEY";
pub async fn handle_phantom_commands(
    project_path: PathBuf,
    command: &PhantomSubcommands,
) -> Result<(), Box<dyn Error>> {
    validate_rindexer_yaml_exist(&project_path);
    match command {
        PhantomSubcommands::Init => handle_phantom_init(&project_path).await,
        PhantomSubcommands::Clone { contract_name, network } => handle_phantom_clone(
            &project_path,
            &PhantomBaseArgs {
                contract_name: contract_name.to_owned(),
                network: network.to_owned(),
            },
        ),
        PhantomSubcommands::Compile { contract_name, network } => handle_phantom_compile(
            &project_path,
            &PhantomBaseArgs {
                contract_name: contract_name.to_owned(),
                network: network.to_owned(),
            },
        ),
        PhantomSubcommands::Deploy { contract_name, network } => {
            handle_phantom_deploy(
                &project_path,
                &PhantomBaseArgs {
                    contract_name: contract_name.to_owned(),
                    network: network.to_owned(),
                },
            )
            .await
        }
    }
}
fn install_foundry() -> Result<(), Box<dyn Error>> {
    let foundry_check =
        Command::new("which").arg("foundryup").output().expect("Failed to execute command");
    if foundry_check.status.success() {
        Ok(())
    } else {
        println!("Foundry is not installed. Installing Foundry...");
        let install_command = Command::new("sh")
            .arg("-c")
            .arg("curl -L https://foundry.paradigm.xyz | bash")
            .status()
            .map_err(|e| e.to_string())?;
        if install_command.success() {
            Ok(())
        } else {
            Err("Failed to install Foundry.".into())
        }
    }
}
async fn handle_phantom_init(project_path: &Path) -> Result<(), Box<dyn Error>> {
    let env_file = project_path.join(".env");
    let rindexer_yaml_path = project_path.join(YAML_CONFIG_NAME);
    let mut manifest = read_manifest_raw(&rindexer_yaml_path).inspect_err(|e| {
        print_error_message(&format!("Could not read the rindexer.yaml file: {e}"))
    })?;
    if manifest.phantom.is_some() {
        let error_message = "phantom already setup in rindexer.yaml";
        print_error_message(error_message);
        return Err(error_message.into());
    }
    print_success_message("setting up phantom events on rindexer...");
    install_foundry()?;
    let phantom_provider_choice = prompt_for_input_list(
        "Which provider are you using?",
        &["shadow".to_string(), "dyrpc".to_string()],
        None,
    );
    let mut api_key_value = prompt_for_input(
        if phantom_provider_choice == "dyrpc" {
            "Enter your dyRPC API key (enter to new to generate a new key)"
        } else {
            "Enter your Shadow API key"
        },
        None,
        None,
        None,
    );
    match phantom_provider_choice.as_str() {
        "dyrpc" => {
            if api_key_value == "new" {
                api_key_value = create_dyrpc_api_key().await?;
                println!(
                    "Your API has been created and key is {api_key_value} - it has also been written to your .env file."
                );
            }
            manifest.phantom = Some(Phantom {
                dyrpc: Some(PhantomDyrpc {
                    api_key: format!("${{{RINDEXER_PHANTOM_API_ENV_KEY}}}"),
                }),
                shadow: None,
            });
            write_manifest(&manifest, &rindexer_yaml_path)?;
        }
        "shadow" => {
            let fork_id = prompt_for_input("Enter the fork ID", None, None, None);
            manifest.phantom = Some(Phantom {
                shadow: Some(PhantomShadow {
                    api_key: format!("${{{RINDEXER_PHANTOM_API_ENV_KEY}}}"),
                    fork_id,
                }),
                dyrpc: None,
            });
            write_manifest(&manifest, &rindexer_yaml_path)?;
        }
        value => panic!("Unknown phantom provider: {value}"),
    }
    let env_content = fs::read_to_string(&env_file).unwrap_or_default();
    let value = api_key_value;
    let mut lines: Vec<String> = env_content.lines().map(|line| line.to_string()).collect();
    let mut key_found = false;
    for line in &mut lines {
        if line.starts_with(&format!("{RINDEXER_PHANTOM_API_ENV_KEY}=")) {
            *line = format!("{RINDEXER_PHANTOM_API_ENV_KEY}={value}");
            key_found = true;
            break;
        }
    }
    if !key_found {
        lines.push(format!("{RINDEXER_PHANTOM_API_ENV_KEY}={value}"));
    }
    let new_env_content = lines.join("\n");
    let mut file = OpenOptions::new().write(true).truncate(true).create(true).open(&env_file)?;
    writeln!(file, "{new_env_content}")?;
    print_success_message("rindexer Phantom events are now setup.\nYou can now use `rindexer phantom clone --contract-name <contract> --network <network>` to start adding your own custom events.");
    Ok(())
}
fn forge_clone_contract(
    clone_in: &Path,
    network: &Network,
    address: &Address,
    contract_name: &str,
    etherscan_api_key: &str,
) -> Result<(), Box<dyn Error>> {
    print_success_message(&format!(
        "Cloning contract {} on network {} at address {:?} this may take a little moment...",
        contract_name, network.name, address
    ));
    let output = Command::new("forge")
        .arg("clone")
        .arg("--no-commit")
        .arg(format!("{address:?}"))
        //.arg(format!("--chain {}", network.chain_id))
        .arg("--etherscan-api-key")
        .arg(etherscan_api_key)
        .arg(contract_name)
        .current_dir(clone_in)
        .output()?;
    if output.status.success() {
        Ok(())
    } else {
        print_error_message(&format!(
            "Failed to clone contract: {contract_name} at address: {address:?}"
        ));
        print_error_message(&format!("Error: {}", String::from_utf8_lossy(&output.stderr)));
        Err("Failed to clone contract".into())
    }
}
fn handle_phantom_clone(project_path: &Path, args: &PhantomBaseArgs) -> Result<(), Box<dyn Error>> {
    let rindexer_yaml_path = project_path.join(YAML_CONFIG_NAME);
    let manifest = read_manifest(&rindexer_yaml_path).inspect_err(|e| {
        print_error_message(&format!("Could not read the rindexer.yaml file: {e}"))
    })?;
    if manifest.phantom.is_none() {
        let error_message =
            "phantom not setup in rindexer.yaml. Please run `rindexer phantom init` first.";
        print_error_message(error_message);
        return Err(error_message.into());
    }
    let cloning_location =
        project_path.join("phantom").join(&args.network).join(args.contract_name.as_str());
    if cloning_location.exists() {
        let error_message = format!("Phantom contract {} on network {} already cloned in {}. If you want to clone it again please delete the folder first.", args.contract_name, args.network, cloning_location.display());
        print_error_message(&error_message);
        return Err(error_message.into());
    }
    let contract = manifest.contracts.iter().find(|c| c.name == args.contract_name);
    match contract {
        Some(contract) => {
            let network = manifest.networks.iter().find(|n| n.name == args.network);
            if network.is_none() {
                let error_message = format!("Network {} not found in rindexer.yaml", args.network);
                print_error_message(&error_message);
                return Err(error_message.into());
            }
            if network.unwrap().chain_id != 1 {
                let error_message = format!("Network {} is not supported", args.network);
                print_error_message(&error_message);
                return Err(error_message.into());
            }
            let contract_network = contract.details.iter().find(|c| c.network == args.network);
            if let Some(contract_network) = contract_network {
                if let Some(address) = contract_network.address() {
                    // pick the first one as the ABI has to match so assume all contracts do
                    let address = match address {
                        ValueOrArray::Value(address) => address,
                        ValueOrArray::Array(addresses) => {
                            print_warn_message(&format!("Multiple addresses found for contract {} on network {} rindexer.yaml, using first one", args.contract_name.as_str(), args.network.as_str()));
                            addresses.first().unwrap()
                        }
                    };
                    if !project_path.join("phantom").exists() {
                        fs::create_dir(project_path.join("phantom"))?;
                    }
                    let clone_in = project_path.join("phantom").join(&args.network);
                    if !clone_in.exists() {
                        fs::create_dir(&clone_in)?;
                    }
                    let etherscan_api_key = manifest.global.etherscan_api_key.as_ref().map_or_else(
                        || BACKUP_ETHERSCAN_API_KEY.to_string(),
                        |key| public_read_env_value(key).unwrap_or_else(|_| key.to_string()),
                    );
                    forge_clone_contract(
                        &clone_in,
                        network.unwrap(),
                        address,
                        contract.name.as_str(),
                        &etherscan_api_key,
                    )
                    .map_err(|e| format!("Failed to clone contract: {e}"))?;
                    print_success_message(format!("\ncloned {} in {} you can start adding your custom events.\nYou can now use `rindexer phantom compile -contract-name {} --network {}` to compile the phantom contract anytime.", contract.name.as_str(), clone_in.display(), contract.name.as_str(), args.network).as_str());
                    Ok(())
                } else {
                    let error_message = format!(
                        "Contract {} in network {} does not have an address in rindexer.yaml",
                        args.contract_name, args.network
                    );
                    print_error_message(&error_message);
                    Err(error_message.into())
                }
            } else {
                let error_message = format!(
                    "Network {} not found in contract {} in rindexer.yaml",
                    args.network, args.contract_name
                );
                print_error_message(&error_message);
                Err(error_message.into())
            }
        }
        None => {
            let error_message =
                format!("Contract {} not found in rindexer.yaml", args.contract_name);
            print_error_message(&error_message);
            Err(error_message.into())
        }
    }
}
fn forge_compile_contract(
    compile_in: &Path,
    network: &Network,
    contract_name: &str,
) -> Result<(), Box<dyn Error>> {
    print_success_message(&format!(
        "Compiling contract {} on network {}...",
        contract_name, network.name
    ));
    let output = Command::new("forge").arg("build").current_dir(compile_in).output()?;
    if output.status.success() {
        Ok(())
    } else {
        print_error_message(&format!(
            "Failed to compile contract: {} for network: {}",
            contract_name, network.name
        ));
        print_error_message(&format!("Error: {}", String::from_utf8_lossy(&output.stderr)));
        Err("Failed to compile contract".into())
    }
}
fn get_phantom_network_name(args: &PhantomBaseArgs) -> String {
    format!("phantom_{}_{}", args.network, args.contract_name)
}
fn handle_phantom_compile(
    project_path: &Path,
    args: &PhantomBaseArgs,
) -> Result<(), Box<dyn Error>> {
    let rindexer_yaml_path = project_path.join(YAML_CONFIG_NAME);
    let manifest = read_manifest(&rindexer_yaml_path).inspect_err(|e| {
        print_error_message(&format!("Could not read the rindexer.yaml file: {e}"))
    })?;
    if manifest.phantom.is_none() {
        let error_message =
            "phantom not setup in rindexer.yaml. Please run `rindexer phantom init` first.";
        print_error_message(error_message);
        return Err(error_message.into());
    }
    if !project_path.join("phantom").exists() {
        let error_message =
            "phantom folder not found in the project. Please run `rindexer phantom init` first.";
        print_error_message(error_message);
        return Err(error_message.into());
    }
    let network_path = project_path.join("phantom").join(&args.network);
    if !network_path.exists() {
        let error_message = format!("phantom network {} folder not found in the project. Please run `rindexer phantom clone` first.", args.network);
        print_error_message(&error_message);
        return Err(error_message.into());
    }
    let compile_in = network_path.join(args.contract_name.as_str());
    if !compile_in.exists() {
        let error_message = format!("phantom contract {} folder not found in the project. Please run `rindexer phantom clone` first.", args.contract_name);
        print_error_message(&error_message);
        return Err(error_message.into());
    }
    let contract = manifest.contracts.iter().find(|c| c.name == args.contract_name);
    match contract {
        Some(contract) => {
            let name = get_phantom_network_name(args);
            let network =
                manifest.networks.iter().find(|n| n.name == args.network || n.name == name);
            if network.is_none() {
                let error_message = format!("Network {} not found in rindexer.yaml", args.network);
                print_error_message(&error_message);
                return Err(error_message.into());
            }
            if network.unwrap().chain_id != 1 {
                let error_message = format!("Network {} is not supported", args.network);
                print_error_message(&error_message);
                return Err(error_message.into());
            }
            let contract_network =
                contract.details.iter().find(|c| c.network == args.network || c.network == name);
            if contract_network.is_some() {
                forge_compile_contract(&compile_in, network.unwrap(), &args.contract_name)
                    .map_err(|e| format!("Failed to compile contract: {e}"))?;
                print_success_message(format!("\ncompiled contract {} for network {} successful.\nYou can use `rindexer phantom deploy --contract-name {} --network {}` to deploy the phantom contract and start indexing your custom events.", args.contract_name, args.network, args.contract_name, args.network).as_str());
                Ok(())
            } else {
                let error_message = format!(
                    "Network {} not found in contract {} in rindexer.yaml",
                    args.network, args.contract_name
                );
                print_error_message(&error_message);
                Err(error_message.into())
            }
        }
        None => {
            let error_message =
                format!("Contract {} not found in rindexer.yaml", args.contract_name);
            print_error_message(&error_message);
            Err(error_message.into())
        }
    }
}
async fn handle_phantom_deploy(
    project_path: &Path,
    args: &PhantomBaseArgs,
) -> Result<(), Box<dyn Error>> {
    let rindexer_yaml_path = project_path.join(YAML_CONFIG_NAME);
    let mut manifest = read_manifest_raw(&rindexer_yaml_path).inspect_err(|e| {
        print_error_message(&format!("Could not read the rindexer.yaml file: {e}"))
    })?;
    if manifest.phantom.is_none() {
        let error_message =
            "phantom not setup in rindexer.yaml. Please run `rindexer phantom init` first.";
        print_error_message(error_message);
        return Err(error_message.into());
    }
    if !project_path.join("phantom").exists() {
        let error_message =
            "phantom folder not found in the project. Please run `rindexer phantom init` first.";
        print_error_message(error_message);
        return Err(error_message.into());
    }
    let network_path = project_path.join("phantom").join(&args.network);
    if !network_path.exists() {
        let error_message = format!("phantom network {} folder not found in the project. Please run `rindexer phantom clone` first.", args.network);
        print_error_message(&error_message);
        return Err(error_message.into());
    }
    let deploy_in = network_path.join(args.contract_name.as_str());
    if !deploy_in.exists() {
        let error_message = format!("phantom contract {} folder not found in the project. Please run `rindexer phantom clone` first.", args.contract_name);
        print_error_message(&error_message);
        return Err(error_message.into());
    }
    let contract = manifest.contracts.iter_mut().find(|c| c.name == args.contract_name);
    match contract {
        Some(contract) => {
            let name = get_phantom_network_name(args);
            let network =
                manifest.networks.iter().find(|n| n.name == args.network || n.name == name);
            if network.is_none() {
                let error_message = format!("Network {} not found in rindexer.yaml", args.network);
                print_error_message(&error_message);
                return Err(error_message.into());
            }
            if network.unwrap().chain_id != 1 {
                let error_message = format!("Network {} is not supported", args.network);
                print_error_message(&error_message);
                return Err(error_message.into());
            }
            let contract_network = contract
                .details
                .iter_mut()
                .find(|c| c.network == args.network || c.network == name);
            if let Some(contract_network) = contract_network {
                let clone_meta = read_contract_clone_metadata(&deploy_in)?;
                let phantom = manifest.phantom.as_ref().expect("Failed to get phantom");
                let rpc_url = if phantom.dyrpc_enabled() {
                    // only compile here as shadow has to do its own compiling to deploy
                    forge_compile_contract(&deploy_in, network.unwrap(), &args.contract_name)
                        .map_err(|e| format!("Failed to compile contract: {e}"))?;
                    deploy_dyrpc_contract(
                        &env::var(RINDEXER_PHANTOM_API_ENV_KEY)
                            .expect("Failed to get phantom api key"),
                        &clone_meta,
                        &read_compiled_contract(&deploy_in, &clone_meta)?,
                    )
                    .await
                    .map_err(|e| format!("Failed to deploy contract: {e}"))?
                } else {
                    println!("deploying shadow contracts, this may take a while....");
                    deploy_shadow_contract(
                        &env::var(RINDEXER_PHANTOM_API_ENV_KEY)
                            .expect("Failed to get phantom api key"),
                        &deploy_in,
                        &clone_meta,
                        phantom.shadow.as_ref().expect("Failed to get phantom shadow"),
                    )
                    .await
                    .map_err(|e| format!("Failed to deploy contract: {e}"))?
                };
                let network_index = manifest.networks.iter().position(|net| net.name == name);
                if let Some(index) = network_index {
                    let net = &mut manifest.networks[index];
                    net.rpc = rpc_url.to_string();
                } else {
                    manifest.networks.push(Network {
                        name: name.to_string(),
                        chain_id: network.unwrap().chain_id,
                        rpc: rpc_url.to_string(),
                        compute_units_per_second: None,
                        block_poll_frequency: None,
                        max_block_range: if phantom.dyrpc_enabled() {
                            Some(U64::from(20_000))
                        } else {
                            Some(U64::from(2_000))
                        },
                        disable_logs_bloom_checks: None,
                        get_logs_settings: None,
                        reth: None,
                    });
                }
                let compiled_contract = read_compiled_contract(&deploy_in, &clone_meta)?;
                let abi_path = project_path.join("abis").join(format!("{name}.abi.json"));
                write_file(
                    &abi_path,
                    serde_json::to_string_pretty(&compiled_contract.abi).unwrap().as_str(),
                )?;
                contract.abi = StringOrArray::Single(format!("./abis/{name}.abi.json"));
                contract_network.network = name;
                write_manifest(&manifest, &rindexer_yaml_path)?;
                print_success_message(format!("\ndeployed contract {} for network {} successful.\nYou can use `rindexer start all` to start indexing the phantom contract", args.contract_name, args.network).as_str());
                Ok(())
            } else {
                let error_message = format!(
                    "Network {} not found in contract {} in rindexer.yaml",
                    args.network, args.contract_name
                );
                print_error_message(&error_message);
                Err(error_message.into())
            }
        }
        None => {
            let error_message =
                format!("Contract {} not found in rindexer.yaml", args.contract_name);
            print_error_message(&error_message);
            Err(error_message.into())
        }
    }
}
</file>

<file path="cli/src/commands/start.rs">
use std::{env, path::PathBuf, process::Command, thread, time::Duration};
use rindexer::{
    manifest::{
        core::ProjectType,
        yaml::{read_manifest, YAML_CONFIG_NAME},
    },
    rindexer_error, rindexer_info, setup_info_logger, start_rindexer_no_code,
    GraphqlOverrideSettings, IndexerNoCodeDetails, PostgresClient, StartNoCodeDetails,
};
use crate::{
    cli_interface::StartSubcommands, console::print_error_message,
    rindexer_yaml::validate_rindexer_yaml_exist,
};
fn check_postgres_connection(conn_str: &str, max_retries: u32) -> Result<(), String> {
    let mut retries = 0;
    while retries < max_retries {
        let status = Command::new("pg_isready").args(["-d", conn_str]).output().map_err(|e| {
            let error = format!("Failed to check Postgres status: {e}");
            rindexer_error!(error);
            error
        })?;
        if status.status.success() {
            return Ok(());
        }
        retries += 1;
        thread::sleep(Duration::from_millis(500));
        rindexer_info!(
            "Waiting for Postgres to become available this may take a few attempts... attempt: {}",
            retries
        );
    }
    Err("Postgres did not become available within the given retries.".into())
}
fn check_docker_compose_status(project_path: &PathBuf, max_retries: u32) -> Result<(), String> {
    let mut retries = 0;
    while retries < max_retries {
        let ps_status = Command::new("docker")
            .args(["compose", "ps"])
            .current_dir(project_path)
            .output()
            .map_err(|e| {
                let error = format!("Failed to check docker compose status: {e}");
                print_error_message(&error);
                error
            })?;
        if ps_status.status.success() {
            let output = String::from_utf8_lossy(&ps_status.stdout);
            if !output.contains("Exit") && output.contains("Up") {
                rindexer_info!("All containers are up and running.");
                return if let Ok(conn_str) = env::var("DATABASE_URL") {
                    check_postgres_connection(&conn_str, max_retries).map_err(|e| {
                        let error = format!("Failed to connect to PostgresSQL: {e}");
                        rindexer_error!(error);
                        error
                    })
                } else {
                    let error = "DATABASE_URL not set.".to_string();
                    rindexer_error!(error);
                    Err(error)
                };
            }
        } else {
            let error = format!("docker compose ps exited with status: {}", ps_status.status);
            rindexer_error!(error);
        }
        retries += 1;
        thread::sleep(Duration::from_millis(200));
        rindexer_info!("Waiting for docker compose containers to start...");
    }
    Err("Docker containers did not start successfully within the given retries.".into())
}
fn start_docker_compose(project_path: &PathBuf) -> Result<(), String> {
    if !project_path.exists() {
        return Err(format!("Project path does not exist: {project_path:?}"));
    }
    let status = Command::new("docker")
        .args(["compose", "up", "-d"])
        .current_dir(project_path)
        .stdout(std::process::Stdio::null())
        .stderr(std::process::Stdio::null())
        .status()
        .map_err(|e| {
            let error = format!("Docker command could not be executed make sure docker is running on the machine: {e}");
            print_error_message(&error);
            error
        })?;
    if !status.success() {
        let error = "Docker compose could not startup the postgres container, please make sure docker is running on the machine".to_string();
        rindexer_error!(error);
        return Err(error);
    }
    rindexer_info!("Docker starting up the postgres container..");
    check_docker_compose_status(project_path, 200)
}
pub async fn start(
    project_path: PathBuf,
    command: &StartSubcommands,
) -> Result<(), Box<dyn std::error::Error>> {
    setup_info_logger();
    validate_rindexer_yaml_exist(&project_path);
    let manifest = read_manifest(&project_path.join(YAML_CONFIG_NAME)).map_err(|e| {
        print_error_message(&format!("Could not read the rindexer.yaml file: {e}"));
        e
    })?;
    if manifest.storage.postgres_enabled() {
        let client = PostgresClient::new().await;
        if client.is_err() {
            // find if docker-compose.yml is present in parent
            let docker_compose_path = project_path.join("docker-compose.yml");
            if !docker_compose_path.exists() {
                return Err(
                    "The DATABASE_URL mapped is not running please make sure it is correct".into(),
                );
            }
            match start_docker_compose(&project_path) {
                Ok(_) => {
                    rindexer_info!("Docker postgres containers started up successfully");
                }
                Err(e) => {
                    return Err(e.into());
                }
            }
            // print_error_message("Could not connect to the postgres database, please make sure it
            // is running. If running locally you can run docker compose up -d");
        }
    }
    match manifest.project_type {
        ProjectType::Rust => {
            let project_cargo_manifest_path = project_path.join("Cargo.toml");
            let status = Command::new("cargo")
                .arg("run")
                .arg("--manifest-path")
                .arg(project_cargo_manifest_path)
                .arg(match command {
                    StartSubcommands::Indexer => "-- --indexer".to_string(),
                    StartSubcommands::Graphql { port } => match port {
                        Some(port) => format!("-- --graphql --port={port}"),
                        None => "-- --graphql".to_string(),
                    },
                    StartSubcommands::All { port } => match port {
                        Some(port) => format!("-- --graphql --indexer --port={port}"),
                        None => "-- --graphql --indexer".to_string(),
                    },
                })
                .status()
                .expect("Failed to execute cargo run.");
            if !status.success() {
                panic!("cargo run failed with status: {status:?}");
            }
        }
        ProjectType::NoCode => match command {
            StartSubcommands::Indexer => {
                let details = StartNoCodeDetails {
                    manifest_path: &project_path.join(YAML_CONFIG_NAME),
                    indexing_details: IndexerNoCodeDetails { enabled: true },
                    graphql_details: GraphqlOverrideSettings {
                        enabled: false,
                        override_port: None,
                    },
                };
                start_rindexer_no_code(details).await.map_err(|e| {
                    print_error_message(&format!("Error starting the server: {e}"));
                    e
                })?;
            }
            StartSubcommands::Graphql { port } => {
                let details = StartNoCodeDetails {
                    manifest_path: &project_path.join(YAML_CONFIG_NAME),
                    indexing_details: IndexerNoCodeDetails { enabled: false },
                    graphql_details: GraphqlOverrideSettings {
                        enabled: true,
                        override_port: port.as_ref().and_then(|port| port.parse().ok()),
                    },
                };
                start_rindexer_no_code(details).await.map_err(|e| {
                    print_error_message(&format!("Error starting the indexer: {e}"));
                    e
                })?;
            }
            StartSubcommands::All { port } => {
                let details = StartNoCodeDetails {
                    manifest_path: &project_path.join(YAML_CONFIG_NAME),
                    indexing_details: IndexerNoCodeDetails { enabled: true },
                    graphql_details: GraphqlOverrideSettings {
                        enabled: true,
                        override_port: port.as_ref().and_then(|port| port.parse().ok()),
                    },
                };
                let _ = start_rindexer_no_code(details).await.map_err(|e| {
                    print_error_message(&format!("Error starting the server: {e}"));
                });
            }
        },
    }
    Ok(())
}
</file>

<file path="cli/src/cli_interface.rs">
use clap::{Args, Parser, Subcommand};
#[allow(clippy::upper_case_acronyms)]
#[derive(Parser, Debug)]
#[clap(name = "rindexer", about, version, author = "Your Name")]
pub struct CLI {
    #[clap(subcommand)]
    pub command: Commands,
}
#[derive(Parser, Debug)]
pub struct NewDetails {
    #[clap(short, long)]
    name: Option<String>,
    #[clap(short, long)]
    project_description: Option<String>,
    #[clap(short, long)]
    repository: Option<String>,
    #[clap(short, long)]
    database: Option<bool>,
}
#[derive(Args, Debug, Clone)]
pub struct RethArgs {
    /// optional - Enable Reth support
    /// Example: rindexer new no-code --reth -- --data-dir /path --http true
    #[clap(long, short)]
    pub reth: bool,
    /// Additional arguments to pass to reth when --reth is enabled
    /// These should be provided after -- e.g. -- --data-dir /path --http true
    #[clap(last = true)]
    pub reth_args: Vec<String>,
}
#[derive(Parser, Debug)]
#[clap(author = "Josh Stevens", version = "1.0", about = "Blazing fast EVM indexing tool built in rust", long_about = None)]
pub enum Commands {
    /// Creates a new rindexer no-code project or rust project.
    ///
    /// no-code = Best choice when starting, no extra code required.
    /// rust = Customise advanced indexer by writing rust code.
    ///
    /// This command initialises a new workspace project with rindexer
    /// with everything populated to start using rindexer.
    ///
    /// Example:
    /// `rindexer new no-code` or `rindexer new rust`
    #[clap(name = "new")]
    New {
        #[clap(subcommand)]
        subcommand: NewSubcommands,
        /// optional - The path to create the project in, default will be where the command is run.
        #[clap(long, short)]
        path: Option<String>,
    },
    /// Start various services like indexers, GraphQL APIs or both together
    ///
    /// `rindexer start indexer` or `rindexer start graphql` or `rindexer start all`
    #[clap(name = "start")]
    Start {
        #[clap(subcommand)]
        subcommand: StartSubcommands,
        /// optional - The path to run the command in, default will be where the command is run.
        #[clap(long, short)]
        path: Option<String>,
    },
    /// Add elements such as contracts to the rindexer.yaml file.
    ///
    /// This command helps you build up your yaml file.
    ///
    /// Example:
    /// `rindexer add`
    #[clap(name = "add")]
    Add {
        #[clap(subcommand)]
        subcommand: AddSubcommands,
        /// optional - The path to run the command in, default will be where the command is run.
        #[clap(long, short)]
        path: Option<String>,
    },
    /// Generates rust code based on rindexer.yaml or graphql queries
    ///
    /// Example:
    /// `rindexer codegen typings` or `rindexer codegen handlers` or `rindexer codegen graphql
    /// --endpoint=graphql_api` or `rindexer codegen rust-all`
    #[clap(name = "codegen")]
    Codegen {
        #[clap(subcommand)]
        subcommand: CodegenSubcommands,
        /// optional - The path to run the command in, default will be where the command is run.
        #[clap(long, short)]
        path: Option<String>,
    },
    /// Delete data from the postgres database or csv files.
    ///
    /// This command deletes rindexer project data from the postgres database or csv files.
    ///
    /// Example:
    /// `rindexer delete`
    Delete {
        /// optional - The path to run the command in, default will be where the command is run.
        #[clap(long, short)]
        path: Option<String>,
    },
    /// Use phantom events to add your own events to contracts
    ///
    /// This command helps you use phantom events within rindexer.
    ///
    /// Example:
    /// `rindexer phantom init` or
    /// `rindexer phantom clone --contract-name <CONTRACT_NAME> --network <NETWORK>` or
    /// `rindexer phantom compile --contract-name <CONTRACT_NAME> --network <NETWORK>` or
    /// `rindexer phantom deploy --contract-name <CONTRACT_NAME> --network <NETWORK>`
    #[clap(name = "phantom")]
    Phantom {
        #[clap(subcommand)]
        subcommand: PhantomSubcommands,
        /// optional - The path to create the project in, default will be where the command is run.
        #[clap(long, short)]
        path: Option<String>,
    },
}
#[derive(Subcommand, Debug)]
pub enum NewSubcommands {
    /// Creates a new no-code project
    ///
    /// Best choice when starting, no extra code required.
    /// Example:
    /// `rindexer new no-code`
    /// `rindexer new no-code --reth -- --data-dir /path --http true`
    #[clap(name = "no-code")]
    NoCode {
        #[clap(flatten)]
        reth: RethArgs,
    },
    /// Creates a new rust project
    ///
    /// Customise advanced indexer by writing rust code
    /// Example:
    /// `rindexer new rust`
    /// `rindexer new rust --reth -- --data-dir /path --http true`
    #[clap(name = "rust")]
    Rust {
        #[clap(flatten)]
        reth: RethArgs,
    },
}
#[derive(Subcommand, Debug)]
pub enum StartSubcommands {
    /// Starts the indexing service based on the rindexer.yaml file.
    ///
    /// Starts an indexer based on the rindexer.yaml file.
    ///
    /// Example:
    /// `rindexer start indexer`
    Indexer,
    /// Starts the GraphQL server based on the rindexer.yaml file.
    ///
    /// Optionally specify a port to override the default.
    ///
    /// Example:
    /// `rindexer start graphql --port 4000`
    Graphql {
        #[clap(short, long, help = "Specify the port number for the GraphQL server")]
        port: Option<String>,
    },
    /// Starts the indexers and the GraphQL together based on the rindexer.yaml file.
    ///
    /// You can specify a port which will be used by all services that require one.
    ///
    /// Example:
    /// `rindexer start all --port 3000`
    All {
        #[clap(short, long, help = "Specify the port number for all services")]
        port: Option<String>,
    },
}
#[derive(Subcommand, Debug)]
pub enum AddSubcommands {
    /// Add a contract from a network to the rindexer.yaml file. It will download the ABI and add
    /// it to the abis folder and map it in the yaml file.
    ///
    /// Example:
    /// `rindexer add contract`
    Contract,
}
#[derive(Subcommand, Debug)]
pub enum CodegenSubcommands {
    /// Generates the rindexer rust typings based on the rindexer.yaml file.
    ///
    /// This should not be edited manually and always generated.
    ///
    /// This is not relevant for no-code projects.
    ///
    /// Example:
    /// `rindexer codegen typings`
    Typings,
    /// Generates the rindexer rust indexers handlers based on the rindexer.yaml file.
    ///
    /// You can use these as the foundations to build your advanced indexers.
    ///
    /// This is not relevant for no-code projects.
    ///
    /// Example:
    /// `rindexer codegen indexer`
    Indexer,
    /// Generates the GraphQL queries from a GraphQL schema
    ///
    /// You can then use this in your dApp instantly to interact with the GraphQL API
    ///
    /// Example:
    /// `rindexer codegen graphql`
    #[clap(name = "graphql")]
    GraphQL {
        #[clap(long, help = "The graphql endpoint - defaults to localhost:3001")]
        endpoint: Option<String>,
    },
}
#[derive(Args, Debug)]
pub struct PhantomBaseArgs {
    /// The name of the contract
    #[clap(value_parser)]
    pub contract_name: String,
    /// The network the contract is on
    #[clap(value_parser)]
    pub network: String,
}
#[derive(Subcommand, Debug)]
pub enum PhantomSubcommands {
    /// Sets up phantom events on rindexer
    ///
    /// Want to add your own custom events to contracts? This command will help you do that.
    ///
    /// Example:
    /// `rindexer phantom init`
    #[clap(name = "init")]
    Init,
    /// Clone the contract with the network you wish to add phantom events to.
    ///
    /// Note contract name and network are your values in your rindexer.yaml file.
    ///
    /// Example:
    /// `rindexer phantom clone --contract-name <CONTRACT_NAME> --network <NETWORK>`
    #[clap(name = "clone")]
    Clone {
        /// The name of the contract to clone
        #[arg(long)]
        contract_name: String,
        /// The network
        #[arg(long)]
        network: String,
    },
    /// Compiles the phantom contract
    ///
    /// Note contract name and network are your values in your rindexer.yaml file.
    ///
    /// Example:
    /// `rindexer phantom compile --contract-name <CONTRACT_NAME> --network <NETWORK>`
    #[clap(name = "compile")]
    Compile {
        /// The name of the contract to clone
        #[arg(long)]
        contract_name: String,
        /// The network
        #[arg(long)]
        network: String,
    },
    /// Deploy the modified phantom contract
    ///
    /// This will compile and update your rindexer project with the phantom events.
    ///
    /// Example:
    /// `rindexer phantom deploy --contract-name <CONTRACT_NAME> --network <NETWORK>`
    #[clap(name = "deploy")]
    Deploy {
        /// The name of the contract to clone
        #[arg(long)]
        contract_name: String,
        /// The network
        #[arg(long)]
        network: String,
    },
}
</file>

<file path="cli/src/console.rs">
use std::{io, io::Write, str::FromStr};
use colored::Colorize;
use regex::Regex;
pub fn print_error_message(error_message: &str) {
    println!("{}", error_message.red());
}
pub fn print_warn_message(error_message: &str) {
    println!("{}", error_message.yellow());
}
pub fn print_success_message(success_message: &str) {
    println!("{}", success_message.green());
}
pub fn prompt_for_optional_input<T: FromStr>(prompt: &str, pattern: Option<&str>) -> Option<T> {
    let regex = pattern.map(|p| Regex::new(p).expect("Invalid regex pattern"));
    loop {
        print!("{} (skip by pressing Enter): ", prompt.yellow());
        io::stdout().flush().expect("Failed to flush stdout");
        let mut input = String::new();
        io::stdin().read_line(&mut input).expect("Failed to read line");
        let trimmed = input.trim();
        if trimmed.is_empty() {
            return None;
        }
        if let Some(ref regex) = regex {
            if regex.is_match(trimmed) {
                match trimmed.parse::<T>() {
                    Ok(value) => return Some(value),
                    Err(_) => println!(
                        "{}",
                        "Invalid format. Please try again or press Enter to skip.".red()
                    ),
                }
            } else {
                println!(
                    "{}",
                    "Invalid input according to regex. Please try again or press Enter to skip."
                        .red()
                );
            }
        } else {
            match trimmed.parse::<T>() {
                Ok(value) => return Some(value),
                Err(_) => println!("{}", "Invalid format. Please try again.".red()),
            }
        }
    }
}
pub fn prompt_for_input_list(
    field_name: &str,
    options: &[String],
    current_value: Option<&str>,
) -> String {
    let options_str = options.join(", ");
    if let Some(value) = current_value {
        return value.to_string();
    }
    loop {
        print!("{} [{}]: ", field_name.to_string().green(), options_str.yellow());
        io::stdout().flush().expect("Failed to flush stdout");
        let mut input = String::new();
        io::stdin().read_line(&mut input).expect("Failed to read line");
        let trimmed = input.trim().to_lowercase();
        if options.contains(&trimmed) {
            return trimmed;
        } else {
            println!(
                "{}",
                format!("Invalid option. Please choose one of the following: {options_str}").red()
            );
        }
    }
}
pub fn prompt_for_input(
    field_name: &str,
    pattern: Option<&str>,
    pattern_failure_message: Option<&str>,
    current_value: Option<&str>,
) -> String {
    let regex = pattern.map(|p| Regex::new(p).expect("Invalid regex pattern"));
    match current_value {
        Some(value) => value.to_string(),
        None => loop {
            print!("{}: ", field_name.yellow());
            io::stdout().flush().expect("Failed to flush stdout");
            let mut input = String::new();
            io::stdin().read_line(&mut input).expect("Failed to read line");
            let trimmed = input.trim();
            if let Some(ref regex) = regex {
                if regex.is_match(trimmed) {
                    return trimmed.to_string();
                } else {
                    let message = pattern_failure_message
                        .unwrap_or("Invalid input according to regex. Please try again.");
                    println!("{}", message.red());
                }
            } else if !trimmed.is_empty() {
                return trimmed.to_string();
            } else {
                println!("{}", "Input cannot be empty. Please try again.".red());
            }
        },
    }
}
</file>

<file path="cli/src/main.rs">
use std::env;
#[cfg(feature = "jemalloc")]
use jemallocator::Jemalloc;
#[cfg(feature = "jemalloc")]
#[global_allocator]
static GLOBAL: Jemalloc = Jemalloc;
mod cli_interface;
mod commands;
mod console;
mod rindexer_yaml;
use std::{path::PathBuf, str::FromStr};
use clap::Parser;
use rindexer::{load_env_from_project_path, manifest::core::ProjectType};
#[cfg(feature = "reth")]
use rindexer::manifest::reth::RethConfig;
use crate::{
    cli_interface::{AddSubcommands, Commands, NewSubcommands, CLI},
    commands::{
        add::handle_add_contract_command, codegen::handle_codegen_command,
        delete::handle_delete_command, new::handle_new_command, phantom::handle_phantom_commands,
        start::start,
    },
    console::print_error_message,
};
fn resolve_path(override_path: &Option<String>) -> Result<PathBuf, String> {
    match override_path {
        Some(path) => {
            let path = PathBuf::from_str(path).map_err(|_| "Invalid path provided.".to_string())?;
            Ok(path)
        }
        None => Ok(env::current_dir().map_err(|_| "Failed to get current directory.".to_string())?),
    }
}
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let cli = CLI::parse();
    match &cli.command {
        Commands::New { subcommand, path } => {
            let resolved_path = resolve_path(path).inspect_err(|e| print_error_message(e))?;
            load_env_from_project_path(&resolved_path);
            #[allow(unused_variables)]
            let (project_type, reth_args) = match subcommand {
                NewSubcommands::NoCode { reth } => (ProjectType::NoCode, reth),
                NewSubcommands::Rust { reth } => (ProjectType::Rust, reth),
            };
            #[cfg(feature = "reth")]
            let reth_config = if reth_args.reth {
                match RethConfig::from_cli_args(reth_args.reth_args.clone()) {
                    Ok(config) => Some(config),
                    Err(e) => {
                        print_error_message(&format!("Invalid reth arguments: {e}"));
                        return Err(e.into());
                    }
                }
            } else {
                None
            };
            #[cfg(not(feature = "reth"))]
            let reth_config = None;
            handle_new_command(resolved_path, project_type, reth_config)
        }
        Commands::Add { subcommand, path } => {
            let resolved_path = resolve_path(path).inspect_err(|e| print_error_message(e))?;
            load_env_from_project_path(&resolved_path);
            match subcommand {
                AddSubcommands::Contract => handle_add_contract_command(resolved_path).await,
            }
        }
        Commands::Codegen { subcommand, path } => {
            let resolved_path = resolve_path(path).inspect_err(|e| print_error_message(e))?;
            load_env_from_project_path(&resolved_path);
            handle_codegen_command(resolved_path, subcommand).await
        }
        Commands::Start { subcommand, path } => {
            let resolved_path = resolve_path(path).inspect_err(|e| print_error_message(e))?;
            load_env_from_project_path(&resolved_path);
            start(resolved_path, subcommand).await
        }
        Commands::Delete { path } => {
            let resolved_path = resolve_path(path).inspect_err(|e| print_error_message(e))?;
            load_env_from_project_path(&resolved_path);
            handle_delete_command(resolved_path).await
        }
        Commands::Phantom { subcommand, path } => {
            let resolved_path = resolve_path(path).inspect_err(|e| print_error_message(e))?;
            load_env_from_project_path(&resolved_path);
            handle_phantom_commands(resolved_path, subcommand).await
        }
    }
}
</file>

<file path="cli/src/rindexer_yaml.rs">
use std::{fs, path::Path};
use rindexer::manifest::yaml::YAML_CONFIG_NAME;
use crate::console::print_error_message;
pub fn rindexer_yaml_exists(project_path: &Path) -> bool {
    fs::metadata(project_path.join(YAML_CONFIG_NAME)).is_ok()
}
pub fn rindexer_yaml_does_not_exist(project_path: &Path) -> bool {
    !rindexer_yaml_exists(project_path)
}
pub fn validate_rindexer_yaml_exist(project_path: &Path) {
    if rindexer_yaml_does_not_exist(project_path) {
        print_error_message("rindexer.yaml does not exist in the current directory. Please use rindexer new to create a new project.");
        std::process::exit(1);
    }
}
</file>

<file path="cli/Cargo.toml">
[package]
name = "rindexer_cli"
version = "0.29.0"
edition = "2021"
description = "A no-code or framework to build blazing fast EVM indexers - built in rust."
license = "MIT"
repository = "https://github.com/joshstevens19/rindexer"
readme = "README.md"
resolver = "2"

[dependencies]
# internal dependencies
rindexer = { path = "../core" }

# external dependencies
alloy = { version = "1.1.3", features = ["full"] }
alloy-chains = "0.2.15"# pegged to version in "foundry-block-explorers"
foundry-block-explorers = "0.22.0"
clap = { version = "4.5.50", features = ["derive"] }
regex = "1.12.2"
colored = "3.0.0"
tokio = "1.48.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0.145"

# build
jemallocator = { version = "0.6.1", package = "tikv-jemallocator", optional = true }
jemalloc-ctl = { version = "0.6.1", package = "tikv-jemalloc-ctl", optional = true }

[profile.release]
lto = "fat"
codegen-units = 1
incremental = false

[features]
default = ["kafka"]
jemalloc = ["dep:jemallocator", "dep:jemalloc-ctl"]
reth = ["rindexer/reth"]
kafka = ["rindexer/kafka"]
</file>

<file path="cli/Makefile">
prod_build:
	RUSTFLAGS='-C target-cpu=native' cargo build --release --features jemalloc,reth
new_no_code:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- new --path $(CURDIR)/../examples no-code
new_rust:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- new --path $(CURDIR)/../examples rust
start_indexer:
	RUSTFLAGS='-C target-cpu=native' RUST_BACKTRACE='full' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/rindexer_demo_cli indexer
start_all:
	RUSTFLAGS='-C target-cpu=native' RUST_BACKTRACE='full' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/rindexer_demo_cli all
start_graphql:
	RUSTFLAGS='-C target-cpu=native' RUST_BACKTRACE='full' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/rindexer_demo_cli graphql
playground_codegen_typings:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- codegen --path $(CURDIR)/../examples/rindexer_rust_playground typings
playground_codegen_indexer:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- codegen --path $(CURDIR)/../examples/rindexer_rust_playground indexer
codegen_graphql:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- codegen --path $(CURDIR)/../examples/rindexer_demo_cli graphql --endpoint  http://0.0.0.0:5005/graphql
add_contract:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- add --path $(CURDIR)/../examples/rindexer_demo_cli contract
delete:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- delete --path $(CURDIR)/../examples/rindexer_demo_cli
phantom_init:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- phantom --path $(CURDIR)/../examples/rindexer_demo_cli init
phantom_clone:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- phantom --path $(CURDIR)/../examples/rindexer_demo_cli clone --contract-name RocketPoolETH --network ethereum
phantom_compile:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- phantom --path $(CURDIR)/../examples/rindexer_demo_cli compile --contract-name RocketPoolETH --network ethereum
phantom_deploy:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- phantom --path $(CURDIR)/../examples/rindexer_demo_cli deploy --contract-name RocketPoolETH --network ethereum
help:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- phantom --help


################################################################################
# FACTORY INDEXING EXAMPLE COMMANDS
################################################################################
factory_indexing_codegen_typings:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- codegen --path $(CURDIR)/../examples/rindexer_factory_indexing typings
factory_indexing_codegen_indexer:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- codegen --path $(CURDIR)/../examples/rindexer_factory_indexing indexer
factory_indexing_codegen_graphql:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- codegen --path $(CURDIR)/../examples/rindexer_factory_indexing graphql

################################################################################
# LOCAL NONE CHECKED IN PROJECT COMMANDS
################################################################################
start_indexer_base_paint:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/base_paint indexer
start_graphql_base_paint:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/base_paint graphql
start_indexer_lens_mirrors:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/lens_mirrors indexer
start_uniswap:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/uniswap_v3_factory all
start_sophon:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/sophon_testnet_contract_deploy all
start_indexer_issue:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- start --path $(CURDIR)/../../rindexer-issue/smallIndexer indexer
codegen_typings_lens:
	cargo run -- codegen --path $(CURDIR)/../../lens-backend/crates/lens-indexer typings
codegen_indexer_lens:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- codegen --path $(CURDIR)/../../lens-backend/crates/indexer indexer
start_uniswap_base:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/uniswap_v3_base indexer
start_indexer_redis_stream:
    RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/redis-stream indexer
start_indexer_hyperwarp:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/hyperwarp indexer
start_factory_indexing:
	RUSTFLAGS='-C target-cpu=native' RUST_BACKTRACE='full' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/rindexer_factory_indexing all
hyperwarp_codegen_typings:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- codegen --path $(CURDIR)/../examples/hyperevm typings
hyperwarp_codegen_indexer:
	RUSTFLAGS='-C target-cpu=native' cargo run --release --features jemalloc -- codegen --path $(CURDIR)/../examples/hyperevm indexer
start_cloudflare_indexer:
	RUSTFLAGS='-C target-cpu=native' RUST_BACKTRACE='full' cargo run --release --features jemalloc -- start --path $(CURDIR)/../examples/rindexer_cloudflare_queues indexer
</file>

<file path="cli/README.md">
# rindexer_cli

This is the cli for rindexer, it contains all the logic for the cli and is how users interact with rindexer.

You can get to the full rindexer [documentation](https://rindexer.xyz/docs/introduction/installation).

## Install

```bash
curl -L https://rindexer.xyz/install.sh | bash
```

If you’re on Windows, you will need to install and use Git BASH or WSL, as your terminal,
since rindexer installation does not support Powershell or Cmd.

## Use rindexer

Once installed you can run `rindexer --help` in your terminal to see all the commands available to you.

```bash
rindexer --help
```

```bash
Blazing fast EVM indexing tool built in rust

Usage: rindexer [COMMAND]

Commands:
  new           Creates a new rindexer no-code project or rust project
  start         Start various services like indexers, GraphQL APIs or both together
  add           Add elements such as contracts to the rindexer.yaml file
  codegen       Generates rust code based on rindexer.yaml or graphql queries
  delete        Delete data from the postgres database or csv files
  phantom       Use phantom events to add your own events to contracts
  help          Print this message or the help of the given subcommand(s)

Options:
  -h, --help     Print help
  -V, --version  Print version
```

## Working with CLI locally

The best way to work with the CLI is to use the `Makefile` predefined commands.

You can also run your own commands using cargo run, example below would create a new no-code project in the path you specified.

```bash
cargo run -- new --path PATH_TO_CREATE_PROJECT no-code
```
</file>

<file path="core/src/api/generate_operations.rs">
use std::{
    fs::{self, File},
    io::Write,
    path::Path,
};
use serde_json::Value;
#[derive(thiserror::Error, Debug)]
pub enum GenerateOperationsError {
    #[error("File system error: {0}")]
    Io(#[from] std::io::Error),
    #[error("Schema generation error: {0}")]
    SchemaGeneration(String),
}
fn generate_query(name: &str, fields: &[String]) -> String {
    let base_name = name.trim_start_matches("all");
    let condition_type = format!("{}Condition", &base_name[..base_name.len() - 1]);
    let order_by_type = format!("{base_name}OrderBy");
    // yes it is meant to be formatted like the below to make the graphql query readable
    let args = if name.starts_with("all") {
        format!(
            r#"$after: Cursor,
    $first: Int = 50,
    $condition: {condition_type} = {{}},
    $orderBy: [{order_by_type}!] = BLOCK_NUMBER_DESC"#
        )
    } else {
        "$nodeId: ID!".to_string()
    };
    // yes it is meant to be formatted like the below to make the graphql query readable
    if name.starts_with("all") {
        format!(
            r#"query {}Query(
    {}
) {{
    {}(
        first: $first,
        after: $after,
        condition: $condition,
        orderBy: $orderBy
    ) {{
        nodes {{
            {}
        }}
        pageInfo {{
            endCursor
            hasNextPage
            hasPreviousPage
            startCursor
        }}
    }}
}}"#,
            name,
            args,
            name,
            fields.join("\n            ")
        )
    } else {
        format!(
            r#"query {}Query({}) {{
    {}(nodeId: $nodeId) {{
        {}
    }}
}}"#,
            name,
            args,
            name,
            fields.join("\n        ")
        )
    }
}
fn extract_node_fields(singular_type_name: &str, schema: &Value) -> Vec<String> {
    if let Some(types) = schema["types"].as_array() {
        for type_obj in types {
            if let Some(type_name) = type_obj["name"].as_str() {
                if type_name.eq_ignore_ascii_case(singular_type_name) {
                    if let Some(fields) = type_obj["fields"].as_array() {
                        return fields
                            .iter()
                            .filter_map(|field| field["name"].as_str().map(|s| s.to_string()))
                            .collect();
                    }
                }
            }
        }
    }
    vec![]
}
pub fn generate_operations(
    schema: &Value,
    generate_path: &Path,
) -> Result<(), GenerateOperationsError> {
    let queries_path = generate_path.join("queries");
    fs::create_dir_all(&queries_path)?;
    let types = schema["types"].as_array().ok_or_else(|| {
        GenerateOperationsError::SchemaGeneration("Invalid schema format".to_string())
    })?;
    for type_obj in types {
        if let Some(type_name) = type_obj["name"].as_str() {
            if type_name == "Query" {
                let fields = type_obj["fields"].as_array().ok_or_else(|| {
                    GenerateOperationsError::SchemaGeneration("Invalid fields format".to_string())
                })?;
                for field in fields {
                    if let Some(field_name) = field["name"].as_str() {
                        let is_paged_query = field_name.starts_with("all");
                        let mut singular_type_name = field_name.trim_start_matches("all");
                        if is_paged_query {
                            singular_type_name =
                                &singular_type_name[..singular_type_name.len() - 1];
                        }
                        let node_fields = extract_node_fields(singular_type_name, schema);
                        if node_fields.is_empty() {
                            continue;
                        }
                        let query = generate_query(field_name, &node_fields);
                        let file_path = queries_path.join(format!("{field_name}.graphql"));
                        let mut file = File::create(file_path)?;
                        file.write_all(query.as_bytes())?;
                    }
                }
            }
        }
    }
    Ok(())
}
#[cfg(test)]
mod tests {
    use serde_json::json;
    use super::*;
    #[test]
    fn test_generate_query_single() {
        let query = generate_query("node", &["id".to_string(), "name".to_string()]);
        let expected = r#"query nodeQuery($nodeId: ID!) {
    node(nodeId: $nodeId) {
        id
        name
    }
}"#;
        assert_eq!(query, expected);
    }
    #[test]
    fn test_extract_node_fields() {
        let schema = json!({
            "types": [
                {
                    "name": "Node",
                    "fields": [
                        {"name": "id"},
                        {"name": "name"}
                    ]
                }
            ]
        });
        let fields = extract_node_fields("Node", &schema);
        assert_eq!(fields, vec!["id".to_string(), "name".to_string()]);
    }
    #[test]
    fn test_extract_node_fields_case_insensitive() {
        let schema = json!({
            "types": [
                {
                    "name": "node",
                    "fields": [
                        {"name": "id"},
                        {"name": "name"}
                    ]
                }
            ]
        });
        let fields = extract_node_fields("Node", &schema);
        assert_eq!(fields, vec!["id".to_string(), "name".to_string()]);
    }
    #[test]
    fn test_extract_node_fields_not_found() {
        let schema = json!({
            "types": [
                {
                    "name": "Node",
                    "fields": [
                        {"name": "id"},
                        {"name": "name"}
                    ]
                }
            ]
        });
        let fields = extract_node_fields("NonExistentNode", &schema);
        assert!(fields.is_empty());
    }
}
</file>

<file path="core/src/api/generate_schema.rs">
use std::path::Path;
use crate::api::generate_operations::{generate_operations, GenerateOperationsError};
use reqwest::Client;
use serde_json::Value;
#[derive(thiserror::Error, Debug)]
pub enum GenerateGraphqlQueriesError {
    #[error("Network request failed: {0}")]
    Network(#[from] reqwest::Error),
    #[error("No data in response")]
    NoData,
    #[error("Invalid response. Make sure that {0} can receive GraphQL introspection query.")]
    InvalidData(String),
    #[error("Failed to generate operations: {0}")]
    GenerateOperationsError(#[from] GenerateOperationsError),
}
pub async fn generate_graphql_queries(
    endpoint: &str,
    generate_path: &Path,
) -> Result<(), GenerateGraphqlQueriesError> {
    let client = Client::new();
    let introspection_query = r#"
    {
      __schema {
        types {
          name
          fields {
            name
            args {
              name
              type {
                name
                kind
                ofType {
                  name
                  kind
                  ofType {
                    name
                    kind
                    ofType {
                      name
                      kind
                    }
                  }
                }
              }
            }
            type {
              name
              kind
              ofType {
                name
                kind
                fields {
                  name
                  type {
                    name
                    kind
                    ofType {
                      name
                      kind
                      ofType {
                        name
                        kind
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
    "#;
    let res = client
        .post(endpoint)
        .json(&serde_json::json!({ "query": introspection_query }))
        .send()
        .await?
        .json::<Value>()
        .await
        .map_err(|_| GenerateGraphqlQueriesError::InvalidData(endpoint.to_string()))?;
    let schema = res["data"]["__schema"].clone();
    if schema.is_null() {
        return Err(GenerateGraphqlQueriesError::NoData);
    }
    generate_operations(&schema, generate_path)?;
    Ok(())
}
#[cfg(test)]
mod tests {
    use tempfile::tempdir;
    use tokio::runtime::Runtime;
    use super::*;
    #[test]
    fn test_generate_graphql_queries_no_data() {
        let mut server = mockito::Server::new();
        server
            .mock("POST", "/")
            .with_status(200)
            .with_header("content-type", "application/json")
            .with_body(r#"{"data": {}}"#)
            .create();
        let dir = tempdir().unwrap();
        let generate_path = dir.path();
        let rt = Runtime::new().unwrap();
        let result = rt.block_on(generate_graphql_queries(&server.url(), generate_path));
        assert!(matches!(result, Err(GenerateGraphqlQueriesError::NoData)));
    }
}
</file>

<file path="core/src/api/graphql.rs">
use std::{
    env, fs,
    path::{Path, PathBuf},
    process::{Child, Command, Stdio},
    sync::{
        atomic::{AtomicBool, Ordering},
        Arc, Mutex,
    },
    time::Duration,
};
use reqwest::{Client, Error};
use serde_json::{json, Value};
use tokio::sync::{oneshot, oneshot::Sender};
use tracing::{error, info};
use crate::{
    database::{
        generate::generate_indexer_contract_schema_name, postgres::client::connection_string,
    },
    helpers::{kill_process_on_port, set_thread_no_logging},
    indexer::Indexer,
    manifest::graphql::GraphQLSettings,
};
pub struct GraphqlOverrideSettings {
    pub enabled: bool,
    pub override_port: Option<u16>,
}
fn get_graphql_exe() -> Result<PathBuf, Box<dyn std::error::Error>> {
    // Try the build-time path first (for development)
    let build_path = PathBuf::from(env!("RINDEXER_GRAPHQL_EXE"));
    if build_path.exists() {
        return Ok(build_path);
    }
    // Fall back to embedded binary (for deployed/installed versions)
    const GRAPHQL_BINARY: &[u8] = include_bytes!(env!("RINDEXER_GRAPHQL_EMBED"));
    let temp_dir = std::env::temp_dir();
    let exe_name = if cfg!(windows) {
        format!("rindexer-graphql-{}.exe", std::process::id())
    } else {
        format!("rindexer-graphql-{}", std::process::id())
    };
    let temp_path = temp_dir.join(exe_name);
    fs::write(&temp_path, GRAPHQL_BINARY)?;
    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        let mut perms = fs::metadata(&temp_path)?.permissions();
        perms.set_mode(0o755);
        fs::set_permissions(&temp_path, perms)?;
    }
    Ok(temp_path)
}
#[allow(dead_code)]
pub struct GraphQLServer {
    pid: u32,
}
#[derive(thiserror::Error, Debug)]
pub enum StartGraphqlServerError {
    #[error("Can not read database environment variable: {0}")]
    UnableToReadDatabaseUrl(#[from] env::VarError),
    #[error("Could not start up GraphQL server {0}")]
    GraphQLServerStartupError(String),
}
pub async fn start_graphql_server(
    indexer: &Indexer,
    settings: &GraphQLSettings,
) -> Result<GraphQLServer, StartGraphqlServerError> {
    info!("Starting GraphQL server");
    let schemas: Vec<String> = indexer
        .contracts
        .iter()
        .map(move |contract| {
            generate_indexer_contract_schema_name(
                &indexer.name,
                &contract.before_modify_name_if_filter_readonly(),
            )
        })
        .collect();
    let connection_string = connection_string()?;
    let port = settings.port;
    let graphql_endpoint = format!("http://localhost:{}/graphql", &port);
    let graphql_playground = format!("http://localhost:{}/playground", &port);
    let rindexer_graphql_exe = get_graphql_exe().map_err(|_| {
        StartGraphqlServerError::GraphQLServerStartupError(
            "rindexer-graphql executable not found".to_string(),
        )
    })?;
    // kill any existing process on the port
    kill_process_on_port(port).map_err(StartGraphqlServerError::GraphQLServerStartupError)?;
    let (tx, rx) = oneshot::channel();
    let tx_arc = Arc::new(Mutex::new(Some(tx)));
    spawn_start_server(
        tx_arc,
        rindexer_graphql_exe,
        connection_string,
        schemas.join(","),
        Arc::new(port),
        settings.filter_only_on_indexed_columns,
        settings.disable_advanced_filters,
    );
    // Do not need now with the main shutdown keeping around in-case
    // setup_ctrlc_handler(Arc::new(Mutex::new(None::<Child>)));
    // Wait for the initial server startup
    let pid = rx.await.map_err(|e| {
        StartGraphqlServerError::GraphQLServerStartupError(format!(
            "Failed to receive initial PID: {e}"
        ))
    })?;
    perform_health_check(&graphql_endpoint, &graphql_playground).await?;
    Ok(GraphQLServer { pid })
}
static MANUAL_STOP: AtomicBool = AtomicBool::new(false);
fn spawn_start_server(
    tx_arc: Arc<Mutex<Option<Sender<u32>>>>,
    rindexer_graphql_exe: PathBuf,
    connection_string: String,
    schemas: String,
    port: Arc<u16>,
    filter_only_on_indexed_columns: bool,
    disable_advanced_filters: bool,
) {
    tokio::spawn(async move {
        loop {
            if MANUAL_STOP.load(Ordering::SeqCst) {
                break;
            }
            match start_server(
                &rindexer_graphql_exe,
                &connection_string,
                &schemas,
                &port,
                filter_only_on_indexed_columns,
                disable_advanced_filters,
            )
            .await
            {
                Ok(child) => {
                    let pid = child.id();
                    let child_arc = Arc::new(Mutex::new(Some(child)));
                    let child_inner_for_thread = Arc::clone(&child_arc);
                    if let Some(tx) = tx_arc.lock().expect("Failed to lock tx arc").take() {
                        if let Err(e) = tx.send(pid) {
                            error!("Failed to send PID: {}", e);
                            break;
                        }
                    }
                    let port_inner = Arc::clone(&port);
                    tokio::spawn(async move {
                        set_thread_no_logging();
                        match child_inner_for_thread.lock() {
                            Ok(mut guard) => match guard.as_mut() {
                                Some(ref mut child) => match child.wait() {
                                    Ok(status) => {
                                        if status.success() {
                                            info!(
                                                "🦀GraphQL API ready at http://0.0.0.0:{}/",
                                                port_inner
                                            );
                                        } else {
                                            error!("GraphQL: Could not start up API: Child process exited with errors");
                                        }
                                    }
                                    Err(e) => {
                                        error!("GraphQL: Failed to wait on child process: {}", e);
                                    }
                                },
                                None => error!("GraphQL: Child process is None"),
                            },
                            Err(e) => {
                                error!("GraphQL: Failed to lock child process for waiting: {}", e);
                            }
                        }
                    });
                    if let Err(e) = child_arc
                        .lock()
                        .expect("Failed to lock child arc")
                        .as_mut()
                        .expect("Failed to get child")
                        .wait()
                    {
                        error!("Failed to wait on child process: {}", e);
                    }
                    if !MANUAL_STOP.load(Ordering::SeqCst) {
                        tokio::time::sleep(Duration::from_secs(1)).await;
                    } else {
                        break;
                    }
                }
                Err(e) => {
                    error!("Failed to start GraphQL server: {}", e);
                    tokio::time::sleep(Duration::from_secs(1)).await;
                }
            }
        }
    });
}
async fn start_server(
    rindexer_graphql_exe: &Path,
    connection_string: &str,
    schemas: &str,
    port: &u16,
    filter_only_on_indexed_columns: bool,
    disable_advanced_filters: bool,
) -> Result<Child, String> {
    Command::new(rindexer_graphql_exe)
        .arg(connection_string)
        .arg(schemas)
        .arg(port.to_string())
        // graphql_limit
        .arg("1000")
        // graphql_timeout
        .arg("10000")
        .arg(filter_only_on_indexed_columns.to_string())
        .arg(disable_advanced_filters.to_string())
        .stdout(Stdio::null())
        .stderr(Stdio::null())
        .spawn()
        .map_err(|e| e.to_string())
}
async fn perform_health_check(
    graphql_endpoint: &str,
    graphql_playground: &str,
) -> Result<(), StartGraphqlServerError> {
    let client = Client::new();
    let health_check_query = json!({
        "query": "query MyQuery { nodeId }"
    });
    let mut health_check_attempts = 0;
    while health_check_attempts < 40 {
        match client.post(graphql_endpoint).json(&health_check_query).send().await {
            Ok(response) if response.status().is_success() => {
                let response_json: Result<Value, Error> = response.json().await;
                match response_json {
                    Ok(response_json) => {
                        if response_json.get("errors").is_none() {
                            info!(
                                "🦀 GraphQL API ready at {} Playground - {} 🦀",
                                graphql_endpoint, graphql_playground
                            );
                            break;
                        }
                    }
                    Err(_) => {
                        // try again
                        info!("🦀 GraphQL API not healthy yet...");
                        continue;
                    }
                }
            }
            _ => {}
        }
        health_check_attempts += 1;
        tokio::time::sleep(Duration::from_millis(250)).await;
    }
    if health_check_attempts >= 40 {
        error!("GraphQL API did not become ready in time");
        return Err(StartGraphqlServerError::GraphQLServerStartupError(
            "GraphQL API did not become ready in time".to_string(),
        ));
    }
    Ok(())
}
// Do not need now with the main shutdown keeping around in-case
// fn setup_ctrlc_handler(child_arc: Arc<Mutex<Option<Child>>>) {
//     ctrlc::set_handler(move || {
//         MANUAL_STOP.store(true, Ordering::SeqCst);
//         if let Ok(mut guard) = child_arc.lock() {
//             if let Some(child) = guard.as_mut() {
//                 if let Err(e) = kill_process_tree(child.id()) {
//                     error!("Failed to kill child process: {}", e);
//                 } else {
//                     info!("GraphQL server process killed");
//                 }
//             }
//         }
//         std::process::exit(0);
//     })
//     .expect("Error setting Ctrl-C handler");
// }
// Do not need now with the main shutdown keeping around in-case
// fn kill_process_tree(pid: u32) -> Result<(), String> {
//     if cfg!(target_os = "windows") {
//         Command::new("taskkill")
//             .args(["/PID", &pid.to_string(), "/T", "/F"])
//             .output()
//             .map_err(|e| e.to_string())?;
//     } else {
//         Command::new("pkill")
//             .args(["-TERM", "-P", &pid.to_string()])
//             .output()
//             .map_err(|e| e.to_string())?;
//     }
//     Ok(())
// }
</file>

<file path="core/src/api/mod.rs">
mod generate_operations;
mod generate_schema;
mod graphql;
pub use generate_schema::generate_graphql_queries;
pub use graphql::{start_graphql_server, GraphqlOverrideSettings, StartGraphqlServerError};
</file>

<file path="core/src/blockclock/fetcher.rs">
use crate::provider::{JsonRpcCachedProvider, ProviderError, RECOMMENDED_RPC_CHUNK_SIZE};
use alloy::primitives::U64;
use alloy::rpc::types::Log;
use alloy_chains::Chain;
use std::collections::BTreeMap;
use std::sync::Arc;
use thiserror::Error;
#[derive(Error, Debug)]
pub enum BlockFetcherError {
    #[error("Failed to attach timestamps: {0}")]
    ProviderError(#[from] ProviderError),
    #[error("{0} missing block {1} in requested range")]
    MissingBlockInRange(Chain, U64),
}
#[derive(Debug, Clone)]
pub struct BlockFetcher {
    sample_rate: f32,
    pub(super) provider: Arc<JsonRpcCachedProvider>,
}
impl BlockFetcher {
    pub fn new(sample_rate: Option<f32>, provider: Arc<JsonRpcCachedProvider>) -> Self {
        let bounded_sampling = sample_rate.unwrap_or(1.0).clamp(0.001, 1.0);
        Self { provider, sample_rate: bounded_sampling }
    }
    /// Given the defined confidence score and a starting block, get the maximum set of block
    /// timestamps available to us in a single provider request.
    fn block_range_samples(&self, from: u64, to: u64) -> Vec<u64> {
        if from > to {
            return vec![];
        }
        if from == to {
            return vec![to];
        }
        let range = to - from;
        let total_blocks = range + 1;
        let desired_sample_count =
            ((total_blocks as f64) * self.sample_rate as f64).ceil() as usize;
        let sample_count = desired_sample_count.clamp(2, total_blocks as usize);
        // Small ranges can just be executed directly as there will be minimal overhead
        // and sampling doesn't add value in these cases necessary.
        if range <= (RECOMMENDED_RPC_CHUNK_SIZE as u64 / 2) {
            return (from..=to).collect();
        }
        if sample_count >= total_blocks as usize {
            return (from..=to).collect();
        }
        let step = total_blocks as f64 / (sample_count - 1) as f64;
        let mut samples = Vec::with_capacity(sample_count);
        samples.push(from);
        for i in 0..sample_count {
            let block = from + (i as f64 * step).round() as u64;
            if block > to {
                break;
            }
            samples.push(block);
        }
        samples.push(to);
        samples.dedup();
        samples
    }
    /// Interpolates blocks between known "anchors"
    ///
    /// # Note
    ///
    /// This method works most accurately when blocks in the same window have similar block-times.
    /// However, this does not hold true for blocks near ethereum genesis block.
    fn interpolate(&self, anchors: &mut [(u64, u64)]) -> BTreeMap<u64, u64> {
        let mut dates = BTreeMap::new();
        if anchors.is_empty() {
            return dates;
        }
        if anchors.len() == 1 {
            dates.insert(anchors[0].0, anchors[0].1);
            return dates;
        }
        anchors.sort_by(|(block_a, _), (block_b, _)| block_a.partial_cmp(block_b).unwrap());
        let (first, _) = *anchors.first().unwrap();
        let (last, _) = *anchors.last().unwrap();
        let blocks = first..=last;
        for block in blocks.clone() {
            let (left, right) = anchors
                .windows(2)
                .find(|window| window[0].0 <= block && block <= window[1].0)
                .map(|w| (w[0], w[1]))
                .unwrap_or_else(|| {
                    (anchors[anchors.len().saturating_sub(2)], anchors[anchors.len() - 1])
                });
            // Cast to signed floats to allow for negative values in the case of a future block
            // being older than a past block (it has happened). See example:
            //  - https://optimistic.etherscan.io/block/464390
            //  - https://optimistic.etherscan.io/block/464400
            let (block_l, time_l) = (left.0 as f64, left.1 as f64);
            let (block_r, time_r) = (right.0 as f64, right.1 as f64);
            let range_avg = (time_r - time_l) / (block_r - block_l);
            let range_time = (block as f64 - block_l) * range_avg;
            let timestamp = time_l + range_time;
            assert!(timestamp > 0.0, "timestamp cannot be negative or zero");
            dates.insert(block, timestamp as u64);
        }
        dates
    }
    /// Accept an input of blocks we care to fetch, and go about obtaining those blocks in an
    /// efficient mannger whilst respecting the confidence configuration.
    ///
    /// This is a "reactive" method used for when we don't already have a store of "known block
    /// timestamp" estimations to pull from without going over the network.
    ///
    /// This method assumes the caller **will not** call out-of-range blocks.
    async fn get_blocks(&self, blocks: &[u64]) -> Result<BTreeMap<u64, u64>, BlockFetcherError> {
        let mut blocks = blocks.to_vec();
        blocks.sort_unstable();
        blocks.dedup();
        let first = blocks[0];
        let last = blocks[blocks.len() - 1];
        let sampling = self.block_range_samples(first, last);
        let blocks = blocks.into_iter().map(|b| U64::from(b)).collect::<Vec<U64>>();
        if blocks.len() <= sampling.len() {
            let block = self.provider.get_block_by_number_batch(&blocks, false).await?;
            let timestamps = block.iter().map(|n| (n.header.number, n.header.timestamp));
            let timestamps = BTreeMap::from_iter(timestamps);
            return Ok(timestamps);
        }
        let mut anchors = self
            .provider
            .get_block_by_number_batch(&blocks, false)
            .await?
            .iter()
            .map(|n| (n.header.number, n.header.timestamp))
            .collect::<Vec<_>>();
        let dates = self.interpolate(&mut anchors);
        for block in blocks {
            if !dates.contains_key(&block.to()) {
                return Err(BlockFetcherError::MissingBlockInRange(self.provider.chain, block));
            }
        }
        Ok(dates)
    }
    /// Intelligently attaches timestamps to any logs.
    ///
    /// - Will not make fetches if the log already has a timestamp
    /// - Will sample block ranges where doing so will minimize networking time
    /// - Will use local-first compressed "delta run-encoded" block timestamps where possible
    pub async fn attach_log_timestamps(
        &self,
        logs: Vec<Log>,
    ) -> Result<Vec<Log>, BlockFetcherError> {
        let blocks_without_ts = logs
            .iter()
            .filter_map(|n| if n.block_timestamp.is_none() { n.block_number } else { None })
            .collect::<Vec<_>>();
        if blocks_without_ts.is_empty() {
            return Ok(logs);
        };
        let timestamps = self.get_blocks(&blocks_without_ts).await?;
        let (logs_with_ts, logs_without_ts) = logs
            .into_iter()
            .map(|mut log| {
                if let Some(block_number) = log.block_number {
                    if let Some(timestamp) = timestamps.get(&block_number) {
                        log.block_timestamp = Some(*timestamp);
                    }
                }
                log
            })
            .partition::<Vec<_>, _>(|x| x.block_timestamp.is_some());
        if !logs_without_ts.is_empty() {
            let mut completed = Box::pin(self.attach_log_timestamps(logs_without_ts)).await?;
            completed.extend(logs_with_ts);
            return Ok(completed);
        }
        Ok(logs_with_ts)
    }
}
#[cfg(test)]
mod tests {
    use super::*;
    fn mock_provider() -> Arc<JsonRpcCachedProvider> {
        JsonRpcCachedProvider::mock(1)
    }
    #[test]
    fn test_simple_interpolation() {
        let clock = BlockFetcher::new(None, mock_provider());
        // Lets assume perfect times, 1 block = 1 second.
        let mut anchors = vec![(1, 1), (10, 10), (20, 20), (30, 30), (10_000, 10_000)];
        let interpolated_blocks = clock.interpolate(&mut anchors);
        for (block, timestamp) in interpolated_blocks {
            assert_eq!(block, timestamp)
        }
        // Lets assume perfect times, 1 block = 5 second.
        let mut anchors = vec![(1, 5), (10_001, 50_005)];
        let interpolated_blocks = clock.interpolate(&mut anchors);
        for (block, timestamp) in interpolated_blocks {
            assert_eq!(block * 5, timestamp)
        }
    }
    #[test]
    fn test_ratio_sampling_min() {
        let clock = BlockFetcher::new(Some(1.0), mock_provider());
        let samples = clock.block_range_samples(1000, 1234);
        let actual_sampling_ratio = samples.len() as f32 / (1234 - 1000) as f32;
        assert!(actual_sampling_ratio >= 1.0);
        let clock = BlockFetcher::new(Some(0.1), mock_provider());
        let samples = clock.block_range_samples(1000, 1019);
        let actual_sampling_ratio = samples.len() as f32 / (1019 - 1000) as f32;
        assert!(actual_sampling_ratio >= 0.1);
        let clock = BlockFetcher::new(Some(0.1), mock_provider());
        let samples = clock.block_range_samples(1000, 1600);
        let actual_sampling_ratio = samples.len() as f32 / (1600 - 1000) as f32;
        assert!(actual_sampling_ratio >= 0.1);
        assert!(actual_sampling_ratio <= 0.15); // Ensure we don't oversample here either
        let clock = BlockFetcher::new(Some(0.5), mock_provider());
        let samples = clock.block_range_samples(1000, 1300);
        let actual_sampling_ratio = samples.len() as f32 / (1300 - 1000) as f32;
        assert!(actual_sampling_ratio >= 0.5);
        assert!(actual_sampling_ratio <= 0.6); // Ensure we don't oversample here either
    }
    #[test]
    fn test_sampling() {
        let clock = BlockFetcher::new(Some(0.1), mock_provider());
        assert_eq!(clock.block_range_samples(10, 10), vec![10]);
        assert_eq!(clock.block_range_samples(10, 11), vec![10, 11]);
        assert_eq!(clock.block_range_samples(100, 105), vec![100, 101, 102, 103, 104, 105]);
        // Handles bad input data without panic
        assert!(clock.block_range_samples(11, 10).is_empty());
    }
    #[test]
    fn test_sampling_unique() {
        let clock = BlockFetcher::new(Some(0.1), mock_provider());
        let samples = clock.block_range_samples(1000, 1600);
        let unique_samples = samples.iter().cloned().collect::<std::collections::HashSet<_>>();
        assert_eq!(unique_samples.len(), samples.len());
        assert!(samples.contains(&1000));
        assert!(samples.contains(&1600));
        let wide_samples = clock.block_range_samples(0, 20_000);
        assert!(wide_samples.contains(&0));
        assert!(wide_samples.contains(&20_000));
    }
    #[test]
    fn test_sampling_window_sequence() {
        let clock = BlockFetcher::new(Some(0.1), mock_provider());
        let samples = clock.block_range_samples(1000, 1600);
        assert!(samples.windows(2).all(|w| w[0] < w[1]));
        // Small range, sampling would result in too few blocks, min interval forces more
        // min_interval = 50 → expect at least one mid-sample
        let tight_samples = clock.block_range_samples(5000, 5050);
        let mid_steps: Vec<_> = tight_samples.windows(2).map(|w| w[1] - w[0]).collect();
        assert!(tight_samples.len() > 2);
        assert!(mid_steps.iter().any(|&step| step < 50));
    }
}
</file>

<file path="core/src/blockclock/fixed.rs">
use alloy_chains::Chain;
use alloy_chains::NamedChain;
#[allow(unused)]
#[derive(Debug)]
pub enum SpacedNetwork {
    Base(Chain),
    Blast(Chain),
    Soneium(Chain),
    Worldchain(Chain),
}
#[allow(unused)]
impl SpacedNetwork {
    /// The genesis unix timestamp for the network (the zero block).
    pub fn genesis_time(&self) -> u64 {
        match &self {
            SpacedNetwork::Base(_) => 1686789347,
            SpacedNetwork::Blast(_) => 1708809815,
            SpacedNetwork::Soneium(_) => 1733134751,
            SpacedNetwork::Worldchain(_) => 1719335639,
        }
    }
    /// Get the blocktime for a chain if present
    pub fn inner(&self) -> &Chain {
        match &self {
            SpacedNetwork::Base(a)
            | SpacedNetwork::Blast(a)
            | SpacedNetwork::Soneium(a)
            | SpacedNetwork::Worldchain(a) => a,
        }
    }
    /// Get the blocktime for a chain if present
    pub fn block_spacing(&self) -> Option<u64> {
        self.inner().average_blocktime_hint().map(|b| b.as_secs())
    }
    /// The maximum block for which we are sure a consistent-spacing holds true.
    pub fn max_safe_block(&self) -> u64 {
        match &self {
            SpacedNetwork::Base(_) => 32853624,
            SpacedNetwork::Blast(_) => 21843498,
            SpacedNetwork::Soneium(_) => 9746802,
            SpacedNetwork::Worldchain(_) => 16647415,
        }
    }
    /// Get the timestamp for a block in the consistently spaced network.
    ///
    /// Will return [`None`] if the block time cannot be safely or accurately calculated, in which
    /// case it is up to the caller to find the timestamp with an alternate method.
    pub fn get_block_time(&self, block: u64) -> Option<u64> {
        if let Some(spacing) = &self.block_spacing() {
            let start = &self.genesis_time();
            let block_time = start + spacing * block;
            if block <= self.max_safe_block() {
                return Some(block_time);
            }
        }
        None
    }
}
impl TryFrom<&Chain> for SpacedNetwork {
    type Error = String;
    fn try_from(value: &Chain) -> Result<Self, Self::Error> {
        match value.named() {
            Some(NamedChain::Base) => Ok(SpacedNetwork::Base(value.to_owned())),
            Some(NamedChain::Blast) => Ok(SpacedNetwork::Blast(value.to_owned())),
            Some(NamedChain::Soneium) => Ok(SpacedNetwork::Soneium(value.to_owned())),
            Some(NamedChain::World) => Ok(SpacedNetwork::Worldchain(value.to_owned())),
            _ => Err(format!("{:?} is not a spaced network", value)),
        }
    }
}
impl TryFrom<NamedChain> for SpacedNetwork {
    type Error = String;
    fn try_from(value: NamedChain) -> Result<Self, Self::Error> {
        SpacedNetwork::try_from(&Chain::from(value))
    }
}
#[cfg(test)]
mod tests {
    use super::*;
    use crate::provider::create_client;
    use alloy::primitives::U64;
    use rand::Rng;
    use std::collections::HashMap;
    /// A generic function to help with spot-checking random block numbers against timestamps.
    ///
    /// Each provider has different limits on their free-tiers for batch-sizes, so we include a
    /// "runs" property to control the number of blocks we check in a single test.
    #[allow(dead_code)]
    async fn check_block_times(rpc: &str, runs: usize, network: SpacedNetwork) {
        let mut blocks = Vec::with_capacity(runs);
        for _ in 0..runs {
            blocks.push(rand::rng().random_range(1u64..=network.max_safe_block()));
        }
        let blocks_req = blocks.iter().map(|&n| U64::from(n)).collect::<Vec<_>>();
        let block_time = create_client(
            rpc,
            network.inner().id(),
            None,
            None,
            None,
            Default::default(),
            None,
            None,
        )
        .await
        .unwrap()
        .get_block_by_number_batch(&blocks_req, false)
        .await
        .unwrap()
        .into_iter()
        .map(|t| (t.header.number, t.header.timestamp))
        .collect::<HashMap<_, _>>();
        for (k, v) in block_time {
            let time = network.get_block_time(k).unwrap_or_else(|| {
                panic!("{:?}: Missing expected time for block {}", network, k);
            });
            assert_eq!(v, time, "{:?}: Mismatch for block {}", network, k);
        }
    }
    // TODO: Fix flaky tests
    // #[tokio::test]
    // async fn base_block_time() {
    //     check_block_times(
    //         "https://base.gateway.tenderly.co",
    //         25,
    //         SpacedNetwork::try_from(NamedChain::Base).unwrap(),
    //     )
    //     .await;
    // }
    //
    // #[tokio::test]
    // async fn blast_block_time() {
    //     check_block_times(
    //         "https://rpc.ankr.com/blast",
    //         10,
    //         SpacedNetwork::try_from(NamedChain::Blast).unwrap(),
    //     )
    //     .await;
    // }
    //
    // #[tokio::test]
    // async fn soneium_block_time() {
    //     check_block_times(
    //         "https://rpc.soneium.org",
    //         10,
    //         SpacedNetwork::try_from(NamedChain::Soneium).unwrap(),
    //     )
    //     .await;
    // }
    //
    // #[tokio::test]
    // async fn worldchain_block_time() {
    //     check_block_times(
    //         "https://worldchain-mainnet.gateway.tenderly.co",
    //         10,
    //         SpacedNetwork::try_from(NamedChain::World).unwrap(),
    //     )
    //     .await;
    // }
}
</file>

<file path="core/src/blockclock/mod.rs">
//! # Return validated block timestamps
//!
//! We can return block-timestamps more intelligently than calling RPCs in two ways:
//!   1. Returned in logs
//!   2. Delta run-length encoded
//!   3. Fixed spaced block-timestamps (an extreme variant on case 1)
//!
//! ## Returned in logs
//!
//! This is the best way to get a timestamp. It requires no additional work. We simply receive the
//! timestamp from the log itself and can skip additional calculations or lookups.
//!
//! ## Delta run-length encoded
//!
//! Delta run-length encoding is an effective way to support block-timestamps that are not
//! necessarily sequential but generally follow a pattern.
//!
//! Most chains will have a roughly "fixed" block-time, and this can be used to encode the
//! block-timestamps more efficiently via "runs" of the delta between times.
//!
//! This process requires more upfront-work and more storage/memory, but can be a great way to save
//! on network requests and IO time.
//!
//! ## Fixed spaced block-timestamps
//!
//! These are the simplest of the networks, it is the most extreme delta-run-length encoding
//! and can therefore be optimized even more.
//!
//! Rather than storing "runs", we consider the whole chain to be a single "run" and can simply
//! calculate any timestamp for a block.
//!
//! Due to the lack of any strong guarantee, we can only do this up to a "known" block number where
//! the fixed-timestamp consistency has been validated. If at any time a chain breaks this pattern
//! we must drop back to delta run length encoding.
mod fetcher;
mod fixed;
mod runlencoder;
use crate::blockclock::fetcher::BlockFetcherError;
use crate::blockclock::fixed::SpacedNetwork;
use crate::provider::JsonRpcCachedProvider;
use alloy::rpc::types::Log;
pub use fetcher::BlockFetcher;
pub use runlencoder::DeltaEncoder;
use std::env;
use std::env::VarError;
use std::path::PathBuf;
use std::sync::Arc;
use thiserror::Error;
#[derive(Error, Debug)]
pub enum BlockClockError {
    #[error("Failed to attach fetched timestamps: {0}")]
    BlockFetcherError(#[from] BlockFetcherError),
    #[error("Failed to get encoded filepath: {0}")]
    DeltaFilepathError(#[from] VarError),
    #[error("Failed to get decode encoded `.blockclock` file: {0}")]
    DeltaFileDecoderError(#[from] anyhow::Error),
}
#[derive(Debug, Clone)]
pub struct BlockClock {
    #[allow(unused)]
    network_id: u64,
    /// The run-length encoder to use for calculating timestamps in memory.
    ///
    /// Will be set to `None` if there is no backing file with pre-encoded data and the
    /// encoder lookup will be bypassed.
    fetcher: BlockFetcher,
    /// The run-length encoder to use for calculating timestamps in memory.
    ///
    /// Will be set to `None` if there is no backing file with pre-encoded data and the
    /// encoder lookup will be bypassed.
    runlencoder: Option<Arc<DeltaEncoder>>,
}
impl BlockClock {
    pub fn new(
        enabled: Option<bool>,
        sample_rate: Option<f32>,
        provider: Arc<JsonRpcCachedProvider>,
    ) -> Self {
        let network_id = provider.chain.id();
        let fetcher = BlockFetcher::new(sample_rate, provider);
        // Filepath based blockclock increases startup time so only proceed to enable if
        // log timestamps are explicitly enabled.
        if !enabled.unwrap_or(false) {
            return Self { network_id, fetcher, runlencoder: None };
        }
        let filepath = get_blockclock_filepath(network_id);
        let runlencoder = filepath.and_then(|path| {
            path.is_file().then(|| {
                let coder = DeltaEncoder::from_file(network_id as u32, None, &path).ok()?;
                Some(coder)
            })?
        });
        Self { network_id, fetcher, runlencoder }
    }
    /// Attach timestamps to logs in the most efficient way possible.
    ///
    /// The following strategies are tried in order:
    ///
    /// 1. Use timestamps present in logs and return early
    /// 2. Use fixed-interval chains to compute timestamps and return
    /// 3. Use precomputed delta-encoded network timestamps
    /// 4. Fetch an optionally sampled-and-interpolated set of timestamps from RPC calls
    pub async fn attach_log_timestamps(&self, logs: Vec<Log>) -> Result<Vec<Log>, BlockClockError> {
        if logs.is_empty() {
            return Ok(logs);
        }
        // Heuristic to transform if it's too large, it's probably in milliseconds
        //
        // The timestamps can be either in milliseconds or seconds, it's unpredictable so we
        // need to check and convert to seconds of we detect milliseconds.
        let logs: Vec<Log> = logs
            .into_iter()
            .map(|mut l| {
                if let Some(ts) = l.block_timestamp {
                    if ts > 1_000_000_000_000 {
                        l.block_timestamp = Some(ts / 1000);
                    }
                }
                l
            })
            .collect();
        // 1. Use timestamps present in logs and return early.
        if logs.iter().all(|log| log.block_timestamp.is_some()) {
            return Ok(logs);
        }
        // 2. Use fixed-interval chains to compute timestamps and return
        let logs = if let Ok(spaced) = SpacedNetwork::try_from(&self.fetcher.provider.chain) {
            logs.into_iter()
                .map(|mut log| {
                    log.block_timestamp = spaced.get_block_time(log.block_number.unwrap());
                    log
                })
                .collect()
        } else {
            logs
        };
        // 3. Use precomputed delta-encoded network timestamps
        let logs = if let Some(deltas) = &self.runlencoder {
            deltas.try_attach_log_timestamps(logs)
        } else {
            logs
        };
        // 4. Fetch an optionally sampled-and-interpolated set of timestamps from RPC calls
        let logs = self.fetcher.attach_log_timestamps(logs).await?;
        Ok(logs)
    }
}
fn get_blockclock_filepath(network: u64) -> Option<PathBuf> {
    let filename = &format!("{}.blockclock", network);
    let mut paths = vec![];
    // Assume `resources` directory is in the same directory as the executable (installed)
    if let Ok(executable_path) = env::current_exe() {
        let mut path = executable_path.to_path_buf();
        path.pop(); // Remove the executable name
        path.push("resources");
        path.push("blockclock");
        path.push(filename);
        paths.push(path);
        // Also consider when running from within the `rindexer` directory
        let mut path = executable_path;
        path.pop(); // Remove the executable name
        path.pop(); // Remove the 'release' or 'debug' directory
        path.push("resources");
        path.push("blockclock");
        path.push(filename);
        paths.push(path);
    }
    // Check additional common paths
    if let Ok(home_dir) = env::var("HOME") {
        let mut path = PathBuf::from(home_dir);
        path.push(".rindexer");
        path.push("resources");
        path.push("blockclock");
        path.push(filename);
        paths.push(path);
    }
    // Return the first valid path
    for path in &paths {
        if path.exists() {
            return Some(path.to_path_buf());
        }
    }
    let extra_looking =
        paths.into_iter().next().expect("Failed to determine rindexer blockclock path");
    if !extra_looking.exists() {
        return None;
    }
    Some(extra_looking)
}
</file>

<file path="core/src/blockclock/runlencoder.rs">
//! # Run Length Encoder
//!
//! The run length encoder leverages the quality of "regularity" in blockchain timestamps.
//!
//! Most chains have consistent patterns or sequences of block time deltas. For example, polygon
//! is mostly consistent 2s blocktimes, but occasionally has a 6s timestamp.
//!
//! We can therefore make a two tradeoffs:
//!  1. Compute in exchange for storage bytes (disk; or RAM during runtime execution)
//!  2. Storage bytes in exchange for Network IO latency
//!
//! ## Encoding
//!
//! We encode the timestamps in a delta-run-length format, and then serialize it in a binary [`zstd`]
//! compressed file. We recommend ending with a `.blockclock` extension.
//!
//! This allows us to potentially use a few kB to store the timestamps of every chain on a network.
//!
//! ## Maximum Block
//!
//! One limitation of this approach is that we must compute the timestamps in advance. Therefore the
//! primary use-case for this encoding is to speed-up large-scale backfill operations.
//!
//! Backfills can often span many networks over long periods of time and require timestamps for tens
//! or hundreds of millions of block. This can add significant latency and so instead we can use a
//! tiny amount of CPU to compute these values instead.
//!
//! For optimal use it is beneficial to regularly extend the compressed binary files with more recent
//! blocks and so the encoding should be run and published semi-regularly.
use alloy::eips::BlockNumberOrTag;
use alloy::network::AnyRpcBlock;
use alloy::rpc::client::RpcClient;
use alloy::rpc::types::Log;
use anyhow::Context;
use bincode::{config, Decode, Encode};
use cfg_if::cfg_if;
use futures::future::try_join_all;
use once_cell::sync::Lazy;
use serde::{Deserialize, Serialize};
use std::collections::{BTreeMap, HashMap};
use std::fs::File;
use std::io::{BufReader, BufWriter};
use std::path::PathBuf;
use std::sync::{Arc, RwLock};
use std::time::Duration;
use tokio::select;
use tokio::time::{sleep, Instant};
use zstd::{Decoder, Encoder};
static BLOCKCLOCK_CACHE: Lazy<RwLock<HashMap<u32, Arc<DeltaEncoder>>>> =
    Lazy::new(|| RwLock::new(HashMap::new()));
/// The maximum size of an RPC call for the application
const RPC_CHUNK_SIZE: usize = 1000;
/// The magic number of "spacing" at which we create indexed checkpoints.
///
/// The smaller the number, the faster the lookups at the cost of some "memory".
const MAGIC_INDEX_INTERVAL: u64 = 100_000;
/// A timestamp delta run length.
#[derive(Debug, Deserialize, Serialize, Encode, Decode)]
pub struct DeltaRunLen {
    /// The length for this specific delta run.
    len: u64,
    /// The difference between each timestamp in seconds (delta).
    delta: u64,
}
/// Metadata and [`DeltaRunLength`] associated with a networks timestamps.
#[derive(Debug, Deserialize, Serialize, Encode, Decode)]
pub struct EncodedDeltas {
    /// The network ID.
    network: u32,
    /// The timestamp of the first block in the chain.
    genesis_timestamp: Option<u64>,
    /// The maximum block number that has been encoded.
    max_block: u64,
    /// The maximum block timestamp that has been encoded, required for continuity when
    /// encoding across multiple runs.
    max_block_timestamp: Option<u64>,
    /// A continuous vector of delta run-lengths.
    runs: Vec<DeltaRunLen>,
}
impl EncodedDeltas {
    pub fn new(network: u32) -> Self {
        Self {
            network,
            genesis_timestamp: None,
            max_block: 0,
            max_block_timestamp: None,
            runs: vec![],
        }
    }
}
/// Delta run length encoder.
///
/// Polls a network's blockchain and persists the runs of timestamp deltas
/// to efficiently allow a timestamp lookup for non-fixed-length networks.
#[derive(Debug)]
pub struct DeltaEncoder {
    /// The network ID.
    network: u32,
    /// An RPC Client corresponding to the provided network.
    ///
    /// If rpc is set to `None`, we will only be able to `decode` from a file and compute
    /// timestamps. An `Error` will be returned on any attempt to encode new blocks.
    rpc: Option<RpcClient>,
    /// The run length encoded deltas.
    encoded_deltas: EncodedDeltas,
    /// A fully qualified Path to the file used for persistence.
    file_path: PathBuf,
    /// An index we built in-memory when deserializing the file, assists with fast lookups.
    ///
    /// `block_number → (run_index, cumulative_ts)`
    index: BTreeMap<u64, (usize, u64)>,
}
impl DeltaEncoder {
    /// Create a new [`DeltaEncoder`] for a network.
    pub fn new(network: u32, rpc_url: Option<&str>, file_path: impl Into<PathBuf>) -> Self {
        let client =
            rpc_url.map(|url| RpcClient::new_http(url.parse().expect("RPC URL is invalid")));
        let deltas = EncodedDeltas::new(network);
        Self {
            network,
            rpc: client,
            encoded_deltas: deltas,
            file_path: file_path.into(),
            index: BTreeMap::new(),
        }
    }
    /// Get a blockclock with the benefit of a global cache layer
    pub fn from_file(
        network: u32,
        rpc_url: Option<&str>,
        file_path: &PathBuf,
    ) -> anyhow::Result<Arc<Self>> {
        {
            let cache = BLOCKCLOCK_CACHE.read().unwrap();
            if let Some(clock) = cache.get(&network) {
                return Ok(clock.clone());
            }
        }
        let encoder = DeltaEncoder::from_file_inner(network, rpc_url, file_path)?;
        let encoder = Arc::new(encoder);
        let mut cache = BLOCKCLOCK_CACHE.write().unwrap();
        Ok(cache.entry(network).or_insert_with(|| encoder.clone()).clone())
    }
    /// Create or restore a [`DeltaEncoder`] for a network from the filesystem persistence.
    pub fn from_file_inner(
        network: u32,
        rpc_url: Option<&str>,
        file_path: &PathBuf,
    ) -> anyhow::Result<Self> {
        let encoded_deltas = if file_path.exists() {
            let file = File::open(file_path)?;
            let reader = BufReader::new(file);
            let mut zstd_decoder = Decoder::new(reader)?;
            tracing::info!("Preparing blockclock for network: {}", network);
            bincode::decode_from_std_read(&mut zstd_decoder, config::standard())?
        } else {
            return Ok(Self::new(network, rpc_url, file_path));
        };
        let mut encoder = Self {
            network,
            rpc: rpc_url.map(|url| RpcClient::new_http(url.parse().expect("RPC URL is invalid"))),
            encoded_deltas,
            file_path: file_path.clone(),
            index: BTreeMap::new(),
        };
        encoder.build_index(MAGIC_INDEX_INTERVAL);
        Ok(encoder)
    }
    /// Build the index when deserializing from the file.
    pub fn build_index(&mut self, interval: u64) {
        let Some(genesis) = self.encoded_deltas.genesis_timestamp else { return };
        let mut bnum = 0;
        let mut agg_ts = genesis;
        for (i, run) in self.encoded_deltas.runs.iter().enumerate() {
            if bnum % interval == 0 {
                self.index.insert(bnum, (i, agg_ts));
            }
            agg_ts += run.delta * run.len;
            bnum += run.len;
        }
    }
    /// Write the contents of the in-memory datastructure to disk in [`zstd`] encoded binary format.
    fn serialize_to_file(&self) -> anyhow::Result<()> {
        let bin_file = File::create(&self.file_path)?;
        let bin_writer = BufWriter::new(bin_file);
        let mut zstd_encoder = Encoder::new(bin_writer, 0)?;
        bincode::encode_into_std_write(
            &self.encoded_deltas,
            &mut zstd_encoder,
            config::standard(),
        )?;
        zstd_encoder.finish()?;
        cfg_if! {
            if #[cfg(feature = "debug-json")] {
                let json_path = self.file_path.with_extension("json");
                let json_file = File::create(json_path)?;
                let json_writer = BufWriter::new(json_file);
                serde_json::to_writer_pretty(json_writer, &self.encoded_deltas)?;
            }
        }
        Ok(())
    }
    /// Fetch a set of blocks and their timestamps from RPC calls.
    async fn get_block_by_number_batch(
        &self,
        block_numbers: &[u64],
        include_txs: bool,
    ) -> anyhow::Result<Vec<(u64, u64)>> {
        let chain = self.network;
        if block_numbers.is_empty() {
            return Ok(Vec::new());
        }
        let mut block_numbers = block_numbers.to_vec();
        block_numbers.dedup();
        let client = self.rpc.clone().context("cannot fetch batch when rpc client is None")?;
        // Use less than the max chunk as recommended in most provider docs
        let futures = block_numbers
            .chunks(RPC_CHUNK_SIZE)
            .map(|chunk| {
                let owned_chunk = chunk.to_vec();
                let client = client.clone();
                tokio::spawn(async move {
                    let mut batch = client.new_batch();
                    let mut request_futures = Vec::with_capacity(owned_chunk.len());
                    for block_num in owned_chunk {
                        let params = (BlockNumberOrTag::Number(block_num), include_txs);
                        let call = batch.add_call("eth_getBlockByNumber", &params)?;
                        request_futures.push(call)
                    }
                    if let Err(e) = batch.send().await {
                        tracing::error!(
                            "Failed to send {} batch 'eth_getBlockByNumber' request for {}: {:?}",
                            request_futures.len(),
                            chain,
                            e
                        );
                        return Err(e);
                    }
                    try_join_all(request_futures).await
                })
            })
            .collect::<Vec<_>>();
        let chunk_results: Vec<Result<Vec<AnyRpcBlock>, _>> = try_join_all(futures).await?;
        let results = chunk_results
            .into_iter()
            .collect::<Result<Vec<_>, _>>()?
            .into_iter()
            .flatten()
            .map(|r| (r.header.number, r.header.timestamp))
            .collect();
        tracing::debug!(
            "Fetched blocks: {}..{}",
            block_numbers.iter().min().unwrap(),
            block_numbers.iter().max().unwrap()
        );
        Ok(results)
    }
    /// Get the latest block for the network.
    async fn get_head_block(&self) -> anyhow::Result<u64> {
        let client = self.rpc.clone().context("cannot fetch batch when rpc client is None")?;
        let res: String = client.request("eth_blockNumber", &()).await?;
        let hex = res.replace("0x", "");
        let block = u64::from_str_radix(&hex, 16)?;
        Ok(block)
    }
    /// Compress the runlen data.
    ///
    /// Accept a list of block numbers and their timestamps, update the run-lengths. Enforces that
    /// the incoming vec of blocks is sequential.
    fn encode_deltas(&mut self, mut blocks: Vec<(u64, u64)>) -> anyhow::Result<&EncodedDeltas> {
        blocks.sort_by_key(|(block, _)| *block);
        if blocks.is_empty() {
            return Ok(&self.encoded_deltas);
        }
        tracing::debug!("{:-<32}", "");
        tracing::debug!("{:<10} | {:<20}", "Block", "Timestamp");
        tracing::debug!("{:-<32}", "");
        for (block, timestamp) in &blocks {
            tracing::debug!("{:<10} | {:<20}", block, timestamp);
        }
        let first_block_in_batch = blocks.first().expect("non-zero already checked").0;
        let expected_block = self.encoded_deltas.max_block + 1;
        if first_block_in_batch != expected_block
            && self.encoded_deltas.max_block == 0
            && first_block_in_batch < 2
        {
            anyhow::bail!(
                "Deltas must be encoded in sequential order, expected {:?} got {:?}",
                expected_block,
                first_block_in_batch
            );
        }
        let mut previous_timestamp = if self.encoded_deltas.max_block == 0 {
            self.encoded_deltas.genesis_timestamp
        } else {
            self.encoded_deltas.max_block_timestamp
        };
        for (_block_num, block_ts) in &blocks {
            if let Some(prev_ts) = previous_timestamp {
                let delta = block_ts.saturating_sub(prev_ts);
                if let Some(last_run) = self.encoded_deltas.runs.last_mut() {
                    if last_run.delta == delta {
                        last_run.len += 1;
                    } else {
                        self.encoded_deltas.runs.push(DeltaRunLen { len: 1, delta });
                    }
                } else {
                    self.encoded_deltas.runs.push(DeltaRunLen { len: 1, delta });
                }
            }
            previous_timestamp = Some(*block_ts);
        }
        if let Some((last_block_num, last_block_ts)) = blocks.last() {
            self.encoded_deltas.max_block = *last_block_num;
            self.encoded_deltas.max_block_timestamp = Some(*last_block_ts);
        }
        Ok(&self.encoded_deltas)
    }
    /// Some RPC do not return timestamp in block `0` so we are forced to provide genesis time
    /// manually instead.
    pub fn genesis_time(&self) -> Option<u64> {
        match &self.network {
            1 => Some(1438269973),
            10 => Some(1610639500),
            42161 => Some(1622240000),
            _ => Some(0),
        }
    }
    /// Fetch and encode deltas and persist to a file path.
    async fn fetch_encode_persist(&mut self, batch_size: u64) -> anyhow::Result<()> {
        if self.encoded_deltas.genesis_timestamp.is_none() {
            match self.genesis_time() {
                Some(ts) => self.encoded_deltas.genesis_timestamp = Some(ts),
                None => {
                    if let Some(genesis) =
                        self.get_block_by_number_batch(&[0], false).await?.first()
                    {
                        if genesis.1 == 0 {
                            anyhow::bail!(
                                "Rpc returned a zero timestamp for genesis block. Please add \
                                network {} to list of manually defined genesis timestamps.",
                                self.network
                            );
                        }
                        self.encoded_deltas.genesis_timestamp = Some(genesis.1);
                    }
                }
            }
            tracing::info!(
                "Fetched genesis timestamp: {}",
                self.encoded_deltas.genesis_timestamp.unwrap()
            );
        }
        let next_block = self.encoded_deltas.max_block + 1;
        let end_block = next_block + batch_size - 1;
        let block_numbers: Vec<u64> = (next_block..=end_block).collect();
        let blocks = match self.get_block_by_number_batch(&block_numbers, false).await {
            Ok(b) => b,
            Err(e) => {
                tracing::error!(
                    "Error fetching blocks {}-{} for network {}: {:?}",
                    next_block,
                    end_block,
                    self.network,
                    e
                );
                sleep(Duration::from_millis(1000)).await;
                return Ok(());
            }
        };
        self.encode_deltas(blocks)?;
        tracing::info!("Encoded blocks {}–{} (network {})", next_block, end_block, self.network);
        Ok(())
    }
    /// Continuously poll for new blocks, encode deltas, and persist to disk.
    pub async fn poll_encode_loop(&mut self, batch_size: u64) -> anyhow::Result<()> {
        let max_block_for_network = self.get_head_block().await?;
        let flush_duration_secs = 180;
        let mut flush_interval = Instant::now();
        tracing::info!(
            "Beginning poll-based block-timestamp encoding. Max block {max_block_for_network}."
        );
        loop {
            if self.encoded_deltas.max_block + batch_size >= max_block_for_network {
                tracing::info!("Exiting poll loop. Max block reached.");
                if let Err(e) = self.serialize_to_file() {
                    tracing::error!("Error flushing to disk: {:?}", e);
                }
                tracing::info!("Finished flushing file to disk.");
                break;
            }
            select! {
                _ = tokio::signal::ctrl_c() => {
                    tracing::info!("Ctrl+C received, exiting poll loop.");
                    if let Err(e) = self.serialize_to_file() {
                        tracing::error!("Error flushing to disk: {:?}", e);
                    }
                    tracing::info!("Finished flushing file to disk.");
                    break;
                }
                result = self.fetch_encode_persist(batch_size) => {
                    if flush_interval.elapsed().as_secs() >= flush_duration_secs {
                        match self.serialize_to_file() {
                            Ok(_) => tracing::info!("✅ Checkpoint reached. Flushed file to disk."),
                            Err(e) => tracing::error!("Error flushing to disk: {:?}", e)
                        }
                        flush_interval = Instant::now();
                    };
                    if let Err(e) = result {
                        tracing::error!("Error fetching and encoding blocks: {:?}", e)
                    }
                }
            }
        }
        Ok(())
    }
    /// Get the timestamp for any block.
    ///
    /// Returns `Some(timestamp)` if we can compute the block or `None` when the block is outside
    /// of the range we have indexed and can safely compute.
    pub fn get_block_timestamp(&self, block_number: &u64) -> Option<u64> {
        let genesis = self.encoded_deltas.genesis_timestamp?;
        if *block_number > self.encoded_deltas.max_block {
            return None;
        }
        let (mut bnum, mut agg_ts, run_idx) =
            if let Some((&checkpoint, &(idx, ts))) = self.index.range(..=*block_number).last() {
                (checkpoint, ts, idx)
            } else {
                (0, genesis, 0)
            };
        for run in &self.encoded_deltas.runs[run_idx..] {
            if bnum + run.len >= *block_number {
                let offset = block_number - bnum;
                return Some(agg_ts + run.delta * offset);
            } else {
                agg_ts += run.delta * run.len;
                bnum += run.len;
            }
        }
        None
    }
    /// Returns a BTreeMap of block_number → timestamp for all block numbers
    /// in the input slice that can be resolved.
    pub fn get_block_timestamps(&self, block_numbers: &[u64]) -> BTreeMap<u64, u64> {
        let genesis = match self.encoded_deltas.genesis_timestamp {
            Some(ts) => ts,
            None => return BTreeMap::new(),
        };
        let max_block = self.encoded_deltas.max_block;
        // Sort and dedup the input for efficient lookup
        let mut requested: Vec<u64> =
            block_numbers.iter().copied().filter(|&b| b <= max_block).collect();
        if requested.is_empty() {
            return BTreeMap::new();
        }
        requested.sort_unstable();
        requested.dedup();
        // Prepare output
        let mut output = BTreeMap::new();
        // Use latest checkpoint <= max requested block
        let start_block = *requested.last().unwrap();
        let (mut bnum, mut agg_ts, run_idx) =
            if let Some((&checkpoint, &(idx, ts))) = self.index.range(..=start_block).last() {
                (checkpoint, ts, idx)
            } else {
                (0, genesis, 0)
            };
        // Walk runs and assign timestamps
        let mut req_idx = 0;
        for run in &self.encoded_deltas.runs[run_idx..] {
            let end = bnum + run.len;
            while req_idx < requested.len() {
                let blk = requested[req_idx];
                if blk < bnum {
                    // Earlier than current run start — should not happen
                    req_idx += 1;
                    continue;
                } else if blk < end {
                    let offset = blk - bnum;
                    output.insert(blk, agg_ts + run.delta * offset);
                    req_idx += 1;
                } else {
                    break;
                }
            }
            bnum = end;
            agg_ts += run.delta * run.len;
        }
        output
    }
    /// Attaches timestamps to any logs it can and returns the full set.
    ///
    /// This is a simple pass-through method and does not guarantee all logs in the batch
    /// will actually get timestamps attached.
    pub fn try_attach_log_timestamps(&self, logs: Vec<Log>) -> Vec<Log> {
        let blocks_without_ts = logs
            .iter()
            .filter_map(|n| if n.block_timestamp.is_none() { n.block_number } else { None })
            .collect::<Vec<_>>();
        if blocks_without_ts.is_empty() {
            return logs;
        };
        let timestamps = self.get_block_timestamps(&blocks_without_ts);
        let logs_with_maybe_ts = logs
            .into_iter()
            .map(|mut log| {
                if let Some(block_number) = log.block_number {
                    if let Some(timestamp) = timestamps.get(&block_number) {
                        log.block_timestamp = Some(*timestamp);
                    }
                }
                log
            })
            .collect();
        logs_with_maybe_ts
    }
}
#[cfg(test)]
mod tests {
    use super::*;
    use rand::random_range;
    use tempfile::tempdir;
    #[test]
    fn test_encode_deltas_run_length_encoding() {
        let mut encoder = DeltaEncoder {
            network: 1,
            index: BTreeMap::new(),
            rpc: None,
            encoded_deltas: EncodedDeltas::new(1),
            file_path: tempdir().unwrap().keep(),
        };
        let blocks = vec![(100, 1000), (101, 1012), (102, 1024), (103, 1036), (104, 1051)];
        let result = encoder.encode_deltas(blocks).unwrap();
        assert_eq!(result.network, 1);
        assert_eq!(result.max_block, 104);
        assert_eq!(result.runs.len(), 2);
        assert_eq!(result.runs[0].len, 3);
        assert_eq!(result.runs[0].delta, 12);
        assert_eq!(result.runs[1].len, 1);
        assert_eq!(result.runs[1].delta, 15);
    }
    #[test]
    fn test_serialize_and_deserialize_file_roundtrip() {
        let file_path = tempdir().unwrap().keep().join("block-deltas");
        let mut encoder = DeltaEncoder {
            network: 99,
            rpc: None,
            file_path: file_path.clone(),
            index: BTreeMap::new(),
            encoded_deltas: EncodedDeltas {
                network: 99,
                genesis_timestamp: Some(0),
                max_block: 0,
                max_block_timestamp: None,
                runs: vec![],
            },
        };
        let blocks = vec![(1, 10), (2, 20), (3, 30), (4, 31), (5, 32)];
        encoder.encode_deltas(blocks).unwrap();
        encoder.serialize_to_file().unwrap();
        let reloaded = DeltaEncoder::from_file(99, None, &file_path).unwrap();
        assert_eq!(reloaded.encoded_deltas.network, 99);
        assert_eq!(reloaded.encoded_deltas.max_block, 5);
        assert_eq!(reloaded.encoded_deltas.max_block_timestamp, Some(32));
        assert_eq!(reloaded.encoded_deltas.runs.len(), 2);
        assert_eq!(reloaded.encoded_deltas.runs[0].len, 3);
        assert_eq!(reloaded.encoded_deltas.runs[0].delta, 10);
        assert_eq!(reloaded.encoded_deltas.runs[1].len, 2);
        assert_eq!(reloaded.encoded_deltas.runs[1].delta, 1);
        assert_eq!(reloaded.get_block_timestamp(&0), Some(0));
        assert_eq!(reloaded.get_block_timestamp(&1), Some(10));
        assert_eq!(reloaded.get_block_timestamp(&2), Some(20));
        assert_eq!(reloaded.get_block_timestamp(&3), Some(30));
        assert_eq!(reloaded.get_block_timestamp(&4), Some(31));
        assert_eq!(reloaded.get_block_timestamp(&5), Some(32));
        assert_eq!(reloaded.get_block_timestamp(&6), None);
    }
    /// E2E Test
    ///
    /// This test should ideally be moved to an e2e test and run less frequently
    /// as it makes a number of real RPC calls and may experience network issues.
    #[tokio::test]
    #[ignore]
    async fn test_fetch_encode_persist() -> anyhow::Result<()> {
        // This should be filled in!
        let rpc_url = "https://polygon-mainnet.g.alchemy.com/v2/XXXXX";
        let network_id = 137;
        let file_path = tempdir()?.keep().join("block-deltas");
        if rpc_url.is_empty() {
            return Ok(());
        }
        let mut encoder = DeltaEncoder::new(network_id, Some(&rpc_url), &file_path);
        encoder.fetch_encode_persist(100).await?;
        drop(encoder);
        let mut reloaded = DeltaEncoder::from_file_inner(network_id, Some(&rpc_url), &file_path)?;
        reloaded.fetch_encode_persist(100).await?;
        assert_eq!(reloaded.encoded_deltas.network, network_id);
        assert_eq!(reloaded.encoded_deltas.max_block, 200);
        let range: Vec<u64> = (1..=200).collect();
        let blocks = reloaded.get_block_by_number_batch(&range, false).await?;
        for (block, actual_timestamp) in blocks {
            let timestamp = reloaded.get_block_timestamp(&block).unwrap();
            assert_eq!(actual_timestamp, timestamp, "Block {} timestamp mismatch", block);
        }
        Ok(())
    }
    /// E2E Test (Disabled, manually enable as required)
    ///
    /// Spotcheck the files on each run to ensure we don't contain errors.
    #[tokio::test]
    // #[ignore]
    async fn spotcheck_file() -> anyhow::Result<()> {
        let rpc_url = "https://arb-mainnet.g.alchemy.com/v2/LfCdXVQaAS7hctR3N78Sj";
        let network_id = 42161;
        let sample_count = 100;
        let base_path = PathBuf::from(env!("CARGO_MANIFEST_DIR"))
            .parent()
            .unwrap()
            .join("core")
            .join("resources")
            .join("blockclock")
            .join(network_id.to_string())
            .with_extension("blockclock");
        println!("Base path: {:?}", base_path);
        if rpc_url.is_empty() {
            return Ok(());
        }
        let reloaded = DeltaEncoder::from_file(network_id, Some(&rpc_url), &base_path)?;
        let mut samples = Vec::with_capacity(sample_count);
        for _ in 1..=sample_count {
            samples.push(random_range(1..=reloaded.encoded_deltas.max_block));
        }
        samples.push(231191);
        let blocks = reloaded.get_block_by_number_batch(&samples, false).await?;
        for (block, actual_timestamp) in blocks {
            let start = Instant::now();
            let timestamp = reloaded.get_block_timestamp(&block).unwrap();
            let end = start.elapsed();
            assert_eq!(actual_timestamp, timestamp, "Block {} timestamp mismatch", block);
            println!("Block {} timestamp: {} in {}ms", block, timestamp, end.as_millis());
        }
        Ok(())
    }
}
</file>

<file path="core/src/chat/clients.rs">
use std::sync::Arc;
use alloy::primitives::U64;
use futures::future::join_all;
use serde_json::Value;
use serenity::all::ChannelId;
use teloxide::types::ChatId;
use thiserror::Error;
use tokio::{
    task,
    task::{JoinError, JoinHandle},
};
use crate::{
    chat::{
        discord::{DiscordBot, DiscordError},
        slack::{SlackBot, SlackError},
        telegram::{TelegramBot, TelegramError},
        template::Template,
    },
    event::{filter_by_expression, filter_event_data_by_conditions, EventMessage},
    manifest::chat::{
        ChatConfig, DiscordConfig, DiscordEvent, SlackConfig, SlackEvent, TelegramConfig,
        TelegramEvent,
    },
};
type SendMessage = Vec<JoinHandle<Result<(), ChatError>>>;
#[derive(Error, Debug)]
pub enum ChatError {
    #[error("Telegram error: {0}")]
    Telegram(#[from] TelegramError),
    #[error("Discord error: {0}")]
    Discord(#[from] DiscordError),
    #[error("Slack error: {0}")]
    Slack(#[from] SlackError),
    #[error("Task failed: {0}")]
    JoinError(JoinError),
}
#[derive(Debug, Clone)]
struct TelegramInstance {
    config: TelegramConfig,
    client: Arc<TelegramBot>,
}
#[derive(Debug)]
struct DiscordInstance {
    config: DiscordConfig,
    client: Arc<DiscordBot>,
}
#[derive(Debug)]
struct SlackInstance {
    config: SlackConfig,
    client: Arc<SlackBot>,
}
pub struct ChatClients {
    telegram: Option<Vec<TelegramInstance>>,
    discord: Option<Vec<DiscordInstance>>,
    slack: Option<Vec<SlackInstance>>,
}
impl ChatClients {
    pub async fn new(chat_config: ChatConfig) -> Self {
        let telegram = chat_config.telegram.map(|config| {
            config
                .into_iter()
                .map(|config| {
                    let client = Arc::new(TelegramBot::new(&config.bot_token));
                    TelegramInstance { config, client }
                })
                .collect()
        });
        let discord = chat_config.discord.map(|config| {
            config
                .into_iter()
                .map(|config| {
                    let client = Arc::new(DiscordBot::new(&config.bot_token));
                    DiscordInstance { config, client }
                })
                .collect()
        });
        let slack = chat_config.slack.map(|config| {
            config
                .into_iter()
                .map(|config| {
                    let client = Arc::new(SlackBot::new(config.bot_token.clone()));
                    SlackInstance { config, client }
                })
                .collect()
        });
        Self { telegram, discord, slack }
    }
    fn find_accepted_block_range(&self, from_block: &U64, to_block: &U64) -> U64 {
        if from_block > to_block {
            panic!("Invalid range: from_block must be less than or equal to to_block");
        }
        match from_block.overflowing_add(to_block - from_block) {
            (result, false) => result,
            (_, true) => U64::MAX,
        }
    }
    pub fn is_in_block_range_to_send(&self, from_block: &U64, to_block: &U64) -> bool {
        // only 10 blocks at a time else rate limits will kick in
        U64::from(10) <= self.find_accepted_block_range(from_block, to_block)
    }
    fn has_any_chat(&self) -> bool {
        self.telegram.is_some() || self.discord.is_some() || self.slack.is_some()
    }
    fn telegram_send_message_tasks(
        &self,
        instance: &TelegramInstance,
        event_for: &TelegramEvent,
        events_data: &[Value],
    ) -> SendMessage {
        let tasks: Vec<_> = events_data
            .iter()
            .filter(|event_data| {
                // Filter expression has priority over conditions
                // If both are present, the filter expression will be used
                // If neither is present, all events will be sent
                if let Some(expression) = &event_for.filter_expression {
                    let result = filter_by_expression(expression, event_data);
                    return match result {
                        Ok(res) => res,
                        Err(e) => {
                            tracing::error!("Error evaluating filter expression: {}", e);
                            false
                        }
                    };
                }
                if let Some(conditions) = &event_for.conditions {
                    return filter_event_data_by_conditions(event_data, conditions);
                }
                true
            })
            .map(|event_data| {
                let client = Arc::clone(&instance.client);
                let chat_id = ChatId(instance.config.chat_id);
                let message = Template::new(event_for.template_inline.clone())
                    .parse_template_inline(event_data);
                task::spawn(async move {
                    client.send_message(chat_id, &message).await?;
                    Ok(())
                })
            })
            .collect();
        tasks
    }
    fn discord_send_message_tasks(
        &self,
        instance: &DiscordInstance,
        event_for: &DiscordEvent,
        events_data: &[Value],
    ) -> SendMessage {
        let tasks: Vec<_> = events_data
            .iter()
            .filter(|event_data| {
                // Filter expression has priority over conditions
                // If both are present, the filter expression will be used
                // If neither is present, all events will be sent
                if let Some(expression) = &event_for.filter_expression {
                    let result = filter_by_expression(expression, event_data);
                    return match result {
                        Ok(res) => res,
                        Err(e) => {
                            tracing::error!("Error evaluating filter expression: {}", e);
                            false
                        }
                    };
                }
                if let Some(conditions) = &event_for.conditions {
                    return filter_event_data_by_conditions(event_data, conditions);
                }
                true
            })
            .map(|event_data| {
                let client = Arc::clone(&instance.client);
                let channel_id = ChannelId::new(instance.config.channel_id);
                let message = Template::new(event_for.template_inline.clone())
                    .parse_template_inline(event_data);
                task::spawn(async move {
                    client.send_message(channel_id, &message).await?;
                    Ok(())
                })
            })
            .collect();
        tasks
    }
    fn slack_send_message_tasks(
        &self,
        instance: &SlackInstance,
        event_for: &SlackEvent,
        events_data: &[Value],
    ) -> SendMessage {
        let tasks: Vec<_> = events_data
            .iter()
            .filter(|event_data| {
                // Filter expression has priority over conditions
                // If both are present, the filter expression will be used
                // If neither is present, all events will be sent
                if let Some(expression) = &event_for.filter_expression {
                    let result = filter_by_expression(expression, event_data);
                    return match result {
                        Ok(res) => res,
                        Err(e) => {
                            tracing::error!("Error evaluating filter expression: {}", e);
                            false
                        }
                    };
                }
                if let Some(conditions) = &event_for.conditions {
                    return filter_event_data_by_conditions(event_data, conditions);
                }
                true
            })
            .map(|event_data| {
                let client = Arc::clone(&instance.client);
                let channel = instance.config.channel.clone();
                let message = Template::new(event_for.template_inline.clone())
                    .parse_template_inline(event_data);
                task::spawn(async move {
                    client.send_message(&channel, &message).await?;
                    Ok(())
                })
            })
            .collect();
        tasks
    }
    pub async fn send_message(
        &self,
        event_message: &EventMessage,
        index_event_in_order: bool,
        from_block: &U64,
        to_block: &U64,
    ) -> Result<usize, ChatError> {
        if !self.has_any_chat() || !self.is_in_block_range_to_send(from_block, to_block) {
            return Ok(0);
        }
        // will always have something even if the event has no parameters due to the tx_information
        if let Value::Array(data_array) = &event_message.event_data {
            let mut messages: Vec<SendMessage> = Vec::new();
            if let Some(telegram) = &self.telegram {
                for instance in telegram {
                    if instance.config.networks.contains(&event_message.network) {
                        let telegram_event = instance
                            .config
                            .messages
                            .iter()
                            .find(|e| e.event_name == event_message.event_name);
                        if let Some(telegram_event) = telegram_event {
                            let message = self.telegram_send_message_tasks(
                                instance,
                                telegram_event,
                                data_array,
                            );
                            messages.push(message);
                        }
                    }
                }
            }
            if let Some(discord) = &self.discord {
                for instance in discord {
                    if instance.config.networks.contains(&event_message.network) {
                        let discord_event = instance
                            .config
                            .messages
                            .iter()
                            .find(|e| e.event_name == event_message.event_name);
                        if let Some(discord_event) = discord_event {
                            let message = self.discord_send_message_tasks(
                                instance,
                                discord_event,
                                data_array,
                            );
                            messages.push(message);
                        }
                    }
                }
            }
            if let Some(slack) = &self.slack {
                for instance in slack {
                    if instance.config.networks.contains(&event_message.network) {
                        let slack_event = instance
                            .config
                            .messages
                            .iter()
                            .find(|e| e.event_name == event_message.event_name);
                        if let Some(slack_event) = slack_event {
                            let message =
                                self.slack_send_message_tasks(instance, slack_event, data_array);
                            messages.push(message);
                        }
                    }
                }
            }
            let mut messages_sent = 0;
            if index_event_in_order {
                for message in messages {
                    for publish in message {
                        match publish.await {
                            Ok(Ok(_)) => messages_sent += 1,
                            Ok(Err(e)) => return Err(e),
                            Err(e) => return Err(ChatError::JoinError(e)),
                        }
                    }
                }
            } else {
                let tasks: Vec<_> = messages.into_iter().flatten().collect();
                let results = join_all(tasks).await;
                for result in results {
                    match result {
                        Ok(Ok(_)) => messages_sent += 1,
                        Ok(Err(e)) => return Err(e),
                        Err(e) => return Err(ChatError::JoinError(e)),
                    }
                }
            }
            Ok(messages_sent)
        } else {
            unreachable!("Event data should be an array");
        }
    }
}
</file>

<file path="core/src/chat/discord.rs">
use serenity::{http::Http, model::id::ChannelId};
use thiserror::Error;
#[derive(Error, Debug)]
pub enum DiscordError {
    #[error("Discord API error: {0}")]
    ApiError(#[from] serenity::Error),
}
#[derive(Debug)]
pub struct DiscordBot {
    http: Http,
}
impl DiscordBot {
    pub fn new(token: &str) -> Self {
        let http = Http::new(token);
        Self { http }
    }
    pub async fn send_message(
        &self,
        channel_id: ChannelId,
        message: &str,
    ) -> Result<(), DiscordError> {
        channel_id.say(&self.http, message).await?;
        Ok(())
    }
}
</file>

<file path="core/src/chat/mod.rs">
mod clients;
pub use clients::ChatClients;
mod discord;
mod slack;
mod telegram;
mod template;
</file>

<file path="core/src/chat/slack.rs">
use reqwest::Client;
use serde_json::json;
use thiserror::Error;
#[derive(Error, Debug)]
pub enum SlackError {
    #[error("HTTP request error: {0}")]
    ReqwestError(#[from] reqwest::Error),
    #[error("Could not parse response: {0}")]
    CouldNotParseResponse(#[from] serde_json::Error),
    #[error("Slack API error: {0}")]
    ApiError(String),
}
#[derive(Debug, Clone)]
pub struct SlackBot {
    client: Client,
    token: String,
}
impl SlackBot {
    pub fn new(token: String) -> Self {
        let client = Client::new();
        Self { client, token }
    }
    pub async fn send_message(&self, channel: &str, message: &str) -> Result<(), SlackError> {
        let url = "https://slack.com/api/chat.postMessage";
        let response = self
            .client
            .post(url)
            .header("Authorization", format!("Bearer {}", self.token))
            .header("Content-Type", "application/json")
            .json(&json!({
                "channel": channel,
                "blocks": [
                    {
                        "type": "section",
                        "text": {
                            "type": "mrkdwn",
                            "text": message
                        }
                    }
                ]
            }))
            .send()
            .await?;
        let response_text = response.text().await?;
        let response_json: serde_json::Value = serde_json::from_str(&response_text)?;
        if response_json["ok"].as_bool().unwrap_or(false) {
            Ok(())
        } else {
            Err(SlackError::ApiError(
                response_json["error"].as_str().unwrap_or("Unknown error").to_string(),
            ))
        }
    }
}
</file>

<file path="core/src/chat/telegram.rs">
use teloxide::{prelude::*, types::ParseMode, RequestError};
use thiserror::Error;
#[derive(Error, Debug)]
pub enum TelegramError {
    #[error("Telegram API error: {0}")]
    ApiError(#[from] RequestError),
}
#[derive(Debug, Clone)]
pub struct TelegramBot {
    bot: Bot,
}
impl TelegramBot {
    pub fn new(token: &str) -> Self {
        let bot = Bot::new(token);
        Self { bot }
    }
    pub async fn send_message(&self, chat_id: ChatId, message: &str) -> Result<(), TelegramError> {
        let escaped_message = self.escape_markdown_v2(message);
        self.bot.send_message(chat_id, &escaped_message).parse_mode(ParseMode::MarkdownV2).await?;
        Ok(())
    }
    fn escape_markdown_v2(&self, text: &str) -> String {
        text.chars()
            .map(|c| match c {
                '_' | '*' | '[' | ']' | '(' | ')' | '~' | '`' | '>' | '#' | '+' | '-' | '='
                | '|' | '{' | '}' | '.' | '!' | '\\' => {
                    format!("\\{}", c)
                }
                _ => c.to_string(),
            })
            .collect()
    }
}
</file>

<file path="core/src/chat/template.rs">
use std::str::FromStr;
use alloy::primitives::U64;
use regex::Regex;
use serde_json::Value;
#[derive(Debug, Clone)]
pub struct Template {
    value: String,
}
impl Template {
    pub fn new(value: String) -> Self {
        Self { value }
    }
    pub fn parse_template_inline(&self, event_data: &Value) -> String {
        let mut template = self.value.clone();
        let placeholders = self.extract_placeholders(&template);
        for placeholder in placeholders {
            if placeholder.contains('(') {
                if let Some(value) = self.evaluate_function(&placeholder, event_data) {
                    template = template.replace(&format!("{{{{{placeholder}}}}}"), &value);
                }
            } else if let Some(value) = self.get_nested_value(event_data, &placeholder) {
                template = template.replace(&format!("{{{{{placeholder}}}}}"), &value);
            }
        }
        template
    }
    fn extract_placeholders(&self, template: &str) -> Vec<String> {
        let mut placeholders = Vec::new();
        let mut start = 0;
        while let Some(start_index) = template[start..].find("{{") {
            if let Some(end_index) = template[start + start_index + 2..].find("}}") {
                let placeholder =
                    &template[start + start_index + 2..start + start_index + 2 + end_index];
                placeholders.push(placeholder.to_string());
                start += start_index + 2 + end_index + 2;
            } else {
                break;
            }
        }
        placeholders
    }
    fn get_nested_value(&self, data: &Value, path: &str) -> Option<String> {
        let keys: Vec<&str> = path.split('.').collect();
        let mut current = data;
        for key in keys {
            if let Some(value) = current.get(key) {
                current = value;
            } else {
                return None;
            }
        }
        Some(current.to_string().replace('"', ""))
    }
    fn evaluate_function(&self, function_call: &str, event_data: &Value) -> Option<String> {
        let re = Regex::new(r"(\w+)\(([^)]+)\)").unwrap();
        if let Some(captures) = re.captures(function_call) {
            let function_name = &captures[1];
            let args: Vec<&str> = captures[2].split(',').map(|s| s.trim()).collect();
            if function_name == "format_value" && args.len() == 2 {
                if let Some(value_str) = self.get_nested_value(event_data, args[0]) {
                    if let Ok(decimals) = args[1].parse::<u32>() {
                        return Some(self.format_value(&value_str, decimals));
                    }
                }
            }
        }
        None
    }
    fn format_value(&self, value: &str, decimals: u32) -> String {
        match U64::from_str(value) {
            Ok(v) => {
                let divisor = U64::from(10).pow(U64::from(decimals));
                let integer_part = v / divisor;
                let fractional_part = v % divisor;
                if fractional_part.is_zero() {
                    return integer_part.to_string();
                }
                format!("{integer_part}.{fractional_part}")
            }
            Err(_) => value.to_string(),
        }
    }
}
</file>

<file path="core/src/database/clickhouse/client.rs">
use crate::EthereumSqlTypeWrapper;
use clickhouse::{Client, Row};
use dotenv::dotenv;
use serde::Deserialize;
use std::env;
use tracing::info;
pub struct ClickhouseConnection {
    url: String,
    user: String,
    password: String,
    db: String,
}
pub fn clickhouse_connection() -> Result<ClickhouseConnection, env::VarError> {
    dotenv().ok();
    let connection = ClickhouseConnection {
        url: env::var("CLICKHOUSE_URL")?,
        user: env::var("CLICKHOUSE_USER")?,
        password: env::var("CLICKHOUSE_PASSWORD")?,
        db: env::var("CLICKHOUSE_DB")?,
    };
    Ok(connection)
}
#[derive(thiserror::Error, Debug)]
pub enum ClickhouseConnectionError {
    #[error("The clickhouse env vars are wrong please check your environment: {0}")]
    ClickhouseConnectionConfigWrong(#[from] env::VarError),
    #[error("Could not connect to clickhouse database: {0}")]
    ClickhouseNetworkError(#[from] clickhouse::error::Error),
}
#[derive(thiserror::Error, Debug)]
pub enum ClickhouseError {
    #[error("ClickhouseError: {0}")]
    ClickhouseError(#[from] clickhouse::error::Error),
}
pub struct ClickhouseClient {
    pub(crate) conn: Client,
}
impl ClickhouseClient {
    pub async fn new() -> Result<Self, ClickhouseConnectionError> {
        let connection = clickhouse_connection()?;
        let client = Client::default()
            .with_url(connection.url)
            .with_user(connection.user)
            .with_database(connection.db)
            .with_password(connection.password);
        client.query("select 1").execute().await?;
        info!("Clickhouse client connected successfully!");
        Ok(ClickhouseClient { conn: client })
    }
    pub async fn query_one<T>(&self, sql: &str) -> Result<T, ClickhouseError>
    where
        T: Row + for<'b> Deserialize<'b>,
    {
        let data = self.conn.query(sql).fetch_one().await?;
        Ok(data)
    }
    pub async fn query<T>(&self, sql: &str) -> Result<T, ClickhouseError>
    where
        T: Row + for<'b> Deserialize<'b>,
    {
        let data = self.conn.query(sql).fetch_one().await?;
        Ok(data)
    }
    pub async fn execute(&self, sql: &str) -> Result<(), ClickhouseError> {
        self.conn.query(sql).execute().await?;
        Ok(())
    }
    pub async fn execute_batch(&self, sql: &str) -> Result<(), ClickhouseError> {
        let statements: Vec<&str> =
            sql.split(';').map(str::trim).filter(|s| !s.is_empty()).collect();
        for statement in statements {
            self.execute(statement).await?;
        }
        Ok(())
    }
    pub(crate) async fn bulk_insert_via_query(
        &self,
        table_name: &str,
        column_names: &[String],
        bulk_data: &[Vec<EthereumSqlTypeWrapper>],
    ) -> Result<u64, ClickhouseError> {
        let values = bulk_data
            .iter()
            .map(|row| row.iter().map(|v| v.to_clickhouse_value()).collect::<Vec<_>>().join(", "))
            .map(|row| format!("({})", row))
            .collect::<Vec<_>>()
            .join(", ");
        self.execute(&format!(
            "INSERT INTO {} ({}) VALUES {}",
            table_name,
            column_names.join(", "),
            values
        ))
        .await?;
        Ok(bulk_data.len() as u64)
    }
    pub async fn insert_bulk(
        &self,
        table_name: &str,
        column_names: &[String],
        bulk_data: &[Vec<EthereumSqlTypeWrapper>],
    ) -> Result<u64, ClickhouseError> {
        self.bulk_insert_via_query(table_name, column_names, bulk_data).await
    }
}
</file>

<file path="core/src/database/clickhouse/create_batch_db_operation.rs">
/// Creates a batch operation function for ClickHouse database.
///
/// # Example
///
/// ```ignore
/// // Define the batch operation
/// create_batch_clickhouse_operation!(
///     update_reserve_supplied_shares,
///     UpdateReserveInfo,
///     "spoke.reserve",
///     BatchOperationType::Update,
///     |result: &UpdateReserveInfo| {
///         vec![
///             column(
///                 "spoke",
///                 EthereumSqlTypeWrapper::AddressBytes(result.withdraw.tx_information.address),
///                 BatchOperationSqlType::Bytea,
///                 BatchOperationColumnBehavior::Distinct,
///                 BatchOperationAction::Where,
///             ),
///             column(
///                 "reserve_id",
///                 EthereumSqlTypeWrapper::U256Numeric(result.withdraw.event_data.reserveId),
///                 BatchOperationSqlType::Numeric,
///                 BatchOperationColumnBehavior::Distinct,
///                 BatchOperationAction::Where,
///             ),
///             column(
///                 "chain_id",
///                 EthereumSqlTypeWrapper::U64BigInt(result.withdraw.tx_information.chain_id),
///                 BatchOperationSqlType::Bigint,
///                 BatchOperationColumnBehavior::Distinct,
///                 BatchOperationAction::Where,
///             ),
///             column(
///                 "supplied_shares",
///                 EthereumSqlTypeWrapper::U256Numeric(result.withdrawn_shares),
///                 BatchOperationSqlType::Numeric,
///                 BatchOperationColumnBehavior::Normal,
///                 BatchOperationAction::Subtract,
///             ),
///         ]
///     },
///     "Spoke::Withdraw - Update spoke.reserve"
/// );
///
/// // Call the generated function
/// update_reserve_supplied_shares(&context.database, &reserve_update_data).await?;
/// ```
#[macro_export]
macro_rules! create_batch_clickhouse_operation {
    (
        $func_name:ident,
        $result_type:ty,
        $table_name:expr,
        $op_type:expr,
        $columns_def:expr,
        $event_name:expr
    ) => {
        async fn $func_name(
            database: &ClickhouseClient,
            filtered_results: &[$result_type],
        ) -> Result<(), String> {
            use $crate::database::batch_operations::{
                BatchOperationAction, BatchOperationColumnBehavior, BatchOperationType,
                RESERVED_KEYWORDS,
            };
            async fn execute_batch(
                database: &ClickhouseClient,
                batch: &[$result_type],
            ) -> Result<(), String> {
                if batch.is_empty() {
                    return Ok(());
                }
                let columns = $columns_def(&batch[0]);
                // Get column names for INSERT
                let column_names: Vec<&str> = columns.iter().map(|col| col.name).collect();
                // Get WHERE columns for DELETE operations
                let where_columns: Vec<&str> = columns
                    .iter()
                    .filter_map(|col| match col.action {
                        BatchOperationAction::Where => Some(col.name),
                        _ => None,
                    })
                    .collect();
                // Get DISTINCT columns (also used for DELETE WHERE if no explicit WHERE columns)
                let distinct_cols: Vec<&str> = columns
                    .iter()
                    .filter_map(|col| match col.behavior {
                        BatchOperationColumnBehavior::Distinct => Some(col.name),
                        _ => None,
                    })
                    .collect();
                // Helper to quote column names if reserved
                let quote_col = |col: &str| -> String {
                    if RESERVED_KEYWORDS.contains(&col) {
                        format!("`{}`", col)
                    } else {
                        col.to_string()
                    }
                };
                // Handle table name (ClickHouse uses backticks for quoting)
                let formatted_table_name = if $table_name.contains('.') {
                    let parts: Vec<&str> = $table_name.split('.').collect();
                    if parts.len() == 2 {
                        let schema = parts[0].trim_matches('"').trim_matches('`');
                        let table = parts[1].trim_matches('"').trim_matches('`');
                        format!("`{}`.`{}`", schema, table)
                    } else {
                        $table_name.to_string()
                    }
                } else {
                    $table_name.to_string()
                };
                match $op_type {
                    BatchOperationType::Update | BatchOperationType::Upsert => {
                        // In ClickHouse, both Update and Upsert map to INSERT
                        // ReplacingMergeTree automatically keeps the latest version
                        // based on ORDER BY columns
                        let formatted_columns = column_names
                            .iter()
                            .map(|col| quote_col(col))
                            .collect::<Vec<_>>()
                            .join(", ");
                        let mut values_parts: Vec<String> = Vec::new();
                        for result in batch.iter() {
                            let columns_for_result = $columns_def(result);
                            let row_values: Vec<String> = columns_for_result
                                .iter()
                                .map(|col| col.value.to_clickhouse_value())
                                .collect();
                            values_parts.push(format!("({})", row_values.join(", ")));
                        }
                        let query = format!(
                            "INSERT INTO {} ({}) VALUES {}",
                            formatted_table_name,
                            formatted_columns,
                            values_parts.join(", ")
                        );
                        database.execute(&query).await.map_err(|e| e.to_string())?;
                    }
                    BatchOperationType::Delete => {
                        // In ClickHouse, DELETE is performed via ALTER TABLE mutation
                        // Build WHERE clause: (col1 = val1 AND col2 = val2) OR (col1 = val3 AND col2 = val4)
                        // Determine which columns to use for matching
                        let match_columns: Vec<&str> = if !where_columns.is_empty() {
                            where_columns.clone()
                        } else {
                            distinct_cols.clone()
                        };
                        if match_columns.is_empty() {
                            return Err(
                                "Delete operation requires WHERE or DISTINCT columns".to_string()
                            );
                        }
                        let mut or_conditions: Vec<String> = Vec::new();
                        for result in batch.iter() {
                            let columns_for_result = $columns_def(result);
                            let mut and_conditions: Vec<String> = Vec::new();
                            for match_col in &match_columns {
                                if let Some(col_def) =
                                    columns_for_result.iter().find(|c| c.name == *match_col)
                                {
                                    let quoted_col = quote_col(match_col);
                                    let value = col_def.value.to_clickhouse_value();
                                    and_conditions.push(format!("{} = {}", quoted_col, value));
                                }
                            }
                            if !and_conditions.is_empty() {
                                or_conditions.push(format!("({})", and_conditions.join(" AND ")));
                            }
                        }
                        if or_conditions.is_empty() {
                            return Ok(());
                        }
                        let query = format!(
                            "ALTER TABLE {} DELETE WHERE {}",
                            formatted_table_name,
                            or_conditions.join(" OR ")
                        );
                        database.execute(&query).await.map_err(|e| e.to_string())?;
                    }
                }
                Ok(())
            }
            for batch in filtered_results.chunks(1000) {
                if let Err(e) = execute_batch(database, batch).await {
                    rindexer_error!("{} - Batch operation failed: {}", $event_name, e);
                    return Err(e);
                }
            }
            Ok(())
        }
    };
}
</file>

<file path="core/src/database/clickhouse/generate.rs">
use std::path::Path;
use tracing::{error, info};
use crate::{
    abi::{ABIInput, ABIItem, EventInfo, GenerateAbiPropertiesType},
    helpers::camel_to_snake,
    indexer::Indexer,
    types::code::Code,
};
use crate::database::generate::{
    generate_indexer_contract_schema_name, generate_internal_factory_event_table_name,
    generate_internal_factory_event_table_name_no_shorten, GenerateTablesForIndexerSqlError,
};
use crate::database::postgres::generate::{
    generate_internal_event_table_name_no_shorten, GenerateInternalFactoryEventTableNameParams,
};
use crate::manifest::contract::FactoryDetailsYaml;
pub fn generate_tables_for_indexer_clickhouse(
    project_path: &Path,
    indexer: &Indexer,
    disable_event_tables: bool,
) -> Result<Code, GenerateTablesForIndexerSqlError> {
    let mut sql = "CREATE DATABASE IF NOT EXISTS rindexer_internal;".to_string();
    for contract in &indexer.contracts {
        let contract_name = contract.before_modify_name_if_filter_readonly();
        let abi_items = ABIItem::read_abi_items(project_path, contract)?;
        let event_names = ABIItem::extract_event_names_and_signatures_from_abi(abi_items)?;
        let schema_name = generate_indexer_contract_schema_name(&indexer.name, &contract_name);
        let networks: Vec<&str> = contract.details.iter().map(|d| d.network.as_str()).collect();
        let factories = contract.details.iter().flat_map(|d| d.factory.clone()).collect::<Vec<_>>();
        if !disable_event_tables {
            sql.push_str(format!("CREATE DATABASE IF NOT EXISTS {};", schema_name).as_str());
            info!("Creating database if not exists: {}", schema_name);
            sql.push_str(&generate_event_table_clickhouse(&event_names, &schema_name));
        }
        sql.push_str(&generate_internal_event_table_clickhouse(
            &event_names,
            &schema_name,
            networks,
        ));
        sql.push_str(&generate_internal_factory_event_table_sql(&indexer.name, &factories));
    }
    Ok(Code::new(sql))
}
fn generate_event_table_clickhouse(abi_inputs: &[EventInfo], schema_name: &str) -> String {
    abi_inputs
        .iter()
        .map(|event_info| {
            let table_name = format!("{}.{}", schema_name, camel_to_snake(&event_info.name));
            info!("Creating table if not exists: {}", table_name);
            let event_columns = if event_info.inputs.is_empty() {
                "".to_string()
            } else {
                generate_columns_with_data_types(&event_info.inputs).join(", ") + ","
            };
            let create_table_sql = format!(
                r#"CREATE TABLE IF NOT EXISTS {} (
                    contract_address FixedString(42),
                    {}
                    tx_hash FixedString(66),
                    block_number UInt64,
                    block_timestamp Nullable(DateTime('UTC')),
                    block_hash FixedString(66),
                    network String,
                    tx_index UInt64,
                    log_index UInt64,
                    index idx_block_num (block_number) type minmax granularity 1,
                    index idx_timestamp (block_timestamp) type minmax granularity 1,
                    index idx_network (network) type bloom_filter granularity 1,
                    index idx_tx_hash (tx_hash) type bloom_filter granularity 1
                )
                ENGINE = ReplacingMergeTree
                ORDER BY (network, block_number, tx_hash, log_index);"#,
                table_name, event_columns
            );
            create_table_sql
        })
        .collect::<Vec<_>>()
        .join("\n")
}
fn generate_internal_event_table_clickhouse(
    abi_inputs: &[EventInfo],
    schema_name: &str,
    networks: Vec<&str>,
) -> String {
    abi_inputs.iter().map(|event_info| {
        let table_name = format!(
            "rindexer_internal.{}_{}",
            schema_name,
            camel_to_snake(&event_info.name)
        );
        let create_table_query = format!(
            r#"
                CREATE TABLE IF NOT EXISTS {} (
                    "network" String,
                    "last_synced_block" UInt64
                )
                    ENGINE = ReplacingMergeTree(last_synced_block)
                    ORDER BY network;"#,
            table_name
        );
        let insert_queries = networks.iter().map(|network| {
            format!(
                r#"INSERT INTO {} ("network", "last_synced_block") VALUES ('{}', 0);"#,
                table_name,
                network
            )
        }).collect::<Vec<_>>().join("\n");
        let create_latest_block_query = r#"
            CREATE TABLE IF NOT EXISTS rindexer_internal.latest_block (
                "network" String,
                "block" UInt64
              )
              ENGINE = ReplacingMergeTree(block)
                ORDER BY network;
        "#.to_string();
        let latest_block_insert_queries = networks.iter().map(|network| {
            format!(
                r#"INSERT INTO rindexer_internal.latest_block ("network", "block") VALUES ('{network}', 0);"#
            )
        }).collect::<Vec<_>>().join("\n");
        format!("{} {} {} {}", create_table_query, insert_queries, create_latest_block_query, latest_block_insert_queries)
    }).collect::<Vec<_>>().join("\n")
}
fn generate_internal_factory_event_table_sql(
    indexer_name: &str,
    factories: &[FactoryDetailsYaml],
) -> String {
    factories
        .iter()
        .map(|factory| {
            let params = GenerateInternalFactoryEventTableNameParams {
                indexer_name: indexer_name.to_string(),
                contract_name: factory.name.to_string(),
                event_name: factory.event_name.to_string(),
                input_names: factory.input_names(),
            };
            let table_name = generate_internal_factory_event_table_name_no_shorten(&params);
            let create_table_query = format!(
                r#"
                CREATE TABLE IF NOT EXISTS rindexer_internal.{table_name} (
                    "factory_address" FixedString(42),
                    "factory_deployed_address" FixedString(42),
                    "network" String
                )
                ENGINE = ReplacingMergeTree()
                    ORDER BY ("network", "factory_address", "factory_deployed_address");
                "#
            );
            create_table_query
        })
        .collect::<Vec<_>>()
        .join("\n")
}
fn generate_columns(inputs: &[ABIInput], property_type: &GenerateAbiPropertiesType) -> Vec<String> {
    ABIInput::generate_abi_name_properties(inputs, property_type, None)
        .into_iter()
        .map(|m| m.value)
        .collect()
}
pub fn generate_columns_with_data_types(inputs: &[ABIInput]) -> Vec<String> {
    generate_columns(inputs, &GenerateAbiPropertiesType::ClickhouseWithDataTypes)
}
pub fn drop_tables_for_indexer_clickhouse(project_path: &Path, indexer: &Indexer) -> Code {
    let mut sql = String::new();
    sql.push_str("DROP TABLE IF EXISTS rindexer_internal.latest_block;");
    for contract in &indexer.contracts {
        let contract_name = contract.before_modify_name_if_filter_readonly();
        let schema_name = generate_indexer_contract_schema_name(&indexer.name, &contract_name);
        sql.push_str(format!("DROP DATABASE IF EXISTS {schema_name};").as_str());
        // drop last synced blocks for contracts
        let abi_items = ABIItem::read_abi_items(project_path, contract);
        if let Ok(abi_items) = abi_items {
            for abi_item in abi_items.iter() {
                let table_name =
                    generate_internal_event_table_name_no_shorten(&schema_name, &abi_item.name);
                sql.push_str(
                    format!("DROP TABLE IF EXISTS rindexer_internal.{table_name};").as_str(),
                );
            }
        } else {
            error!(
                "Could not read ABI items for contract moving on clearing the other data up: {}",
                contract.name
            );
        }
        // drop factory indexing tables
        for factory in contract.details.iter().flat_map(|d| d.factory.as_ref()) {
            let params = GenerateInternalFactoryEventTableNameParams {
                indexer_name: indexer.name.clone(),
                contract_name: factory.name.clone(),
                event_name: factory.event_name.clone(),
                input_names: factory.input_names(),
            };
            let table_name = generate_internal_factory_event_table_name(&params);
            sql.push_str(format!("DROP TABLE IF EXISTS rindexer_internal.{table_name};").as_str())
        }
    }
    Code::new(sql)
}
#[allow(clippy::manual_strip)]
pub fn solidity_type_to_clickhouse_type(abi_type: &str) -> String {
    let is_array = abi_type.ends_with("[]");
    let base_type = abi_type.trim_end_matches("[]");
    let sql_type = match base_type {
        "address" => "FixedString(42)",
        "bool" => "Bool",
        "string" => "String",
        t if t.starts_with("bytes") => "String",
        t if t.starts_with("int") || t.starts_with("uint") => {
            let (prefix, size): (&str, usize) = if t.starts_with("int") {
                ("int", t[3..].parse().unwrap_or(256))
            } else {
                ("uint", t[4..].parse().unwrap_or(256))
            };
            let rounded_size = match size {
                0..=8 => 8,
                9..=16 => 16,
                17..=32 => 32,
                33..=64 => 64,
                65..=128 => 128,
                129..=256 => 256,
                _ => 512, // fallback to String
            };
            let int = match rounded_size {
                8 => "Int8",
                16 => "Int16",
                32 => "Int32",
                64 => "Int64",
                128 => "Int128",
                256 => "Int256",
                512 => "String",
                _ => unreachable!(),
            };
            if prefix == "uint" && rounded_size <= 256 {
                &format!("U{}", int)
            } else {
                int
            }
        }
        _ => panic!("Unsupported type: {}", base_type),
    };
    // Return the SQL type, appending array brackets if necessary
    if is_array {
        // ClickHouse does not have native array types with specific sizes like PostgreSQL
        // Represent arrays as Array(T) where T is the base type
        format!("Array({})", sql_type)
    } else {
        sql_type.to_string()
    }
}
</file>

<file path="core/src/database/clickhouse/mod.rs">
pub mod client;
pub mod create_batch_db_operation;
pub mod generate;
pub mod setup;
</file>

<file path="core/src/database/clickhouse/setup.rs">
use crate::database::clickhouse::client::{
    ClickhouseClient, ClickhouseConnectionError, ClickhouseError,
};
use crate::database::clickhouse::generate::{
    drop_tables_for_indexer_clickhouse, generate_tables_for_indexer_clickhouse,
};
use crate::database::generate::GenerateTablesForIndexerSqlError;
use crate::manifest::core::Manifest;
use std::path::Path;
use tracing::info;
#[allow(clippy::enum_variant_names)]
#[derive(thiserror::Error, Debug)]
pub enum SetupClickhouseError {
    #[error("Clickhouse connection error {0}")]
    ClickhouseConnectionError(#[from] ClickhouseConnectionError),
    #[error("Failed to generate tables for indexer: {0}")]
    ClickhouseTableGenerationError(#[from] GenerateTablesForIndexerSqlError),
    #[error("Clickhouse execution error {0}")]
    ClickhouseExecutionError(#[from] ClickhouseError),
}
pub async fn setup_clickhouse(
    project_path: &Path,
    manifest: &Manifest,
) -> Result<ClickhouseClient, SetupClickhouseError> {
    info!("Setting up clickhouse");
    let client =
        ClickhouseClient::new().await.map_err(SetupClickhouseError::ClickhouseConnectionError)?;
    let disable_event_tables = manifest.storage.clickhouse_disable_create_tables();
    if manifest.storage.clickhouse_drop_each_run() {
        info!(
            "`drop_each_run` enabled so dropping all data for {} before starting",
            &manifest.name
        );
        let sql = drop_tables_for_indexer_clickhouse(project_path, &manifest.to_indexer());
        client.execute_batch(sql.as_str()).await?;
        info!("Dropped all data for {}", manifest.name);
    }
    if disable_event_tables {
        info!("Creating internal rindexer tables for {}", manifest.name);
    } else {
        info!("Creating tables for {}", manifest.name);
    }
    let sql = generate_tables_for_indexer_clickhouse(
        project_path,
        &manifest.to_indexer(),
        disable_event_tables,
    )
    .map_err(SetupClickhouseError::ClickhouseTableGenerationError)?;
    client.execute_batch(sql.as_str()).await?;
    if disable_event_tables {
        info!("Created tables for {}", manifest.name);
    } else {
        info!("Created internal rindexer tables for {}", manifest.name);
    }
    Ok(client)
}
</file>

<file path="core/src/database/postgres/client.rs">
use std::{env, future::Future, time::Duration};
use bb8::{Pool, PooledConnection, RunError};
use bb8_postgres::PostgresConnectionManager;
use bytes::Buf;
use dotenv::dotenv;
use futures::pin_mut;
use native_tls::TlsConnector;
use postgres_native_tls::MakeTlsConnector;
use tokio::{task, time::timeout};
pub use tokio_postgres::types::{ToSql, Type as PgType};
use tokio_postgres::{
    binary_copy::BinaryCopyInWriter, config::SslMode, Config, CopyInSink, Error as PgError, Row,
    Statement, ToStatement, Transaction as PgTransaction,
};
use tracing::error;
use crate::database::generate::generate_event_table_columns_names_sql;
use crate::database::sql_type_wrapper::EthereumSqlTypeWrapper;
pub fn connection_string() -> Result<String, env::VarError> {
    dotenv().ok();
    let connection = env::var("DATABASE_URL")?;
    Ok(connection)
}
#[derive(thiserror::Error, Debug)]
pub enum PostgresConnectionError {
    #[error("The database connection string is wrong please check your environment: {0}")]
    DatabaseConnectionConfigWrong(#[from] env::VarError),
    #[error("Connection pool error: {0}")]
    ConnectionPoolError(#[from] tokio_postgres::Error),
    #[error("Connection pool runtime error: {0}")]
    ConnectionPoolRuntimeError(#[from] RunError<tokio_postgres::Error>),
    #[error("Can not connect to the database please make sure your connection string is correct")]
    CanNotConnectToDatabase,
    #[error("Could not parse connection string make sure it is correctly formatted")]
    CouldNotParseConnectionString,
    #[error("Could not create tls connector")]
    CouldNotCreateTlsConnector,
}
#[derive(thiserror::Error, Debug)]
pub enum PostgresError {
    #[error("PgError {0}")]
    PgError(#[from] PgError),
    #[error("Connection pool error: {0}")]
    ConnectionPoolError(#[from] RunError<tokio_postgres::Error>),
}
#[allow(unused)]
pub struct PostgresTransaction<'a> {
    pub transaction: PgTransaction<'a>,
}
impl PostgresTransaction<'_> {
    #[allow(unused)]
    pub async fn execute(
        &mut self,
        query: &str,
        params: &[&(dyn ToSql + Sync)],
    ) -> Result<u64, PostgresError> {
        self.transaction.execute(query, params).await.map_err(PostgresError::PgError)
    }
    #[allow(unused)]
    pub async fn commit(self) -> Result<(), PostgresError> {
        self.transaction.commit().await.map_err(PostgresError::PgError)
    }
    #[allow(unused)]
    pub async fn rollback(self) -> Result<(), PostgresError> {
        self.transaction.rollback().await.map_err(PostgresError::PgError)
    }
}
#[derive(thiserror::Error, Debug)]
pub enum BulkInsertPostgresError {
    #[error("{0}")]
    PostgresError(#[from] PostgresError),
    #[error("{0}")]
    CouldNotWriteDataToPostgres(#[from] tokio_postgres::Error),
}
pub struct PostgresClient {
    pool: Pool<PostgresConnectionManager<MakeTlsConnector>>,
}
impl PostgresClient {
    pub async fn new() -> Result<Self, PostgresConnectionError> {
        async fn _new(disable_ssl: bool) -> Result<PostgresClient, PostgresConnectionError> {
            let connection_str = connection_string()?;
            let mut config: Config = connection_str
                .parse()
                .map_err(|_| PostgresConnectionError::CouldNotParseConnectionString)?;
            if disable_ssl {
                config.ssl_mode(SslMode::Disable);
            }
            let connector = TlsConnector::builder()
                .build()
                .map_err(|_| PostgresConnectionError::CouldNotCreateTlsConnector)?;
            let tls_connector = MakeTlsConnector::new(connector);
            // Perform a direct connection test
            let (client, connection) =
                match timeout(Duration::from_millis(5000), config.connect(tls_connector.clone()))
                    .await
                {
                    Ok(Ok((client, connection))) => (client, connection),
                    Ok(Err(e)) => {
                        // retry without ssl if ssl has been attempted and failed
                        if !disable_ssl
                            && config.get_ssl_mode() != SslMode::Disable
                            && !connection_str.contains("sslmode=require")
                        {
                            return Box::pin(_new(true)).await;
                        }
                        error!("Error connecting to database: {}", e);
                        return Err(PostgresConnectionError::CanNotConnectToDatabase);
                    }
                    Err(e) => {
                        error!("Timeout connecting to database: {}", e);
                        return Err(PostgresConnectionError::CanNotConnectToDatabase);
                    }
                };
            // Spawn the connection future to ensure the connection is established
            let connection_handle = task::spawn(connection);
            // Perform a simple query to check the connection
            match client.query_one("SELECT 1", &[]).await {
                Ok(_) => {}
                Err(_) => return Err(PostgresConnectionError::CanNotConnectToDatabase),
            };
            // Drop the client and ensure the connection handle completes
            drop(client);
            match connection_handle.await {
                Ok(Ok(())) => (),
                Ok(Err(_)) => return Err(PostgresConnectionError::CanNotConnectToDatabase),
                Err(_) => return Err(PostgresConnectionError::CanNotConnectToDatabase),
            }
            let manager = PostgresConnectionManager::new(config, tls_connector);
            // TODO: It's important for users to be able to define the pool size they want.
            // Rust binding projects can use this client, but it's critical to have config access.
            let pool = Pool::builder().build(manager).await?;
            Ok(PostgresClient { pool })
        }
        _new(false).await
    }
    pub async fn from_connection(
        pool: Pool<PostgresConnectionManager<MakeTlsConnector>>,
    ) -> Result<Self, PostgresConnectionError> {
        Ok(Self { pool })
    }
    pub async fn batch_execute(&self, sql: &str) -> Result<(), PostgresError> {
        let conn = self.pool.get().await?;
        conn.batch_execute(sql).await.map_err(PostgresError::PgError)
    }
    pub async fn execute<T>(
        &self,
        query: &T,
        params: &[&(dyn ToSql + Sync)],
    ) -> Result<u64, PostgresError>
    where
        T: ?Sized + ToStatement,
    {
        let conn = self.pool.get().await?;
        conn.execute(query, params).await.map_err(PostgresError::PgError)
    }
    pub async fn prepare(
        &self,
        query: &str,
        parameter_types: &[PgType],
    ) -> Result<Statement, PostgresError> {
        let conn = self.pool.get().await?;
        conn.prepare_typed(query, parameter_types).await.map_err(PostgresError::PgError)
    }
    pub async fn with_transaction<F, Fut, T, Q>(
        &self,
        query: &Q,
        params: &[&(dyn ToSql + Sync)],
        f: F,
    ) -> Result<T, PostgresError>
    where
        F: FnOnce(u64) -> Fut + Send,
        Fut: Future<Output = Result<T, PostgresError>> + Send,
        Q: ?Sized + ToStatement,
    {
        let mut conn = self.pool.get().await.map_err(PostgresError::ConnectionPoolError)?;
        let transaction = conn.transaction().await.map_err(PostgresError::PgError)?;
        let count = transaction.execute(query, params).await.map_err(PostgresError::PgError)?;
        let result = f(count).await?;
        transaction.commit().await.map_err(PostgresError::PgError)?;
        Ok(result)
    }
    pub async fn query<T>(
        &self,
        query: &T,
        params: &[&(dyn ToSql + Sync)],
    ) -> Result<Vec<Row>, PostgresError>
    where
        T: ?Sized + ToStatement,
    {
        let conn = self.pool.get().await?;
        let rows = conn.query(query, params).await.map_err(PostgresError::PgError)?;
        Ok(rows)
    }
    pub async fn query_one<T>(
        &self,
        query: &T,
        params: &[&(dyn ToSql + Sync)],
    ) -> Result<Row, PostgresError>
    where
        T: ?Sized + ToStatement,
    {
        let conn = self.pool.get().await?;
        let row = conn.query_one(query, params).await.map_err(PostgresError::PgError)?;
        Ok(row)
    }
    pub async fn query_one_or_none<T>(
        &self,
        query: &T,
        params: &[&(dyn ToSql + Sync)],
    ) -> Result<Option<Row>, PostgresError>
    where
        T: ?Sized + ToStatement,
    {
        let conn = self.pool.get().await?;
        let row = conn.query_opt(query, params).await.map_err(PostgresError::PgError)?;
        Ok(row)
    }
    pub async fn batch_insert<T>(
        &self,
        query: &T,
        params_list: Vec<Vec<Box<dyn ToSql + Send + Sync>>>,
    ) -> Result<(), PostgresError>
    where
        T: ?Sized + ToStatement,
    {
        let mut conn = self.pool.get().await?;
        let transaction = conn.transaction().await.map_err(PostgresError::PgError)?;
        for params in params_list {
            let params_refs: Vec<&(dyn ToSql + Sync)> =
                params.iter().map(|param| param.as_ref() as &(dyn ToSql + Sync)).collect();
            transaction.execute(query, &params_refs).await.map_err(PostgresError::PgError)?;
        }
        transaction.commit().await.map_err(PostgresError::PgError)?;
        Ok(())
    }
    pub async fn copy_in<T, U>(&self, statement: &T) -> Result<CopyInSink<U>, PostgresError>
    where
        T: ?Sized + ToStatement,
        U: Buf + 'static + Send,
    {
        let conn = self.pool.get().await?;
        conn.copy_in(statement).await.map_err(PostgresError::PgError)
    }
    // Internal method used by insert_bulk for large datasets (>100 rows).
    // Uses PostgreSQL COPY command for optimal performance with large data.
    // Made pub(crate) to allow crate-internal access while keeping insert_bulk as the primary API.
    pub(crate) async fn bulk_insert_via_copy(
        &self,
        table_name: &str,
        column_names: &[String],
        column_types: &[PgType],
        data: &[Vec<EthereumSqlTypeWrapper>],
    ) -> Result<(), BulkInsertPostgresError> {
        let stmt = format!(
            "COPY {} ({}) FROM STDIN WITH (FORMAT binary)",
            table_name,
            generate_event_table_columns_names_sql(column_names),
        );
        // info!("Bulk insert statement: {}", stmt);
        let prepared_data: Vec<Vec<&(dyn ToSql + Sync)>> = data
            .iter()
            .map(|row| row.iter().map(|param| param as &(dyn ToSql + Sync)).collect())
            .collect();
        // info!("Prepared data: {:?}", prepared_data);
        let sink = self.copy_in(&stmt).await?;
        let writer = BinaryCopyInWriter::new(sink, column_types);
        pin_mut!(writer);
        // This can cause issues with Binary Copy command not completing and leaving hanging
        // processes. See similar: https://github.com/sfackler/rust-postgres/issues/1109
        //
        // We have to call `finish` manually on any write error.
        for row in prepared_data.iter() {
            if let Err(e) = writer.as_mut().write(row).await {
                error!("Error writing binary data, aborting early: {}", e);
                writer.finish().await?;
                return Err(e)?;
            };
        }
        writer.finish().await?;
        Ok(())
    }
    // Internal method used by insert_bulk for small datasets (≤100 rows).
    // Uses standard INSERT queries which are more efficient for smaller data volumes.
    // Made pub(crate) to allow crate-internal access while keeping insert_bulk as the primary API.
    pub(crate) async fn bulk_insert_via_query(
        &self,
        table_name: &str,
        column_names: &[String],
        bulk_data: &[Vec<EthereumSqlTypeWrapper>],
    ) -> Result<u64, PostgresError> {
        let total_columns = column_names.len();
        // good for debugging
        // for (i, row) in bulk_data.iter().enumerate() {
        //     for (j, param) in row.iter().enumerate() {
        //         tracing::info!(
        //             "Row {} Column {} ({:?}) -> Value: {:?}, Type: {:?}",
        //             i,
        //             j,
        //             column_names.get(j),
        //             param,
        //             param.to_type()
        //         );
        //     }
        // }
        let mut query = format!(
            "INSERT INTO {} ({}) VALUES ",
            table_name,
            generate_event_table_columns_names_sql(column_names),
        );
        let mut params: Vec<&(dyn ToSql + Sync)> = Vec::new();
        for (i, row) in bulk_data.iter().enumerate() {
            if i > 0 {
                query.push(',');
            }
            let mut placeholders = vec![];
            for j in 0..total_columns {
                placeholders.push(format!("${}", i * total_columns + j + 1));
            }
            query.push_str(&format!("({})", placeholders.join(",")));
            for param in row {
                params.push(param as &(dyn ToSql + Sync));
            }
        }
        // Good for debugging
        // tracing::info!("query: {:?}", query);
        // tracing::info!(
        //     "params original types: {:?}",
        //     bulk_data.iter().flat_map(|row| row.iter().map(|p|
        // p.to_type())).collect::<Vec<_>>()     );
        self.execute(&query, &params).await
    }
    /// This will use COPY to insert the data into the database
    /// or use the normal bulk inserts if the data is not large enough to
    /// need a COPY. This uses `bulk_insert` and `bulk_insert_via_copy` under the hood
    pub async fn insert_bulk(
        &self,
        table_name: &str,
        columns: &[String],
        postgres_bulk_data: &[Vec<EthereumSqlTypeWrapper>],
    ) -> Result<(), String> {
        if postgres_bulk_data.is_empty() {
            return Ok(());
        }
        let total_params = postgres_bulk_data.len() * columns.len();
        // PostgreSQL has a maximum of 65535 parameters in a single query
        // (see https://www.postgresql.org/docs/current/limits.html#LIMITS-TABLE)
        // If we exceed this limit, force use of COPY method
        if postgres_bulk_data.len() > 100 || total_params > 65535 {
            let column_types: Vec<PgType> =
                postgres_bulk_data[0].iter().map(|param| param.to_type()).collect();
            self.bulk_insert_via_copy(table_name, columns, &column_types, postgres_bulk_data)
                .await
                .map_err(|e| e.to_string())
        } else {
            self.bulk_insert_via_query(table_name, columns, postgres_bulk_data)
                .await
                .map(|_| ())
                .map_err(|e| e.to_string())
        }
    }
    pub async fn raw_connection(
        &self,
    ) -> Result<PooledConnection<'_, PostgresConnectionManager<MakeTlsConnector>>, PostgresError>
    {
        let conn = self.pool.get().await?;
        Ok(conn)
    }
}
</file>

<file path="core/src/database/postgres/create_batch_db_operation.rs">
/// Creates a batch operation function for PostgreSQL database.
///
/// # Example
///
/// ```ignore
/// // Define the batch operation
/// create_batch_postgres_operation!(
///     update_reserve_supplied_shares,
///     UpdateReserveInfo,
///     "spoke.reserve",
///     BatchOperationType::Update,
///     |result: &UpdateReserveInfo| {
///         vec![
///             column(
///                 "spoke",
///                 EthereumSqlTypeWrapper::AddressBytes(result.withdraw.tx_information.address),
///                 BatchOperationSqlType::Bytea,
///                 BatchOperationColumnBehavior::Distinct,
///                 BatchOperationAction::Where,
///             ),
///             column(
///                 "reserve_id",
///                 EthereumSqlTypeWrapper::U256Numeric(result.withdraw.event_data.reserveId),
///                 BatchOperationSqlType::Numeric,
///                 BatchOperationColumnBehavior::Distinct,
///                 BatchOperationAction::Where,
///             ),
///             column(
///                 "chain_id",
///                 EthereumSqlTypeWrapper::U64BigInt(result.withdraw.tx_information.chain_id),
///                 BatchOperationSqlType::Bigint,
///                 BatchOperationColumnBehavior::Distinct,
///                 BatchOperationAction::Where,
///             ),
///             column(
///                 "supplied_shares",
///                 EthereumSqlTypeWrapper::U256Numeric(result.withdrawn_shares),
///                 BatchOperationSqlType::Numeric,
///                 BatchOperationColumnBehavior::Normal,
///                 BatchOperationAction::Subtract,
///             ),
///         ]
///     },
///     "Spoke::Withdraw - Update spoke.reserve"
/// );
///
/// // Call the generated function
/// update_reserve_supplied_shares(&context.database, &reserve_update_data).await?;
/// ```
#[macro_export]
macro_rules! create_batch_postgres_operation {
    (
        $func_name:ident,
        $result_type:ty,
        $table_name:expr,
        $op_type:expr,
        $columns_def:expr,
        $event_name:expr
    ) => {
        async fn $func_name(
            database: &PostgresClient,
            filtered_results: &[$result_type],
        ) -> Result<(), String> {
            use $crate::database::batch_operations::{
                BatchOperationAction, BatchOperationColumnBehavior, BatchOperationType,
                RESERVED_KEYWORDS,
            };
            async fn execute_batch(
                database: &PostgresClient,
                batch: &[$result_type],
            ) -> Result<(), String> {
                if batch.is_empty() {
                    return Ok(());
                }
                let columns = $columns_def(&batch[0]);
                let cte_cols_and_types: Vec<(&str, &str)> =
                    columns.iter().map(|col| (col.name, col.sql_type.as_str())).collect();
                let distinct_cols: Vec<&str> = columns
                    .iter()
                    .filter_map(|col| match col.behavior {
                        BatchOperationColumnBehavior::Distinct => Some(col.name),
                        _ => None,
                    })
                    .collect();
                let sequence_col = columns.iter().find_map(|col| match col.behavior {
                    BatchOperationColumnBehavior::Sequence => Some(col.name),
                    _ => None,
                });
                let set_columns: Vec<&str> = columns
                    .iter()
                    .filter_map(|col| match col.action {
                        BatchOperationAction::Set => Some(col.name),
                        _ => None,
                    })
                    .collect();
                let add_columns: Vec<&str> = columns
                    .iter()
                    .filter_map(|col| match col.action {
                        BatchOperationAction::Add => Some(col.name),
                        _ => None,
                    })
                    .collect();
                let subtract_columns: Vec<&str> = columns
                    .iter()
                    .filter_map(|col| match col.action {
                        BatchOperationAction::Subtract => Some(col.name),
                        _ => None,
                    })
                    .collect();
                let where_columns: Vec<&str> = columns
                    .iter()
                    .filter_map(|col| match col.action {
                        BatchOperationAction::Where => Some(col.name),
                        _ => None,
                    })
                    .collect();
                let formatted_cte_cols = cte_cols_and_types
                    .iter()
                    .map(|(col, _)| {
                        if RESERVED_KEYWORDS.contains(col) {
                            format!("\"{}\"", col)
                        } else {
                            col.to_string()
                        }
                    })
                    .collect::<Vec<_>>()
                    .join(", ");
                let mut query = format!(
                    "
                    WITH raw_data ({}) AS (
                        VALUES
                    ",
                    formatted_cte_cols
                );
                let mut placeholders = Vec::new();
                let mut owned_params: Vec<EthereumSqlTypeWrapper> = Vec::new();
                for (i, result) in batch.iter().enumerate() {
                    let columns_for_result = $columns_def(result);
                    let base = i * columns_for_result.len() + 1;
                    let placeholder = columns_for_result
                        .iter()
                        .enumerate()
                        .map(|(j, col)| format!("${}::{}", base + j, col.sql_type.as_str()))
                        .collect::<Vec<_>>()
                        .join(", ");
                    placeholders.push(format!("({})", placeholder));
                    for col in &columns_for_result {
                        owned_params.push(col.value.clone());
                    }
                }
                query.push_str(&placeholders.join(", "));
                query.push_str(")");
                if !distinct_cols.is_empty() && sequence_col.is_some() {
                    let seq_col = sequence_col.unwrap();
                    let quoted_distinct_cols = distinct_cols
                        .iter()
                        .map(|col| {
                            if RESERVED_KEYWORDS.contains(col) {
                                format!("\"{}\"", col)
                            } else {
                                col.to_string()
                            }
                        })
                        .collect::<Vec<_>>()
                        .join(", ");
                    let quoted_order_cols = quoted_distinct_cols.clone();
                    let quoted_seq_col = if RESERVED_KEYWORDS.contains(&seq_col) {
                        format!("\"{}\"", seq_col)
                    } else {
                        seq_col.to_string()
                    };
                    query.push_str(&format!(
                        ",
                    to_process AS (
                        SELECT DISTINCT ON ({}) *
                        FROM raw_data
                        ORDER BY {}, {} DESC
                    )",
                        quoted_distinct_cols, quoted_order_cols, quoted_seq_col
                    ));
                } else {
                    query.push_str(
                        ",
                    to_process AS (
                        SELECT * FROM raw_data
                    )",
                    );
                }
                // Handle table name escaping for schema.table format
                let formatted_table_name = if $table_name.contains('.') {
                    let parts: Vec<&str> = $table_name.split('.').collect();
                    if parts.len() == 2 {
                        let schema = parts[0].trim_matches('"');
                        let table = parts[1].trim_matches('"');
                        format!("\"{}\".\"{}\"", schema, table)
                    } else {
                        $table_name.to_string()
                    }
                } else {
                    $table_name.to_string()
                };
                match $op_type {
                    BatchOperationType::Update => {
                        query.push_str(&format!("\nUPDATE {} am\nSET ", formatted_table_name));
                        let mut all_set_clauses: Vec<String> = Vec::new();
                        let set_clauses: Vec<String> = set_columns
                            .iter()
                            .map(|col| {
                                let column_def = columns.iter().find(|c| c.name == *col).unwrap();
                                let table_col_name =
                                    column_def.table_column.unwrap_or(column_def.name);
                                let cte_col_name = column_def.name;
                                let column_name = if RESERVED_KEYWORDS.contains(&table_col_name) {
                                    format!("\"{}\"", table_col_name)
                                } else {
                                    table_col_name.to_string()
                                };
                                let tp_col = if RESERVED_KEYWORDS.contains(&cte_col_name) {
                                    format!("tp.\"{}\"", cte_col_name)
                                } else {
                                    format!("tp.{}", cte_col_name)
                                };
                                format!("{} = {}", column_name, tp_col)
                            })
                            .collect();
                        all_set_clauses.extend(set_clauses);
                        let add_clauses: Vec<String> = add_columns
                            .iter()
                            .map(|col| {
                                let column_def = columns.iter().find(|c| c.name == *col).unwrap();
                                let table_col_name =
                                    column_def.table_column.unwrap_or(column_def.name);
                                let cte_col_name = column_def.name;
                                let column_name = if RESERVED_KEYWORDS.contains(&table_col_name) {
                                    format!("\"{}\"", table_col_name)
                                } else {
                                    table_col_name.to_string()
                                };
                                let tp_col = if RESERVED_KEYWORDS.contains(&cte_col_name) {
                                    format!("tp.\"{}\"", cte_col_name)
                                } else {
                                    format!("tp.{}", cte_col_name)
                                };
                                format!("{} = am.{} + {}", column_name, column_name, tp_col)
                            })
                            .collect();
                        all_set_clauses.extend(add_clauses);
                        let subtract_clauses: Vec<String> = subtract_columns
                            .iter()
                            .map(|col| {
                                let column_def = columns.iter().find(|c| c.name == *col).unwrap();
                                let table_col_name =
                                    column_def.table_column.unwrap_or(column_def.name);
                                let cte_col_name = column_def.name;
                                let column_name = if RESERVED_KEYWORDS.contains(&table_col_name) {
                                    format!("\"{}\"", table_col_name)
                                } else {
                                    table_col_name.to_string()
                                };
                                let tp_col = if RESERVED_KEYWORDS.contains(&cte_col_name) {
                                    format!("tp.\"{}\"", cte_col_name)
                                } else {
                                    format!("tp.{}", cte_col_name)
                                };
                                format!("{} = am.{} - {}", column_name, column_name, tp_col)
                            })
                            .collect();
                        all_set_clauses.extend(subtract_clauses);
                        query.push_str(&all_set_clauses.join(", "));
                        query.push_str("\nFROM to_process tp");
                    }
                    BatchOperationType::Delete => {
                        query.push_str(&format!("\nDELETE FROM {} am", formatted_table_name));
                        query.push_str("\nUSING to_process tp");
                    }
                    BatchOperationType::Upsert => {
                        let all_columns: Vec<&str> = columns.iter().map(|col| col.name).collect();
                        let formatted_columns = all_columns
                            .iter()
                            .map(|col| {
                                if RESERVED_KEYWORDS.contains(col) {
                                    format!("\"{}\"", col)
                                } else {
                                    col.to_string()
                                }
                            })
                            .collect::<Vec<_>>()
                            .join(", ");
                        let tp_columns = all_columns
                            .iter()
                            .map(|col| {
                                if RESERVED_KEYWORDS.contains(col) {
                                    format!("tp.\"{}\"", col)
                                } else {
                                    format!("tp.{}", col)
                                }
                            })
                            .collect::<Vec<_>>()
                            .join(", ");
                        query.push_str(&format!(
                            "\nINSERT INTO {} ({})\nSELECT {}\nFROM to_process tp",
                            formatted_table_name, formatted_columns, tp_columns
                        ));
                        let conflict_columns = if !where_columns.is_empty() {
                            where_columns
                                .iter()
                                .map(|col| {
                                    if RESERVED_KEYWORDS.contains(col) {
                                        format!("\"{}\"", col)
                                    } else {
                                        col.to_string()
                                    }
                                })
                                .collect::<Vec<_>>()
                                .join(", ")
                        } else {
                            distinct_cols
                                .iter()
                                .map(|col| {
                                    if RESERVED_KEYWORDS.contains(col) {
                                        format!("\"{}\"", col)
                                    } else {
                                        col.to_string()
                                    }
                                })
                                .collect::<Vec<_>>()
                                .join(", ")
                        };
                        if !conflict_columns.is_empty() {
                            query.push_str(&format!("\nON CONFLICT ({})", conflict_columns));
                            let mut update_clauses: Vec<String> = Vec::new();
                            let set_clauses: Vec<String> = set_columns
                                .iter()
                                .filter(|col| {
                                    !where_columns.contains(col) && !distinct_cols.contains(col)
                                })
                                .map(|col| {
                                    let column_name = if RESERVED_KEYWORDS.contains(col) {
                                        format!("\"{}\"", col)
                                    } else {
                                        col.to_string()
                                    };
                                    format!("{} = EXCLUDED.{}", column_name, column_name)
                                })
                                .collect();
                            update_clauses.extend(set_clauses);
                            let add_clauses: Vec<String> = add_columns
                                .iter()
                                .filter(|col| {
                                    !where_columns.contains(col) && !distinct_cols.contains(col)
                                })
                                .map(|col| {
                                    let column_name = if RESERVED_KEYWORDS.contains(col) {
                                        format!("\"{}\"", col)
                                    } else {
                                        col.to_string()
                                    };
                                    format!(
                                        "{} = {}.{} + EXCLUDED.{}",
                                        column_name, formatted_table_name, column_name, column_name
                                    )
                                })
                                .collect();
                            update_clauses.extend(add_clauses);
                            let subtract_clauses: Vec<String> = subtract_columns
                                .iter()
                                .filter(|col| {
                                    !where_columns.contains(col) && !distinct_cols.contains(col)
                                })
                                .map(|col| {
                                    let column_name = if RESERVED_KEYWORDS.contains(col) {
                                        format!("\"{}\"", col)
                                    } else {
                                        col.to_string()
                                    };
                                    format!(
                                        "{} = {}.{} - EXCLUDED.{}",
                                        column_name, formatted_table_name, column_name, column_name
                                    )
                                })
                                .collect();
                            update_clauses.extend(subtract_clauses);
                            if !update_clauses.is_empty() {
                                query.push_str(&format!(
                                    "\nDO UPDATE SET {}",
                                    update_clauses.join(", ")
                                ));
                            } else {
                                query.push_str("\nDO NOTHING");
                            }
                        } else {
                            query.push_str("\nON CONFLICT DO NOTHING");
                        }
                        let params: Vec<&(dyn ToSql + Sync)> =
                            owned_params.iter().map(|param| param as &(dyn ToSql + Sync)).collect();
                        database
                            .with_transaction(&query, &params, |_| async move { Ok(()) })
                            .await
                            .map_err(|e| e.to_string())?;
                        return Ok(());
                    }
                }
                let mut where_conditions = Vec::new();
                for col in &where_columns {
                    let column_def = columns.iter().find(|c| c.name == *col).unwrap();
                    let table_col = column_def.table_column.unwrap_or(column_def.name);
                    let am_col = if RESERVED_KEYWORDS.contains(&table_col) {
                        format!("am.\"{}\"", table_col)
                    } else {
                        format!("am.{}", table_col)
                    };
                    let tp_col = if RESERVED_KEYWORDS.contains(col) {
                        format!("tp.\"{}\"", col)
                    } else {
                        format!("tp.{}", col)
                    };
                    where_conditions.push(format!("{} = {}", am_col, tp_col));
                }
                for col in &distinct_cols {
                    if !where_columns.contains(col) {
                        let column_def = columns.iter().find(|c| c.name == *col).unwrap();
                        let table_col = column_def.table_column.unwrap_or(column_def.name);
                        let am_col = if RESERVED_KEYWORDS.contains(&table_col) {
                            format!("am.\"{}\"", table_col)
                        } else {
                            format!("am.{}", table_col)
                        };
                        let tp_col = if RESERVED_KEYWORDS.contains(col) {
                            format!("tp.\"{}\"", col)
                        } else {
                            format!("tp.{}", col)
                        };
                        where_conditions.push(format!("{} = {}", am_col, tp_col));
                    }
                }
                if let Some(seq_col) = sequence_col {
                    let seq_col_name = if RESERVED_KEYWORDS.contains(&seq_col) {
                        format!("\"{}\"", seq_col)
                    } else {
                        seq_col.to_string()
                    };
                    match $op_type {
                        BatchOperationType::Update => {
                            where_conditions
                                .push(format!("tp.{} > am.{}", seq_col_name, seq_col_name));
                        }
                        BatchOperationType::Delete => {
                            where_conditions
                                .push(format!("tp.{} >= am.{}", seq_col_name, seq_col_name));
                        }
                        BatchOperationType::Upsert => {
                            // Sequence handling already done above
                        }
                    }
                }
                if !where_conditions.is_empty() {
                    query.push_str("\nWHERE ");
                    query.push_str(&where_conditions.join("\n  AND "));
                }
                let params: Vec<&(dyn ToSql + Sync)> =
                    owned_params.iter().map(|param| param as &(dyn ToSql + Sync)).collect();
                // println!("{}", query);
                // println!("{:?}", params);
                database
                    .with_transaction(&query, &params, |_| async move { Ok(()) })
                    .await
                    .map_err(|e| e.to_string())?;
                Ok(())
            }
            for batch in filtered_results.chunks(1000) {
                if let Err(e) = execute_batch(database, batch).await {
                    rindexer_error!("{} - Batch operation failed: {}", $event_name, e);
                    return Err(e);
                }
            }
            Ok(())
        }
    };
}
</file>

<file path="core/src/database/postgres/generate.rs">
use crate::database::generate::compact_table_name_if_needed;
use crate::helpers::parse_solidity_integer_type;
use crate::{
    abi::{ABIInput, GenerateAbiPropertiesType},
    helpers::camel_to_snake,
};
fn generate_columns(inputs: &[ABIInput], property_type: &GenerateAbiPropertiesType) -> Vec<String> {
    ABIInput::generate_abi_name_properties(inputs, property_type, None)
        .into_iter()
        .map(|m| m.value)
        .collect()
}
pub fn generate_columns_with_data_types(inputs: &[ABIInput]) -> Vec<String> {
    generate_columns(inputs, &GenerateAbiPropertiesType::PostgresWithDataTypes)
}
fn generate_columns_names_only(inputs: &[ABIInput]) -> Vec<String> {
    generate_columns(inputs, &GenerateAbiPropertiesType::PostgresColumnsNamesOnly)
}
pub fn generate_column_names_only_with_base_properties(inputs: &[ABIInput]) -> Vec<String> {
    let mut column_names: Vec<String> = vec!["contract_address".to_string()];
    column_names.extend(generate_columns_names_only(inputs));
    column_names.extend(vec![
        "tx_hash".to_string(),
        "block_number".to_string(),
        "block_timestamp".to_string(),
        "block_hash".to_string(),
        "network".to_string(),
        "tx_index".to_string(),
        "log_index".to_string(),
    ]);
    column_names
}
pub fn generate_internal_event_table_name(schema_name: &str, event_name: &str) -> String {
    let table_name = format!("{}_{}", schema_name, camel_to_snake(event_name));
    compact_table_name_if_needed(table_name)
}
pub fn generate_internal_event_table_name_no_shorten(
    schema_name: &str,
    event_name: &str,
) -> String {
    format!("{}_{}", schema_name, camel_to_snake(event_name))
}
pub struct GenerateInternalFactoryEventTableNameParams {
    pub indexer_name: String,
    pub contract_name: String,
    pub event_name: String,
    pub input_names: Vec<String>,
}
#[allow(clippy::manual_strip)]
pub fn solidity_type_to_db_type(abi_type: &str) -> String {
    let is_array = abi_type.ends_with("[]");
    let base_type = abi_type.trim_end_matches("[]");
    let sql_type = match base_type {
        "address" => "CHAR(42)",
        "bool" => "BOOLEAN",
        "string" => "TEXT",
        t if t.starts_with("bytes") => "BYTEA",
        t if t.starts_with("int") || t.starts_with("uint") => {
            // Handling fixed-size integers (intN and uintN where N can be 8 to 256 in steps of 8)
            let (prefix, size) = parse_solidity_integer_type(t);
            match size {
                8 | 16 => "SMALLINT",
                24 | 32 => "INTEGER",
                40 | 48 | 56 | 64 | 72 | 80 | 88 | 96 | 104 | 112 | 120 | 128 => "NUMERIC",
                136 | 144 | 152 | 160 | 168 | 176 | 184 | 192 | 200 | 208 | 216 | 224 | 232
                | 240 | 248 | 256 => "VARCHAR(78)",
                _ => panic!("Unsupported {prefix}N size: {size}"),
            }
        }
        _ => panic!("Unsupported type: {base_type}"),
    };
    // Return the SQL type, appending array brackets if necessary
    if is_array {
        // CHAR(42)[] does not work nicely with parsers so using
        // TEXT[] works out the box and CHAR(42) doesnt protect much anyway
        // as its already in type Address
        if base_type == "address" {
            return "TEXT[]".to_string();
        }
        format!("{sql_type}[]")
    } else {
        sql_type.to_string()
    }
}
</file>

<file path="core/src/database/postgres/indexes.rs">
use std::{path::Path, sync::Arc};
use futures::future::join_all;
use tracing::info;
use crate::{
    abi::{get_abi_item_with_db_map, ABIItem, GetAbiItemWithDbMapError, ReadAbiError},
    database::postgres::client::{PostgresClient, PostgresConnectionError, PostgresError},
    helpers::camel_to_snake,
    manifest::{contract::Contract, storage::PostgresIndexes},
    types::code::Code,
};
#[derive(Debug, Clone)]
pub struct PostgresIndexResult {
    db_table_name: String,
    db_table_columns: Vec<String>,
}
#[derive(thiserror::Error, Debug)]
pub enum ApplyPostgresIndexesError {
    #[error("{0}")]
    PostgresConnectionError(#[from] PostgresConnectionError),
    #[error("Could not apply indexes: {0}")]
    ApplyIndexesError(#[from] PostgresError),
}
impl PostgresIndexResult {
    pub fn apply_index_sql(&self) -> Code {
        info!(
            "Applying index after historic resync complete: table - {} constraint - {}",
            self.db_table_name,
            self.index_name()
        );
        // CONCURRENTLY is used to avoid locking the table for writes
        Code::new(format!(
            r#"
                CREATE INDEX CONCURRENTLY {index_name}
                ON {db_table_name} ({db_table_columns});
            "#,
            index_name = self.index_name(),
            db_table_name = self.db_table_name,
            db_table_columns = self.db_table_columns.join(", "),
        ))
    }
    fn drop_index_sql(&self) -> Code {
        info!(
            "Dropping index for historic resync: table - {} index - {}",
            self.db_table_name,
            self.index_name()
        );
        Code::new(format!(
            // CONCURRENTLY is used to avoid locking the table for writes
            "DROP INDEX CONCURRENTLY IF EXISTS {}.{};",
            // get schema else drop won't work
            self.db_table_name.split('.').next().unwrap_or_else(|| panic!(
                "Failed to split and then get schema for table: {}",
                self.db_table_name
            )),
            self.index_name(),
        ))
    }
    pub fn index_name(&self) -> String {
        format!(
            "idx_{db_table_name}_{db_table_columns}",
            db_table_name = self.db_table_name.split('.').next_back().unwrap_or_else(|| panic!(
                "Failed to split and then get schema for table: {}",
                self.db_table_name
            )),
            db_table_columns = self.db_table_columns.join("_"),
        )
    }
    pub async fn apply_indexes(
        indexes: Vec<PostgresIndexResult>,
    ) -> Result<(), ApplyPostgresIndexesError> {
        if indexes.is_empty() {
            return Ok(());
        }
        info!("Applying indexes if any back to the database as historic resync is complete");
        let client = PostgresClient::new().await?;
        // do a loop due to deadlocks on concurrent execution
        for postgres_index in indexes {
            let sql = postgres_index.apply_index_sql();
            client.execute(sql.as_str(), &[]).await?;
        }
        info!("Applied indexes back to database");
        Ok(())
    }
}
#[derive(thiserror::Error, Debug)]
pub enum GetLastKnownIndexesDroppingSqlError {
    #[error("Could not read last known indexes: {0}")]
    CouldNotReadLastKnownIndexes(#[from] PostgresError),
    #[error("Could not serialize indexes: {0}")]
    CouldNotParseIndexesToJson(#[from] serde_json::Error),
}
async fn get_last_known_indexes_dropping_sql(
    client: &PostgresClient,
    manifest_name: &str,
) -> Result<Vec<Code>, GetLastKnownIndexesDroppingSqlError> {
    let row_opt = client
        .query_one_or_none(
            &format!(
                r#"
                    SELECT value FROM rindexer_internal.{}_last_known_indexes_dropping_sql WHERE key = 1
                "#,
                camel_to_snake(manifest_name)
            ),
            &[],
        )
        .await?;
    if let Some(row) = row_opt {
        let value: &str = row.get(0);
        let foreign_keys: Vec<String> = serde_json::from_str(value)?;
        Ok(foreign_keys.iter().map(|foreign_key| Code::new(foreign_key.to_string())).collect())
    } else {
        Ok(Vec::new())
    }
}
#[derive(thiserror::Error, Debug)]
pub enum DropLastKnownIndexesError {
    #[error("Could not connect to Postgres: {0}")]
    PostgresConnection(#[from] PostgresConnectionError),
    #[error("{0}")]
    GetLastKnownIndexesDroppingSql(#[from] GetLastKnownIndexesDroppingSqlError),
    #[error("Could not execute dropping sql: {0}")]
    PostgresError(#[from] PostgresError),
    #[error("Could not drop indexes: {0}")]
    CouldNotDropIndexes(PostgresError),
}
pub async fn drop_last_known_indexes(manifest_name: &str) -> Result<(), DropLastKnownIndexesError> {
    let client = Arc::new(PostgresClient::new().await?);
    // people can edit the indexes, so we have to drop old stuff
    // we save all drops in the database, so we can drop them all at once
    // even if old stuff has been changed
    let last_known_indexes_dropping_sql =
        get_last_known_indexes_dropping_sql(&client, manifest_name).await?;
    let futures = last_known_indexes_dropping_sql.into_iter().map(|sql| {
        let client = Arc::clone(&client);
        async move {
            client
                .execute(sql.as_str(), &[])
                .await
                .map_err(DropLastKnownIndexesError::CouldNotDropIndexes)
        }
    });
    let results = join_all(futures).await;
    for result in results {
        result?;
    }
    Ok(())
}
#[derive(thiserror::Error, Debug)]
pub enum PrepareIndexesError {
    #[error("{0}")]
    PostgresConnectionError(#[from] PostgresConnectionError),
    #[error("{0}")]
    GetAbiParameterError(#[from] GetAbiItemWithDbMapError),
    #[error("Contract {0} not found in `contracts` make sure it is defined")]
    ContractMissing(String),
    #[error("{0}")]
    ReadAbiError(#[from] ReadAbiError),
    #[error("Could not serialize foreign keys: {0}")]
    CouldNotParseIndexToJson(#[from] serde_json::Error),
    #[error("Could not save indexes to postgres: {0}")]
    SaveIndexesError(#[from] PostgresError),
}
pub async fn prepare_indexes(
    project_path: &Path,
    manifest_name: &str,
    postgres_indexes: &PostgresIndexes,
    contracts: &[Contract],
) -> Result<Vec<PostgresIndexResult>, PrepareIndexesError> {
    let mut index_results: Vec<PostgresIndexResult> = vec![];
    let mut dropping_sql: Vec<Code> = vec![];
    let client = Arc::new(PostgresClient::new().await?);
    // global first
    if let Some(global_injected_parameters) = &postgres_indexes.global_injected_parameters {
        for contract in contracts {
            let abi_items = ABIItem::read_abi_items(project_path, contract)?;
            for abi_item in abi_items {
                let contract_name = contract.before_modify_name_if_filter_readonly();
                let db_table_name = format!(
                    "{}_{}.{}",
                    camel_to_snake(manifest_name),
                    camel_to_snake(&contract_name),
                    camel_to_snake(&abi_item.name)
                );
                for global_parameter_column_name in global_injected_parameters {
                    let index_result = PostgresIndexResult {
                        db_table_name: db_table_name.clone(),
                        db_table_columns: vec![global_parameter_column_name.clone()],
                    };
                    dropping_sql.push(index_result.drop_index_sql());
                    index_results.push(index_result);
                }
            }
        }
    }
    // then contracts
    if let Some(contract_events_indexes) = &postgres_indexes.contracts {
        for contract_event_indexes in contract_events_indexes.iter() {
            let contract = contracts.iter().find(|c| c.name == contract_event_indexes.name);
            match contract {
                None => {
                    return Err(PrepareIndexesError::ContractMissing(
                        contract_event_indexes.name.clone(),
                    ));
                }
                Some(contract) => {
                    let abi_items = ABIItem::read_abi_items(project_path, contract)?;
                    if let Some(injected_parameters) = &contract_event_indexes.injected_parameters {
                        for abi_item in &abi_items {
                            let contract_name = contract.before_modify_name_if_filter_readonly();
                            let db_table_name = format!(
                                "{}_{}.{}",
                                camel_to_snake(manifest_name),
                                camel_to_snake(&contract_name),
                                camel_to_snake(&abi_item.name)
                            );
                            for injected_parameter in injected_parameters {
                                let index_result = PostgresIndexResult {
                                    db_table_name: db_table_name.clone(),
                                    db_table_columns: vec![injected_parameter.clone()],
                                };
                                dropping_sql.push(index_result.drop_index_sql());
                                index_results.push(index_result);
                            }
                        }
                    }
                    for event_indexes in &contract_event_indexes.events {
                        let contract_name = contract.before_modify_name_if_filter_readonly();
                        let db_table_name = format!(
                            "{}_{}.{}",
                            camel_to_snake(manifest_name),
                            camel_to_snake(&contract_name),
                            camel_to_snake(&event_indexes.name)
                        );
                        if let Some(injected_parameters) = &event_indexes.injected_parameters {
                            for injected_parameter in injected_parameters {
                                let index_result = PostgresIndexResult {
                                    db_table_name: db_table_name.clone(),
                                    db_table_columns: vec![injected_parameter.clone()],
                                };
                                dropping_sql.push(index_result.drop_index_sql());
                                index_results.push(index_result);
                            }
                        }
                        for index in &event_indexes.indexes {
                            let mut db_table_columns = vec![];
                            for parameter in &index.event_input_names {
                                let abi_parameter = get_abi_item_with_db_map(
                                    &abi_items,
                                    &event_indexes.name,
                                    &parameter.split('.').collect::<Vec<&str>>(),
                                )?;
                                db_table_columns.push(abi_parameter.db_column_name);
                            }
                            let index_result = PostgresIndexResult {
                                db_table_name: db_table_name.clone(),
                                db_table_columns,
                            };
                            dropping_sql.push(index_result.drop_index_sql());
                            index_results.push(index_result);
                        }
                    }
                }
            }
        }
    }
    let indexes_dropping_sql_json = serde_json::to_string(
        &dropping_sql.iter().map(|code| code.as_str()).collect::<Vec<&str>>(),
    )?;
    client
        .execute(
            &format!(r#"
                INSERT INTO rindexer_internal.{manifest_name}_last_known_indexes_dropping_sql (key, value) VALUES (1, $1)
                ON CONFLICT (key) DO UPDATE SET value = $1;
            "#,
                     manifest_name = camel_to_snake(manifest_name)
            ),
            &[&indexes_dropping_sql_json],
        )
        .await?;
    Ok(index_results)
}
</file>

<file path="core/src/database/postgres/migrations.rs">
use crate::database::generate::{
    generate_indexer_contract_schema_name, GenerateTablesForIndexerSqlError,
};
use crate::helpers::camel_to_snake;
use crate::indexer::Indexer;
use crate::{ABIItem, PostgresClient};
use std::path::Path;
/// Atomically incrementing version number for migrations.
///
/// Note: if all migrations are idempotent on re-run we can avoid tracking last-known migration
/// entirely, however the cost associated with checking already run migrations is low enough that
/// we can design it to be future-proof at least until are clear it's not worth it.
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub enum Migration {
    /// Add `block_timestamp` column to all tables, nullable by default.
    V1,
}
impl Migration {
    pub const ALL: &'static [Migration] = &[Migration::V1];
    pub fn as_u32(&self) -> u32 {
        match self {
            Migration::V1 => 1,
        }
    }
    pub fn from_u32(v: u32) -> Option<Self> {
        match v {
            1 => Some(Migration::V1),
            _ => None,
        }
    }
    pub fn generate_table_migration_sql(&self, table_name: &str) -> String {
        match self {
            Migration::V1 => {
                format!("ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS block_timestamp TIMESTAMPTZ;")
            }
        }
    }
}
/// Execute any unapplied migrations for rindexer project.
///
/// ## Migration flow
///
/// 1. Check any already applied migrations
/// 2. Get the migrations that have not yet been applied
/// 3. Generate the migration SQL and apply them
/// 4. Insert new rows reflecting that these migrations have been applied
pub async fn execute_migrations_for_indexer_sql(
    client: &PostgresClient,
    project_path: &Path,
    indexer: &Indexer,
    disable_event_tables: bool,
) -> Result<Vec<Migration>, GenerateTablesForIndexerSqlError> {
    if disable_event_tables {
        return Ok(vec![]);
    }
    let applied = client
        .query(
            &format!(
                r#"
                 SELECT version FROM rindexer_internal.{indexer_name}_last_run_migrations_sql
                 WHERE migration_applied IS TRUE
                "#,
                indexer_name = camel_to_snake(&indexer.name)
            ),
            &[],
        )
        .await?;
    let applied = applied
        .iter()
        .filter_map(|row| {
            let id: i32 = row.get(0);
            Migration::from_u32(id as u32)
        })
        .collect::<Vec<_>>();
    let unapplied =
        Migration::ALL.iter().cloned().filter(|v| !applied.contains(v)).collect::<Vec<_>>();
    let mut statements = String::new();
    for version in &unapplied {
        for contract in &indexer.contracts {
            let contract_name = contract.before_modify_name_if_filter_readonly();
            let abi_items = ABIItem::read_abi_items(project_path, contract)?;
            let events = ABIItem::extract_event_names_and_signatures_from_abi(abi_items)?;
            let schema_name = generate_indexer_contract_schema_name(&indexer.name, &contract_name);
            for event_info in events {
                let table_name = format!("{}.{}", schema_name, camel_to_snake(&event_info.name));
                statements.push_str(&version.generate_table_migration_sql(&table_name));
            }
        }
    }
    client.batch_execute(&statements).await?;
    client
        .query(
            &format!(
                r#"
                 INSERT INTO rindexer_internal.{indexer_name}_last_run_migrations_sql (version, migration_applied)
                 SELECT UNNEST($1::INT[]), TRUE
                 ON CONFLICT (version) DO NOTHING
                "#,
                indexer_name = camel_to_snake(&indexer.name),
            ),
            &[&unapplied.iter().map(|v| v.as_u32() as i32).collect::<Vec<_>>()],
        )
        .await?;
    Ok(unapplied)
}
</file>

<file path="core/src/database/postgres/mod.rs">
pub mod client;
pub mod create_batch_db_operation;
pub mod generate;
pub mod indexes;
pub mod migrations;
pub mod relationship;
pub mod setup;
</file>

<file path="core/src/database/postgres/relationship.rs">
use std::path::Path;
use serde::{Deserialize, Serialize};
use tracing::info;
use crate::{
    abi::{get_abi_item_with_db_map, ABIInput, ABIItem, GetAbiItemWithDbMapError, ReadAbiError},
    database::postgres::client::{PostgresClient, PostgresConnectionError, PostgresError},
    helpers::camel_to_snake,
    manifest::{contract::Contract, storage::ForeignKeys},
    types::code::Code,
};
#[derive(thiserror::Error, Debug)]
pub enum CreateRelationshipError {
    #[error("{0}")]
    PostgresConnectionError(#[from] PostgresConnectionError),
    #[error("Contract missing: {0}")]
    ContractMissing(String),
    #[error("{0}")]
    ReadAbiError(#[from] ReadAbiError),
    #[error("Type mismatch: {0}")]
    TypeMismatch(String),
    #[error("{0}")]
    GetAbiParameterError(#[from] GetAbiItemWithDbMapError),
    #[error("Dropping relationship failed: {0}")]
    DropRelationshipError(#[from] PostgresError),
    #[error("Could not save relationships to postgres: {0}")]
    SaveRelationshipsError(PostgresError),
    #[error("Could not serialize foreign keys: {0}")]
    CouldNotParseRelationshipToJson(#[from] serde_json::Error),
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct LinkTo {
    pub contract_name: String,
    pub event: String,
    pub abi_input: ABIInput,
    pub db_table_name: String,
    pub db_table_column: String,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Relationship {
    pub contract_name: String,
    pub event: String,
    pub abi_input: ABIInput,
    pub db_table_name: String,
    pub db_table_column: String,
    pub linked_to: LinkTo,
}
#[derive(thiserror::Error, Debug)]
pub enum ApplyAllRelationships {
    #[error("{0}")]
    PostgresConnectionError(#[from] PostgresConnectionError),
    #[error("Could not apply relationship - {0}")]
    ApplyRelationshipError(#[from] PostgresError),
}
impl Relationship {
    pub fn has_cross_contract_dependency(relationships: &[Relationship]) -> bool {
        for relationship in relationships {
            if relationship.linked_to.contract_name != relationship.contract_name {
                return true;
            }
        }
        false
    }
    fn apply_foreign_key_construct_sql(&self) -> Code {
        Code::new(format!(
            r#"
                ALTER TABLE {db_table_name}
                ADD CONSTRAINT {foreign_key_construct_name}
                FOREIGN KEY ({db_table_column}) REFERENCES {linked_db_table_name}({linked_db_table_column});
            "#,
            foreign_key_construct_name = self.foreign_key_construct_name(),
            db_table_name = self.db_table_name,
            db_table_column = self.db_table_column,
            linked_db_table_name = self.linked_to.db_table_name,
            linked_db_table_column = self.linked_to.db_table_column
        ))
    }
    fn drop_foreign_key_construct_sql(&self) -> Code {
        Code::new(format!(
            r#"
                ALTER TABLE {db_table_name}
                DROP CONSTRAINT IF EXISTS {foreign_key_construct_name};
            "#,
            foreign_key_construct_name = self.foreign_key_construct_name(),
            db_table_name = self.db_table_name,
        ))
    }
    fn foreign_key_construct_name(&self) -> String {
        format!(
            "fk_{linked_db_table_name}_{linked_db_table_column}",
            linked_db_table_name =
                self.linked_to.db_table_name.split('.').next_back().unwrap_or_else(|| panic!(
                    "Failed to split and then get schema for table: {}",
                    self.linked_to.db_table_column
                )),
            linked_db_table_column = self.linked_to.db_table_column
        )
    }
    // IF NOT EXISTS does not work on unique constraints, so we only want to
    // apply if it's not already applied
    fn apply_unique_construct_sql(&self) -> Code {
        Code::new(format!(
            r#"
            DO $$
            BEGIN
                IF NOT EXISTS (
                    SELECT 1
                    FROM pg_constraint
                    WHERE conname = '{unique_construct_name}'
                    AND conrelid = '{linked_db_table_name}'::regclass
                ) THEN
                    ALTER TABLE {linked_db_table_name}
                    ADD CONSTRAINT {unique_construct_name}
                    UNIQUE ({linked_db_table_column});
                END IF;
            END $$;
        "#,
            unique_construct_name = self.unique_construct_name(),
            linked_db_table_name = self.linked_to.db_table_name,
            linked_db_table_column = self.linked_to.db_table_column
        ))
    }
    fn drop_unique_construct_sql(&self) -> Code {
        Code::new(format!(
            r#"
                ALTER TABLE {linked_db_table_name}
                DROP CONSTRAINT IF EXISTS {unique_construct_name};
            "#,
            unique_construct_name = self.unique_construct_name(),
            linked_db_table_name = self.linked_to.db_table_name,
        ))
    }
    fn unique_construct_name(&self) -> String {
        format!(
            "unique_{linked_db_table_name}_{linked_db_table_column}",
            linked_db_table_name =
                self.linked_to.db_table_name.split('.').next_back().unwrap_or_else(|| panic!(
                    "Failed to split and then get schema for table: {}",
                    self.linked_to.db_table_column
                )),
            linked_db_table_column = self.linked_to.db_table_column
        )
    }
    fn apply_index_sql(&self) -> Code {
        // CONCURRENTLY is used to avoid locking the table for writes
        Code::new(format!(
            r#"
                CREATE INDEX CONCURRENTLY IF NOT EXISTS {index_name}
                ON {db_table_name} ({db_table_column});
            "#,
            index_name = self.index_name(),
            db_table_name = self.db_table_name,
            db_table_column = self.db_table_column,
        ))
    }
    fn drop_index_sql(&self) -> Code {
        Code::new(format!(
            // CONCURRENTLY is used to avoid locking the table for writes
            "DROP INDEX CONCURRENTLY IF EXISTS {}.{};",
            // get schema else drop won't work
            self.db_table_name.split('.').next().unwrap_or_else(|| panic!(
                "Failed to split and then get schema for table: {}",
                self.db_table_column
            )),
            self.index_name(),
        ))
    }
    pub async fn drop_sql(&self) -> Result<Vec<Code>, PostgresError> {
        let mut codes = vec![];
        let sql = format!(
            r#"
            {}
            {}
          "#,
            self.drop_foreign_key_construct_sql(),
            self.drop_unique_construct_sql()
        );
        codes.push(Code::new(sql));
        info!(
            "Dropped foreign key for relationship for historic resync: table - {} constraint - {}",
            self.db_table_name,
            self.foreign_key_construct_name()
        );
        info!(
            "Dropped unique constraint key for relationship for historic resync: table - {} constraint - {}",
            self.linked_to.db_table_name,
            self.unique_construct_name()
        );
        let drop_index_sql = self.drop_index_sql();
        codes.push(drop_index_sql);
        info!(
            "Dropped index for relationship for historic resync: table - {} index - {}",
            self.db_table_name,
            self.index_name()
        );
        Ok(codes)
    }
    pub fn index_name(&self) -> String {
        format!(
            "idx_{db_table_name}_{db_table_column}",
            db_table_name = self.db_table_name.split('.').next_back().unwrap_or_else(|| panic!(
                "Failed to split and then get schema for table: {}",
                self.db_table_column
            )),
            db_table_column = self.db_table_column,
        )
    }
    pub async fn apply(&self, client: &PostgresClient) -> Result<(), PostgresError> {
        // apply on its own as it's in a DO block
        client.execute(self.apply_unique_construct_sql().as_str(), &[]).await?;
        info!(
            "Applied unique constraint key for relationship after historic resync complete: table - {} constraint - {}",
            self.linked_to.db_table_name,
            self.unique_construct_name()
        );
        client.execute(self.apply_foreign_key_construct_sql().as_str(), &[]).await?;
        info!(
            "Applied foreign key for relationship after historic resync complete: table - {} constraint - {}",
            self.db_table_name,
            self.foreign_key_construct_name()
        );
        // CONCURRENTLY is used to avoid locking the table for writes
        client.execute(&self.apply_index_sql().to_string(), &[]).await?;
        info!(
            "Applied index for relationship after historic resync complete: table - {} index - {}",
            self.db_table_name,
            self.index_name()
        );
        Ok(())
    }
    pub async fn apply_all(relationships: &Vec<Relationship>) -> Result<(), ApplyAllRelationships> {
        if relationships.is_empty() {
            return Ok(());
        }
        let client = PostgresClient::new().await?;
        for relationship in relationships {
            relationship.apply(&client).await?;
        }
        Ok(())
    }
}
#[derive(thiserror::Error, Debug)]
pub enum GetLastKnownRelationshipsDroppingSqlError {
    #[error("Could not read last known relationship: {0}")]
    CouldNotReadLastKnownRelationship(#[from] PostgresError),
    #[error("Could not serialize foreign keys: {0}")]
    CouldNotParseRelationshipToJson(#[from] serde_json::Error),
}
async fn get_last_known_relationships_dropping_sql(
    client: &PostgresClient,
    manifest_name: &str,
) -> Result<Vec<Code>, GetLastKnownRelationshipsDroppingSqlError> {
    let row_opt = client
        .query_one_or_none(
            &format!(
                r#"
                    SELECT value FROM rindexer_internal.{}_last_known_relationship_dropping_sql WHERE key = 1
                "#,
                camel_to_snake(manifest_name)
            ),
            &[],
        )
        .await?;
    if let Some(row) = row_opt {
        let value: &str = row.get(0);
        let foreign_keys: Vec<String> = serde_json::from_str(value)?;
        Ok(foreign_keys.iter().map(|foreign_key| Code::new(foreign_key.to_string())).collect())
    } else {
        Ok(Vec::new())
    }
}
#[derive(thiserror::Error, Debug)]
pub enum DropLastKnownRelationshipsError {
    #[error("Could not connect to Postgres: {0}")]
    PostgresConnection(#[from] PostgresConnectionError),
    #[error("{0}")]
    GetLastKnownRelationshipsDroppingSql(#[from] GetLastKnownRelationshipsDroppingSqlError),
    #[error("Could not execute dropping sql: {0}")]
    PostgresError(#[from] PostgresError),
}
pub async fn drop_last_known_relationships(
    manifest_name: &str,
) -> Result<(), DropLastKnownRelationshipsError> {
    let client = PostgresClient::new().await?;
    // people can edit the relationships, so we have to drop old stuff
    // we save all drops in the database, so we can drop them all at once
    // even if old stuff has been changed
    let last_known_relationships_dropping_sql =
        get_last_known_relationships_dropping_sql(&client, manifest_name).await?;
    for drop_sql in last_known_relationships_dropping_sql {
        client.batch_execute(drop_sql.as_str()).await?;
    }
    Ok(())
}
pub async fn create_relationships(
    project_path: &Path,
    manifest_name: &str,
    contracts: &[Contract],
    foreign_keys: &[ForeignKeys],
) -> Result<Vec<Relationship>, CreateRelationshipError> {
    let mut relationships = vec![];
    let mut dropping_sql: Vec<Code> = vec![];
    for foreign_key in foreign_keys {
        let contract = contracts.iter().find(|c| c.name == foreign_key.contract_name);
        match contract {
            None => {
                return Err(CreateRelationshipError::ContractMissing(format!(
                    "Contract {} not found in `contracts` make sure it is defined",
                    foreign_key.contract_name
                )));
            }
            Some(contract) => {
                let abi_items = ABIItem::read_abi_items(project_path, contract)?;
                for linked_key in &foreign_key.foreign_keys {
                    let parameter_mapping =
                        foreign_key.event_input_name.split('.').collect::<Vec<&str>>();
                    let abi_parameter = get_abi_item_with_db_map(
                        &abi_items,
                        &foreign_key.event_name,
                        &parameter_mapping,
                    )?;
                    let linked_key_contract = contracts
                        .iter()
                        .find(|c| c.name == linked_key.contract_name)
                        .ok_or_else(|| {
                            CreateRelationshipError::ContractMissing(format!(
                                "Contract {} not found in `contracts` and linked in relationships. Make sure it is defined.",
                                linked_key.contract_name
                            ))
                        })?;
                    let linked_abi_items =
                        ABIItem::read_abi_items(project_path, linked_key_contract)?;
                    let linked_parameter_mapping =
                        linked_key.event_input_name.split('.').collect::<Vec<&str>>();
                    let linked_abi_parameter = get_abi_item_with_db_map(
                        &linked_abi_items,
                        &linked_key.event_name,
                        &linked_parameter_mapping,
                    )?;
                    if abi_parameter.abi_item.type_ != linked_abi_parameter.abi_item.type_ {
                        return Err(CreateRelationshipError::TypeMismatch(format!(
                            "Type mismatch between {}.{} ({}) and {}.{} ({})",
                            &foreign_key.contract_name,
                            &foreign_key.event_input_name,
                            &abi_parameter.abi_item.type_,
                            &linked_key.contract_name,
                            &linked_key.event_input_name,
                            &linked_abi_parameter.abi_item.type_
                        )));
                    }
                    let relationship = Relationship {
                        contract_name: foreign_key.contract_name.clone(),
                        event: foreign_key.event_name.clone(),
                        db_table_column: camel_to_snake(&abi_parameter.db_column_name),
                        db_table_name: format!(
                            "{}_{}.{}",
                            camel_to_snake(manifest_name),
                            camel_to_snake(&contract.name),
                            camel_to_snake(&foreign_key.event_name)
                        ),
                        abi_input: abi_parameter.abi_item,
                        linked_to: LinkTo {
                            contract_name: linked_key.contract_name.clone(),
                            event: linked_key.event_name.clone(),
                            db_table_column: camel_to_snake(&linked_abi_parameter.db_column_name),
                            db_table_name: format!(
                                "{}_{}.{}",
                                camel_to_snake(manifest_name),
                                camel_to_snake(&linked_key_contract.name),
                                camel_to_snake(&linked_key.event_name)
                            ),
                            abi_input: linked_abi_parameter.abi_item,
                        },
                    };
                    let sql = relationship.drop_sql().await?;
                    dropping_sql.extend(sql);
                    relationships.push(relationship);
                }
            }
        }
    }
    let relationships_dropping_sql_json = serde_json::to_string(
        &dropping_sql.iter().map(|code| code.as_str()).collect::<Vec<&str>>(),
    )?;
    // save relationships in postgres
    let client = PostgresClient::new().await?;
    client
        .execute(
            &format!(r#"
                INSERT INTO rindexer_internal.{manifest_name}_last_known_relationship_dropping_sql (key, value) VALUES (1, $1)
                ON CONFLICT (key) DO UPDATE SET value = $1;
            "#,
                     manifest_name = camel_to_snake(manifest_name)
            ),
            &[&relationships_dropping_sql_json],
        )
        .await
        .map_err(CreateRelationshipError::SaveRelationshipsError)?;
    Ok(relationships)
}
</file>

<file path="core/src/database/postgres/setup.rs">
use std::path::Path;
use tracing::{debug, info};
use crate::database::generate::{
    drop_tables_for_indexer_sql, generate_tables_for_indexer_sql, GenerateTablesForIndexerSqlError,
};
use crate::database::postgres::migrations::execute_migrations_for_indexer_sql;
use crate::{
    database::postgres::client::{PostgresClient, PostgresConnectionError, PostgresError},
    manifest::core::Manifest,
};
#[derive(thiserror::Error, Debug)]
pub enum SetupPostgresError {
    #[error("{0}")]
    PostgresConnection(#[from] PostgresConnectionError),
    #[error("{0}")]
    PostgresError(#[from] PostgresError),
    #[error("Error creating tables for indexer: {0}")]
    GeneratingTables(#[from] GenerateTablesForIndexerSqlError),
}
pub async fn setup_postgres(
    project_path: &Path,
    manifest: &Manifest,
) -> Result<PostgresClient, SetupPostgresError> {
    info!("Setting up postgres");
    let client = PostgresClient::new().await?;
    let disable_event_tables = manifest.storage.postgres_disable_create_tables();
    if manifest.storage.postgres_drop_each_run() {
        info!(
            "`drop_each_run` enabled so dropping all data for {} before starting",
            &manifest.name
        );
        let sql = drop_tables_for_indexer_sql(project_path, &manifest.to_indexer());
        client.batch_execute(sql.as_str()).await?;
        info!("Dropped all data for {}", manifest.name);
    }
    if disable_event_tables {
        info!("Creating internal rindexer tables for {}", manifest.name);
    } else {
        info!("Creating tables for {}", manifest.name);
    }
    let sql = generate_tables_for_indexer_sql(
        project_path,
        &manifest.to_indexer(),
        disable_event_tables,
    )?;
    debug!("{}", sql);
    client.batch_execute(sql.as_str()).await?;
    if disable_event_tables {
        info!("Created tables for {}", manifest.name);
    } else {
        info!("Created internal rindexer tables for {}", manifest.name);
    }
    let version_applied = execute_migrations_for_indexer_sql(
        &client,
        project_path,
        &manifest.to_indexer(),
        disable_event_tables,
    )
    .await?;
    for version in version_applied {
        info!("Applied migration V{} for {}", version.as_u32(), manifest.name);
    }
    Ok(client)
}
</file>

<file path="core/src/database/batch_operations.rs">
// Types are used by exported macros, not internally
#![allow(dead_code)]
use crate::EthereumSqlTypeWrapper;
/// The type of batch operation to perform.
pub enum BatchOperationType {
    Update,
    Delete,
    Upsert,
}
/// Column behavior determines how columns are used in deduplication and ordering.
pub enum BatchOperationColumnBehavior {
    /// Normal column with no special behavior
    Normal,
    /// Column used for deduplication (DISTINCT ON in Postgres, part of ORDER BY in ClickHouse)
    Distinct,
    /// Column used for ordering/sequencing (determines which row to keep when deduplicating)
    Sequence,
}
/// SQL type mapping for batch operations.
pub enum BatchOperationSqlType {
    Bytea,
    Numeric,
    Bool,
    Jsonb,
    Varchar,
    Bigint,
    DateTime,
    Custom(&'static str),
}
impl BatchOperationSqlType {
    /// Returns the PostgreSQL type string representation.
    pub fn as_str(&self) -> &'static str {
        match self {
            BatchOperationSqlType::Bytea => "BYTEA",
            BatchOperationSqlType::Numeric => "NUMERIC",
            BatchOperationSqlType::Bool => "BOOL",
            BatchOperationSqlType::Jsonb => "JSONB",
            BatchOperationSqlType::Varchar => "VARCHAR",
            BatchOperationSqlType::Bigint => "BIGINT",
            BatchOperationSqlType::DateTime => "TIMESTAMPTZ",
            BatchOperationSqlType::Custom(type_name) => type_name,
        }
    }
    /// Returns the ClickHouse type string representation.
    pub fn as_clickhouse_str(&self) -> &'static str {
        match self {
            BatchOperationSqlType::Bytea => "String",
            BatchOperationSqlType::Numeric => "String",
            BatchOperationSqlType::Bool => "Bool",
            BatchOperationSqlType::Jsonb => "String",
            BatchOperationSqlType::Varchar => "String",
            BatchOperationSqlType::Bigint => "Int64",
            BatchOperationSqlType::DateTime => "DateTime('UTC')",
            BatchOperationSqlType::Custom(type_name) => type_name,
        }
    }
}
/// Action to perform on a column during batch operations.
pub enum BatchOperationAction {
    /// No action (column is included but not updated)
    Nothing,
    /// Set the column to the new value
    Set,
    /// Use column in WHERE clause for matching
    Where,
    /// Add the new value to the existing value
    Add,
    /// Subtract the new value from the existing value
    Subtract,
}
/// Definition of a column for batch operations.
pub struct ColumnDefinition {
    pub name: &'static str,
    pub table_column: Option<&'static str>,
    pub value: EthereumSqlTypeWrapper,
    pub sql_type: BatchOperationSqlType,
    pub behavior: BatchOperationColumnBehavior,
    pub action: BatchOperationAction,
}
/// Creates a column definition for batch operations.
pub fn column(
    name: &'static str,
    value: EthereumSqlTypeWrapper,
    sql_type: BatchOperationSqlType,
    behavior: BatchOperationColumnBehavior,
    action: BatchOperationAction,
) -> ColumnDefinition {
    ColumnDefinition { name, table_column: None, value, sql_type, behavior, action }
}
/// Creates a column definition with a separate table column name.
#[allow(clippy::too_many_arguments)]
pub fn column_with_table_name(
    name: &'static str,
    table_column: &'static str,
    value: EthereumSqlTypeWrapper,
    sql_type: BatchOperationSqlType,
    behavior: BatchOperationColumnBehavior,
    action: BatchOperationAction,
) -> ColumnDefinition {
    ColumnDefinition { name, table_column: Some(table_column), value, sql_type, behavior, action }
}
/// Reserved SQL keywords that need quoting.
pub const RESERVED_KEYWORDS: &[&str] =
    &["group", "user", "order", "table", "index", "primary", "key"];
</file>

<file path="core/src/database/generate.rs">
use crate::abi::{EventInfo, ParamTypeError, ReadAbiError};
use crate::database::postgres::client::PostgresError;
use crate::database::postgres::generate::{
    generate_columns_with_data_types, generate_internal_event_table_name,
    GenerateInternalFactoryEventTableNameParams,
};
use crate::helpers::camel_to_snake;
use crate::indexer::native_transfer::{NATIVE_TRANSFER_ABI, NATIVE_TRANSFER_CONTRACT_NAME};
use crate::indexer::Indexer;
use crate::manifest::contract::{Contract, FactoryDetailsYaml};
use crate::types::code::Code;
use crate::ABIItem;
use alloy::primitives::keccak256;
use std::path::Path;
use tracing::{error, info};
fn generate_event_table_sql_with_comments(
    abi_inputs: &[EventInfo],
    contract_name: &str,
    schema_name: &str,
    apply_full_name_comment_for_events: Vec<String>,
) -> String {
    abi_inputs
        .iter()
        .map(|event_info| {
            let table_name = format!("{}.{}", schema_name, camel_to_snake(&event_info.name));
            info!("Creating table if not exists: {}", table_name);
            let event_columns = if event_info.inputs.is_empty() {
                "".to_string()
            } else {
                generate_columns_with_data_types(&event_info.inputs).join(", ") + ","
            };
            let create_table_sql = format!(
                "CREATE TABLE IF NOT EXISTS {table_name} (\
                    rindexer_id SERIAL PRIMARY KEY NOT NULL, \
                    contract_address CHAR(42) NOT NULL, \
                    {event_columns} \
                    tx_hash CHAR(66) NOT NULL, \
                    block_number NUMERIC NOT NULL, \
                    block_timestamp TIMESTAMPTZ, \
                    block_hash CHAR(66) NOT NULL, \
                    network VARCHAR(50) NOT NULL, \
                    tx_index NUMERIC NOT NULL, \
                    log_index VARCHAR(78) NOT NULL\
                );"
            );
            if !apply_full_name_comment_for_events.contains(&event_info.name) {
                return create_table_sql;
            }
            // smart comments needed to avoid clashing of order by graphql names
            let table_comment = format!(
                "COMMENT ON TABLE {} IS E'@name {}{}';",
                table_name, contract_name, event_info.name
            );
            format!("{create_table_sql}\n{table_comment}")
        })
        .collect::<Vec<_>>()
        .join("\n")
}
fn generate_internal_factory_event_table_sql(
    indexer_name: &str,
    factories: &[FactoryDetailsYaml],
) -> String {
    factories.iter().map(|factory| {
        let params = GenerateInternalFactoryEventTableNameParams {
            indexer_name: indexer_name.to_string(),
            contract_name: factory.name.to_string(),
            event_name: factory.event_name.to_string(),
            input_names: factory.input_names(),
        };
        let table_name = generate_internal_factory_event_table_name(&params);
        let create_table_query = format!(
            r#"CREATE TABLE IF NOT EXISTS rindexer_internal.{table_name} ("factory_address" CHAR(42), "factory_deployed_address" CHAR(42), "network" TEXT, PRIMARY KEY ("factory_address", "factory_deployed_address", "network"));"#
        );
        create_table_query
    }).collect::<Vec<_>>().join("\n")
}
fn generate_internal_event_table_sql(
    abi_inputs: &[EventInfo],
    schema_name: &str,
    networks: Vec<&str>,
) -> String {
    abi_inputs.iter().map(|event_info| {
        let table_name = generate_internal_event_table_name(schema_name, &event_info.name);
        let create_table_query = format!(
            r#"CREATE TABLE IF NOT EXISTS rindexer_internal.{table_name} ("network" TEXT PRIMARY KEY, "last_synced_block" NUMERIC);"#
        );
        let insert_queries = networks.iter().map(|network| {
            format!(
                r#"INSERT INTO rindexer_internal.{table_name} ("network", "last_synced_block") VALUES ('{network}', 0) ON CONFLICT ("network") DO NOTHING;"#,
            )
        }).collect::<Vec<_>>().join("\n");
        let create_latest_block_query = r#"CREATE TABLE IF NOT EXISTS rindexer_internal.latest_block ("network" TEXT PRIMARY KEY, "block" NUMERIC);"#.to_string();
        let latest_block_insert_queries = networks.iter().map(|network| {
            format!(
                r#"INSERT INTO rindexer_internal.latest_block ("network", "block") VALUES ('{network}', 0) ON CONFLICT ("network") DO NOTHING;"#
            )
        }).collect::<Vec<_>>().join("\n");
        format!("{create_table_query}\n{insert_queries}\n{create_latest_block_query}\n{latest_block_insert_queries}")
    }).collect::<Vec<_>>().join("\n")
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateTablesForIndexerSqlError {
    #[error("{0}")]
    ReadAbiError(#[from] ReadAbiError),
    #[error("{0}")]
    ParamTypeError(#[from] ParamTypeError),
    #[error("failed to execute {0}")]
    Postgres(#[from] PostgresError),
}
/// If any event names match the whole table name should be exposed differently on graphql
/// to avoid clashing of graphql namings
pub fn find_clashing_event_names(
    project_path: &Path,
    current_contract_name: &str,
    other_contracts: &[Contract],
    current_event_names: &[EventInfo],
) -> Result<Vec<String>, GenerateTablesForIndexerSqlError> {
    let mut clashing_events = Vec::new();
    for other_contract in other_contracts {
        if other_contract.name == current_contract_name {
            continue;
        }
        let other_abi_items = ABIItem::read_abi_items(project_path, other_contract)?;
        let other_event_names =
            ABIItem::extract_event_names_and_signatures_from_abi(other_abi_items)?;
        for event_name in current_event_names {
            if other_event_names.iter().any(|e| e.name == event_name.name)
                && !clashing_events.contains(&event_name.name)
            {
                clashing_events.push(event_name.name.clone());
            }
        }
    }
    Ok(clashing_events)
}
pub fn generate_tables_for_indexer_sql(
    project_path: &Path,
    indexer: &Indexer,
    disable_event_tables: bool,
) -> Result<Code, GenerateTablesForIndexerSqlError> {
    let mut sql = "CREATE SCHEMA IF NOT EXISTS rindexer_internal;".to_string();
    for contract in &indexer.contracts {
        let contract_name = contract.before_modify_name_if_filter_readonly();
        let abi_items = ABIItem::read_abi_items(project_path, contract)?;
        let events = ABIItem::extract_event_names_and_signatures_from_abi(abi_items)?;
        let schema_name = generate_indexer_contract_schema_name(&indexer.name, &contract_name);
        let networks: Vec<&str> = contract.details.iter().map(|d| d.network.as_str()).collect();
        let factories = contract.details.iter().flat_map(|d| d.factory.clone()).collect::<Vec<_>>();
        if !disable_event_tables {
            sql.push_str(format!("CREATE SCHEMA IF NOT EXISTS {schema_name};").as_str());
            info!("Creating schema if not exists: {}", schema_name);
            let event_matching_name_on_other = find_clashing_event_names(
                project_path,
                &contract_name,
                &indexer.contracts,
                &events,
            )?;
            sql.push_str(&generate_event_table_sql_with_comments(
                &events,
                &contract.name,
                &schema_name,
                event_matching_name_on_other,
            ));
        }
        // we still need to create the internal tables for the contract
        sql.push_str(&generate_internal_event_table_sql(&events, &schema_name, networks));
        // generate internal tables for contract factories indexing
        sql.push_str(&generate_internal_factory_event_table_sql(&indexer.name, &factories));
    }
    if indexer.native_transfers.enabled {
        let contract_name = NATIVE_TRANSFER_CONTRACT_NAME.to_string();
        let abi_str = NATIVE_TRANSFER_ABI;
        let abi_items: Vec<ABIItem> =
            serde_json::from_str(abi_str).expect("JSON was not well-formatted");
        let event_names = ABIItem::extract_event_names_and_signatures_from_abi(abi_items)?;
        let schema_name = generate_indexer_contract_schema_name(&indexer.name, &contract_name);
        let networks = indexer.clone().native_transfers.networks.unwrap();
        let networks: Vec<&str> = networks.iter().map(|d| d.network.as_str()).collect();
        if !disable_event_tables {
            sql.push_str(format!("CREATE SCHEMA IF NOT EXISTS {schema_name};").as_str());
            info!("Creating schema if not exists: {}", schema_name);
            let event_matching_name_on_other = find_clashing_event_names(
                project_path,
                &contract_name,
                &indexer.contracts,
                &event_names,
            )?;
            sql.push_str(&generate_event_table_sql_with_comments(
                &event_names,
                &contract_name,
                &schema_name,
                event_matching_name_on_other,
            ));
        }
        sql.push_str(&generate_internal_event_table_sql(&event_names, &schema_name, networks));
    }
    sql.push_str(&format!(
        r#"
        CREATE TABLE IF NOT EXISTS rindexer_internal.{indexer_name}_last_known_relationship_dropping_sql (
            key INT PRIMARY KEY,
            value TEXT NOT NULL
        );
    "#,
        indexer_name = camel_to_snake(&indexer.name)
    ));
    sql.push_str(&format!(
        r#"
        CREATE TABLE IF NOT EXISTS rindexer_internal.{indexer_name}_last_known_indexes_dropping_sql (
            key INT PRIMARY KEY,
            value TEXT NOT NULL
        );
    "#,
        indexer_name = camel_to_snake(&indexer.name)
    ));
    sql.push_str(&format!(
        r#"
        CREATE TABLE IF NOT EXISTS rindexer_internal.{indexer_name}_last_run_migrations_sql (
            version INT PRIMARY KEY,
            migration_applied BOOL NOT NULL
        );
    "#,
        indexer_name = camel_to_snake(&indexer.name)
    ));
    Ok(Code::new(sql))
}
pub fn generate_event_table_full_name(
    indexer_name: &str,
    contract_name: &str,
    event_name: &str,
) -> String {
    let schema_name = generate_indexer_contract_schema_name(indexer_name, contract_name);
    format!("{}.{}", schema_name, camel_to_snake(event_name))
}
pub fn generate_event_table_columns_names_sql(column_names: &[String]) -> String {
    column_names.iter().map(|name| format!("\"{name}\"")).collect::<Vec<String>>().join(", ")
}
pub fn generate_indexer_contract_schema_name(indexer_name: &str, contract_name: &str) -> String {
    format!("{}_{}", camel_to_snake(indexer_name), camel_to_snake(contract_name))
}
pub fn generate_internal_factory_event_table_name(
    params: &GenerateInternalFactoryEventTableNameParams,
) -> String {
    let schema_name =
        generate_indexer_contract_schema_name(&params.indexer_name, &params.contract_name);
    let table_name = format!(
        "{}_{}_{}",
        schema_name,
        camel_to_snake(&params.event_name),
        &params.input_names.iter().map(|v| camel_to_snake(v)).collect::<Vec<String>>().join("-")
    );
    compact_table_name_if_needed(table_name)
}
pub fn generate_internal_factory_event_table_name_no_shorten(
    params: &GenerateInternalFactoryEventTableNameParams,
) -> String {
    let schema_name =
        generate_indexer_contract_schema_name(&params.indexer_name, &params.contract_name);
    format!(
        "{}_{}_{}",
        schema_name,
        camel_to_snake(&params.event_name),
        &params.input_names.iter().map(|v| camel_to_snake(v)).collect::<Vec<String>>().join("_")
    )
}
pub fn compact_table_name_if_needed(table_name: String) -> String {
    // sql table names cant be as long as 63
    if table_name.len() > 63 {
        let hash_bytes = keccak256(table_name.as_bytes());
        let hash = hash_bytes.iter().map(|byte| format!("{byte:02x}")).collect::<String>();
        let hash_prefix = &hash[0..10];
        // Preserve the beginning of the original name, but leave room for the hash
        let preserved_length = 63 - 11; // 10 for hash plus 1 for underscore
        let prefix = &table_name[0..preserved_length];
        return format!("{prefix}_{hash_prefix}");
    }
    table_name
}
pub fn drop_tables_for_indexer_sql(project_path: &Path, indexer: &Indexer) -> Code {
    let mut sql = format!(
        "DROP TABLE IF EXISTS rindexer_internal.{}_last_known_indexes_dropping_sql CASCADE;",
        camel_to_snake(&indexer.name)
    );
    sql.push_str(format!("DROP TABLE IF EXISTS rindexer_internal.{}_last_known_relationship_dropping_sql CASCADE;", camel_to_snake(&indexer.name)).as_str());
    sql.push_str("DROP TABLE IF EXISTS rindexer_internal.latest_block;");
    for contract in &indexer.contracts {
        let contract_name = contract.before_modify_name_if_filter_readonly();
        let schema_name = generate_indexer_contract_schema_name(&indexer.name, &contract_name);
        sql.push_str(format!("DROP SCHEMA IF EXISTS {schema_name} CASCADE;").as_str());
        // drop last synced blocks for contracts
        let abi_items = ABIItem::read_abi_items(project_path, contract);
        if let Ok(abi_items) = abi_items {
            for abi_item in abi_items.iter() {
                let table_name = generate_internal_event_table_name(&schema_name, &abi_item.name);
                sql.push_str(
                    format!("DROP TABLE IF EXISTS rindexer_internal.{table_name} CASCADE;")
                        .as_str(),
                );
            }
        } else {
            error!(
                "Could not read ABI items for contract moving on clearing the other data up: {}",
                contract.name
            );
        }
        // drop factory indexing tables
        for factory in contract.details.iter().flat_map(|d| d.factory.as_ref()) {
            let params = GenerateInternalFactoryEventTableNameParams {
                indexer_name: indexer.name.clone(),
                contract_name: factory.name.clone(),
                event_name: factory.event_name.clone(),
                input_names: factory.input_names(),
            };
            let table_name = generate_internal_factory_event_table_name(&params);
            sql.push_str(
                format!("DROP TABLE IF EXISTS rindexer_internal.{table_name} CASCADE;").as_str(),
            )
        }
    }
    Code::new(sql)
}
</file>

<file path="core/src/database/mod.rs">
pub mod batch_operations;
pub mod clickhouse;
pub mod generate;
pub mod postgres;
pub mod sql_type_wrapper;
</file>

<file path="core/src/database/sql_type_wrapper.rs">
use std::str::FromStr;
use crate::helpers::parse_solidity_integer_type;
use crate::{abi::ABIInput, event::callback_registry::TxInformation, types::core::LogParam};
#[allow(deprecated)]
use alloy::{
    dyn_abi::DynSolValue,
    primitives::{Address, Bytes, B128, B256, B512, I256, U256, U512},
};
use bytes::{BufMut, BytesMut};
use chrono::{DateTime, Utc};
use rust_decimal::Decimal;
use serde_json::{json, Value};
use tokio_postgres::types::{to_sql_checked, IsNull, ToSql, Type as PgType};
use tracing::error;
use uuid::Uuid;
#[derive(Debug, Clone)]
pub enum EthereumSqlTypeWrapper {
    // Boolean
    Bool(bool),
    VecBool(Vec<bool>),
    // 8-bit integers
    U8(u8),
    I8(i8),
    VecU8(Vec<u8>),
    VecI8(Vec<i8>),
    // 16-bit integers
    U16(u16),
    I16(i16),
    VecU16(Vec<u16>),
    VecI16(Vec<i16>),
    // 32-bit integers
    U32(u32),
    I32(i32),
    VecU32(Vec<u32>),
    VecI32(Vec<i32>),
    // 64-bit integers
    U64(u64),
    U64Nullable(u64),
    U64BigInt(u64),
    I64(i64),
    VecU64(Vec<u64>),
    VecI64(Vec<i64>),
    // 128-bit integers
    U128(u128),
    I128(i128),
    VecU128(Vec<u128>),
    VecI128(Vec<i128>),
    // 256-bit integers
    U256(U256),
    U256Numeric(U256),
    U256NumericNullable(Option<U256>),
    U256Nullable(U256),
    U256Bytes(U256),
    U256BytesNullable(U256),
    I256(I256),
    I256Numeric(I256),
    I256Nullable(I256),
    I256Bytes(I256),
    I256BytesNullable(I256),
    VecU256(Vec<U256>),
    VecU256Bytes(Vec<U256>),
    VecU256Numeric(Vec<U256>),
    VecI256(Vec<I256>),
    VecI256Bytes(Vec<I256>),
    // 512-bit integers
    U512(U512),
    VecU512(Vec<U512>),
    // Hashes
    B128(B128),
    #[deprecated(note = "Use Address instead")]
    #[allow(deprecated)]
    H160(Address),
    B256(B256),
    B256Bytes(B256),
    B512(B512),
    VecB128(Vec<B128>),
    #[deprecated(note = "Use Address instead")]
    #[allow(deprecated)]
    VecH160(Vec<Address>),
    VecB256(Vec<B256>),
    VecB256Bytes(Vec<B256>),
    VecB512(Vec<B512>),
    // Address
    Address(Address),
    AddressNullable(Address),
    AddressBytes(Address),
    AddressBytesNullable(Address),
    VecAddress(Vec<Address>),
    VecAddressBytes(Vec<Address>),
    // Strings and Bytes
    String(String),
    StringVarchar(String),
    StringChar(String),
    StringNullable(String),
    StringVarcharNullable(String),
    StringCharNullable(String),
    VecString(Vec<String>),
    VecStringVarchar(Vec<String>),
    VecStringChar(Vec<String>),
    Bytes(Bytes),
    BytesNullable(Bytes),
    VecBytes(Vec<Bytes>),
    Uuid(Uuid),
    DateTime(DateTime<Utc>),
    DateTimeNullable(Option<DateTime<Utc>>),
    JSONB(Value),
}
impl EthereumSqlTypeWrapper {
    pub fn raw_name(&self) -> &'static str {
        match self {
            // Boolean
            EthereumSqlTypeWrapper::Bool(_) => "Bool",
            EthereumSqlTypeWrapper::VecBool(_) => "VecBool",
            // 8-bit integers
            EthereumSqlTypeWrapper::U8(_) => "U8",
            EthereumSqlTypeWrapper::I8(_) => "I8",
            EthereumSqlTypeWrapper::VecU8(_) => "VecU8",
            EthereumSqlTypeWrapper::VecI8(_) => "VecI8",
            // 16-bit integers
            EthereumSqlTypeWrapper::U16(_) => "U16",
            EthereumSqlTypeWrapper::I16(_) => "I16",
            EthereumSqlTypeWrapper::VecU16(_) => "VecU16",
            EthereumSqlTypeWrapper::VecI16(_) => "VecI16",
            // 32-bit integers
            EthereumSqlTypeWrapper::U32(_) => "U32",
            EthereumSqlTypeWrapper::I32(_) => "I32",
            EthereumSqlTypeWrapper::VecU32(_) => "VecU32",
            EthereumSqlTypeWrapper::VecI32(_) => "VecI32",
            // 64-bit integers
            EthereumSqlTypeWrapper::U64(_) => "U64",
            EthereumSqlTypeWrapper::U64Nullable(_) => "U64Nullable",
            EthereumSqlTypeWrapper::U64BigInt(_) => "U64BigInt",
            EthereumSqlTypeWrapper::I64(_) => "I64",
            EthereumSqlTypeWrapper::VecU64(_) => "VecU64",
            EthereumSqlTypeWrapper::VecI64(_) => "VecI64",
            // 128-bit integers
            EthereumSqlTypeWrapper::U128(_) => "U128",
            EthereumSqlTypeWrapper::I128(_) => "I128",
            EthereumSqlTypeWrapper::VecU128(_) => "VecU128",
            EthereumSqlTypeWrapper::VecI128(_) => "VecI128",
            // 256-bit integers
            EthereumSqlTypeWrapper::U256(_) => "U256",
            EthereumSqlTypeWrapper::U256Nullable(_) => "U256Nullable",
            EthereumSqlTypeWrapper::U256Numeric(_) => "U256Numeric",
            EthereumSqlTypeWrapper::U256NumericNullable(_) => "U256NumericNullable",
            EthereumSqlTypeWrapper::U256Bytes(_) => "U256Bytes",
            EthereumSqlTypeWrapper::U256BytesNullable(_) => "U256BytesNullable",
            EthereumSqlTypeWrapper::I256(_) => "I256",
            EthereumSqlTypeWrapper::I256Numeric(_) => "I256Numeric",
            EthereumSqlTypeWrapper::I256Nullable(_) => "I256Nullable",
            EthereumSqlTypeWrapper::I256Bytes(_) => "I256Bytes",
            EthereumSqlTypeWrapper::I256BytesNullable(_) => "I256BytesNullable",
            EthereumSqlTypeWrapper::VecU256(_) => "VecU256",
            EthereumSqlTypeWrapper::VecU256Bytes(_) => "VecU256Bytes",
            EthereumSqlTypeWrapper::VecU256Numeric(_) => "VecU256Numeric",
            EthereumSqlTypeWrapper::VecI256(_) => "VecI256",
            EthereumSqlTypeWrapper::VecI256Bytes(_) => "VecI256Bytes",
            // 512-bit integers
            EthereumSqlTypeWrapper::U512(_) => "U512",
            EthereumSqlTypeWrapper::VecU512(_) => "VecU512",
            // Hashes
            EthereumSqlTypeWrapper::B128(_) => "B128",
            #[allow(deprecated)]
            EthereumSqlTypeWrapper::H160(_) => "H160",
            EthereumSqlTypeWrapper::B256(_) => "B256",
            EthereumSqlTypeWrapper::B256Bytes(_) => "B256Bytes",
            EthereumSqlTypeWrapper::B512(_) => "B512",
            EthereumSqlTypeWrapper::VecB128(_) => "VecB128",
            #[allow(deprecated)]
            EthereumSqlTypeWrapper::VecH160(_) => "VecH160",
            EthereumSqlTypeWrapper::VecB256(_) => "VecB256",
            EthereumSqlTypeWrapper::VecB256Bytes(_) => "VecB256Bytes",
            EthereumSqlTypeWrapper::VecB512(_) => "VecB512",
            // Address
            EthereumSqlTypeWrapper::Address(_) => "Address",
            EthereumSqlTypeWrapper::AddressNullable(_) => "AddressNullable",
            EthereumSqlTypeWrapper::AddressBytes(_) => "AddressBytes",
            EthereumSqlTypeWrapper::AddressBytesNullable(_) => "AddressBytesNullable",
            EthereumSqlTypeWrapper::VecAddress(_) => "VecAddress",
            EthereumSqlTypeWrapper::VecAddressBytes(_) => "VecAddressBytes",
            // Strings and Bytes
            EthereumSqlTypeWrapper::String(_) => "String",
            EthereumSqlTypeWrapper::StringVarchar(_) => "StringVarchar",
            EthereumSqlTypeWrapper::StringChar(_) => "StringChar",
            EthereumSqlTypeWrapper::StringNullable(_) => "StringNullable",
            EthereumSqlTypeWrapper::StringVarcharNullable(_) => "StringVarcharNullable",
            EthereumSqlTypeWrapper::StringCharNullable(_) => "StringCharNullable",
            EthereumSqlTypeWrapper::VecString(_) => "VecString",
            EthereumSqlTypeWrapper::VecStringVarchar(_) => "VecStringVarchar",
            EthereumSqlTypeWrapper::VecStringChar(_) => "VecStringChar",
            EthereumSqlTypeWrapper::Bytes(_) => "Bytes",
            EthereumSqlTypeWrapper::BytesNullable(_) => "BytesNullable",
            EthereumSqlTypeWrapper::VecBytes(_) => "VecBytes",
            EthereumSqlTypeWrapper::Uuid(_) => "Uuid",
            EthereumSqlTypeWrapper::DateTime(_) => "DateTime",
            EthereumSqlTypeWrapper::DateTimeNullable(_) => "DateTimeNullable",
            EthereumSqlTypeWrapper::JSONB(_) => "JSONB",
        }
    }
    pub fn to_type(&self) -> PgType {
        match self {
            // Boolean
            EthereumSqlTypeWrapper::Bool(_) => PgType::BOOL,
            EthereumSqlTypeWrapper::VecBool(_) => PgType::BOOL_ARRAY,
            // 8-bit integers
            EthereumSqlTypeWrapper::U8(_) => PgType::INT2,
            EthereumSqlTypeWrapper::I8(_) => PgType::INT2,
            EthereumSqlTypeWrapper::VecU8(_) => PgType::INT2_ARRAY,
            EthereumSqlTypeWrapper::VecI8(_) => PgType::INT2_ARRAY,
            // 16-bit integers
            EthereumSqlTypeWrapper::U16(_) => PgType::INT2,
            EthereumSqlTypeWrapper::I16(_) => PgType::INT2,
            EthereumSqlTypeWrapper::VecU16(_) => PgType::INT2_ARRAY,
            EthereumSqlTypeWrapper::VecI16(_) => PgType::INT2_ARRAY,
            // 32-bit integers
            EthereumSqlTypeWrapper::U32(_) => PgType::INT4,
            EthereumSqlTypeWrapper::I32(_) => PgType::INT4,
            EthereumSqlTypeWrapper::VecU32(_) => PgType::INT4_ARRAY,
            EthereumSqlTypeWrapper::VecI32(_) => PgType::INT4_ARRAY,
            // 64-bit integers
            EthereumSqlTypeWrapper::U64(_)
            | EthereumSqlTypeWrapper::U64Nullable(_)
            | EthereumSqlTypeWrapper::U64BigInt(_) => PgType::INT8,
            EthereumSqlTypeWrapper::I64(_) => PgType::INT8,
            EthereumSqlTypeWrapper::VecU64(_) => PgType::INT8_ARRAY,
            EthereumSqlTypeWrapper::VecI64(_) => PgType::INT8_ARRAY,
            // 128-bit integers
            EthereumSqlTypeWrapper::U128(_) => PgType::NUMERIC,
            EthereumSqlTypeWrapper::I128(_) => PgType::NUMERIC,
            EthereumSqlTypeWrapper::VecU128(_) => PgType::NUMERIC_ARRAY,
            EthereumSqlTypeWrapper::VecI128(_) => PgType::NUMERIC_ARRAY,
            // 256-bit integers (kept as VARCHAR for decimal string representation)
            EthereumSqlTypeWrapper::U256(_) | EthereumSqlTypeWrapper::U256Nullable(_) => {
                PgType::VARCHAR
            }
            // 256-bit unsigned integers opt in numeric representation (numeric(78))
            EthereumSqlTypeWrapper::U256Numeric(_)
            | EthereumSqlTypeWrapper::U256NumericNullable(_) => PgType::NUMERIC,
            EthereumSqlTypeWrapper::U256Bytes(_) | EthereumSqlTypeWrapper::U256BytesNullable(_) => {
                PgType::BYTEA
            }
            EthereumSqlTypeWrapper::I256(_) | EthereumSqlTypeWrapper::I256Nullable(_) => {
                PgType::VARCHAR
            }
            EthereumSqlTypeWrapper::I256Numeric(_) => PgType::NUMERIC,
            EthereumSqlTypeWrapper::I256Bytes(_) | EthereumSqlTypeWrapper::I256BytesNullable(_) => {
                PgType::BYTEA
            }
            EthereumSqlTypeWrapper::VecU256(_) => PgType::VARCHAR_ARRAY,
            EthereumSqlTypeWrapper::VecU256Bytes(_) => PgType::BYTEA_ARRAY,
            EthereumSqlTypeWrapper::VecU256Numeric(_) => PgType::NUMERIC_ARRAY,
            EthereumSqlTypeWrapper::VecI256(_) => PgType::VARCHAR_ARRAY,
            EthereumSqlTypeWrapper::VecI256Bytes(_) => PgType::BYTEA_ARRAY,
            // 512-bit integers
            EthereumSqlTypeWrapper::U512(_) => PgType::TEXT,
            EthereumSqlTypeWrapper::VecU512(_) => PgType::TEXT_ARRAY,
            // Hashes
            EthereumSqlTypeWrapper::B128(_) => PgType::BYTEA,
            #[allow(deprecated)]
            EthereumSqlTypeWrapper::H160(_) => PgType::BYTEA,
            // TODO! LOOK AT THIS TYPE AS IT IS SAVED AS CHAR IN NO CODE
            EthereumSqlTypeWrapper::B256(_) => PgType::BYTEA,
            EthereumSqlTypeWrapper::B256Bytes(_) => PgType::BYTEA,
            EthereumSqlTypeWrapper::B512(_) => PgType::BYTEA,
            EthereumSqlTypeWrapper::VecB128(_) => PgType::BYTEA_ARRAY,
            #[allow(deprecated)]
            EthereumSqlTypeWrapper::VecH160(_) => PgType::BYTEA_ARRAY,
            // TODO! LOOK AT THIS TYPE AS IT IS SAVED AS CHAR IN NO CODE
            EthereumSqlTypeWrapper::VecB256(_) => PgType::BYTEA_ARRAY,
            EthereumSqlTypeWrapper::VecB256Bytes(_) => PgType::BYTEA_ARRAY,
            EthereumSqlTypeWrapper::VecB512(_) => PgType::BYTEA_ARRAY,
            // Address
            EthereumSqlTypeWrapper::Address(_) | EthereumSqlTypeWrapper::AddressNullable(_) => {
                PgType::BPCHAR
            }
            EthereumSqlTypeWrapper::AddressBytes(_)
            | EthereumSqlTypeWrapper::AddressBytesNullable(_) => PgType::BYTEA,
            EthereumSqlTypeWrapper::VecAddress(_) => PgType::TEXT_ARRAY,
            EthereumSqlTypeWrapper::VecAddressBytes(_) => PgType::BYTEA_ARRAY,
            // Strings and Bytes
            EthereumSqlTypeWrapper::String(_) | EthereumSqlTypeWrapper::StringNullable(_) => {
                PgType::TEXT
            }
            EthereumSqlTypeWrapper::StringVarchar(_)
            | EthereumSqlTypeWrapper::StringVarcharNullable(_) => PgType::VARCHAR,
            EthereumSqlTypeWrapper::StringChar(_)
            | EthereumSqlTypeWrapper::StringCharNullable(_) => PgType::CHAR,
            EthereumSqlTypeWrapper::VecString(_) => PgType::TEXT_ARRAY,
            EthereumSqlTypeWrapper::VecStringVarchar(_) => PgType::VARCHAR_ARRAY,
            EthereumSqlTypeWrapper::VecStringChar(_) => PgType::CHAR_ARRAY,
            EthereumSqlTypeWrapper::Bytes(_) | EthereumSqlTypeWrapper::BytesNullable(_) => {
                PgType::BYTEA
            }
            EthereumSqlTypeWrapper::VecBytes(_) => PgType::BYTEA_ARRAY,
            EthereumSqlTypeWrapper::Uuid(_) => PgType::UUID,
            // DateTime
            EthereumSqlTypeWrapper::DateTime(_) | EthereumSqlTypeWrapper::DateTimeNullable(_) => {
                PgType::TIMESTAMPTZ
            }
            EthereumSqlTypeWrapper::JSONB(_) => PgType::JSONB,
        }
    }
    pub fn to_clickhouse_value(&self) -> String {
        match self {
            // Boolean
            EthereumSqlTypeWrapper::Bool(value) => value.to_string(),
            EthereumSqlTypeWrapper::VecBool(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            // 8-bit integers
            EthereumSqlTypeWrapper::U8(value) => value.to_string(),
            EthereumSqlTypeWrapper::I8(value) => value.to_string(),
            EthereumSqlTypeWrapper::VecU8(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            EthereumSqlTypeWrapper::VecI8(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            // 16-bit integers
            EthereumSqlTypeWrapper::U16(value) => value.to_string(),
            EthereumSqlTypeWrapper::I16(value) => value.to_string(),
            EthereumSqlTypeWrapper::VecU16(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            EthereumSqlTypeWrapper::VecI16(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            // 32-bit integers
            EthereumSqlTypeWrapper::U32(value) => value.to_string(),
            EthereumSqlTypeWrapper::I32(value) => value.to_string(),
            EthereumSqlTypeWrapper::VecU32(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            EthereumSqlTypeWrapper::VecI32(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            // 64-bit integers
            EthereumSqlTypeWrapper::U64(value) => value.to_string(),
            EthereumSqlTypeWrapper::I64(value) => value.to_string(),
            EthereumSqlTypeWrapper::VecU64(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            EthereumSqlTypeWrapper::VecI64(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            // 128-bit integers
            EthereumSqlTypeWrapper::U128(value) => value.to_string(),
            EthereumSqlTypeWrapper::I128(value) => value.to_string(),
            EthereumSqlTypeWrapper::VecU128(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            EthereumSqlTypeWrapper::VecI128(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            // 256-bit integers
            EthereumSqlTypeWrapper::U256(value) => value.to_string(),
            EthereumSqlTypeWrapper::I256(value) => value.to_string(),
            EthereumSqlTypeWrapper::VecU256(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            EthereumSqlTypeWrapper::VecI256(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            // 512-bit integers
            EthereumSqlTypeWrapper::U512(value) => value.to_string(),
            EthereumSqlTypeWrapper::VecU512(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            // Hashes
            EthereumSqlTypeWrapper::B128(value) => format!("'{value:?}'"),
            EthereumSqlTypeWrapper::B256(value) => format!("'{value:?}'"),
            EthereumSqlTypeWrapper::B512(value) => format!("'{value:?}'"),
            EthereumSqlTypeWrapper::VecB128(values) => format!(
                "[{}]",
                values.iter().map(|v| format!("'{v:?}'")).collect::<Vec<_>>().join(", ")
            ),
            EthereumSqlTypeWrapper::VecB256(values) => format!(
                "[{}]",
                values.iter().map(|v| format!("'{v:?}'")).collect::<Vec<_>>().join(", ")
            ),
            EthereumSqlTypeWrapper::VecB512(values) => format!(
                "[{}]",
                values.iter().map(|v| format!("'{v:?}'")).collect::<Vec<_>>().join(", ")
            ),
            // Address
            EthereumSqlTypeWrapper::Address(address) => format!("'{address}'"),
            EthereumSqlTypeWrapper::VecAddress(addresses) => format!(
                "[{}]",
                addresses.iter().map(|addr| format!("'{}'", addr)).collect::<Vec<_>>().join(", ")
            ),
            // Strings and Bytes
            EthereumSqlTypeWrapper::String(value) => format!("'{}'", value.replace("'", "\\'")),
            EthereumSqlTypeWrapper::VecString(values) => format!(
                "[{}]",
                values
                    .iter()
                    .map(|v| format!("'{}'", v.replace("'", "\\'")))
                    .collect::<Vec<_>>()
                    .join(", ")
            ),
            EthereumSqlTypeWrapper::Bytes(value) => format!("'0x{}'", hex::encode(value)),
            EthereumSqlTypeWrapper::VecBytes(values) => format!(
                "[{}]",
                values
                    .iter()
                    .map(|v| format!("'0x{}'", hex::encode(v)))
                    .collect::<Vec<_>>()
                    .join(", ")
            ),
            // DateTime
            EthereumSqlTypeWrapper::DateTime(value) => {
                let timestamp = value.to_rfc3339();
                let (datetime, _) =
                    timestamp.split_once('+').expect("DateTime should have a timezone");
                format!("'{datetime}'",)
            }
            EthereumSqlTypeWrapper::DateTimeNullable(value) => {
                if let Some(value) = value {
                    let timestamp = value.to_rfc3339();
                    let (datetime, _) =
                        timestamp.split_once('+').expect("DateTime should have a timezone");
                    format!("'{datetime}'")
                } else {
                    "NULL".to_string()
                }
            }
            EthereumSqlTypeWrapper::I256Nullable(v) => v.to_string(),
            EthereumSqlTypeWrapper::U64Nullable(v) => v.to_string(),
            EthereumSqlTypeWrapper::U256Nullable(v) => v.to_string(),
            EthereumSqlTypeWrapper::U64BigInt(v) => v.to_string(),
            EthereumSqlTypeWrapper::StringVarchar(v) => v.to_string(),
            EthereumSqlTypeWrapper::StringChar(v) => v.to_string(),
            EthereumSqlTypeWrapper::StringNullable(v) => v.to_string(),
            EthereumSqlTypeWrapper::StringVarcharNullable(v) => v.to_string(),
            EthereumSqlTypeWrapper::StringCharNullable(v) => v.to_string(),
            EthereumSqlTypeWrapper::AddressNullable(v) => v.to_string(),
            EthereumSqlTypeWrapper::BytesNullable(v) => format!("'0x{}'", hex::encode(v)),
            #[allow(deprecated)]
            EthereumSqlTypeWrapper::H160(v) => v.to_string(),
            #[allow(deprecated)]
            EthereumSqlTypeWrapper::VecH160(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            EthereumSqlTypeWrapper::VecStringVarchar(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            EthereumSqlTypeWrapper::VecStringChar(values) => {
                format!("[{}]", values.iter().map(|v| v.to_string()).collect::<Vec<_>>().join(", "))
            }
            EthereumSqlTypeWrapper::Uuid(_)
            | EthereumSqlTypeWrapper::VecB256Bytes(_)
            | EthereumSqlTypeWrapper::VecI256Bytes(_)
            | EthereumSqlTypeWrapper::B256Bytes(_)
            | EthereumSqlTypeWrapper::JSONB(_)
            | EthereumSqlTypeWrapper::U256Numeric(_)
            | EthereumSqlTypeWrapper::U256NumericNullable(_)
            | EthereumSqlTypeWrapper::VecU256Bytes(_)
            | EthereumSqlTypeWrapper::VecU256Numeric(_)
            | EthereumSqlTypeWrapper::U256Bytes(_)
            | EthereumSqlTypeWrapper::U256BytesNullable(_)
            | EthereumSqlTypeWrapper::I256Numeric(_)
            | EthereumSqlTypeWrapper::AddressBytes(_)
            | EthereumSqlTypeWrapper::AddressBytesNullable(_)
            | EthereumSqlTypeWrapper::VecAddressBytes(_)
            | EthereumSqlTypeWrapper::I256Bytes(_)
            | EthereumSqlTypeWrapper::I256BytesNullable(_) => {
                panic!(
                    "Clickhouse in no-code should never encounter these types. Clickhouse rust projects should use prefer the native-protocol. Unsupported '{}' EthereumSqlTypeWrapper variant for ClickHouse serialization",
                    self.raw_name()
                )
            }
        }
    }
    fn serialize_vec_decimal<T: ToString>(
        values: &Vec<T>,
        ty: &PgType,
        out: &mut BytesMut,
    ) -> Result<IsNull, Box<dyn std::error::Error + Sync + Send>> {
        if values.is_empty() {
            return Ok(IsNull::Yes);
        }
        let mut buf = BytesMut::new();
        buf.extend_from_slice(&(1i32.to_be_bytes())); // Number of dimensions
        buf.extend_from_slice(&(0i32.to_be_bytes())); // Has nulls flag
        buf.extend_from_slice(&PgType::NUMERIC.oid().to_be_bytes()); // Element type OID for numeric
        // Upper and lower bounds for dimensions
        buf.extend_from_slice(&(values.len() as i32).to_be_bytes()); // Length of the array
        buf.extend_from_slice(&(1i32.to_be_bytes())); // Index lower bound
        for value in values {
            let value_str = value.to_string();
            let decimal_value = Decimal::from_str(&value_str)?;
            let mut elem_buf = BytesMut::new();
            Decimal::to_sql(&decimal_value, ty, &mut elem_buf)?;
            buf.extend_from_slice(&(elem_buf.len() as i32).to_be_bytes()); // Length of the element
            buf.extend_from_slice(&elem_buf); // The element itself
        }
        out.extend_from_slice(&buf);
        Ok(IsNull::No)
    }
    fn convert_to_base_10000_numeric_digits<T: Into<u128> + Copy>(value: T) -> Vec<i16> {
        let mut groups = Vec::new();
        let mut num: u128 = value.into();
        while num > 0 {
            groups.push((num % 10000) as i16);
            num /= 10000;
        }
        groups.reverse();
        groups
    }
    fn convert_u256_to_base_10000_numeric_digits(value: &U256) -> Vec<i16> {
        let mut groups = Vec::new();
        let mut num = *value;
        if num.is_zero() {
            return vec![0];
        }
        while !num.is_zero() {
            let remainder = num % U256::from(10000);
            let bytes: [u8; 32] = remainder.to_be_bytes();
            let bytes: [u8; 2] = bytes[30..].try_into().unwrap();
            let remainder_i16 = i16::from_be_bytes(bytes);
            groups.push(remainder_i16);
            num /= U256::from(10000);
        }
        groups.reverse();
        groups
    }
    fn write_numeric_to_postgres<T>(
        value: T,
        is_negative: bool,
        out: &mut BytesMut,
    ) -> Result<IsNull, Box<dyn std::error::Error + Sync + Send>>
    where
        T: Into<u128> + Copy,
    {
        let groups = Self::convert_to_base_10000_numeric_digits(value);
        if groups.is_empty() {
            // Handle zero case
            out.put_i16(0); // ndigits
            out.put_i16(0); // weight
            out.put_i16(0x0000); // sign
            out.put_i16(0); // dscale
            return Ok(IsNull::No);
        }
        out.put_i16(groups.len() as i16); // ndigits
        out.put_i16((groups.len() - 1) as i16); // weight - safe now as we checked for empty
        out.put_i16(if is_negative { 0x4000 } else { 0x0000 }); // sign
        out.put_i16(0); // dscale
        for group in groups {
            out.put_i16(group);
        }
        Ok(IsNull::No)
    }
    fn write_u256_numeric_to_postgres<T>(
        value: T,
        is_negative: bool,
        out: &mut BytesMut,
    ) -> Result<IsNull, Box<dyn std::error::Error + Sync + Send>>
    where
        T: Into<U256>,
    {
        let groups = Self::convert_u256_to_base_10000_numeric_digits(&value.into());
        if groups.is_empty() {
            // Handle zero case
            out.put_i16(0); // ndigits
            out.put_i16(0); // weight
            out.put_i16(0x0000); // sign
            out.put_i16(0); // dscale
            return Ok(IsNull::No);
        }
        out.put_i16(groups.len() as i16); // ndigits
        out.put_i16((groups.len() - 1) as i16); // weight - safe now as we checked for empty
        out.put_i16(if is_negative { 0x4000 } else { 0x0000 }); // sign
        out.put_i16(0); // dscale
        for group in groups {
            out.put_i16(group);
        }
        Ok(IsNull::No)
    }
    fn serialize_numeric_array<T>(
        values: &[T],
        out: &mut BytesMut,
        value_converter: impl Fn(&T) -> (u128, bool), // (absolute value, is_negative)
    ) -> Result<IsNull, Box<dyn std::error::Error + Sync + Send>> {
        if values.is_empty() {
            return Ok(IsNull::Yes);
        }
        let mut buf = BytesMut::new();
        buf.extend_from_slice(&(1i32.to_be_bytes()));
        buf.extend_from_slice(&(0i32.to_be_bytes()));
        buf.extend_from_slice(&PgType::NUMERIC.oid().to_be_bytes());
        buf.extend_from_slice(&(values.len() as i32).to_be_bytes());
        buf.extend_from_slice(&(1i32.to_be_bytes()));
        for value in values {
            let (abs_value, is_negative) = value_converter(value);
            let mut elem_buf = BytesMut::new();
            Self::write_numeric_to_postgres(abs_value, is_negative, &mut elem_buf)?;
            buf.extend_from_slice(&(elem_buf.len() as i32).to_be_bytes());
            buf.extend_from_slice(&elem_buf);
        }
        out.extend_from_slice(&buf);
        Ok(IsNull::No)
    }
    fn serialize_numeric_u256_array<T>(
        values: &[T],
        out: &mut BytesMut,
        value_converter: impl Fn(&T) -> (U256, bool), // (absolute value, is_negative)
    ) -> Result<IsNull, Box<dyn std::error::Error + Sync + Send>> {
        if values.is_empty() {
            return Ok(IsNull::Yes);
        }
        let mut buf = BytesMut::new();
        buf.extend_from_slice(&(1i32.to_be_bytes()));
        buf.extend_from_slice(&(0i32.to_be_bytes()));
        buf.extend_from_slice(&PgType::NUMERIC.oid().to_be_bytes());
        buf.extend_from_slice(&(values.len() as i32).to_be_bytes());
        buf.extend_from_slice(&(1i32.to_be_bytes()));
        for value in values {
            let (abs_value, is_negative) = value_converter(value);
            let mut elem_buf = BytesMut::new();
            Self::write_u256_numeric_to_postgres(abs_value, is_negative, &mut elem_buf)?;
            buf.extend_from_slice(&(elem_buf.len() as i32).to_be_bytes());
            buf.extend_from_slice(&elem_buf);
        }
        out.extend_from_slice(&buf);
        Ok(IsNull::No)
    }
}
impl ToSql for EthereumSqlTypeWrapper {
    fn to_sql(
        &self,
        ty: &PgType,
        out: &mut BytesMut,
    ) -> Result<IsNull, Box<dyn std::error::Error + Sync + Send>> {
        match self {
            EthereumSqlTypeWrapper::U64(value) => Decimal::to_sql(&Decimal::from(*value), ty, out),
            EthereumSqlTypeWrapper::U64BigInt(value) => {
                // Convert u64 directly to i64 for BIGINT
                let pg_value = *value as i64;
                pg_value.to_sql(ty, out)
            }
            EthereumSqlTypeWrapper::U64Nullable(value) => {
                if *value == 0 {
                    return Ok(IsNull::Yes);
                }
                Decimal::to_sql(&Decimal::from(*value), ty, out)
            }
            EthereumSqlTypeWrapper::I64(value) => value.to_sql(ty, out),
            EthereumSqlTypeWrapper::VecU64(values) => Self::serialize_vec_decimal(values, ty, out),
            EthereumSqlTypeWrapper::VecI64(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    values.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::U128(value) => {
                Self::write_numeric_to_postgres(*value, false, out)
            }
            EthereumSqlTypeWrapper::I128(value) => {
                Self::write_numeric_to_postgres(value.unsigned_abs(), *value < 0, out)
            }
            EthereumSqlTypeWrapper::VecU128(values) => {
                Self::serialize_numeric_array(values, out, |v| (*v, false))
            }
            EthereumSqlTypeWrapper::VecI128(values) => {
                Self::serialize_numeric_array(values, out, |v| (v.unsigned_abs(), *v < 0))
            }
            EthereumSqlTypeWrapper::U256(value) => String::to_sql(&value.to_string(), ty, out),
            EthereumSqlTypeWrapper::U256Nullable(value) => {
                if value.is_zero() {
                    return Ok(IsNull::Yes);
                }
                String::to_sql(&value.to_string(), ty, out)
            }
            EthereumSqlTypeWrapper::U256Numeric(value) => {
                Self::write_u256_numeric_to_postgres(*value, false, out)
            }
            EthereumSqlTypeWrapper::U256NumericNullable(value) => {
                if let Some(v) = value {
                    Self::write_u256_numeric_to_postgres(*v, false, out)
                } else {
                    Ok(IsNull::Yes)
                }
            }
            EthereumSqlTypeWrapper::U256Bytes(value) => {
                let bytes: [u8; 32] = value.to_be_bytes();
                let bytes = Bytes::from(bytes);
                out.extend_from_slice(&bytes);
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::U256BytesNullable(value) => {
                if value.is_zero() {
                    return Ok(IsNull::Yes);
                }
                let bytes: [u8; 32] = value.to_be_bytes();
                let bytes = Bytes::from(bytes);
                out.extend_from_slice(&bytes);
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::VecU256(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    let values_strings: Vec<String> =
                        values.iter().map(|v| v.to_string()).collect();
                    EthereumSqlTypeWrapper::VecStringVarchar(values_strings).to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::VecU256Bytes(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    for value in values {
                        let bytes: [u8; 32] = value.to_be_bytes();
                        let bytes = Bytes::from(bytes);
                        out.extend_from_slice(&bytes);
                    }
                    Ok(IsNull::No)
                }
            }
            EthereumSqlTypeWrapper::VecU256Numeric(values) => {
                Self::serialize_numeric_u256_array(values, out, |v| (*v, false))
            }
            EthereumSqlTypeWrapper::I256(value) => {
                let value = value.to_string();
                String::to_sql(&value, ty, out)
            }
            EthereumSqlTypeWrapper::I256Numeric(value) => {
                let is_negative = value.is_negative();
                let abs_value: U256 =
                    if is_negative { value.abs().into_raw() } else { value.into_raw() };
                Self::write_u256_numeric_to_postgres(abs_value, is_negative, out)
            }
            EthereumSqlTypeWrapper::I256Nullable(value) => {
                if value.is_zero() {
                    return Ok(IsNull::Yes);
                }
                let value = value.to_string();
                String::to_sql(&value, ty, out)
            }
            EthereumSqlTypeWrapper::I256Bytes(value) => {
                let bytes: [u8; 32] = value.to_be_bytes();
                let bytes = Bytes::from(bytes);
                out.extend_from_slice(&bytes);
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::I256BytesNullable(value) => {
                if value.is_zero() {
                    return Ok(IsNull::Yes);
                }
                let bytes: [u8; 32] = value.to_be_bytes();
                let bytes = Bytes::from(bytes);
                out.extend_from_slice(&bytes);
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::VecI256(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    let values_strings: Vec<String> =
                        values.iter().map(|v| v.to_string()).collect();
                    let formatted_str = values_strings.join(",");
                    String::to_sql(&formatted_str, ty, out)
                }
            }
            EthereumSqlTypeWrapper::VecI256Bytes(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    for value in values {
                        let bytes: [u8; 32] = value.to_be_bytes();
                        let bytes = Bytes::from(bytes);
                        out.extend_from_slice(&bytes);
                    }
                    Ok(IsNull::No)
                }
            }
            EthereumSqlTypeWrapper::U512(value) => {
                let value = value.to_string();
                String::to_sql(&value, ty, out)
            }
            EthereumSqlTypeWrapper::VecU512(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    let values_strings: Vec<String> =
                        values.iter().map(|v| v.to_string()).collect();
                    let formatted_str = values_strings.join(",");
                    String::to_sql(&formatted_str, ty, out)
                }
            }
            EthereumSqlTypeWrapper::B128(value) => {
                let hex = format!("{value:?}");
                out.extend_from_slice(hex.as_bytes());
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::VecB128(values) => {
                let hexes: Vec<String> = values.iter().map(|s| format!("{s:?}")).collect();
                if hexes.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    hexes.to_sql(ty, out)
                }
            }
            #[allow(deprecated)]
            EthereumSqlTypeWrapper::H160(value) => {
                let hex = format!("{value:?}");
                out.extend_from_slice(hex.as_bytes());
                Ok(IsNull::No)
            }
            #[allow(deprecated)]
            EthereumSqlTypeWrapper::VecH160(values) => {
                let hexes: Vec<String> = values.iter().map(|s| format!("{s:?}")).collect();
                if hexes.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    hexes.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::B256(value) => {
                let hex = format!("{value:?}");
                out.extend_from_slice(hex.as_bytes());
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::B256Bytes(value) => {
                let bytes = Bytes::from(value.0);
                out.extend_from_slice(&bytes);
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::VecB256(values) => {
                let hexes: Vec<String> = values.iter().map(|s| format!("{s:?}")).collect();
                if hexes.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    hexes.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::VecB256Bytes(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    for value in values {
                        let bytes = Bytes::from(value.0);
                        out.extend_from_slice(&bytes);
                    }
                    Ok(IsNull::No)
                }
            }
            EthereumSqlTypeWrapper::B512(value) => {
                let hex = format!("{value:?}");
                out.extend_from_slice(hex.as_bytes());
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::VecB512(values) => {
                let hexes: Vec<String> = values.iter().map(|s| format!("{s:?}")).collect();
                if hexes.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    hexes.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::Address(value) => {
                let hex = format!("{value:?}");
                String::to_sql(&hex, ty, out)
            }
            EthereumSqlTypeWrapper::AddressNullable(value) => {
                if value.is_zero() {
                    return Ok(IsNull::Yes);
                }
                let hex = format!("{value:?}");
                String::to_sql(&hex, ty, out)
            }
            EthereumSqlTypeWrapper::AddressBytes(value) => {
                let bytes = Bytes::from(value.0);
                out.extend_from_slice(&bytes);
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::AddressBytesNullable(value) => {
                if value.is_zero() {
                    return Ok(IsNull::Yes);
                }
                let bytes = Bytes::from(value.0);
                out.extend_from_slice(&bytes);
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::VecAddress(values) => {
                let addresses: Vec<String> = values.iter().map(|s| format!("{s:?}")).collect();
                if addresses.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    addresses.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::VecAddressBytes(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    for value in values {
                        let bytes = Bytes::from(value.0);
                        out.extend_from_slice(&bytes);
                    }
                    Ok(IsNull::No)
                }
            }
            EthereumSqlTypeWrapper::Bool(value) => bool::to_sql(value, ty, out),
            EthereumSqlTypeWrapper::VecBool(values) => {
                if values.is_empty() {
                    return Ok(IsNull::Yes);
                }
                // yes this looks mad but only way i could get bool[] working in postgres
                // it correctly serialize the boolean values into the binary format for boolean
                // arrays
                let mut buf = BytesMut::new();
                buf.extend_from_slice(&(1i32.to_be_bytes())); // Number of dimensions
                buf.extend_from_slice(&(0i32.to_be_bytes())); // Has nulls flag
                buf.extend_from_slice(&PgType::BOOL.oid().to_be_bytes()); // Element type OID for boolean
                // Upper and lower bounds for dimensions
                buf.extend_from_slice(&(values.len() as i32).to_be_bytes()); // Length of the array
                buf.extend_from_slice(&(1i32.to_be_bytes())); // Index lower bound
                for value in values {
                    buf.extend_from_slice(&1i32.to_be_bytes()); // Length of the element
                    buf.extend_from_slice(&(*value as u8).to_be_bytes()); // The element itself
                }
                out.extend_from_slice(&buf);
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::String(value)
            | EthereumSqlTypeWrapper::StringVarchar(value)
            | EthereumSqlTypeWrapper::StringChar(value) => String::to_sql(value, ty, out),
            EthereumSqlTypeWrapper::StringNullable(value)
            | EthereumSqlTypeWrapper::StringVarcharNullable(value)
            | EthereumSqlTypeWrapper::StringCharNullable(value) => {
                if value.is_empty() {
                    return Ok(IsNull::Yes);
                }
                String::to_sql(value, ty, out)
            }
            EthereumSqlTypeWrapper::VecString(values)
            | EthereumSqlTypeWrapper::VecStringVarchar(values)
            | EthereumSqlTypeWrapper::VecStringChar(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    values.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::Bytes(value) => {
                out.extend_from_slice(value);
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::BytesNullable(value) => {
                if value.is_empty() {
                    return Ok(IsNull::Yes);
                }
                out.extend_from_slice(value);
                Ok(IsNull::No)
            }
            EthereumSqlTypeWrapper::VecBytes(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    for value in values {
                        out.extend_from_slice(value);
                    }
                    Ok(IsNull::No)
                }
            }
            EthereumSqlTypeWrapper::U32(value) => {
                let int_value: i32 = *value as i32;
                int_value.to_sql(ty, out)
            }
            EthereumSqlTypeWrapper::I32(value) => value.to_sql(ty, out),
            EthereumSqlTypeWrapper::VecU32(values) => {
                let int_values: Vec<i32> = values.iter().map(|&s| s as i32).collect();
                if int_values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    int_values.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::VecI32(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    values.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::U16(value) => {
                let int_value: i16 = *value as i16;
                int_value.to_sql(ty, out)
            }
            EthereumSqlTypeWrapper::I16(value) => value.to_sql(ty, out),
            EthereumSqlTypeWrapper::VecU16(values) => {
                let int_values: Vec<i16> = values.iter().map(|&s| s as i16).collect();
                if int_values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    int_values.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::VecI16(values) => {
                if values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    values.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::U8(value) => {
                let int_value: i16 = *value as i16;
                int_value.to_sql(ty, out)
            }
            EthereumSqlTypeWrapper::I8(value) => {
                let int_value: i16 = *value as i16;
                int_value.to_sql(ty, out)
            }
            EthereumSqlTypeWrapper::VecU8(values) => {
                let int_values: Vec<i16> = values.iter().map(|&s| s as i16).collect();
                if int_values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    int_values.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::VecI8(values) => {
                let int_values: Vec<i16> = values.iter().map(|&s| s as i16).collect();
                if int_values.is_empty() {
                    Ok(IsNull::Yes)
                } else {
                    int_values.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::DateTime(value) => value.to_sql(ty, out),
            EthereumSqlTypeWrapper::DateTimeNullable(value) => {
                if value.is_none() {
                    Ok(IsNull::Yes)
                } else {
                    value.to_sql(ty, out)
                }
            }
            EthereumSqlTypeWrapper::JSONB(value) => value.to_sql(ty, out),
            EthereumSqlTypeWrapper::Uuid(value) => value.to_sql(ty, out),
        }
    }
    fn accepts(_ty: &PgType) -> bool {
        true // We accept all types
    }
    to_sql_checked!();
}
#[allow(clippy::manual_strip)]
pub fn solidity_type_to_ethereum_sql_type_wrapper(
    abi_type: &str,
) -> Option<EthereumSqlTypeWrapper> {
    let is_array = abi_type.ends_with("[]");
    let base_type = abi_type.trim_end_matches("[]");
    match base_type {
        "string" => Some(if is_array {
            EthereumSqlTypeWrapper::VecString(Vec::new())
        } else {
            EthereumSqlTypeWrapper::String(String::new())
        }),
        "address" => Some(if is_array {
            EthereumSqlTypeWrapper::VecAddress(Vec::new())
        } else {
            EthereumSqlTypeWrapper::Address(Address::ZERO)
        }),
        "bool" => Some(if is_array {
            EthereumSqlTypeWrapper::VecBool(Vec::new())
        } else {
            EthereumSqlTypeWrapper::Bool(false)
        }),
        t if t.starts_with("bytes") => Some(if is_array {
            EthereumSqlTypeWrapper::VecBytes(Vec::new())
        } else {
            EthereumSqlTypeWrapper::Bytes(Bytes::new())
        }),
        t if t.starts_with("int") || t.starts_with("uint") => {
            let (prefix, size) = parse_solidity_integer_type(t);
            let is_signed = prefix.eq("int");
            Some(match (size, is_signed) {
                (8, false) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecU8(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::U8(0)
                    }
                }
                (8, true) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecI8(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::I8(0)
                    }
                }
                (16, false) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecU16(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::U16(0)
                    }
                }
                (16, true) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecI16(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::I16(0)
                    }
                }
                (24 | 32, false) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecU32(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::U32(0)
                    }
                }
                (24 | 32, true) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecI32(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::I32(0)
                    }
                }
                (40 | 48 | 56 | 64, false) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecU64(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::U64(0)
                    }
                }
                (40 | 48 | 56 | 64, true) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecI64(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::I64(0)
                    }
                }
                (72 | 80 | 88 | 96 | 104 | 112 | 120 | 128, false) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecU128(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::U128(0)
                    }
                }
                (72 | 80 | 88 | 96 | 104 | 112 | 120 | 128, true) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecI128(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::I128(0)
                    }
                }
                (
                    136 | 144 | 152 | 160 | 168 | 176 | 184 | 192 | 200 | 208 | 216 | 224 | 232
                    | 240 | 248 | 256,
                    false,
                ) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecU256(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::U256(U256::ZERO)
                    }
                }
                (
                    136 | 144 | 152 | 160 | 168 | 176 | 184 | 192 | 200 | 208 | 216 | 224 | 232
                    | 240 | 248 | 256,
                    true,
                ) => {
                    if is_array {
                        EthereumSqlTypeWrapper::VecI256(Vec::new())
                    } else {
                        EthereumSqlTypeWrapper::I256(I256::ZERO)
                    }
                }
                _ => return None,
            })
        }
        _ => None,
    }
}
pub fn map_log_params_to_ethereum_wrapper(
    abi_inputs: &[ABIInput],
    params: &[LogParam],
) -> Vec<EthereumSqlTypeWrapper> {
    let mut wrappers = vec![];
    for (index, param) in params.iter().enumerate() {
        if let Some(abi_input) = abi_inputs.get(index) {
            wrappers.extend(map_log_token_to_ethereum_wrapper(abi_input, &param.value))
        } else {
            panic!("No ABI input found for log param at index: {index}")
        }
    }
    wrappers
}
fn process_tuple(abi_inputs: &[ABIInput], tokens: &[DynSolValue]) -> Vec<EthereumSqlTypeWrapper> {
    let mut wrappers = vec![];
    for (index, token) in tokens.iter().enumerate() {
        if let Some(abi_input) = abi_inputs.get(index) {
            wrappers.extend(map_log_token_to_ethereum_wrapper(abi_input, token));
        } else {
            panic!("No ABI input found for log param at index: {index}")
        }
    }
    wrappers
}
fn tuple_solidity_type_to_ethereum_sql_type_wrapper(
    abi_inputs: &[ABIInput],
) -> Option<Vec<EthereumSqlTypeWrapper>> {
    let mut wrappers = vec![];
    for abi_input in abi_inputs {
        match &abi_input.components {
            Some(components) => {
                wrappers.extend(tuple_solidity_type_to_ethereum_sql_type_wrapper(components)?)
            }
            None => {
                wrappers.push(solidity_type_to_ethereum_sql_type_wrapper(&abi_input.type_)?);
            }
        }
    }
    Some(wrappers)
}
fn low_u128(value: &U256) -> u128 {
    // Referenced from: https://github.com/paritytech/parity-common/blob/a2b580d9fd5a340cea81....rs#L499
    let arr = value.as_limbs();
    ((arr[1] as u128) << 64) + arr[0] as u128
}
fn low_u128_from_int(value: &I256) -> u128 {
    // Referenced from: https://github.com/paritytech/parity-common/blob/a2b580d9fd5a340cea81....rs#L499
    let arr = value.as_limbs();
    ((arr[1] as u128) << 64) + arr[0] as u128
}
fn low_u32(value: &U256) -> u32 {
    value.to::<u32>()
}
fn as_u64(value: &U256) -> u64 {
    let low = value.into_limbs()[0];
    if value > &U256::from(low) {
        panic!("Integer overflow when casting to u64")
    }
    low
}
fn convert_int(value: &I256, target_type: &EthereumSqlTypeWrapper) -> EthereumSqlTypeWrapper {
    match target_type {
        EthereumSqlTypeWrapper::I256(_) | EthereumSqlTypeWrapper::VecI256(_) => {
            EthereumSqlTypeWrapper::I256(*value)
        }
        EthereumSqlTypeWrapper::U128(_) | EthereumSqlTypeWrapper::VecU128(_) => {
            EthereumSqlTypeWrapper::U128(low_u128_from_int(value))
        }
        EthereumSqlTypeWrapper::I128(_) | EthereumSqlTypeWrapper::VecI128(_) => {
            EthereumSqlTypeWrapper::I128(low_u128_from_int(value) as i128)
        }
        EthereumSqlTypeWrapper::U64(_) | EthereumSqlTypeWrapper::VecU64(_) => {
            EthereumSqlTypeWrapper::U64(value.as_u64())
        }
        EthereumSqlTypeWrapper::I64(_) | EthereumSqlTypeWrapper::VecI64(_) => {
            EthereumSqlTypeWrapper::I64(value.as_u64() as i64)
        }
        EthereumSqlTypeWrapper::U32(_) | EthereumSqlTypeWrapper::VecU32(_) => {
            EthereumSqlTypeWrapper::U32(value.low_u32())
        }
        EthereumSqlTypeWrapper::I32(_) | EthereumSqlTypeWrapper::VecI32(_) => {
            EthereumSqlTypeWrapper::I32(value.low_u32() as i32)
        }
        EthereumSqlTypeWrapper::U16(_) | EthereumSqlTypeWrapper::VecU16(_) => {
            EthereumSqlTypeWrapper::U16(value.low_u32() as u16)
        }
        EthereumSqlTypeWrapper::I16(_) | EthereumSqlTypeWrapper::VecI16(_) => {
            EthereumSqlTypeWrapper::I16(value.low_u32() as i16)
        }
        EthereumSqlTypeWrapper::U8(_) | EthereumSqlTypeWrapper::VecU8(_) => {
            EthereumSqlTypeWrapper::U8(value.low_u32() as u8)
        }
        EthereumSqlTypeWrapper::I8(_) | EthereumSqlTypeWrapper::VecI8(_) => {
            EthereumSqlTypeWrapper::I8(value.low_u32() as i8)
        }
        _ => {
            let error_message = format!("Unsupported target type - {target_type:?}");
            error!("{}", error_message);
            panic!("{}", error_message)
        }
    }
}
fn convert_uint(value: &U256, target_type: &EthereumSqlTypeWrapper) -> EthereumSqlTypeWrapper {
    match target_type {
        EthereumSqlTypeWrapper::U256(_) | EthereumSqlTypeWrapper::VecU256(_) => {
            EthereumSqlTypeWrapper::U256(*value)
        }
        EthereumSqlTypeWrapper::I256(_) | EthereumSqlTypeWrapper::VecI256(_) => {
            EthereumSqlTypeWrapper::I256(I256::from_raw(*value))
        }
        EthereumSqlTypeWrapper::U128(_) | EthereumSqlTypeWrapper::VecU128(_) => {
            EthereumSqlTypeWrapper::U128(low_u128(value))
        }
        EthereumSqlTypeWrapper::I128(_) | EthereumSqlTypeWrapper::VecI128(_) => {
            EthereumSqlTypeWrapper::I128(low_u128(value) as i128)
        }
        EthereumSqlTypeWrapper::U64(_) | EthereumSqlTypeWrapper::VecU64(_) => {
            EthereumSqlTypeWrapper::U64(as_u64(value))
        }
        EthereumSqlTypeWrapper::I64(_) | EthereumSqlTypeWrapper::VecI64(_) => {
            EthereumSqlTypeWrapper::I64(as_u64(value) as i64)
        }
        EthereumSqlTypeWrapper::U32(_) | EthereumSqlTypeWrapper::VecU32(_) => {
            EthereumSqlTypeWrapper::U32(low_u32(value))
        }
        EthereumSqlTypeWrapper::I32(_) | EthereumSqlTypeWrapper::VecI32(_) => {
            EthereumSqlTypeWrapper::I32(low_u32(value) as i32)
        }
        EthereumSqlTypeWrapper::U16(_) | EthereumSqlTypeWrapper::VecU16(_) => {
            EthereumSqlTypeWrapper::U16(low_u32(value) as u16)
        }
        EthereumSqlTypeWrapper::I16(_) | EthereumSqlTypeWrapper::VecI16(_) => {
            EthereumSqlTypeWrapper::I16(low_u32(value) as i16)
        }
        EthereumSqlTypeWrapper::U8(_) | EthereumSqlTypeWrapper::VecU8(_) => {
            EthereumSqlTypeWrapper::U8(low_u32(value) as u8)
        }
        EthereumSqlTypeWrapper::I8(_) | EthereumSqlTypeWrapper::VecI8(_) => {
            EthereumSqlTypeWrapper::I8(low_u32(value) as i8)
        }
        _ => {
            let error_message = format!("Unsupported target type - {target_type:?}");
            error!("{}", error_message);
            panic!("{}", error_message)
        }
    }
}
fn map_dynamic_int_to_ethereum_sql_type_wrapper(
    abi_input: &ABIInput,
    value: &I256,
) -> EthereumSqlTypeWrapper {
    let sql_type_wrapper = solidity_type_to_ethereum_sql_type_wrapper(&abi_input.type_);
    if let Some(target_type) = sql_type_wrapper {
        convert_int(value, &target_type)
    } else {
        let error_message = format!("Unknown int type for abi input: {abi_input:?}");
        error!("{}", error_message);
        panic!("{}", error_message);
    }
}
fn map_dynamic_uint_to_ethereum_sql_type_wrapper(
    abi_input: &ABIInput,
    value: &U256,
) -> EthereumSqlTypeWrapper {
    let sql_type_wrapper = solidity_type_to_ethereum_sql_type_wrapper(&abi_input.type_);
    if let Some(target_type) = sql_type_wrapper {
        convert_uint(value, &target_type)
    } else {
        let error_message = format!("Unknown int type for abi input: {abi_input:?}");
        error!("{}", error_message);
        panic!("{}", error_message);
    }
}
fn map_log_token_to_ethereum_wrapper(
    abi_input: &ABIInput,
    token: &DynSolValue,
) -> Vec<EthereumSqlTypeWrapper> {
    match &token {
        DynSolValue::Address(address) => vec![EthereumSqlTypeWrapper::Address(*address)],
        DynSolValue::Int(value, _) => {
            vec![map_dynamic_int_to_ethereum_sql_type_wrapper(abi_input, value)]
        }
        DynSolValue::Uint(value, _) => {
            vec![map_dynamic_uint_to_ethereum_sql_type_wrapper(abi_input, value)]
        }
        DynSolValue::Bool(b) => vec![EthereumSqlTypeWrapper::Bool(*b)],
        DynSolValue::String(s) => vec![EthereumSqlTypeWrapper::String(s.clone())],
        DynSolValue::FixedBytes(bytes, _) => {
            vec![EthereumSqlTypeWrapper::Bytes(Bytes::from(*bytes))]
        }
        DynSolValue::Bytes(bytes) => {
            vec![EthereumSqlTypeWrapper::Bytes(Bytes::from(bytes.clone()))]
        }
        DynSolValue::FixedArray(tokens) | DynSolValue::Array(tokens) => {
            match tokens.first() {
                None => match &abi_input.components {
                    Some(components) => tuple_solidity_type_to_ethereum_sql_type_wrapper(
                        components,
                    )
                    .unwrap_or_else(|| {
                        panic!(
                            "map_log_token_to_ethereum_wrapper:: Unknown type: {}",
                            abi_input.type_
                        )
                    }),
                    None => {
                        vec![solidity_type_to_ethereum_sql_type_wrapper(&abi_input.type_)
                            .unwrap_or_else(|| {
                                panic!(
                                    "map_log_token_to_ethereum_wrapper:: Unknown type: {}",
                                    abi_input.type_
                                )
                            })]
                    }
                },
                Some(first_token) => {
                    // events arrays can only be one type so get it from the first one
                    let token_type = first_token;
                    match token_type {
                        DynSolValue::Address(_) => {
                            let mut vec: Vec<Address> = vec![];
                            for token in tokens {
                                if let DynSolValue::Address(address) = token {
                                    vec.push(*address);
                                }
                            }
                            vec![EthereumSqlTypeWrapper::VecAddress(vec)]
                        }
                        DynSolValue::FixedBytes(_, _) | DynSolValue::Bytes(_) => {
                            let mut vec: Vec<Bytes> = vec![];
                            for token in tokens {
                                if let DynSolValue::FixedBytes(bytes, _) = token {
                                    vec.push(Bytes::from(*bytes));
                                }
                            }
                            vec![EthereumSqlTypeWrapper::VecBytes(vec)]
                        }
                        DynSolValue::Int(_, _) | DynSolValue::Uint(_, _) => {
                            let sql_type_wrapper =
                                solidity_type_to_ethereum_sql_type_wrapper(&abi_input.type_)
                                    .unwrap_or_else(|| {
                                        panic!("Unknown int type for abi input: {abi_input:?}")
                                    });
                            let vec_wrapper = tokens
                                .iter()
                                .map(|token| {
                                    if let DynSolValue::Uint(uint, _) = token {
                                        return convert_uint(uint, &sql_type_wrapper);
                                    }
                                    if let DynSolValue::Int(uint, _) = token {
                                        return convert_int(uint, &sql_type_wrapper);
                                    }
                                    panic!(
                                        "Expected uint or int token in array for abi input: {abi_input:?}"
                                    );
                                })
                                .collect::<Vec<_>>();
                            match sql_type_wrapper {
                                EthereumSqlTypeWrapper::U256(_)
                                | EthereumSqlTypeWrapper::VecU256(_) => {
                                    vec![EthereumSqlTypeWrapper::VecU256(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::U256(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::I256(_)
                                | EthereumSqlTypeWrapper::VecI256(_) => {
                                    vec![EthereumSqlTypeWrapper::VecI256(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::I256(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::U128(_)
                                | EthereumSqlTypeWrapper::VecU128(_) => {
                                    vec![EthereumSqlTypeWrapper::VecU128(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::U128(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::I128(_)
                                | EthereumSqlTypeWrapper::VecI128(_) => {
                                    vec![EthereumSqlTypeWrapper::VecI128(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::I128(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::U64(_)
                                | EthereumSqlTypeWrapper::VecU64(_) => {
                                    vec![EthereumSqlTypeWrapper::VecU64(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::U64(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::I64(_)
                                | EthereumSqlTypeWrapper::VecI64(_) => {
                                    vec![EthereumSqlTypeWrapper::VecI64(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::I64(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::U32(_)
                                | EthereumSqlTypeWrapper::VecU32(_) => {
                                    vec![EthereumSqlTypeWrapper::VecU32(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::U32(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::I32(_)
                                | EthereumSqlTypeWrapper::VecI32(_) => {
                                    vec![EthereumSqlTypeWrapper::VecI32(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::I32(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::U16(_)
                                | EthereumSqlTypeWrapper::VecU16(_) => {
                                    vec![EthereumSqlTypeWrapper::VecU16(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::U16(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::I16(_)
                                | EthereumSqlTypeWrapper::VecI16(_) => {
                                    vec![EthereumSqlTypeWrapper::VecI16(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::I16(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::U8(_)
                                | EthereumSqlTypeWrapper::VecU8(_) => {
                                    vec![EthereumSqlTypeWrapper::VecU8(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::U8(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                EthereumSqlTypeWrapper::I8(_)
                                | EthereumSqlTypeWrapper::VecI8(_) => {
                                    vec![EthereumSqlTypeWrapper::VecI8(
                                        vec_wrapper
                                            .into_iter()
                                            .map(|w| match w {
                                                EthereumSqlTypeWrapper::I8(v) => v,
                                                _ => unreachable!(),
                                            })
                                            .collect(),
                                    )]
                                }
                                _ => panic!("Unknown int type for abi input: {abi_input:?}"),
                            }
                        }
                        DynSolValue::Bool(_) => {
                            let mut vec: Vec<bool> = vec![];
                            for token in tokens {
                                if let DynSolValue::Bool(b) = token {
                                    vec.push(*b);
                                }
                            }
                            vec![EthereumSqlTypeWrapper::VecBool(vec)]
                        }
                        DynSolValue::String(_) => {
                            let mut vec: Vec<String> = vec![];
                            for token in tokens {
                                if let DynSolValue::String(s) = token {
                                    vec.push(s.clone());
                                }
                            }
                            vec![EthereumSqlTypeWrapper::VecString(vec)]
                        }
                        DynSolValue::FixedArray(_) | DynSolValue::Array(_) => {
                            unreachable!("Nested arrays are not supported by the EVM")
                        }
                        DynSolValue::Tuple(tuple) => process_tuple(
                            abi_input
                                .components
                                .as_ref()
                                .expect("Tuple should have a component ABI on"),
                            tuple,
                        ),
                        _ => {
                            unimplemented!("CustomStruct and Function are not supported yet - please raise issue in github with ABI to recreate")
                        }
                    }
                }
            }
        }
        DynSolValue::Tuple(tuple) => process_tuple(
            abi_input.components.as_ref().expect("Tuple should have a component ABI on"),
            tuple,
        ),
        _ => {
            unimplemented!("CustomStruct and Function are not supported yet - please raise issue in github with ABI to recreate")
        }
    }
}
impl From<&Address> for EthereumSqlTypeWrapper {
    fn from(address: &Address) -> Self {
        EthereumSqlTypeWrapper::Address(*address)
    }
}
fn count_components(components: &[ABIInput]) -> usize {
    components
        .iter()
        .map(|component| {
            if component.type_ == "tuple" {
                let nested_components =
                    component.components.as_ref().expect("Tuple should have components defined");
                1 + count_components(nested_components)
            } else {
                1
            }
        })
        .sum()
}
pub fn map_ethereum_wrapper_to_json(
    abi_inputs: &[ABIInput],
    wrappers: &[EthereumSqlTypeWrapper],
    transaction_information: &TxInformation,
    is_within_tuple: bool,
) -> Value {
    let mut result = serde_json::Map::new();
    let mut current_wrapper_index = 0;
    let mut wrappers_index_processed = Vec::new();
    for abi_input in abi_inputs.iter() {
        // tuples will take in multiple wrapper indexes, so we need to skip them if processed
        if wrappers_index_processed.contains(&current_wrapper_index) {
            continue;
        }
        if let Some(wrapper) = wrappers.get(current_wrapper_index) {
            if abi_input.type_ == "tuple" {
                let components =
                    abi_input.components.as_ref().expect("Tuple should have components defined");
                let total_properties = count_components(components);
                // Extract the correct slice of wrappers for this tuple.
                // We need wrappers[current_index..current_index + count], not wrappers[current_index..count]
                // because current_index is the starting position, and total_properties is the number of components.
                // For example: if we're at index 3 and need 12 components, we want indices 3-14 (12 items),
                // not indices 3-11 (9 items).
                let tuple_value = map_ethereum_wrapper_to_json(
                    components,
                    &wrappers[current_wrapper_index..current_wrapper_index + total_properties],
                    transaction_information,
                    true,
                );
                result.insert(abi_input.name.clone(), tuple_value);
                for i in current_wrapper_index..current_wrapper_index + total_properties {
                    wrappers_index_processed.push(i);
                }
                current_wrapper_index += total_properties;
            } else {
                let value = match wrapper {
                    EthereumSqlTypeWrapper::U64(u)
                    | EthereumSqlTypeWrapper::U64Nullable(u)
                    | EthereumSqlTypeWrapper::U64BigInt(u) => {
                        json!(u)
                    }
                    EthereumSqlTypeWrapper::VecU64(u64s) => json!(u64s),
                    EthereumSqlTypeWrapper::I64(i) => json!(i),
                    EthereumSqlTypeWrapper::VecI64(i64s) => json!(i64s),
                    EthereumSqlTypeWrapper::U128(u) => json!(u.to_string()),
                    EthereumSqlTypeWrapper::VecU128(u128s) => {
                        json!(u128s.iter().map(|u| u.to_string()).collect::<Vec<_>>())
                    }
                    EthereumSqlTypeWrapper::I128(i) => json!(i.to_string()),
                    EthereumSqlTypeWrapper::VecI128(i128s) => {
                        json!(i128s.iter().map(|i| i.to_string()).collect::<Vec<_>>())
                    }
                    EthereumSqlTypeWrapper::U256(u)
                    | EthereumSqlTypeWrapper::U256Numeric(u)
                    | EthereumSqlTypeWrapper::U256Bytes(u)
                    | EthereumSqlTypeWrapper::U256Nullable(u)
                    | EthereumSqlTypeWrapper::U256BytesNullable(u) => {
                        json!(u.to_string())
                    }
                    EthereumSqlTypeWrapper::U256NumericNullable(u) => {
                        json!(u.map(|v| v.to_string()))
                    }
                    EthereumSqlTypeWrapper::VecU256(u256s)
                    | EthereumSqlTypeWrapper::VecU256Numeric(u256s)
                    | EthereumSqlTypeWrapper::VecU256Bytes(u256s) => {
                        json!(u256s.iter().map(|u| u.to_string()).collect::<Vec<_>>())
                    }
                    EthereumSqlTypeWrapper::I256(i)
                    | EthereumSqlTypeWrapper::I256Numeric(i)
                    | EthereumSqlTypeWrapper::I256Bytes(i)
                    | EthereumSqlTypeWrapper::I256Nullable(i)
                    | EthereumSqlTypeWrapper::I256BytesNullable(i) => {
                        json!(i.to_string())
                    }
                    EthereumSqlTypeWrapper::VecI256(i256s)
                    | EthereumSqlTypeWrapper::VecI256Bytes(i256s) => {
                        json!(i256s.iter().map(|i| i.to_string()).collect::<Vec<_>>())
                    }
                    EthereumSqlTypeWrapper::U512(u) => json!(u.to_string()),
                    EthereumSqlTypeWrapper::VecU512(u512s) => {
                        json!(u512s.iter().map(|u| u.to_string()).collect::<Vec<_>>())
                    }
                    EthereumSqlTypeWrapper::B128(h) => json!(h),
                    EthereumSqlTypeWrapper::VecB128(h128s) => json!(h128s),
                    #[allow(deprecated)]
                    EthereumSqlTypeWrapper::H160(h) => json!(h),
                    #[allow(deprecated)]
                    EthereumSqlTypeWrapper::VecH160(h160s) => json!(h160s),
                    EthereumSqlTypeWrapper::B256(h) | EthereumSqlTypeWrapper::B256Bytes(h) => {
                        json!(h)
                    }
                    EthereumSqlTypeWrapper::VecB256(h256s)
                    | EthereumSqlTypeWrapper::VecB256Bytes(h256s) => json!(h256s),
                    EthereumSqlTypeWrapper::B512(h) => json!(h),
                    EthereumSqlTypeWrapper::VecB512(h512s) => json!(h512s),
                    EthereumSqlTypeWrapper::Address(address)
                    | EthereumSqlTypeWrapper::AddressBytes(address)
                    | EthereumSqlTypeWrapper::AddressBytesNullable(address)
                    | EthereumSqlTypeWrapper::AddressNullable(address) => json!(address),
                    EthereumSqlTypeWrapper::VecAddress(addresses)
                    | EthereumSqlTypeWrapper::VecAddressBytes(addresses) => json!(addresses),
                    EthereumSqlTypeWrapper::Bool(b) => json!(b),
                    EthereumSqlTypeWrapper::VecBool(bools) => json!(bools),
                    EthereumSqlTypeWrapper::U32(u) => json!(u),
                    EthereumSqlTypeWrapper::VecU32(u32s) => json!(u32s),
                    EthereumSqlTypeWrapper::I32(i) => json!(i),
                    EthereumSqlTypeWrapper::VecI32(i32s) => json!(i32s),
                    EthereumSqlTypeWrapper::U16(u) => json!(u),
                    EthereumSqlTypeWrapper::VecU16(u16s) => json!(u16s),
                    EthereumSqlTypeWrapper::I16(i) => json!(i),
                    EthereumSqlTypeWrapper::VecI16(i16s) => json!(i16s),
                    EthereumSqlTypeWrapper::U8(u) => json!(u),
                    EthereumSqlTypeWrapper::VecU8(u8s) => json!(u8s),
                    EthereumSqlTypeWrapper::I8(i) => json!(i),
                    EthereumSqlTypeWrapper::VecI8(i8s) => json!(i8s),
                    EthereumSqlTypeWrapper::String(s)
                    | EthereumSqlTypeWrapper::StringNullable(s)
                    | EthereumSqlTypeWrapper::StringVarchar(s)
                    | EthereumSqlTypeWrapper::StringVarcharNullable(s)
                    | EthereumSqlTypeWrapper::StringChar(s)
                    | EthereumSqlTypeWrapper::StringCharNullable(s) => json!(s),
                    EthereumSqlTypeWrapper::VecString(strings)
                    | EthereumSqlTypeWrapper::VecStringVarchar(strings)
                    | EthereumSqlTypeWrapper::VecStringChar(strings) => json!(strings),
                    EthereumSqlTypeWrapper::Bytes(bytes)
                    | EthereumSqlTypeWrapper::BytesNullable(bytes) => json!(hex::encode(bytes)),
                    EthereumSqlTypeWrapper::VecBytes(bytes) => {
                        json!(bytes.iter().map(hex::encode).collect::<Vec<_>>())
                    }
                    EthereumSqlTypeWrapper::DateTime(date_time) => {
                        json!(date_time.to_rfc3339())
                    }
                    EthereumSqlTypeWrapper::DateTimeNullable(date_time) => {
                        json!(date_time.map(|d| d.to_rfc3339()))
                    }
                    EthereumSqlTypeWrapper::JSONB(json) => json.clone(),
                    EthereumSqlTypeWrapper::Uuid(uuid) => json!(uuid.to_string()),
                };
                result.insert(abi_input.name.clone(), value);
                wrappers_index_processed.push(current_wrapper_index);
                current_wrapper_index += 1;
            }
        } else {
            panic!(
                "No wrapper found for ABI input {abi_input:?} and wrapper index {current_wrapper_index} - wrappers {wrappers:?}"
            );
        }
    }
    // only do this at the top level
    if !is_within_tuple {
        result.insert("transaction_information".to_string(), json!(transaction_information));
    }
    Value::Object(result)
}
</file>

<file path="core/src/event/filter/ast.rs">
//! This module defines the abstract syntax tree (AST) for the filter expressions.
//! Parsing module will convert the input string into this AST structure.
//! This AST is then traversed and interpreted by the evaluation module to determine the result of the filter expression.
//!
//! The AST is designed to be a direct representation of the parsed filter expression, capturing it's structure, operators and literal values.
//! Lifetime annotations (`'a`) are used to ensure that the references to string literals are valid for the duration of the expression evaluation.
/// Represents the possible literal values that can be used in filter expressions.
/// The `LiteralValue` enum captures the different constant values that are used on the right side of a condition (RHS).
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum LiteralValue<'a> {
    /// A boolean literal value.
    Bool(bool),
    /// A string literal value. Includes both single-quoted and unquoted strings, includes hexadecimal strings.
    /// e.g., "abc", 'abc', '0x123ABC'
    Str(&'a str),
    /// A numeric literal value. e.g., "123", "-123.456", "0x123" or hexadecimal
    /// Store as string slice to preserve original form until evaluation phase.
    /// Conversion to specific type is done within chain context during evaluation.
    Number(&'a str),
}
/// Represents the possible comparison operators that can be used in filter expressions.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ComparisonOperator {
    /// Equality operator (==)
    Eq,
    /// Inequality operator (!=)
    Ne,
    /// Greater than operator (>)
    Gt,
    /// Greater than or equal to operator (>=)
    Gte,
    /// Less than operator (<)
    Lt,
    /// Less than or equal to operator (<=)
    Lte,
}
/// Represents the possible logical operators that can be used in filter expressions.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum LogicalOperator {
    /// Logical AND operator (&&)
    And,
    /// Logical OR operator (||)
    Or,
}
/// Represents the possible accessors that can be used in filter expressions.
/// Accessors are used to access elements in collections or properties in objects.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum Accessor<'a> {
    /// Accessor for a collection index (e.g., [0], [1], etc.)
    Index(usize),
    /// Accessor for a property name (e.g., .name, .age, etc.)
    Key(&'a str),
}
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct VariablePath<'a> {
    pub base: &'a str,
    pub accessors: Vec<Accessor<'a>>,
}
/// Represents the left side of a condition (LHS) in a filter expression.
/// The left side can either be a simple variable name or a path to a variable.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ConditionLeft<'a> {
    /// A simple variable name (e.g., "name", "age", etc.)
    /// This is a direct reference to a variable in the data structure.
    Simple(&'a str),
    /// A sequence of accessors that form a path to a variable (e.g., "person.name", "person[0].age", etc.)
    Path(VariablePath<'a>),
}
impl<'a> ConditionLeft<'a> {
    /// Helper method get the base name of the variable or path.
    pub fn base_name(&self) -> &'a str {
        match self {
            ConditionLeft::Simple(name) => name,
            ConditionLeft::Path(path) => path.base,
        }
    }
    /// Helper method to get the accessors of the variable path.
    /// If ConditionLeft is a simple variable, it returns an empty slice.
    /// If it is a path, it returns the accessors of that path.
    /// Used during evaluation to traverse nested structures.
    pub fn accessors(&self) -> &[Accessor<'_>] {
        match self {
            ConditionLeft::Simple(_) => &[],
            ConditionLeft::Path(path) => &path.accessors,
        }
    }
}
/// Represents a condition in a filter expression.
/// A condition consists of a left side (LHS), an operator, and a right side (RHS).
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Condition<'a> {
    /// The left side of the condition (LHS).
    /// This can be a simple variable name or a path to a variable.
    pub left: ConditionLeft<'a>,
    /// The operator used in the condition (e.g., ==, !=, >, <, etc.)
    pub operator: ComparisonOperator,
    /// The right side of the condition (RHS).
    pub right: LiteralValue<'a>,
}
/// Represents a complete filter expression.
/// An expression can be a single condition or a logical combination of multiple conditions.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum Expression<'a> {
    /// A simple condition (e.g., "age > 30")
    Condition(Condition<'a>),
    /// A logical combination of two expressions (e.g., "age > 30 && name == 'John'")
    /// `Box` is used to avoid infinite type recursion, as `Expression` can contain other `Expression`s.
    Logical {
        /// The left side sub-expression.
        left: Box<Expression<'a>>,
        /// The logical operator used to combine the two expressions: AND or OR.
        operator: LogicalOperator,
        /// The right side sub-expression.
        right: Box<Expression<'a>>,
    },
}
</file>

<file path="core/src/event/filter/evaluation.rs">
//! This module evaluates a parsed expression AST against a JSON object.
use super::{
    ast::{
        Accessor, ComparisonOperator, Condition, ConditionLeft, Expression, LiteralValue,
        LogicalOperator,
    },
    helpers::{are_same_address, compare_ordered_values, string_to_i256, string_to_u256},
};
use rust_decimal::Decimal;
use serde_json::Value as JsonValue;
use std::str::FromStr;
use thiserror::Error;
/// Represents errors that can occur during expression evaluation.
#[derive(Debug, Error, PartialEq)]
pub enum EvaluationError {
    /// An error indicating a type mismatch during evaluation.
    #[error("Type mismatch: {0}")]
    TypeMismatch(String),
    /// An error indicating that an unsupported operator was used.
    #[error("Unsupported operator: {0}")]
    UnsupportedOperator(String),
    /// An error indicating that a parse operation failed.
    #[error("Parse error: {0}")]
    ParseError(String),
    /// An error indicating that a variable was not found in the provided data.
    #[error("Variable not found: {0}")]
    VariableNotFound(String),
    /// An error indicating that an index was out of bounds for an array.
    #[error("Index out of bounds: {0}")]
    IndexOutOfBounds(String),
}
const UNSIGNED_INTEGER_KINDS: &[&str] =
    &["uint8", "uint16", "uint32", "uint64", "uint128", "uint256", "number"];
const SIGNED_INTEGER_KINDS: &[&str] = &["int8", "int16", "int32", "int64", "int128", "int256"];
const ARRAY_KINDS: &[&str] = &[
    "array",
    "uint8[]",
    "uint16[]",
    "uint32[]",
    "uint64[]",
    "uint128[]",
    "uint256[]",
    "int8[]",
    "int16[]",
    "int32[]",
    "int64[]",
    "int128[]",
    "int256[]",
    "string[]",
    "address[]",
    "bool[]",
    "fixed[]",
    "ufixed[]",
    "bytes[]",
    "bytes32[]",
    "tuple[]",
];
/// The main entry point for evaluation. Traverses the Expression AST and evaluates it against the given data.
///
/// # Arguments
/// * `expression` - The parsed expression AST to evaluate.
/// * `data` - The JSON data against which the expression is evaluated.
/// # Returns
/// * `Ok(bool)` - The result of the evaluation, true if the expression evaluates to true, false otherwise.
/// * `Err(EvaluationError)` - An error if the evaluation fails due to type mismatches, unsupported operators, parsing errors, or missing variables.
pub fn evaluate<'a>(
    expression: &Expression<'a>,
    data: &JsonValue,
) -> Result<bool, EvaluationError> {
    tracing::debug!(?expression, ?data, "Evaluating expression");
    match expression {
        Expression::Logical { left, operator, right } => {
            let left_val = evaluate(left, data)?;
            match operator {
                LogicalOperator::And => {
                    if !left_val {
                        Ok(false) // Short-circuit
                    } else {
                        evaluate(right, data)
                    }
                }
                LogicalOperator::Or => {
                    if left_val {
                        Ok(true) // Short-circuit
                    } else {
                        evaluate(right, data)
                    }
                }
            }
        }
        Expression::Condition(condition) => evaluate_condition(condition, data),
    }
}
/// Evaluates a single condition.
/// # Arguments
/// * `condition` - The condition to evaluate.
/// * `data` - The JSON data against which the condition is evaluated.
/// # Returns
/// * `Ok(bool)` - The result of the condition evaluation, true if the condition is satisfied, false otherwise.
/// * `Err(EvaluationError)` - An error if the evaluation fails due to type mismatches, unsupported operators, parsing errors, or missing variables.
fn evaluate_condition<'a>(
    condition: &Condition<'a>,
    data: &JsonValue,
) -> Result<bool, EvaluationError> {
    tracing::debug!(?condition, ?data, "Evaluating condition");
    let resolved_value = resolve_path(&condition.left, data)?;
    let final_left_kind = get_kind_from_json_value(resolved_value);
    let final_left_value_str = match resolved_value {
        JsonValue::String(s) => s.clone(),
        JsonValue::Number(n) => n.to_string(),
        JsonValue::Bool(b) => b.to_string(),
        JsonValue::Null => "null".to_string(),
        JsonValue::Array(_) | JsonValue::Object(_) => resolved_value.to_string(),
    };
    compare_final_values(
        &final_left_kind,
        &final_left_value_str,
        &condition.operator,
        &condition.right,
    )
}
/// Resolves a path from the AST against the JSON data.
/// # Arguments
/// * `path` - The path to resolve, which may include base names and accessors.
/// * `data` - The JSON data against which the path is resolved.
/// # Returns
/// * `Ok(&JsonValue)` - The resolved value from the JSON data.
/// * `Err(EvaluationError)` - An error if the path cannot be resolved due to missing variables, type mismatches, or index out of bounds errors.
fn resolve_path<'a>(
    path: &ConditionLeft<'a>,
    data: &'a JsonValue,
) -> Result<&'a JsonValue, EvaluationError> {
    tracing::debug!(?path, ?data, "Resolving path");
    let base_name = path.base_name();
    let mut current = data
        .get(base_name)
        .ok_or_else(|| EvaluationError::VariableNotFound(base_name.to_string()))?;
    for accessor in path.accessors() {
        current = match (accessor, current) {
            (Accessor::Key(key), JsonValue::Object(map)) => map
                .get(*key)
                .ok_or_else(|| EvaluationError::VariableNotFound((*key).to_string()))?,
            (Accessor::Index(index), JsonValue::Array(arr)) => arr
                .get(*index)
                .ok_or_else(|| EvaluationError::IndexOutOfBounds(index.to_string()))?,
            _ => {
                return Err(EvaluationError::TypeMismatch(format!(
                    "Cannot apply accessor {accessor:?} to value {current:?}"
                )));
            }
        };
    }
    Ok(current)
}
/// Routes the comparison to the correct type-specific function.
/// # Arguments
/// * `lhs_kind_str` - The kind of the left-hand side value as a string.
/// * `lhs_value_str` - The string representation of the left-hand side value.
/// * `operator` - The comparison operator to use.
/// * `rhs_literal` - The right-hand side literal value to compare against.
/// # Returns
/// * `Ok(bool)` - The result of the comparison, true if the condition is satisfied, false otherwise.
/// * `Err(EvaluationError)` - An error if the comparison fails due to type mismatches, unsupported operators, or parsing errors.
fn compare_final_values(
    lhs_kind_str: &str,
    lhs_value_str: &str,
    operator: &ComparisonOperator,
    rhs_literal: &LiteralValue<'_>,
) -> Result<bool, EvaluationError> {
    tracing::debug!(
        ?lhs_kind_str,
        ?lhs_value_str,
        ?operator,
        ?rhs_literal,
        "Comparing final values"
    );
    let lhs_kind = lhs_kind_str.to_lowercase();
    if SIGNED_INTEGER_KINDS.contains(&lhs_kind.as_str()) {
        return compare_i256(lhs_value_str, operator, rhs_literal);
    }
    if UNSIGNED_INTEGER_KINDS.contains(&lhs_kind.as_str()) {
        return compare_u256(lhs_value_str, operator, rhs_literal);
    }
    if ARRAY_KINDS.contains(&lhs_kind.as_str()) {
        return compare_array(lhs_value_str, operator, rhs_literal);
    }
    match lhs_kind.as_str() {
        "fixed" | "ufixed" => compare_fixed_point(lhs_value_str, operator, rhs_literal),
        "address" => compare_address(lhs_value_str, operator, rhs_literal),
        "string" | "bytes" | "bytes32" => compare_string(lhs_value_str, operator, rhs_literal),
        "bool" => compare_boolean(lhs_value_str, operator, rhs_literal),
        _ => Err(EvaluationError::TypeMismatch(format!(
            "Unsupported parameter kind for comparison: {lhs_kind_str}",
        ))),
    }
}
/// Compares two JSON array values based on the specified operator.
/// # Arguments
/// * `lhs_json_array_str` - The string representation of the left-hand side JSON array.
/// * `operator` - The comparison operator to use.
/// * `rhs_literal` - The right-hand side literal value to compare against, expected to be a string representation of a JSON array.
/// # Returns
/// * `Ok(bool)` - The result of the comparison, true if the condition is satisfied, false otherwise.
/// * `Err(EvaluationError)` - An error if the comparison fails due to type mismatches, unsupported operators, or parsing errors.
fn compare_array(
    lhs_json_array_str: &str,
    operator: &ComparisonOperator,
    rhs_literal: &LiteralValue<'_>,
) -> Result<bool, EvaluationError> {
    tracing::debug!(?lhs_json_array_str, ?operator, ?rhs_literal, "Comparing array values");
    let rhs_target_str = match rhs_literal {
        LiteralValue::Str(s) => *s,
        _ => {
            return Err(EvaluationError::TypeMismatch(format!(
                "Expected string literal for 'array' comparison, found: {rhs_literal:?}"
            )));
        }
    };
    match operator {
        ComparisonOperator::Eq | ComparisonOperator::Ne => {
            let lhs_json = serde_json::from_str::<JsonValue>(lhs_json_array_str)
                .map_err(|e| EvaluationError::ParseError(e.to_string()))?;
            let rhs_json = serde_json::from_str::<JsonValue>(rhs_target_str)
                .map_err(|e| EvaluationError::ParseError(e.to_string()))?;
            if !lhs_json.is_array() || !rhs_json.is_array() {
                return Err(EvaluationError::TypeMismatch(
                    "Both sides of an array comparison must be valid JSON arrays.".to_string(),
                ));
            }
            let are_equal = lhs_json == rhs_json;
            Ok(if *operator == ComparisonOperator::Eq { are_equal } else { !are_equal })
        }
        _ => Err(EvaluationError::UnsupportedOperator(format!(
            "Operator {operator:?} not supported for 'array' type. Supported: Eq, Ne.",
        ))),
    }
}
/// Compares two U256 values based on the specified operator.
/// # Arguments
/// * `left_str` - The string representation of the left-hand side U256 value.
/// * `operator` - The comparison operator to use.
/// * `right_literal` - The right-hand side literal value to compare against, expected to be a string or number literal.
/// # Returns
/// * `Ok(bool)` - The result of the comparison, true if the condition is satisfied, false otherwise.
/// * `Err(EvaluationError)` - An error if the comparison fails due to type mismatches, or parsing errors.
fn compare_u256(
    left_str: &str,
    operator: &ComparisonOperator,
    right_literal: &LiteralValue<'_>,
) -> Result<bool, EvaluationError> {
    tracing::debug!(?left_str, ?operator, ?right_literal, "Comparing U256 values");
    let left = string_to_u256(left_str).map_err(|e| {
        EvaluationError::ParseError(format!("Failed to parse LHS '{left_str}' as U256: {e}"))
    })?;
    let right_str = match right_literal {
        LiteralValue::Number(s) | LiteralValue::Str(s) => s,
        _ => {
            return Err(EvaluationError::TypeMismatch(format!(
                "Expected number or string for U256 comparison, found: {right_literal:?}"
            )))
        }
    };
    let right = string_to_u256(right_str).map_err(|e| {
        EvaluationError::ParseError(format!("Failed to parse RHS '{right_str}' as U256: {e}"))
    })?;
    Ok(compare_ordered_values(&left, operator, &right))
}
/// Compares two I256 values based on the specified operator.
/// # Arguments
/// * `left_str` - The string representation of the left-hand side I256 value.
/// * `operator` - The comparison operator to use.
/// * `right_literal` - The right-hand side literal value to compare against, expected to be a string or number literal.
/// # Returns
/// * `Ok(bool)` - The result of the comparison, true if the condition is satisfied, false otherwise.
/// * `Err(EvaluationError)` - An error if the comparison fails due to type mismatches, or parsing errors.
fn compare_i256(
    left_str: &str,
    operator: &ComparisonOperator,
    right_literal: &LiteralValue<'_>,
) -> Result<bool, EvaluationError> {
    tracing::debug!(?left_str, ?operator, ?right_literal, "Comparing I256 values");
    let left = string_to_i256(left_str).map_err(|e| {
        EvaluationError::ParseError(format!("Failed to parse LHS '{left_str}' as I256: {e}"))
    })?;
    let right_str = match right_literal {
        LiteralValue::Number(s) | LiteralValue::Str(s) => s,
        _ => {
            return Err(EvaluationError::TypeMismatch(format!(
                "Expected number or string for I256 comparison, found: {right_literal:?}"
            )))
        }
    };
    let right = string_to_i256(right_str).map_err(|e| {
        EvaluationError::ParseError(format!("Failed to parse RHS '{right_str}' as I256: {e}"))
    })?;
    Ok(compare_ordered_values(&left, operator, &right))
}
/// Compares two address values based on the specified operator.
/// # Arguments
/// * `left` - The string representation of the left-hand side address value.
/// * `operator` - The comparison operator to use.
/// * `right_literal` - The right-hand side literal value to compare against, expected to be a string literal.
/// # Returns
/// * `Ok(bool)` - The result of the comparison, true if the condition is satisfied, false otherwise.
/// * `Err(EvaluationError)` - An error if the comparison fails due to type mismatches or unsupported operators
fn compare_address(
    left: &str,
    operator: &ComparisonOperator,
    right_literal: &LiteralValue<'_>,
) -> Result<bool, EvaluationError> {
    tracing::debug!(?left, ?operator, ?right_literal, "Comparing address values");
    let right = match right_literal {
        LiteralValue::Str(s) => *s,
        _ => {
            return Err(EvaluationError::TypeMismatch(format!(
                "Expected string literal for address comparison, found: {right_literal:?}"
            )))
        }
    };
    match operator {
        ComparisonOperator::Eq => Ok(are_same_address(left, right)),
        ComparisonOperator::Ne => Ok(!are_same_address(left, right)),
        _ => Err(EvaluationError::UnsupportedOperator(format!(
            "Unsupported operator {operator:?} for address type"
        ))),
    }
}
/// Compares two string values based on the specified operator.
/// # Arguments
/// * `lhs_str` - The string representation of the left-hand side value.
/// * `operator` - The comparison operator to use.
/// * `rhs_literal` - The right-hand side literal value to compare against, expected to be a string literal.
/// # Returns
/// * `Ok(bool)` - The result of the comparison, true if the condition is satisfied, false otherwise.
/// * `Err(EvaluationError)` - An error if the comparison fails due to type mismatches or unsupported operators
fn compare_string(
    lhs_str: &str,
    operator: &ComparisonOperator,
    rhs_literal: &LiteralValue<'_>,
) -> Result<bool, EvaluationError> {
    tracing::debug!(?lhs_str, ?operator, ?rhs_literal, "Comparing string values");
    let left = lhs_str.to_lowercase();
    let right = match rhs_literal {
        LiteralValue::Str(s) => s.to_lowercase(),
        _ => {
            return Err(EvaluationError::TypeMismatch(format!(
                "Expected string literal for string comparison, found: {rhs_literal:?}"
            )))
        }
    };
    match operator {
        ComparisonOperator::Eq => Ok(left == right),
        ComparisonOperator::Ne => Ok(left != right),
        _ => Err(EvaluationError::UnsupportedOperator(format!(
            "Operator {operator:?} not supported for type String",
        ))),
    }
}
/// Compares two fixed point values based on the specified operator.
/// # Arguments
/// * `lhs_str` - The string representation of the left-hand side fixed point value.
/// * `operator` - The comparison operator to use.
/// * `rhs_literal` - The right-hand side literal value to compare against, expected to be a string or number literal.
/// # Returns
/// * `Ok(bool)` - The result of the comparison, true if the condition is satisfied, false otherwise.
/// * `Err(EvaluationError)` - An error if the comparison fails due to type mismatches, or parsing errors.
fn compare_fixed_point(
    lhs_str: &str,
    operator: &ComparisonOperator,
    rhs_literal: &LiteralValue<'_>,
) -> Result<bool, EvaluationError> {
    tracing::debug!(?lhs_str, ?operator, ?rhs_literal, "Comparing fixed point values");
    let left_decimal = Decimal::from_str(lhs_str).map_err(|e| {
        EvaluationError::ParseError(format!("Failed to parse LHS '{lhs_str}' as Decimal: {e}"))
    })?;
    let rhs_str = match rhs_literal {
        LiteralValue::Number(s) | LiteralValue::Str(s) => *s,
        _ => {
            return Err(EvaluationError::TypeMismatch(format!(
                "Expected number or string for Decimal comparison, found: {rhs_literal:?}"
            )))
        }
    };
    let right_decimal = Decimal::from_str(rhs_str).map_err(|e| {
        EvaluationError::ParseError(format!("Failed to parse RHS '{rhs_str}' as Decimal: {e}"))
    })?;
    Ok(compare_ordered_values(&left_decimal, operator, &right_decimal))
}
/// Compares two boolean values based on the specified operator.
/// # Arguments
/// * `lhs_value_str` - The string representation of the left-hand side boolean value.
/// * `operator` - The comparison operator to use.
/// * `rhs_literal` - The right-hand side literal value to compare against, expected to be a boolean literal.
/// # Returns
/// * `Ok(bool)` - The result of the comparison, true if the condition is satisfied, false otherwise.
/// * `Err(EvaluationError)` - An error if the comparison fails due to type mismatches or parsing errors.
fn compare_boolean(
    lhs_value_str: &str,
    operator: &ComparisonOperator,
    rhs_literal: &LiteralValue<'_>,
) -> Result<bool, EvaluationError> {
    tracing::debug!(?lhs_value_str, ?operator, ?rhs_literal, "Comparing boolean values");
    let lhs = lhs_value_str.parse::<bool>().map_err(|e| {
        EvaluationError::ParseError(format!("Failed to parse LHS '{lhs_value_str}' as bool: {e}"))
    })?;
    let rhs = match rhs_literal {
        LiteralValue::Bool(b) => *b,
        _ => {
            return Err(EvaluationError::TypeMismatch(format!(
                "Expected bool literal for Bool comparison, found: {rhs_literal:?}",
            )))
        }
    };
    match operator {
        ComparisonOperator::Eq => Ok(lhs == rhs),
        ComparisonOperator::Ne => Ok(lhs != rhs),
        _ => Err(EvaluationError::UnsupportedOperator(format!(
            "Unsupported operator {operator:?} for Bool comparison",
        ))),
    }
}
/// Determines the kind of a JSON value based on its content.
/// # Arguments
/// * `value` - The JSON value to analyze.
/// # Returns
/// * `String` - The kind of the value, such as "address", "bytes32", "fixed", "number", etc.
fn get_kind_from_json_value(value: &JsonValue) -> String {
    tracing::debug!(?value, "Determining kind from JSON value");
    match value {
        JsonValue::String(s) => {
            let s_lower = s.to_lowercase();
            if s_lower.starts_with("0x")
                && s.len() == 42
                && s.chars().skip(2).all(|c| c.is_ascii_hexdigit())
            {
                "address".to_string()
            } else if s_lower.starts_with("0x") && s.chars().skip(2).all(|c| c.is_ascii_hexdigit())
            {
                if s.len() == 66 {
                    "bytes32".to_string()
                } else {
                    "bytes".to_string()
                }
            } else if Decimal::from_str(s).is_ok() && s.contains('.') {
                "fixed".to_string()
            } else if string_to_u256(s).is_ok() {
                "number".to_string()
            } else if string_to_i256(s).is_ok() {
                "int256".to_string()
            } else {
                "string".to_string()
            }
        }
        JsonValue::Number(n) => {
            if n.is_f64() || n.to_string().contains('.') {
                "fixed".to_string()
            } else if n.is_i64() && n.as_i64().unwrap_or(0) < 0 {
                "int64".to_string()
            } else {
                "number".to_string()
            }
        }
        JsonValue::Bool(_) => "bool".to_string(),
        JsonValue::Array(_) => "array".to_string(),
        JsonValue::Object(_) => "map".to_string(),
        JsonValue::Null => "null".to_string(),
    }
}
#[cfg(test)]
mod tests {
    use super::*;
    use crate::event::filter::parsing::parse;
    use serde_json::json;
    #[test]
    fn test_evaluate_simple_condition_ok() {
        let data = json!({"age": 30, "name": "Alice"});
        let expr = parse("age > 25").unwrap();
        assert!(evaluate(&expr, &data).unwrap());
    }
    #[test]
    fn test_evaluate_simple_condition_fail() {
        let data = json!({"age": 20});
        let expr = parse("age > 25").unwrap();
        assert!(!evaluate(&expr, &data).unwrap());
    }
    #[test]
    fn test_evaluate_logical_and() {
        let data = json!({"age": 30, "city": "Berlin"});
        let expr = parse("age > 25 && city == 'Berlin'").unwrap();
        assert!(evaluate(&expr, &data).unwrap());
        let expr_fail = parse("age < 25 && city == 'Berlin'").unwrap();
        assert!(!evaluate(&expr_fail, &data).unwrap());
    }
    #[test]
    fn test_evaluate_logical_or() {
        let data = json!({"age": 20, "city": "Paris"});
        let expr = parse("age > 25 || city == 'Paris'").unwrap();
        assert!(evaluate(&expr, &data).unwrap());
        let expr_fail = parse("age > 25 || city == 'Berlin'").unwrap();
        assert!(!evaluate(&expr_fail, &data).unwrap());
    }
    #[test]
    fn test_evaluate_nested_path() {
        let data = json!({
            "user": {
                "details": {
                    "age": 42,
                    "tags": ["a", "b"]
                }
            }
        });
        let expr = parse("user.details.age == 42").unwrap();
        assert!(evaluate(&expr, &data).unwrap());
        let expr_index = parse("user.details.tags[0] == 'a'").unwrap();
        assert!(evaluate(&expr_index, &data).unwrap());
    }
    #[test]
    fn test_variable_not_found() {
        let data = json!({"age": 10});
        let expr = parse("name == 'Alice'").unwrap();
        let result = evaluate(&expr, &data);
        assert!(matches!(result, Err(EvaluationError::VariableNotFound(_))));
    }
    #[test]
    fn test_type_mismatch_error() {
        let data = json!({"age": "ten"}); // age is a string
        let expr = parse("age > 5").unwrap(); // comparing string to number
        let result = evaluate(&expr, &data);
        assert!(matches!(result, Err(EvaluationError::TypeMismatch(_))));
    }
    #[test]
    fn test_evaluate_large_unsigned_integer_string() {
        let data = json!({"value": "1996225771303743351"});
        let expr = parse("value > 1000000000000000000").unwrap();
        assert!(evaluate(&expr, &data).unwrap());
        let expr_fail = parse("value < 1000000000000000000").unwrap();
        assert!(!evaluate(&expr_fail, &data).unwrap());
    }
    #[test]
    fn test_evaluate_signed_integer_string() {
        let data = json!({"value": "-50"});
        let expr = parse("value < 0").unwrap();
        assert!(evaluate(&expr, &data).unwrap());
        let expr_fail = parse("value > 0").unwrap();
        assert!(!evaluate(&expr_fail, &data).unwrap());
    }
    #[test]
    fn test_evaluate_non_numeric_string_is_not_a_number() {
        let data = json!({"value": "hello_world"});
        let expr = parse("value == 'hello_world'").unwrap();
        assert!(evaluate(&expr, &data).unwrap());
        // This should fail because "hello_world" cannot be compared to a number
        let expr_fail = parse("value > 100").unwrap();
        assert!(matches!(evaluate(&expr_fail, &data), Err(EvaluationError::TypeMismatch(_))));
    }
    #[test]
    fn test_get_kind_from_json_value_various_types() {
        assert_eq!(
            get_kind_from_json_value(&json!("0xAb5801a7D398351b8bE11C439e05C5B3259aeC9B")),
            "address"
        );
        assert_eq!(
            get_kind_from_json_value(&json!(
                "0x0000000000000000000000000000000000000000000000000000000000000001"
            )),
            "bytes32"
        );
        assert_eq!(get_kind_from_json_value(&json!("0x1234")), "bytes");
        assert_eq!(get_kind_from_json_value(&json!("123.45")), "fixed");
        assert_eq!(get_kind_from_json_value(&json!("12345678901234567890")), "number");
        assert_eq!(get_kind_from_json_value(&json!("-123")), "int256");
        assert_eq!(get_kind_from_json_value(&json!("hello world")), "string");
        assert_eq!(get_kind_from_json_value(&json!(123)), "number");
        assert_eq!(get_kind_from_json_value(&json!(-123)), "int64");
        assert_eq!(get_kind_from_json_value(&json!(123.45)), "fixed");
        assert_eq!(get_kind_from_json_value(&json!(true)), "bool");
        assert_eq!(get_kind_from_json_value(&json!([1, 2])), "array");
        assert_eq!(get_kind_from_json_value(&json!({"a": 1})), "map");
        assert_eq!(get_kind_from_json_value(&json!(null)), "null");
    }
    #[test]
    fn test_resolve_path_errors() {
        let data = json!({ "user": { "tags": ["a", "b"] } });
        // Index out of bounds
        let expr_idx = parse("user.tags[2] == 'c'").unwrap();
        assert!(matches!(evaluate(&expr_idx, &data), Err(EvaluationError::IndexOutOfBounds(_))));
        // Key accessor on array
        let expr_key = parse("user.tags.key == 'a'").unwrap();
        assert!(matches!(evaluate(&expr_key, &data), Err(EvaluationError::TypeMismatch(_))));
        // Index accessor on object
        let expr_obj = parse("user[0] == 'a'").unwrap();
        assert!(matches!(evaluate(&expr_obj, &data), Err(EvaluationError::TypeMismatch(_))));
    }
    #[test]
    fn test_compare_address_case_insensitivity() {
        let data = json!({ "owner": "0xAb5801a7D398351b8bE11C439e05C5B3259aeC9B" });
        let expr = parse("owner == '0xab5801a7d398351b8be11c439e05c5b3259aec9b'").unwrap();
        assert!(evaluate(&expr, &data).unwrap());
    }
    #[test]
    fn test_unsupported_operators() {
        // For string
        let data_str = json!({ "value": "hello" });
        let expr_str = parse("value > 'world'").unwrap();
        assert!(matches!(
            evaluate(&expr_str, &data_str),
            Err(EvaluationError::UnsupportedOperator(_))
        ));
        // For boolean
        let data_bool = json!({ "value": true });
        let expr_bool = parse("value > false").unwrap();
        assert!(matches!(
            evaluate(&expr_bool, &data_bool),
            Err(EvaluationError::UnsupportedOperator(_))
        ));
    }
    #[test]
    fn test_compare_array() {
        let data = json!({ "values": [1, 2, 3] });
        let expr_eq = parse("values == '[1, 2, 3]'").unwrap();
        assert!(evaluate(&expr_eq, &data).unwrap());
        let expr_ne = parse("values != '[1, 2, 4]'").unwrap();
        assert!(evaluate(&expr_ne, &data).unwrap());
        let expr_fail = parse("values == '[1, 2, 4]'").unwrap();
        assert!(!evaluate(&expr_fail, &data).unwrap());
    }
    #[test]
    fn test_compare_fixed_point() {
        let data = json!({ "price": "123.45" });
        let expr_gt = parse("price > 123.4").unwrap();
        assert!(evaluate(&expr_gt, &data).unwrap());
        let expr_lte = parse("price <= 123.45").unwrap();
        assert!(evaluate(&expr_lte, &data).unwrap());
        let expr_ne = parse("price != 123.456").unwrap();
        assert!(evaluate(&expr_ne, &data).unwrap());
    }
    // --- Tests for Error Messages ---
    #[test]
    fn test_variable_not_found_error_message() {
        let data = json!({});
        let expr = parse("non_existent_var == 1").unwrap();
        let err = evaluate(&expr, &data).unwrap_err();
        assert_eq!(err.to_string(), "Variable not found: non_existent_var");
    }
    #[test]
    fn test_index_out_of_bounds_error_message() {
        let data = json!({ "arr": [0, 1] });
        let expr = parse("arr[2] == 0").unwrap();
        let err = evaluate(&expr, &data).unwrap_err();
        assert_eq!(err.to_string(), "Index out of bounds: 2");
    }
    #[test]
    fn test_type_mismatch_on_accessor_error_message() {
        let data = json!({ "arr": [0, 1] });
        let expr = parse("arr.key == 0").unwrap();
        let err = evaluate(&expr, &data).unwrap_err();
        assert_eq!(
            err.to_string(),
            "Type mismatch: Cannot apply accessor Key(\"key\") to value Array [Number(0), Number(1)]"
        );
    }
    #[test]
    fn test_unsupported_operator_error_message() {
        let data = json!({ "value": "hello" });
        let expr = parse("value > 'world'").unwrap();
        let err = evaluate(&expr, &data).unwrap_err();
        assert_eq!(
            err.to_string(),
            "Unsupported operator: Operator Gt not supported for type String"
        );
    }
    #[test]
    fn test_parse_error_message() {
        let data = json!({ "value": 123 });
        let expr = parse("value > 'not-a-number'").unwrap();
        let err = evaluate(&expr, &data).unwrap_err();
        assert!(err
            .to_string()
            .contains("Parse error: Failed to parse RHS 'not-a-number' as U256"));
    }
}
</file>

<file path="core/src/event/filter/helpers.rs">
//! Utility functions for evaluating expressions.
use super::ast::ComparisonOperator;
use alloy::primitives::{I256, U256};
use std::str::FromStr;
/// Compares two addresses for equality, ignoring case and "0x" prefixes.
pub fn are_same_address(address1: &str, address2: &str) -> bool {
    normalize_address(address1) == normalize_address(address2)
}
/// Normalizes an address string by removing "0x" prefix and converting to lowercase.
pub fn normalize_address(address: &str) -> String {
    address.strip_prefix("0x").unwrap_or(address).replace(" ", "").to_lowercase()
}
/// Converts a string to a U256 value.
pub fn string_to_u256(value_str: &str) -> Result<U256, String> {
    let trimmed = value_str.trim();
    if trimmed.is_empty() {
        return Err("Input string is empty".to_string());
    }
    if let Some(hex_val) = trimmed.strip_prefix("0x").or_else(|| trimmed.strip_prefix("0X")) {
        // Hexadecimal parsing
        if hex_val.is_empty() {
            return Err("Hex string '0x' is missing value digits".to_string());
        }
        U256::from_str_radix(hex_val, 16)
            .map_err(|e| format!("Failed to parse hex '{hex_val}': {e}"))
    } else {
        // Decimal parsing
        U256::from_str(trimmed).map_err(|e| format!("Failed to parse decimal '{trimmed}': {e}"))
    }
}
/// Converts a string to an I256 value, handling decimal and hex formats.
pub fn string_to_i256(value_str: &str) -> Result<I256, String> {
    let trimmed = value_str.trim();
    if trimmed.is_empty() {
        return Err("Input string is empty".to_string());
    }
    if let Some(hex_val_no_sign) = trimmed.strip_prefix("0x").or_else(|| trimmed.strip_prefix("0X"))
    {
        if hex_val_no_sign.is_empty() {
            return Err("Hex string '0x' is missing value digits".to_string());
        }
        // Parse hex as U256 first
        U256::from_str_radix(hex_val_no_sign, 16)
            .map_err(|e| format!("Failed to parse hex magnitude '{hex_val_no_sign}': {e}"))
            .map(I256::from_raw)
    } else {
        I256::from_str(trimmed).map_err(|e| format!("Failed to parse decimal '{trimmed}': {e}"))
    }
}
/// Compares two values implementing the Ord trait using the specified comparison operator.
pub fn compare_ordered_values<T: Ord>(left: &T, op: &ComparisonOperator, right: &T) -> bool {
    match op {
        ComparisonOperator::Eq => left == right,
        ComparisonOperator::Ne => left != right,
        ComparisonOperator::Gt => left > right,
        ComparisonOperator::Gte => left >= right,
        ComparisonOperator::Lt => left < right,
        ComparisonOperator::Lte => left <= right,
    }
}
#[cfg(test)]
mod tests {
    use super::*;
    #[test]
    fn test_string_to_u256() {
        // --- Helpers ---
        fn u256_hex_val(hex_str: &str) -> U256 {
            U256::from_str_radix(hex_str.strip_prefix("0x").unwrap_or(hex_str), 16).unwrap()
        }
        // --- Constants for testing ---
        const U256_MAX_STR: &str =
            "115792089237316195423570985008687907853269984665640564039457584007913129639935";
        const U256_MAX_HEX_STR: &str =
            "0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff";
        const U256_OVERFLOW_STR: &str =
            "115792089237316195423570985008687907853269984665640564039457584007913129639936";
        const U256_HEX_OVERFLOW_STR: &str =
            "0x10000000000000000000000000000000000000000000000000000000000000000";
        const ZERO_STR: &str = "0";
        const SMALL_NUM_STR: &str = "123";
        const SMALL_NUM_HEX_STR: &str = "0x7b"; // 123 in hex
        // --- Valid numbers cases ---
        assert_eq!(string_to_u256(ZERO_STR), Ok(U256::ZERO));
        assert_eq!(string_to_u256(SMALL_NUM_STR), Ok(U256::from_str(SMALL_NUM_STR).unwrap()));
        assert_eq!(string_to_u256(U256_MAX_STR), Ok(U256::MAX));
        // --- Valid hex cases ---
        assert_eq!(string_to_u256("0x0"), Ok(U256::ZERO));
        assert_eq!(string_to_u256("0X0"), Ok(U256::ZERO)); // Case insensitive
        assert_eq!(string_to_u256(SMALL_NUM_HEX_STR), Ok(u256_hex_val(SMALL_NUM_HEX_STR)));
        assert_eq!(string_to_u256(U256_MAX_HEX_STR), Ok(U256::MAX));
        // --- Invalid cases ---
        assert!(string_to_u256("").is_err());
        assert!(string_to_u256("   ").is_err());
        assert!(string_to_u256("0x").is_err());
        assert!(string_to_u256("abc").is_err());
        assert!(string_to_u256("-123").is_err());
        assert!(string_to_u256(U256_OVERFLOW_STR).is_err());
        assert!(string_to_u256(U256_HEX_OVERFLOW_STR).is_err());
    }
    #[test]
    fn test_string_to_i256() {
        // --- Constants for testing ---
        const I256_MAX_STR: &str =
            "57896044618658097711785492504343953926634992332820282019728792003956564819967";
        const I256_MAX_HEX_STR: &str =
            "0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff";
        const I256_MIN_STR: &str =
            "-57896044618658097711785492504343953926634992332820282019728792003956564819968";
        const I256_MIN_HEX_STR: &str =
            "0x8000000000000000000000000000000000000000000000000000000000000000";
        const I256_POS_OVERFLOW_STR: &str =
            "57896044618658097711785492504343953926634992332820282019728792003956564819968";
        const I256_NEG_OVERFLOW_STR: &str =
            "-57896044618658097711785492504343953926634992332820282019728792003956564819969";
        const I256_HEX_OVERFLOW_STR: &str =
            "0x10000000000000000000000000000000000000000000000000000000000000000";
        // --- Valid numbers cases ---
        assert_eq!(string_to_i256("0"), Ok(I256::ZERO));
        assert_eq!(string_to_i256("123"), Ok(I256::from_str("123").unwrap()));
        assert_eq!(string_to_i256(I256_MAX_STR), Ok(I256::MAX));
        assert_eq!(string_to_i256(I256_MIN_STR), Ok(I256::MIN));
        assert_eq!(string_to_i256("-123"), Ok(I256::from_str("-123").unwrap()));
        assert_eq!(string_to_i256("-0"), Ok(I256::ZERO));
        // --- Valid hex cases ---
        assert_eq!(string_to_i256("0x0"), Ok(I256::ZERO));
        assert_eq!(string_to_i256("0X0"), Ok(I256::ZERO)); // Case insensitive
        assert_eq!(string_to_i256(I256_MAX_HEX_STR), Ok(I256::MAX));
        assert_eq!(string_to_i256(I256_MIN_HEX_STR), Ok(I256::MIN));
        // --- Invalid cases ---
        assert!(string_to_i256("").is_err());
        assert!(string_to_i256("   ").is_err());
        assert!(string_to_i256("0x").is_err());
        assert!(string_to_i256("abc").is_err());
        assert!(string_to_i256("-abc").is_err());
        assert!(string_to_i256(I256_POS_OVERFLOW_STR).is_err());
        assert!(string_to_i256(I256_NEG_OVERFLOW_STR).is_err());
        assert!(string_to_i256(I256_HEX_OVERFLOW_STR).is_err());
    }
    #[test]
    fn test_are_same_address() {
        assert!(are_same_address(
            "0x0123456789abcdef0123456789abcdef01234567",
            "0x0123456789ABCDEF0123456789ABCDEF01234567"
        ));
        assert!(are_same_address(
            "0123456789abcdef0123456789abcdef01234567",
            "0x0123456789abcdef0123456789abcdef01234567"
        ));
        assert!(!are_same_address(
            "0x0123456789abcdef0123456789abcdef01234567",
            "0x0123456789abcdef0123456789abcdef01234568"
        ));
    }
    #[test]
    fn test_normalize_address() {
        assert_eq!(
            normalize_address("0x0123456789ABCDEF0123456789ABCDEF01234567"),
            "0123456789abcdef0123456789abcdef01234567"
        );
        assert_eq!(
            normalize_address("0123456789ABCDEF0123456789ABCDEF01234567"),
            "0123456789abcdef0123456789abcdef01234567"
        );
        assert_eq!(
            normalize_address("0x0123456789abcdef 0123456789abcdef01234567"),
            "0123456789abcdef0123456789abcdef01234567"
        );
    }
    #[test]
    fn test_compare_ordered_values_integers() {
        assert!(compare_ordered_values(&5, &ComparisonOperator::Eq, &5));
        assert!(compare_ordered_values(&10, &ComparisonOperator::Gt, &5));
        assert!(compare_ordered_values(&5, &ComparisonOperator::Lt, &10));
        assert!(compare_ordered_values(&5, &ComparisonOperator::Gte, &5));
        assert!(compare_ordered_values(&5, &ComparisonOperator::Lte, &5));
        assert!(compare_ordered_values(&5, &ComparisonOperator::Ne, &10));
    }
}
</file>

<file path="core/src/event/filter/mod.rs">
//! This module provides functionality for filtering JSON data based on expressions and conditions.
//! It includes parsing of expressions, evaluation against JSON objects, and utility functions for nested value retrieval.
pub mod ast;
pub mod evaluation;
pub mod helpers;
pub mod parsing;
use self::evaluation::EvaluationError;
use once_cell::sync::Lazy;
use regex::Regex;
use serde_json::{json, Map, Value};
use thiserror::Error;
use winnow::error::{ContextError, ParseError};
/// This regex finds logical operators (&&, ||) possibly surrounded by whitespace.
static LOGICAL_OPERATOR_RE: Lazy<Regex> = Lazy::new(|| Regex::new(r"\s*(&&|\|\|)\s*").unwrap());
/// Error type for the filter module, encapsulating parsing and evaluation errors.
#[derive(Debug, Error)]
pub enum FilterError<'a> {
    /// Error encountered during parsing of the expression.
    #[error("Failed to parse expression: {0}")]
    Parse(ParseError<&'a str, ContextError>),
    /// Error encountered during evaluation of the expression.
    #[error("Failed to evaluate expression: {0}")]
    Evaluation(#[from] EvaluationError),
}
/// Evaluates a filter expression against a given JSON object.
///
/// This is the main public entry point for the new conditions module. It orchestrates
/// the parsing of the expression string and the evaluation of the resulting AST
/// against the provided data.
///
/// # Arguments
/// * `expression_str` - A string representing the filter expression to be evaluated.
/// * `data` - A JSON object (serde_json::Value) against which the expression will be evaluated.
/// # Returns
/// * `Ok(bool)` - If the expression evaluates successfully, returns `true` if the expression matches the data, or `false` otherwise.
/// * `Err(FilterError)` - If there is an error in parsing the expression or evaluating it against the data, returns a `FilterError`.
pub fn filter_by_expression<'a>(
    expression_str: &'a str,
    data: &Value,
) -> Result<bool, FilterError<'a>> {
    let parsed_expr = parsing::parse(expression_str).map_err(FilterError::Parse)?;
    evaluation::evaluate(&parsed_expr, data).map_err(FilterError::from)
}
/// Retrieves a nested value from a JSON object using a dot-separated path.
/// If the path does not exist, it returns `None`.
///
/// # Arguments
/// * `data` - A JSON object (serde_json::Value) from which to retrieve the value.
/// * `path` - A dot-separated string representing the path to the desired value.
/// # Returns
/// * `Some(Value)` - If the value exists at the specified path, returns the value.
/// * `None` - If the path does not exist in the JSON object.
fn get_nested_value(data: &Value, path: &str) -> Option<Value> {
    let keys: Vec<&str> = path.split('.').collect();
    let mut current = data;
    for key in keys {
        match current.get(key) {
            Some(value) => current = value,
            None => return None,
        }
    }
    Some(current.clone())
}
/// This function is a compatibility layer to support the legacy `conditions` format.
/// It translates the legacy format into a modern filter expression to ensure
/// correct operator precedence (e.g., for `> 5 && < 10 || == 0`).
///
/// While it now correctly handles complex expressions, it is still recommended
/// to use the `filter_expression` field directly for new implementations, as it
/// avoids this translation step and is more explicit.
///
/// # Arguments
/// * `value` - The value to evaluate against the condition.
/// * `condition` - A string representing the legacy condition to evaluate (e.g., `> 10 && != 20`).
///
/// # Returns
/// * `bool` - Returns `true` if the value satisfies the condition, `false` otherwise.
fn evaluate_condition(value: &Value, condition: &str) -> bool {
    tracing::debug!(?value, ?condition, "Evaluating legacy condition");
    // Fast path for simple equality checks where no complex operators are present.
    if !condition.contains(['&', '|', '>', '<', '=', '!']) {
        return match value {
            Value::String(s) => s == condition,
            Value::Number(n) => n.to_string() == condition,
            Value::Bool(b) => b.to_string() == condition,
            Value::Null => "null" == condition,
            _ => false,
        };
    }
    // For complex expressions, reconstruct the legacy format into a valid
    // expression for the new evaluation engine.
    //
    // Legacy format: `> 5 && < 10`
    // Target format: `_placeholder_ > 5 && _placeholder_ < 10`
    //
    // The regex finds `&&` or `||` and replaces it with ` [OPERATOR] _placeholder_ `,
    // effectively inserting the subject of the expression for each subsequent clause.
    let reconstructed_after_ops =
        LOGICAL_OPERATOR_RE.replace_all(condition.trim(), " $1 _placeholder_ ");
    // Prepend the placeholder for the very first clause.
    let final_expr = format!("_placeholder_ {reconstructed_after_ops}");
    let context = json!({ "_placeholder_": value });
    tracing::debug!(expression = final_expr, ?context, "Evaluating reconstructed expression");
    // Delegate to the proper engine.
    // If evaluation fails, default to `false`.
    filter_by_expression(&final_expr, &context).unwrap_or(false)
}
/// Filters event data based on a set of conditions.
/// This function checks if the event data matches all conditions specified in the `conditions` vector.
///
/// # Arguments
/// * `event_data` - A JSON object (serde_json::Value) representing the event data to be filtered.
/// * `conditions` - A vector of maps, where each map contains key-value pairs representing the conditions to be checked against the event data.
/// # Returns
/// * `bool` - Returns `true` if the event data matches all conditions, `false` otherwise.
pub fn filter_event_data_by_conditions(
    event_data: &Value,
    conditions: &Vec<Map<String, Value>>,
) -> bool {
    if conditions.is_empty() {
        return true;
    }
    tracing::debug!(?event_data, ?conditions, "Filtering event data using legacy conditions");
    for condition in conditions {
        for (key, value) in condition {
            if let Some(event_value) = get_nested_value(event_data, key) {
                if !evaluate_condition(&event_value, value.as_str().unwrap_or("")) {
                    tracing::debug!(key, "Condition failed, returning false");
                    return false;
                }
            } else {
                tracing::debug!(key, "Key not found in event data, returning false");
                return false;
            }
        }
    }
    tracing::debug!("All conditions passed, returning true");
    true
}
#[cfg(test)]
mod tests {
    use super::*;
    use serde_json::json;
    #[test]
    fn test_get_nested_value() {
        let data = json!({
            "a": {
                "b": {
                    "c": "hello"
                }
            },
            "x": "world"
        });
        assert_eq!(get_nested_value(&data, "a.b.c"), Some(json!("hello")));
        assert_eq!(get_nested_value(&data, "x"), Some(json!("world")));
        assert_eq!(get_nested_value(&data, "a.b"), Some(json!({"c": "hello"})));
        assert_eq!(get_nested_value(&data, "a.x"), None);
        assert_eq!(get_nested_value(&data, "d"), None);
    }
    #[test]
    fn test_evaluate_condition_simple_equality() {
        assert!(evaluate_condition(&json!("hello"), "hello"));
        assert!(!evaluate_condition(&json!("hello"), "world"));
        assert!(evaluate_condition(&json!(123), "123"));
        assert!(!evaluate_condition(&json!(123), "456"));
    }
    #[test]
    fn test_evaluate_condition_complex_logical_expressions() {
        // AND conditions
        assert!(evaluate_condition(&json!(10), ">=10&&<=20"));
        assert!(!evaluate_condition(&json!(5), ">=10&&<=20"));
        // OR conditions
        assert!(evaluate_condition(&json!(5), "<10||>20"));
        assert!(evaluate_condition(&json!(25), "<10||>20"));
        assert!(!evaluate_condition(&json!(15), "<10||>20"));
        // Combined AND and OR
        assert!(evaluate_condition(&json!(5), ">=0&&<=10||>=20&&<=30"));
        assert!(evaluate_condition(&json!(25), ">=0&&<=10||>=20&&<=30"));
        assert!(!evaluate_condition(&json!(15), ">=0&&<=10||>=20&&<=30"));
    }
    #[test]
    fn test_evaluate_condition_correct_precedence() {
        // Test with value = 2.
        // `true || (false && false)` -> `true || false` -> `true`
        assert!(evaluate_condition(&json!(2), "== 2 || >= 10 && <= 20"));
        // Test with value = 15.
        // `false || (true && true)` -> `false || true` -> `true`
        assert!(evaluate_condition(&json!(15), "== 2 || >= 10 && <= 20"));
        // Test with value = 30
        // `false || (true && false)` -> `false || false` -> `false`
        assert!(!evaluate_condition(&json!(30), "== 2 || >= 10 && <= 20"));
    }
    #[test]
    fn test_filter_event_data_by_conditions_simple() {
        let event_data = json!({
            "name": "test",
            "value": 100
        });
        let conditions = vec![json!({
            "name": "test"
        })
        .as_object()
        .unwrap()
        .clone()];
        assert!(filter_event_data_by_conditions(&event_data, &conditions));
        let conditions = vec![json!({
            "value": "100"
        })
        .as_object()
        .unwrap()
        .clone()];
        assert!(filter_event_data_by_conditions(&event_data, &conditions));
        let conditions = vec![json!({
            "name": "wrong"
        })
        .as_object()
        .unwrap()
        .clone()];
        assert!(!filter_event_data_by_conditions(&event_data, &conditions));
    }
    #[test]
    fn test_filter_event_data_by_conditions_nested() {
        let event_data = json!({
            "a": {
                "b": {
                    "c": "hello"
                }
            }
        });
        let conditions = vec![json!({
            "a.b.c": "hello"
        })
        .as_object()
        .unwrap()
        .clone()];
        assert!(filter_event_data_by_conditions(&event_data, &conditions));
        let conditions = vec![json!({
            "a.b.c": "world"
        })
        .as_object()
        .unwrap()
        .clone()];
        assert!(!filter_event_data_by_conditions(&event_data, &conditions));
    }
    #[test]
    fn test_filter_event_data_by_conditions_multiple() {
        let event_data = json!({
            "name": "test",
            "value": 100
        });
        let conditions = vec![
            json!({"name": "test"}).as_object().unwrap().clone(),
            json!({"value": "100"}).as_object().unwrap().clone(),
        ];
        assert!(filter_event_data_by_conditions(&event_data, &conditions));
        let conditions = vec![
            json!({"name": "test"}).as_object().unwrap().clone(),
            json!({"value": "200"}).as_object().unwrap().clone(),
        ];
        assert!(!filter_event_data_by_conditions(&event_data, &conditions));
    }
    #[test]
    fn test_filter_event_data_by_conditions_complex_expressions() {
        let event_data = json!({
            "value": 15
        });
        let conditions = vec![json!({
            "value": ">=10&&<=20"
        })
        .as_object()
        .unwrap()
        .clone()];
        assert!(filter_event_data_by_conditions(&event_data, &conditions));
        let conditions = vec![json!({
            "value": "<10||>20"
        })
        .as_object()
        .unwrap()
        .clone()];
        assert!(!filter_event_data_by_conditions(&event_data, &conditions));
    }
    #[test]
    fn test_filter_with_null_value() {
        let event_data = json!({ "key": null });
        let conditions = vec![json!({ "key": "null" }).as_object().unwrap().clone()];
        assert!(filter_event_data_by_conditions(&event_data, &conditions));
    }
    #[test]
    fn test_filter_with_boolean_value() {
        let event_data = json!({ "active": true });
        let conditions = vec![json!({ "active": "true" }).as_object().unwrap().clone()];
        assert!(filter_event_data_by_conditions(&event_data, &conditions));
        let event_data_false = json!({ "active": false });
        let conditions_false = vec![json!({ "active": "false" }).as_object().unwrap().clone()];
        assert!(filter_event_data_by_conditions(&event_data_false, &conditions_false));
    }
    #[test]
    fn test_filter_with_missing_key() {
        let event_data = json!({ "actual_key": "some_value" });
        let conditions =
            vec![json!({ "non_existent_key": "some_value" }).as_object().unwrap().clone()];
        assert!(!filter_event_data_by_conditions(&event_data, &conditions));
    }
    #[test]
    fn test_filter_with_empty_conditions() {
        let event_data = json!({ "key": "value" });
        let conditions: Vec<Map<String, Value>> = vec![];
        assert!(filter_event_data_by_conditions(&event_data, &conditions));
    }
}
</file>

<file path="core/src/event/filter/parsing.rs">
//! This module contains the parser for the filter expression language.
//! It uses the `winnow` library for parsing and defines the grammar for the expression language.
//! The parser converts the input string into an abstract syntax tree (AST) representation of the expression.
use super::ast::{
    Accessor, ComparisonOperator, Condition, ConditionLeft, Expression, LiteralValue,
    LogicalOperator, VariablePath,
};
use winnow::{
    ascii::{digit1, space0, space1},
    combinator::{alt, delimited, eof, opt, peek, repeat, Repeat},
    error::{ContextError, ErrMode, ParseError, StrContext, StrContextValue},
    prelude::*,
    token::{literal, one_of, take_while},
};
/// --- Helper aliases ---
type Input<'a> = &'a str;
/// Result for internal parser functions
type ParserResult<T> = winnow::Result<T, ErrMode<ContextError>>;
// Helper to check for keywords
// These words cannot be used as unquoted string literals or variable names
fn is_keyword(ident: &str) -> bool {
    matches!(ident.to_ascii_lowercase().as_str(), "true" | "false")
}
/// Common delimiters that can follow a literal value
const COMMON_DELIMITERS: [char; 10] = [')', '(', ',', '=', '!', '>', '<', '&', '|', ']'];
/// --- Parser functions ---
/// Parses boolean literals into `LiteralValue::Bool`
fn parse_boolean<'a>(input: &mut Input<'a>) -> ParserResult<LiteralValue<'a>> {
    let parse_true = (
        literal("true"),
        peek(alt((
            // Ensure "true" is followed by a delimiter or EOF
            space1.value(()),
            eof.value(()),
            one_of(COMMON_DELIMITERS).value(()),
        ))),
    )
        .map(|_| LiteralValue::Bool(true));
    let parse_false = (
        literal("false"),
        peek(alt((
            // Ensure "false" is followed by a delimiter or EOF
            space1.value(()),
            eof.value(()),
            one_of(COMMON_DELIMITERS).value(()),
        ))),
    )
        .map(|_| LiteralValue::Bool(false));
    alt((parse_true, parse_false))
        .context(StrContext::Expected(StrContextValue::Description(
            "boolean literal 'true' or 'false'",
        )))
        .parse_next(input)
}
/// Parses any numeric-looking literal (integer or float) into LiteralValue::Number(&'a str).
fn parse_number_or_fixed_str<'a>(input: &mut Input<'a>) -> ParserResult<LiteralValue<'a>> {
    (
        opt(one_of(['+', '-'])),
        digit1,
        opt((literal("."), digit1)), // Optional fractional part
        peek(alt((
            // Ensure it's properly delimited
            space1.value(()),
            eof.value(()),
            one_of(COMMON_DELIMITERS).value(()),
        ))),
    )
        .take()
        .map(|s: &str| LiteralValue::Number(s)) // Store as Number(&str)
        .context(StrContext::Expected(StrContextValue::Description(
            "numeric literal (integer or fixed-point)",
        )))
        .parse_next(input)
}
// Parses an unquoted "0x..." or "0X..." sequence as a string.
fn parse_hex_string<'a>(input: &mut Input<'a>) -> ParserResult<LiteralValue<'a>> {
    (
        alt((literal("0x"), literal("0X"))),
        take_while(1.., |c: char| c.is_ascii_hexdigit()), // Ensure at least one hex digit
        peek(alt((space1.value(()), eof.value(()), one_of(COMMON_DELIMITERS).value(())))),
    )
        .take()
        .map(|s: &str| LiteralValue::Str(s))
        .context(StrContext::Expected(StrContextValue::Description("hexadecimal string literal")))
        .parse_next(input)
}
/// Parses string literals enclosed in single or double quotes into `LiteralValue::Str`
fn parse_quoted_string<'a>(input: &mut Input<'a>) -> ParserResult<LiteralValue<'a>> {
    // Match and consume opening quote, remember which one it was.
    let open_quote: char = one_of(['\'', '"']).parse_next(input)?;
    let character_or_escape_sequence = alt((
        (literal("\\"), one_of([open_quote, '\\'])).void(),
        take_while(1.., move |c: char| c != open_quote && c != '\\').void(),
    ));
    let inner_parser: Repeat<_, &_, _, (), _> = repeat(0.., character_or_escape_sequence);
    let string_content_slice: &'a str = inner_parser.take().parse_next(input)?;
    literal(open_quote)
        .context(StrContext::Expected(StrContextValue::Description(
            "matching closing quote for string literal",
        )))
        .parse_next(input)?;
    Ok(LiteralValue::Str(string_content_slice))
}
/// Fallback parser for unquoted strings (applied last)
fn parse_unquoted_string<'a>(input: &mut Input<'a>) -> ParserResult<LiteralValue<'a>> {
    take_while(1.., |c: char| c.is_alphanum() || c == '_' || c == '-')
        .take()
        .verify(|s: &&str| {
            let word = *s;
            !is_keyword(word)
					// Check if it's NOT an integer-like string
					&& !(word
						.chars()
						.all(|c| c.is_ascii_digit() || c == '+' || c == '-'))
					// Check if it's NOT a hex string
					&& !((word.starts_with("0x") || word.starts_with("0X"))
						&& word.chars().skip(2).all(|c| c.is_ascii_hexdigit()))
					// Check if it's NOT a float-like string
					&& !word.contains('.')
        })
        .map(|s: &str| LiteralValue::Str(s))
        .context(StrContext::Expected(StrContextValue::Description("unquoted string literal")))
        .parse_next(input)
}
/// Parses an accessor (either an index or a key) from the input
fn parse_accessor<'a>(input: &mut Input<'a>) -> ParserResult<Accessor<'a>> {
    let index_parser = delimited(
        literal("["),
        // digit1 itself returns &str, try_map converts it
        digit1.try_map(|s: &str| s.parse::<usize>()),
        literal("]"),
    )
    .map(Accessor::Index)
    .context(StrContext::Expected(StrContextValue::Description("array index accessor like '[0]'")));
    let key_parser = (
        literal("."),
        // Allow key to be purely numeric OR start with alpha/_
        alt((
            // Standard identifier-like key
            (
                one_of(|c: char| c.is_alpha() || c == '_'),
                take_while(0.., |c: char| c.is_alphanum() || c == '_'),
            )
                .take(),
            // Purely numeric key (e.g., ".0", ".123")
            digit1.take(),
        )),
        // Ensure it's properly delimited
        peek(alt((
            space1.value(()),                                 // space
            eof.value(()),                                    // end of input
            literal("[").value(()),                           // start of index accessor
            literal(".").value(()),                           // start of another key accessor
            one_of(['=', '!', '>', '<', ')', '(']).value(()), // Operators or delimiters
        ))),
    )
        .map(|(_, key_slice, _): (_, &str, _)| Accessor::Key(key_slice))
        .context(StrContext::Expected(StrContextValue::Description(
            "object key accessor like '.key' or '.0'",
        )));
    alt((index_parser, key_parser)).parse_next(input)
}
fn parse_base_variable_name<'a>(input: &mut Input<'a>) -> ParserResult<&'a str> {
    alt((
        // Standard identifier
        (
            one_of(|c: char| c.is_alpha() || c == '_'),
            take_while(0.., |c: char| c.is_alphanum() || c == '_'),
        )
            .take(),
        // Purely numeric identifier
        (
            digit1,
            peek(alt((
                // Peek ensures it's properly delimited for an LHS base
                literal('['),
                literal('.'),
                space1,
                eof,
                literal("=="),
                literal("!="),
                literal(">="),
                literal("<="),
                literal(">"),
                literal("<"),
            ))),
        )
            .take(),
    ))
    .verify(|ident_slice: &&str| !is_keyword(ident_slice))
    .map(|s: &str| s)
    .context(StrContext::Expected(StrContextValue::Description(
        "variable base name (e.g., 'request', '0')",
    )))
    .parse_next(input)
}
fn parse_condition_lhs<'a>(input: &mut Input<'a>) -> ParserResult<ConditionLeft<'a>> {
    // Parse the base variable name
    let base = parse_base_variable_name.parse_next(input)?;
    // Parse any accessors (e.g., .key or [0])
    let accessors: Vec<Accessor> = repeat(0.., parse_accessor).parse_next(input)?;
    if accessors.is_empty() {
        Ok(ConditionLeft::Simple(base))
    } else {
        Ok(ConditionLeft::Path(VariablePath { base, accessors }))
    }
}
/// Parses any valid LiteralValue (boolean, number, string, or variable)
/// Handles optional whitespace around the value
fn parse_value<'a>(input: &mut Input<'a>) -> ParserResult<LiteralValue<'a>> {
    delimited(
        space0,
        alt((
            parse_quoted_string,       // "'string'" or '"string"'
            parse_boolean,             // "true" / "false"
            parse_hex_string,          // "0x..."
            parse_number_or_fixed_str, // "123" / "-123" / "123.456"
            parse_unquoted_string,     // "unquoted_string"
        )),
        space0,
    )
    .context(StrContext::Expected(StrContextValue::Description(
        "boolean, number, hex string or string",
    )))
    .parse_next(input)
}
/// Parses a comparison operator (e.g., ==, !=, >, >=, <, <=)
/// Handles optional whitespace around the operator
fn parse_comparison_operator(input: &mut Input<'_>) -> ParserResult<ComparisonOperator> {
    delimited(
        space0,
        alt((
            literal(">=").map(|_| ComparisonOperator::Gte),
            literal("<=").map(|_| ComparisonOperator::Lte),
            literal("==").map(|_| ComparisonOperator::Eq),
            literal("!=").map(|_| ComparisonOperator::Ne),
            literal(">").map(|_| ComparisonOperator::Gt),
            literal("<").map(|_| ComparisonOperator::Lt),
        )),
        space0,
    )
    .context(StrContext::Expected(StrContextValue::Description(
        "comparison operator (e.g., ==, >, >=, <, <=, !=)",
    )))
    .parse_next(input)
}
/// Parses a condition expression (e.g., "a == 1") into an `Expression::Condition`
fn parse_condition<'a>(input: &mut Input<'a>) -> ParserResult<Expression<'a>> {
    let (left, operator, right) = (parse_condition_lhs, parse_comparison_operator, parse_value)
        .context(StrContext::Expected(StrContextValue::Description(
            "condition expression (e.g., variable == value)",
        )))
        .parse_next(input)?;
    let condition = Condition { left, operator, right };
    Ok(Expression::Condition(condition))
}
/// Parses the highest precedence components: conditions and parenthesized expressions
fn parse_term<'a>(input: &mut Input<'a>) -> ParserResult<Expression<'a>> {
    delimited(
        space0,
        alt((
            // Parse a parenthesized expression
            delimited(
                (literal("("), space0),
                parse_expression,
                (space0, literal(")")).context(StrContext::Expected(StrContextValue::Description(
                    "closing parenthesis ')'",
                ))),
            ),
            // Parse a condition
            parse_condition,
        )),
        space0,
    )
    .context(StrContext::Expected(StrContextValue::Description(
        "condition or parenthesized expression",
    )))
    .parse_next(input)
}
/// Parses the AND operator and its components
fn parse_and_expression<'a>(input: &mut Input<'a>) -> ParserResult<Expression<'a>> {
    let left = parse_term.parse_next(input)?;
    let and_operator_parser = delimited(space0, literal("&&").value(LogicalOperator::And), space0)
        .context(StrContext::Expected(StrContextValue::Description("logical operator &&")));
    let trailing_parser = (and_operator_parser, parse_term);
    let folded_and_parser = repeat(0.., trailing_parser).fold(
        move || left.clone(), // Clone the left side for initial value
        |acc, (op, right)| Expression::Logical {
            left: Box::new(acc),
            operator: op,
            right: Box::new(right),
        },
    );
    folded_and_parser
        .context(StrContext::Expected(StrContextValue::Description("AND expression")))
        .parse_next(input)
}
/// Parses the OR operator and its components
fn parse_or_expression<'a>(input: &mut Input<'a>) -> ParserResult<Expression<'a>> {
    let left = parse_and_expression.parse_next(input)?;
    let or_operator_parser = delimited(space0, literal("||").value(LogicalOperator::Or), space0)
        .context(StrContext::Expected(StrContextValue::Description("logical operator ||")));
    let trailing_parser = (or_operator_parser, parse_and_expression);
    let folded_or_parser = repeat(0.., trailing_parser).fold(
        move || left.clone(),
        |acc, (op, right)| Expression::Logical {
            left: Box::new(acc),
            operator: op,
            right: Box::new(right),
        },
    );
    folded_or_parser
        .context(StrContext::Expected(StrContextValue::Description("OR expression")))
        .parse_next(input)
}
/// Parses the entire expression, starting from the highest precedence
fn parse_expression<'a>(input: &mut Input<'a>) -> ParserResult<Expression<'a>> {
    delimited(space0, parse_or_expression, space0)
        .context(StrContext::Expected(StrContextValue::Description("a full expression")))
        .parse_next(input)
}
/// Public method, which parses a string expression into an `Expression` AST
pub fn parse(expression_str: &str) -> Result<Expression<'_>, ParseError<Input<'_>, ContextError>> {
    // Parse the expression and ensure it ends with EOF
    let mut full_expression_parser = (parse_expression, eof).map(|(expr, _)| expr);
    full_expression_parser.parse(expression_str)
}
#[cfg(test)]
mod tests {
    use super::*;
    // helpers
    fn assert_parses_ok<'a, O, P>(
        mut parser: P,
        input: &'a str,
        expected_output: O,
        expected_remaining: &str,
    ) where
        P: FnMut(&mut Input<'a>) -> ParserResult<O>,
        O: PartialEq + std::fmt::Debug,
    {
        let mut mutable_input = input;
        match parser.parse_next(&mut mutable_input) {
            Ok(output) => {
                assert_eq!(output, expected_output, "Output mismatch for input: '{input}'");
                assert_eq!(
                    mutable_input, expected_remaining,
                    "Remaining input mismatch for input: '{input}'"
                );
            }
            Err(e) => panic!("Parser failed for input '{input}': {e:?}"),
        }
    }
    fn assert_parse_fails<'a, O, P>(mut parser: P, input: &'a str)
    where
        P: FnMut(&mut Input<'a>) -> ParserResult<O>,
        O: PartialEq + std::fmt::Debug,
    {
        let mut mutable_input = input;
        assert!(
            parser.parse_next(&mut mutable_input).is_err(),
            "Parser should have failed for input: '{input}'"
        );
    }
    #[test]
    fn test_parse_boolean() {
        // Success cases
        assert_parses_ok(parse_boolean, "true", LiteralValue::Bool(true), "");
        assert_parses_ok(parse_boolean, "false", LiteralValue::Bool(false), "");
        assert_parses_ok(parse_boolean, "true ", LiteralValue::Bool(true), " "); // Consumes only "true"
        // Failures
        assert_parse_fails(parse_boolean, "TRUE"); // Case-sensitive
        assert_parse_fails(parse_boolean, "tru");
        assert_parse_fails(parse_boolean, "  true"); // Does not consume leading space
    }
    #[test]
    fn test_parse_number_or_fixed_str() {
        // Success cases
        assert_parses_ok(parse_number_or_fixed_str, "123", LiteralValue::Number("123"), "");
        assert_parses_ok(parse_number_or_fixed_str, "-456", LiteralValue::Number("-456"), "");
        assert_parses_ok(parse_number_or_fixed_str, "0.5", LiteralValue::Number("0.5"), "");
        assert_parses_ok(parse_number_or_fixed_str, "123.456", LiteralValue::Number("123.456"), "");
        assert_parses_ok(parse_number_or_fixed_str, "-0.789", LiteralValue::Number("-0.789"), "");
        assert_parses_ok(parse_number_or_fixed_str, "123 ", LiteralValue::Number("123"), " "); // Peek space
        assert_parses_ok(parse_number_or_fixed_str, "123)", LiteralValue::Number("123"), ")"); // Peek delimiter
        assert_parses_ok(parse_number_or_fixed_str, "123.45)", LiteralValue::Number("123.45"), ")");
        // Failures
        assert_parse_fails(parse_number_or_fixed_str, "abc");
        assert_parse_fails(parse_number_or_fixed_str, "123a"); // Not delimited
        assert_parse_fails(parse_number_or_fixed_str, "1.2.3"); // Invalid number
        assert_parse_fails(parse_number_or_fixed_str, ".5"); // Requires digit before .
        assert_parse_fails(parse_number_or_fixed_str, "5."); // Requires digit after .
    }
    #[test]
    fn test_parse_hex_string() {
        // Success cases
        assert_parses_ok(parse_hex_string, "0x1a2B", LiteralValue::Str("0x1a2B"), "");
        assert_parses_ok(parse_hex_string, "0XFF", LiteralValue::Str("0XFF"), "");
        assert_parses_ok(
            parse_hex_string,
            "0xabcdef0123456789",
            LiteralValue::Str("0xabcdef0123456789"),
            "",
        );
        assert_parses_ok(parse_hex_string, "0x1 ", LiteralValue::Str("0x1"), " ");
        assert_parses_ok(parse_hex_string, "0xa)", LiteralValue::Str("0xa"), ")");
        // Failures
        assert_parse_fails(parse_hex_string, "0x"); // No digits
        assert_parse_fails(parse_hex_string, "0xG"); // Invalid hex digit
        assert_parse_fails(parse_hex_string, "123"); // Not a hex string
        assert_parse_fails(parse_hex_string, "0x123z"); // Not delimited properly
    }
    #[test]
    fn test_parse_quoted_string() {
        // Success cases
        assert_parses_ok(parse_quoted_string, "'hello'", LiteralValue::Str("hello"), "");
        // Empty string
        assert_parses_ok(parse_quoted_string, "''", LiteralValue::Str(""), "");
        assert_parses_ok(
            parse_quoted_string,
            "'hello world'",
            LiteralValue::Str("hello world"),
            "",
        );
        assert_parses_ok(parse_quoted_string, "'foo\\'bar'", LiteralValue::Str("foo\\'bar"), "");
        assert_parses_ok(parse_quoted_string, "'foo\\\\bar'", LiteralValue::Str("foo\\\\bar"), "");
        assert_parses_ok(parse_quoted_string, "'a\\''", LiteralValue::Str("a\\'"), "");
        // Just an escaped quote
        assert_parses_ok(parse_quoted_string, "'\\''", LiteralValue::Str("\\'"), "");
        // Escaped double quotes
        assert_parses_ok(parse_quoted_string, "'\"hello\"'", LiteralValue::Str("\"hello\""), "");
        assert_parses_ok(parse_quoted_string, "'_'", LiteralValue::Str("_"), "");
        // Failures
        assert_parse_fails(parse_quoted_string, "'hello"); // Missing closing quote
        assert_parse_fails(parse_quoted_string, "hello'"); // Missing opening quote
        assert_parse_fails(parse_quoted_string, "'hello\\"); // Ends with backslash (incomplete escape)
    }
    #[test]
    fn test_parse_unquoted_string() {
        // Success cases
        assert_parses_ok(parse_unquoted_string, "foobar", LiteralValue::Str("foobar"), "");
        assert_parses_ok(parse_unquoted_string, "foo_bar", LiteralValue::Str("foo_bar"), "");
        assert_parses_ok(parse_unquoted_string, "foo-bar", LiteralValue::Str("foo-bar"), "");
        assert_parses_ok(parse_unquoted_string, "a123", LiteralValue::Str("a123"), "");
        assert_parses_ok(parse_unquoted_string, "unquoted ", LiteralValue::Str("unquoted"), " ");
        // Failures
        assert_parse_fails(parse_unquoted_string, "true"); // Keyword
        assert_parse_fails(parse_unquoted_string, "123"); // Should be parsed as number
        assert_parse_fails(parse_unquoted_string, "-45"); // Should be parsed as number
        assert_parse_fails(parse_unquoted_string, "0xFA"); // Should be parsed as hex string
        assert_parse_fails(parse_unquoted_string, "123.45"); // Should be parsed as number (float-like)
        assert_parse_fails(parse_unquoted_string, ""); // Needs 1+ char
    }
    #[test]
    fn test_is_keyword() {
        // Success cases
        assert!(is_keyword("true"));
        assert!(is_keyword("FALSE"));
        // Failures
        assert!(!is_keyword("trueish"));
        assert!(!is_keyword("variable"));
    }
    #[test]
    fn test_parse_accessor() {
        // Success cases
        assert_parses_ok(parse_accessor, "[123]", Accessor::Index(123), "");
        assert_parses_ok(parse_accessor, ".keyName", Accessor::Key("keyName"), "");
        assert_parses_ok(parse_accessor, "._key_Name0", Accessor::Key("_key_Name0"), "");
        assert_parses_ok(parse_accessor, "[0].next", Accessor::Index(0), ".next");
        // Numeric keys
        assert_parses_ok(parse_accessor, ".0", Accessor::Key("0"), "");
        assert_parses_ok(parse_accessor, ".123", Accessor::Key("123"), "");
        assert_parses_ok(parse_accessor, ".0.next", Accessor::Key("0"), ".next"); // Numeric key followed by another accessor
        assert_parses_ok(parse_accessor, ".45[0]", Accessor::Key("45"), "[0]"); // Numeric key followed by index
        // Failures
        assert_parse_fails(parse_accessor, "keyName"); // Missing .
        assert_parse_fails(parse_accessor, "[abc]"); // Index not a number
        assert_parse_fails(parse_accessor, "[]"); // Empty index
        assert_parse_fails(parse_accessor, ".1key"); // Key cannot start with digit
        assert_parse_fails(parse_accessor, ".key-name"); // Hyphen not allowed in key
    }
    #[test]
    fn test_parse_base_variable_name() {
        assert_parses_ok(parse_base_variable_name, "request", "request", "");
        assert_parses_ok(parse_base_variable_name, "_privateVar", "_privateVar", "");
        assert_parses_ok(parse_base_variable_name, "var123", "var123", "");
        assert_parses_ok(parse_base_variable_name, "response ", "response", " ");
        assert_parses_ok(parse_base_variable_name, "0", "0", ""); // Numeric LHS base
        assert_parses_ok(parse_base_variable_name, "123[", "123", "["); // Numeric LHS base, peek '['
        assert_parses_ok(parse_base_variable_name, "45.field", "45", ".field"); // Numeric LHS base, peek '.'
                                                                                // Peek should ensure '123' is taken if followed by space then op
        assert_parses_ok(parse_base_variable_name, "123 ==", "123", " ==");
        assert_parse_fails(parse_base_variable_name, "true"); // Keyword
        assert_parse_fails(parse_base_variable_name, "123true"); // Invalid identifier
        assert_parse_fails(parse_base_variable_name, "123_"); // underscore after numeric not part of it
    }
    #[test]
    fn test_parse_condition_lhs() {
        assert_parses_ok(parse_condition_lhs, "var", ConditionLeft::Simple("var"), "");
        assert_parses_ok(
            parse_condition_lhs,
            "var.key",
            ConditionLeft::Path(VariablePath {
                base: "var",
                accessors: vec![Accessor::Key("key")],
            }),
            "",
        );
        assert_parses_ok(
            parse_condition_lhs,
            "arr[0]",
            ConditionLeft::Path(VariablePath { base: "arr", accessors: vec![Accessor::Index(0)] }),
            "",
        );
        assert_parses_ok(
            parse_condition_lhs,
            "obj.arr[1].field",
            ConditionLeft::Path(VariablePath {
                base: "obj",
                accessors: vec![Accessor::Key("arr"), Accessor::Index(1), Accessor::Key("field")],
            }),
            "",
        );
        assert_parses_ok(
            parse_condition_lhs,
            "0.field",
            ConditionLeft::Path(VariablePath {
                base: "0",
                accessors: vec![Accessor::Key("field")],
            }),
            "",
        );
        assert_parses_ok(
            parse_condition_lhs,
            "obj.0",
            ConditionLeft::Path(VariablePath { base: "obj", accessors: vec![Accessor::Key("0")] }),
            "",
        );
        assert_parses_ok(
            parse_condition_lhs,
            "0.1", // e.g. base_param_named_0.field_named_1
            ConditionLeft::Path(VariablePath { base: "0", accessors: vec![Accessor::Key("1")] }),
            "",
        );
        assert_parses_ok(
            parse_condition_lhs,
            "data.123.field",
            ConditionLeft::Path(VariablePath {
                base: "data",
                accessors: vec![Accessor::Key("123"), Accessor::Key("field")],
            }),
            "",
        );
        assert_parses_ok(
            parse_condition_lhs,
            "map.0[1].name",
            ConditionLeft::Path(VariablePath {
                base: "map",
                accessors: vec![Accessor::Key("0"), Accessor::Index(1), Accessor::Key("name")],
            }),
            "",
        );
    }
    #[test]
    fn test_parse_value_alt_order() {
        // Order: quoted_string, boolean, hex_string, number_or_fixed, unquoted_string
        assert_parses_ok(parse_value, " 'hello' ", LiteralValue::Str("hello"), "");
        assert_parses_ok(parse_value, " true ", LiteralValue::Bool(true), "");
        assert_parses_ok(parse_value, " 0xAB ", LiteralValue::Str("0xAB"), "");
        assert_parses_ok(parse_value, " 123.45 ", LiteralValue::Number("123.45"), "");
        assert_parses_ok(parse_value, " 123 ", LiteralValue::Number("123"), "");
        assert_parses_ok(parse_value, " unquoted_val ", LiteralValue::Str("unquoted_val"), "");
        assert_parses_ok(parse_value, " true_val ", LiteralValue::Str("true_val"), ""); // 'true' is prefix of 'true_val', boolean is tried, fails, then unquoted.
        assert_parses_ok(parse_value, " 0xVal ", LiteralValue::Str("0xVal"), "");
        // '0x' is prefix, hex is tried, fails on 'V', then unquoted.
    }
    #[test]
    fn test_parse_comparison_operator() {
        assert_parses_ok(parse_comparison_operator, "==", ComparisonOperator::Eq, "");
        assert_parses_ok(parse_comparison_operator, "!=", ComparisonOperator::Ne, "");
        assert_parses_ok(parse_comparison_operator, ">", ComparisonOperator::Gt, "");
        assert_parses_ok(parse_comparison_operator, ">=", ComparisonOperator::Gte, "");
        assert_parses_ok(parse_comparison_operator, "<", ComparisonOperator::Lt, "");
        assert_parses_ok(parse_comparison_operator, "<=", ComparisonOperator::Lte, "");
    }
    #[test]
    fn test_parse_condition() {
        let expr = "var == 123";
        let expected = Expression::Condition(Condition {
            left: ConditionLeft::Simple("var"),
            operator: ComparisonOperator::Eq,
            right: LiteralValue::Number("123"),
        });
        assert_parses_ok(parse_condition, expr, expected, "");
        let expr_path = "obj.count > 0.5";
        let expected_path = Expression::Condition(Condition {
            left: ConditionLeft::Path(VariablePath {
                base: "obj",
                accessors: vec![Accessor::Key("count")],
            }),
            operator: ComparisonOperator::Gt,
            right: LiteralValue::Number("0.5"),
        });
        assert_parses_ok(parse_condition, expr_path, expected_path, "");
    }
    #[test]
    fn test_parse_term_parentheses() {
        let expr = "(var == 123)";
        let inner_cond = Condition {
            left: ConditionLeft::Simple("var"),
            operator: ComparisonOperator::Eq,
            right: LiteralValue::Number("123"),
        };
        let expected = Expression::Condition(inner_cond.clone()); // The term itself is the condition
        assert_parses_ok(parse_term, expr, expected, "");
        let expr_nested = "( var1 > 10 && var2 < 'abc' )";
        let expected_nested = Expression::Logical {
            left: Box::new(Expression::Condition(Condition {
                left: ConditionLeft::Simple("var1"),
                operator: ComparisonOperator::Gt,
                right: LiteralValue::Number("10"),
            })),
            operator: LogicalOperator::And,
            right: Box::new(Expression::Condition(Condition {
                left: ConditionLeft::Simple("var2"),
                operator: ComparisonOperator::Lt,
                right: LiteralValue::Str("abc"),
            })),
        };
        // parse_term calls parse_expression for parentheses, parse_expression calls parse_or_expression...
        assert_parses_ok(parse_term, expr_nested, expected_nested, "");
    }
    #[test]
    fn test_parse_logical_expressions() {
        let expr = "a == 1 && b < 2.0";
        let expected = Expression::Logical {
            left: Box::new(Expression::Condition(Condition {
                left: ConditionLeft::Simple("a"),
                operator: ComparisonOperator::Eq,
                right: LiteralValue::Number("1"),
            })),
            operator: LogicalOperator::And,
            right: Box::new(Expression::Condition(Condition {
                left: ConditionLeft::Simple("b"),
                operator: ComparisonOperator::Lt,
                right: LiteralValue::Number("2.0"),
            })),
        };
        // Test parse_and_expression directly or parse_expression for full precedence
        assert_parses_ok(parse_expression, expr, expected.clone(), "");
        // Also test with parse(), which adds eof
        assert_eq!(parse(expr).unwrap(), expected);
        let expr_or = "a == 1 || b < 'text'";
        let expected_or = Expression::Logical {
            left: Box::new(Expression::Condition(Condition {
                left: ConditionLeft::Simple("a"),
                operator: ComparisonOperator::Eq,
                right: LiteralValue::Number("1"),
            })),
            operator: LogicalOperator::Or,
            right: Box::new(Expression::Condition(Condition {
                left: ConditionLeft::Simple("b"),
                operator: ComparisonOperator::Lt,
                right: LiteralValue::Str("text"),
            })),
        };
        assert_eq!(parse(expr_or).unwrap(), expected_or);
        // Precedence: AND over OR
        let expr_mixed = "a == 1 || b < 2 && c > 3";
        let expected_mixed = Expression::Logical {
            left: Box::new(Expression::Condition(Condition {
                left: ConditionLeft::Simple("a"),
                operator: ComparisonOperator::Eq,
                right: LiteralValue::Number("1"),
            })),
            operator: LogicalOperator::Or,
            right: Box::new(Expression::Logical {
                left: Box::new(Expression::Condition(Condition {
                    left: ConditionLeft::Simple("b"),
                    operator: ComparisonOperator::Lt,
                    right: LiteralValue::Number("2"),
                })),
                operator: LogicalOperator::And,
                right: Box::new(Expression::Condition(Condition {
                    left: ConditionLeft::Simple("c"),
                    operator: ComparisonOperator::Gt,
                    right: LiteralValue::Number("3"),
                })),
            }),
        };
        assert_eq!(parse(expr_mixed).unwrap(), expected_mixed);
        // Parentheses overriding precedence
        let expr_parens = "(a == 1 || b < 2) && c > 3";
        let expected_parens = Expression::Logical {
            left: Box::new(Expression::Logical {
                left: Box::new(Expression::Condition(Condition {
                    left: ConditionLeft::Simple("a"),
                    operator: ComparisonOperator::Eq,
                    right: LiteralValue::Number("1"),
                })),
                operator: LogicalOperator::Or,
                right: Box::new(Expression::Condition(Condition {
                    left: ConditionLeft::Simple("b"),
                    operator: ComparisonOperator::Lt,
                    right: LiteralValue::Number("2"),
                })),
            }),
            operator: LogicalOperator::And,
            right: Box::new(Expression::Condition(Condition {
                left: ConditionLeft::Simple("c"),
                operator: ComparisonOperator::Gt,
                right: LiteralValue::Number("3"),
            })),
        };
        assert_eq!(parse(expr_parens).unwrap(), expected_parens);
    }
    #[test]
    fn test_full_parse_with_eof() {
        assert!(parse("var == 123").is_ok());
        assert!(parse("var == 123 && extra_stuff_not_parsed").is_err()); // Fails eof
        assert!(parse("(a == 1 || b < 2)&& c > 3").is_ok()); // No space around AND
    }
}
</file>

<file path="core/src/event/callback_registry.rs">
use std::{any::Any, sync::Arc, time::Duration};
use alloy::consensus::Transaction;
use alloy::network::{AnyRpcTransaction, TransactionResponse};
use alloy::{
    primitives::{Address, BlockHash, Bytes, TxHash, B256, U256, U64},
    rpc::types::{
        trace::parity::{CallAction, LocalizedTransactionTrace},
        Log,
    },
};
use chrono::Utc;
use futures::future::BoxFuture;
use serde::{Deserialize, Serialize};
use tokio::time::sleep;
use tracing::{debug, error, info};
use crate::{
    event::contract_setup::{ContractInformation, NetworkContract, TraceInformation},
    indexer::start::ProcessedNetworkContract,
    is_running,
};
pub type Decoder = Arc<dyn Fn(Vec<TxHash>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync>;
pub fn noop_decoder() -> Decoder {
    Arc::new(move |_topics: Vec<TxHash>, _data: Bytes| {
        Arc::new(String::new()) as Arc<dyn Any + Send + Sync>
    }) as Decoder
}
/// The [`CallbackResult`] enum has two core variants, a Trace and an Event. We implement shared
/// callback logic to sink or stream these "results".
///
/// Since each event is different, and we want `rust` project consumers to not worry about manually
/// mapping their [`EventResult`] into a [`CallbackResult`], we handle this for them internally and
/// this struct allows us to do this behind the scenes.
#[derive(Clone)]
pub enum CallbackResult {
    Event(Vec<EventResult>),
    Trace(Vec<TraceResult>),
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct TxInformation {
    pub chain_id: u64,
    pub network: String,
    pub address: Address,
    pub block_hash: BlockHash,
    pub block_number: u64,
    pub block_timestamp: Option<U256>,
    pub transaction_hash: TxHash,
    pub log_index: U256,
    pub transaction_index: u64,
}
impl TxInformation {
    pub fn block_timestamp_to_datetime(&self) -> Option<chrono::DateTime<Utc>> {
        if let Some(timestamp) = self.block_timestamp {
            let timestamp = timestamp.to::<i64>();
            Some(chrono::DateTime::from_timestamp(timestamp, 0).expect("invalid timestamp"))
        } else {
            None
        }
    }
}
/// Define a trait over any entity that has attached transaction information. This is very useful
/// when working with on-chain generics over many event types.
pub trait HasTxInformation {
    /// Return the transaction information associated with an event.
    fn tx_information(&self) -> &TxInformation;
}
#[derive(Debug, Clone)]
pub struct LogFoundInRequest {
    pub from_block: U64,
    pub to_block: U64,
}
#[derive(Debug, Clone)]
pub struct EventResult {
    pub log: Log,
    pub decoded_data: Arc<dyn Any + Send + Sync>,
    pub tx_information: TxInformation,
    pub found_in_request: LogFoundInRequest,
}
impl EventResult {
    pub fn new(
        network_contract: Arc<NetworkContract>,
        log: Log,
        start_block: U64,
        end_block: U64,
    ) -> Self {
        let log_address = log.inner.address;
        Self {
            log: log.clone(),
            decoded_data: network_contract.decode_log(log.inner),
            tx_information: TxInformation {
                chain_id: network_contract.cached_provider.chain.id(),
                network: network_contract.network.to_string(),
                address: log_address,
                block_hash: log.block_hash.expect("log should contain block_hash"),
                block_number: log.block_number.expect("log should contain block_number"),
                block_timestamp: log.block_timestamp.map(U256::from),
                transaction_hash: log
                    .transaction_hash
                    .expect("log should contain transaction_hash"),
                transaction_index: log
                    .transaction_index
                    .expect("log should contain transaction_index"),
                log_index: U256::from(log.log_index.expect("log should contain log_index")),
            },
            found_in_request: LogFoundInRequest { from_block: start_block, to_block: end_block },
        }
    }
}
pub type EventCallbackResult<T> = Result<T, String>;
pub type EventCallbackType =
    Arc<dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync>;
pub type TraceCallbackType =
    Arc<dyn Fn(Vec<TraceResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync>;
pub struct EventCallbackRegistryInformation {
    pub id: String,
    pub indexer_name: String,
    pub topic_id: B256,
    pub event_name: String,
    pub index_event_in_order: bool,
    pub contract: ContractInformation,
    pub callback: EventCallbackType,
}
impl EventCallbackRegistryInformation {
    pub fn info_log_name(&self) -> String {
        format!("{}::{}", self.contract.name, self.event_name)
    }
    pub fn is_factory_filter_event(&self) -> bool {
        self.contract.details.iter().all(|d| {
            // it's a factory contract if the factory filter matches the contract name and event name
            matches!(
                d.indexing_contract_setup.factory_details(),
                Some(f) if f.contract_name == self.contract.name && f.event.name == self.event_name
            )
        })
    }
}
impl Clone for EventCallbackRegistryInformation {
    fn clone(&self) -> Self {
        EventCallbackRegistryInformation {
            id: self.id.clone(),
            indexer_name: self.indexer_name.clone(),
            topic_id: self.topic_id,
            event_name: self.event_name.clone(),
            index_event_in_order: self.index_event_in_order,
            contract: self.contract.clone(),
            callback: Arc::clone(&self.callback),
        }
    }
}
#[derive(Clone)]
pub struct EventCallbackRegistry {
    pub events: Vec<EventCallbackRegistryInformation>,
}
impl Default for EventCallbackRegistry {
    fn default() -> Self {
        Self::new()
    }
}
impl EventCallbackRegistry {
    pub fn new() -> Self {
        EventCallbackRegistry { events: Vec::new() }
    }
    pub fn find_event(&self, id: &String) -> Option<&EventCallbackRegistryInformation> {
        self.events.iter().find(|e| e.id == *id)
    }
    pub fn register_event(&mut self, event: EventCallbackRegistryInformation) {
        self.events.push(event);
    }
    pub async fn trigger_event(&self, id: &String, data: Vec<EventResult>) -> Result<(), String> {
        if let Some(event_information) = self.find_event(id) {
            trigger_event(
                id,
                data,
                |d| (event_information.callback)(d),
                || event_information.info_log_name(),
                &event_information.topic_id.to_string(),
            )
            .await
        } else {
            let message = format!(
                "EventCallbackRegistry: No event found for id: {}. Data: {:?}",
                id,
                data.first()
            );
            error!(message);
            Err(message)
        }
    }
    pub fn complete(&self) -> Arc<Self> {
        Arc::new(self.clone())
    }
    pub fn reapply_after_historic(
        &mut self,
        processed_network_contracts: Vec<ProcessedNetworkContract>,
    ) -> Arc<EventCallbackRegistry> {
        self.events.iter_mut().for_each(|e| {
            e.contract.details.iter_mut().for_each(|d| {
                if d.end_block.is_none() {
                    if let Some(processed_block) =
                        processed_network_contracts.iter().find(|c| c.id == d.id)
                    {
                        d.start_block = Some(processed_block.processed_up_to);
                    }
                }
            });
        });
        // Retain only the details with `end_block.is_none()`
        self.events.iter_mut().for_each(|e| {
            e.contract.details.retain(|d| d.end_block.is_none());
        });
        // Retain only the events that have details with `end_block.is_none()`
        self.events.retain(|e| !e.contract.details.is_empty());
        self.complete()
    }
}
// --------------------------------
// "Native" Trace Callback Registry
// --------------------------------
#[derive(Debug, Clone)]
pub enum TraceResult {
    NativeTransfer {
        from: Address,
        to: Address,
        value: U256,
        tx_information: TxInformation,
        found_in_request: LogFoundInRequest,
    },
    Block {
        block: Box<alloy::network::AnyRpcBlock>,
        tx_information: TxInformation,
        found_in_request: LogFoundInRequest,
    },
}
impl TraceResult {
    /// Create a "NativeTransfer" TraceResult for sinking and streaming.
    pub fn new_debug_native_transfer(
        action: &CallAction,
        trace: &LocalizedTransactionTrace,
        network: &str,
        chain_id: u64,
        start_block: U64,
        end_block: U64,
    ) -> Self {
        if trace.block_number.is_none() {
            error!(
                "Unexpected block trace None for `block_number` in {} - {}",
                start_block, end_block
            );
        }
        Self::NativeTransfer {
            from: action.from,
            to: action.to,
            value: action.value,
            tx_information: TxInformation {
                chain_id,
                network: network.to_string(),
                address: Address::ZERO,
                // TODO: Unclear in what situation this would be `None`.
                block_number: trace.block_number.unwrap_or(0),
                block_timestamp: None,
                transaction_hash: trace.transaction_hash.unwrap_or(TxHash::ZERO),
                block_hash: trace.block_hash.unwrap_or(BlockHash::ZERO),
                transaction_index: trace.transaction_position.unwrap_or(0),
                log_index: U256::from(0),
            },
            found_in_request: LogFoundInRequest { from_block: start_block, to_block: end_block },
        }
    }
    /// Create a "NativeTransfer" TraceResult from a `eth_getBlockByNumber` Transaction.
    pub fn new_native_transfer(
        tx: AnyRpcTransaction,
        ts: u64,
        to: Address,
        network: &str,
        chain_id: u64,
        start_block: U64,
        end_block: U64,
    ) -> Self {
        Self::NativeTransfer {
            to,
            from: tx.from(),
            value: tx.value(),
            tx_information: TxInformation {
                chain_id,
                network: network.to_string(),
                address: Address::ZERO,
                block_number: tx.block_number.expect("block_number should be present"),
                block_timestamp: Some(U256::from(ts)),
                transaction_hash: tx.tx_hash(),
                block_hash: tx.block_hash.expect("block_hash should be present"),
                transaction_index: tx
                    .transaction_index
                    .expect("transaction_index should be present"),
                log_index: U256::from(0),
            },
            found_in_request: LogFoundInRequest { from_block: start_block, to_block: end_block },
        }
    }
    /// Create a "Block" TraceResult for block events.
    pub fn new_block(
        block: alloy::network::AnyRpcBlock,
        network: &str,
        chain_id: u64,
        start_block: U64,
        end_block: U64,
    ) -> Self {
        Self::Block {
            tx_information: TxInformation {
                chain_id,
                block_timestamp: Some(U256::from(block.header.timestamp)),
                network: network.to_string(),
                block_number: block.header.number,
                block_hash: block.header.hash,
                // Invalid fields for a block event.
                address: Address::ZERO,
                transaction_hash: TxHash::ZERO,
                transaction_index: 0,
                log_index: U256::from(0),
            },
            block: Box::new(block),
            found_in_request: LogFoundInRequest { from_block: start_block, to_block: end_block },
        }
    }
}
pub type TraceCallbackResult<T> = Result<T, String>;
#[derive(Clone)]
pub struct TraceCallbackRegistryInformation {
    pub id: String,
    pub indexer_name: String,
    pub event_name: String,
    pub contract_name: String,
    pub trace_information: TraceInformation,
    pub callback: TraceCallbackType,
}
impl TraceCallbackRegistryInformation {
    pub fn info_log_name(&self) -> String {
        format!("{}::{}", self.indexer_name, self.event_name)
    }
}
#[derive(Clone, Default)]
pub struct TraceCallbackRegistry {
    pub events: Vec<TraceCallbackRegistryInformation>,
}
impl TraceCallbackRegistry {
    pub fn new() -> Self {
        TraceCallbackRegistry { events: Vec::new() }
    }
    pub fn find_event(&self, id: &String) -> Option<&TraceCallbackRegistryInformation> {
        self.events.iter().find(|e| e.id == *id)
    }
    pub fn register_event(&mut self, event: TraceCallbackRegistryInformation) {
        self.events.push(event);
    }
    pub async fn trigger_event(&self, id: &String, data: Vec<TraceResult>) -> Result<(), String> {
        if let Some(event_information) = self.find_event(id) {
            trigger_event(
                id,
                data,
                |d| (event_information.callback)(d),
                || event_information.info_log_name(),
                &event_information.event_name,
            )
            .await
        } else {
            let message = format!("TraceCallbackRegistry: No event found for id: {id}");
            error!("TraceCallbackRegistry: No event found for id: {}", id);
            Err(message)
        }
    }
    pub fn complete(&self) -> Arc<Self> {
        Arc::new(self.clone())
    }
}
async fn trigger_event<T>(
    id: &String,
    data: Vec<T>,
    callback: impl Fn(Vec<T>) -> BoxFuture<'static, EventCallbackResult<()>>,
    info_log_name: impl Fn() -> String,
    event_identifier: &str,
) -> Result<(), String>
where
    T: Clone,
{
    let mut attempts = 0;
    let mut delay = Duration::from_millis(100);
    let len = data.len();
    debug!("{} - Pushed {} events", len, info_log_name());
    loop {
        if !is_running() {
            info!("Detected shutdown, stopping event trigger");
            return Err("Detected shutdown, stopping event trigger".to_string());
        }
        match callback(data.clone()).await {
            Ok(_) => {
                debug!(
                    "Event processing succeeded for id: {} - topic_id: {}",
                    id, event_identifier
                );
                return Ok(());
            }
            Err(e) => {
                if !is_running() {
                    info!("Detected shutdown, stopping event trigger");
                    return Err(e);
                }
                attempts += 1;
                error!(
                    "{} Event processing failed - id: {} - topic_id: {}. Retrying... (attempt {}). Error: {}",
                    info_log_name(), id, event_identifier, attempts, e
                );
                delay = (delay * 2).min(Duration::from_secs(15));
                sleep(delay).await;
            }
        }
    }
}
</file>

<file path="core/src/event/config.rs">
use alloy::json_abi::Event;
use alloy::primitives::{keccak256, Address, B256, U64};
use alloy::rpc::types::ValueOrArray;
use std::{path::PathBuf, sync::Arc};
use tokio::sync::Mutex;
use crate::database::clickhouse::client::ClickhouseClient;
use crate::event::contract_setup::{AddressDetails, IndexingContractSetup};
use crate::event::factory_event_filter_sync::update_known_factory_deployed_addresses;
use crate::event::rindexer_event_filter::FactoryFilter;
use crate::manifest::config::Config;
use crate::manifest::contract::EventInputIndexedFilters;
use crate::{
    event::{
        callback_registry::{
            EventCallbackRegistry, EventResult, TraceCallbackRegistry, TraceResult,
        },
        contract_setup::NetworkContract,
        BuildRindexerFilterError, RindexerEventFilter,
    },
    indexer::IndexingEventsProgressState,
    manifest::{native_transfer::TraceProcessingMethod, storage::CsvDetails},
    PostgresClient,
};
pub struct ContractEventProcessingConfig {
    pub id: String,
    pub project_path: PathBuf,
    pub indexer_name: String,
    pub contract_name: String,
    pub topic_id: B256,
    pub event_name: String,
    pub config: Config,
    pub network_contract: Arc<NetworkContract>,
    pub timestamps: bool,
    pub start_block: U64,
    pub end_block: U64,
    pub registry: Arc<EventCallbackRegistry>,
    pub progress: Arc<Mutex<IndexingEventsProgressState>>,
    pub postgres: Option<Arc<PostgresClient>>,
    pub clickhouse: Option<Arc<ClickhouseClient>>,
    pub csv_details: Option<CsvDetails>,
    pub stream_last_synced_block_file_path: Option<String>,
    pub index_event_in_order: bool,
    pub live_indexing: bool,
    pub indexing_distance_from_head: U64,
}
impl ContractEventProcessingConfig {
    pub fn info_log_name(&self) -> String {
        format!("{}::{}::{}", self.contract_name, self.event_name, self.network_contract.network)
    }
    pub fn to_event_filter(&self) -> Result<RindexerEventFilter, BuildRindexerFilterError> {
        match &self.network_contract.indexing_contract_setup {
            IndexingContractSetup::Address(details) => RindexerEventFilter::new_address_filter(
                &self.topic_id,
                &self.event_name,
                details,
                self.start_block,
                self.end_block,
            ),
            IndexingContractSetup::Filter(details) => RindexerEventFilter::new_filter(
                &self.topic_id,
                &self.event_name,
                details,
                self.start_block,
                self.end_block,
            ),
            IndexingContractSetup::Factory(details) => {
                let index_filter = details.indexed_filters.iter().find_map(|indexed_filters| {
                    indexed_filters.iter().find(|&n| n.event_name == self.event_name)
                });
                Ok(RindexerEventFilter::Factory(FactoryFilter {
                    project_path: self.project_path.clone(),
                    indexer_name: self.indexer_name.clone(),
                    factory_contract_name: details.contract_name.clone(),
                    factory_address: details.address.clone(),
                    factory_event_name: details.event.name.clone(),
                    factory_input_name: details.input_name.clone(),
                    network: self.network_contract.network.clone(),
                    topic_id: self.topic_id,
                    topics: index_filter.cloned().map(Into::into).unwrap_or_default(),
                    clickhouse: self.clickhouse.clone(),
                    postgres: self.postgres.clone(),
                    csv_details: self.csv_details.clone(),
                    current_block: self.start_block,
                    next_block: self.end_block,
                }))
            }
        }
    }
    pub async fn trigger_event(&self, fn_data: Vec<EventResult>) -> Result<(), String> {
        self.registry.trigger_event(&self.id, fn_data).await
    }
}
pub struct FactoryEventProcessingConfig {
    pub id: String,
    pub project_path: PathBuf,
    pub indexer_name: String,
    pub contract_name: String,
    pub address: ValueOrArray<Address>,
    pub input_name: ValueOrArray<String>,
    pub event: Event,
    pub config: Config,
    pub network_contract: Arc<NetworkContract>,
    pub timestamps: bool,
    pub start_block: U64,
    pub end_block: U64,
    pub registry: Arc<EventCallbackRegistry>,
    pub progress: Arc<Mutex<IndexingEventsProgressState>>,
    pub postgres: Option<Arc<PostgresClient>>,
    pub clickhouse: Option<Arc<ClickhouseClient>>,
    pub csv_details: Option<CsvDetails>,
    pub stream_last_synced_block_file_path: Option<String>,
    pub index_event_in_order: bool,
    pub live_indexing: bool,
    pub indexing_distance_from_head: U64,
}
impl FactoryEventProcessingConfig {
    pub fn input_names(&self) -> Vec<String> {
        match &self.input_name {
            ValueOrArray::Value(name) => vec![name.clone()],
            ValueOrArray::Array(names) => names.clone(),
        }
    }
    pub fn to_event_filter(&self) -> Result<RindexerEventFilter, BuildRindexerFilterError> {
        let event_name = self.event.name.clone();
        let event_selector = self.event.selector();
        let details = AddressDetails {
            address: self.address.clone(),
            indexed_filters: Some(vec![EventInputIndexedFilters {
                event_name: event_name.clone(),
                indexed_1: None,
                indexed_2: None,
                indexed_3: None,
            }]),
        };
        RindexerEventFilter::new_address_filter(
            &event_selector,
            &event_name,
            &details,
            self.start_block,
            self.end_block,
        )
    }
    pub async fn trigger_event(&self, events: Vec<EventResult>) -> Result<(), String> {
        self.registry.trigger_event(&self.id, events.clone()).await?;
        update_known_factory_deployed_addresses(self, &events).await.map_err(|e| e.to_string())
    }
    pub fn info_log_name(&self) -> String {
        format!("{}::{}::{}", self.contract_name, self.event.name, self.network_contract.network)
    }
}
pub enum EventProcessingConfig {
    ContractEventProcessing(ContractEventProcessingConfig),
    FactoryEventProcessing(FactoryEventProcessingConfig),
}
impl From<ContractEventProcessingConfig> for EventProcessingConfig {
    fn from(config: ContractEventProcessingConfig) -> Self {
        Self::ContractEventProcessing(config)
    }
}
impl From<FactoryEventProcessingConfig> for EventProcessingConfig {
    fn from(config: FactoryEventProcessingConfig) -> Self {
        Self::FactoryEventProcessing(config)
    }
}
impl EventProcessingConfig {
    pub fn is_factory_event(&self) -> bool {
        match self {
            Self::ContractEventProcessing(_) => false,
            Self::FactoryEventProcessing(_) => true,
        }
    }
    pub fn topic_id(&self) -> B256 {
        match self {
            Self::ContractEventProcessing(config) => config.topic_id,
            Self::FactoryEventProcessing(config) => config.event.selector(),
        }
    }
    pub fn id(&self) -> B256 {
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let network = self.network_contract().network.to_string();
        let combined = format!("{topic_id}{contract_name}{network}");
        keccak256(combined.as_bytes())
    }
    pub fn config(&self) -> &Config {
        match self {
            Self::ContractEventProcessing(config) => &config.config,
            Self::FactoryEventProcessing(config) => &config.config,
        }
    }
    pub fn timestamps(&self) -> bool {
        match self {
            Self::ContractEventProcessing(config) => config.timestamps,
            Self::FactoryEventProcessing(config) => config.timestamps,
        }
    }
    pub fn info_log_name(&self) -> String {
        match self {
            Self::ContractEventProcessing(config) => config.info_log_name().clone(),
            Self::FactoryEventProcessing(config) => config.info_log_name(),
        }
    }
    pub fn network_contract(&self) -> Arc<NetworkContract> {
        match self {
            Self::ContractEventProcessing(config) => config.network_contract.clone(),
            Self::FactoryEventProcessing(config) => config.network_contract.clone(),
        }
    }
    pub fn index_event_in_order(&self) -> bool {
        match self {
            Self::ContractEventProcessing(config) => config.index_event_in_order,
            Self::FactoryEventProcessing(config) => config.index_event_in_order,
        }
    }
    pub fn contract_name(&self) -> String {
        match self {
            Self::ContractEventProcessing(config) => config.contract_name.clone(),
            Self::FactoryEventProcessing(config) => config.contract_name.clone(),
        }
    }
    pub fn indexer_name(&self) -> String {
        match self {
            Self::ContractEventProcessing(config) => config.indexer_name.clone(),
            Self::FactoryEventProcessing(config) => config.indexer_name.clone(),
        }
    }
    pub fn event_name(&self) -> String {
        match self {
            Self::ContractEventProcessing(config) => config.event_name.clone(),
            Self::FactoryEventProcessing(config) => config.event.name.clone(),
        }
    }
    pub fn live_indexing(&self) -> bool {
        match self {
            Self::ContractEventProcessing(config) => config.live_indexing,
            Self::FactoryEventProcessing(config) => config.live_indexing,
        }
    }
    pub fn indexing_distance_from_head(&self) -> U64 {
        match self {
            Self::ContractEventProcessing(config) => config.indexing_distance_from_head,
            Self::FactoryEventProcessing(config) => config.indexing_distance_from_head,
        }
    }
    pub fn progress(&self) -> Arc<Mutex<IndexingEventsProgressState>> {
        match self {
            Self::ContractEventProcessing(config) => config.progress.clone(),
            Self::FactoryEventProcessing(config) => config.progress.clone(),
        }
    }
    pub fn postgres(&self) -> Option<Arc<PostgresClient>> {
        match self {
            Self::ContractEventProcessing(config) => config.postgres.clone(),
            Self::FactoryEventProcessing(config) => config.postgres.clone(),
        }
    }
    pub fn clickhouse(&self) -> Option<Arc<ClickhouseClient>> {
        match self {
            Self::ContractEventProcessing(config) => config.clickhouse.clone(),
            Self::FactoryEventProcessing(config) => config.clickhouse.clone(),
        }
    }
    pub fn csv_details(&self) -> Option<CsvDetails> {
        match self {
            Self::ContractEventProcessing(config) => config.csv_details.clone(),
            Self::FactoryEventProcessing(config) => config.csv_details.clone(),
        }
    }
    pub fn stream_last_synced_block_file_path(&self) -> Option<String> {
        match self {
            Self::ContractEventProcessing(config) => {
                config.stream_last_synced_block_file_path.clone()
            }
            Self::FactoryEventProcessing(config) => {
                config.stream_last_synced_block_file_path.clone()
            }
        }
    }
    pub fn project_path(&self) -> PathBuf {
        match self {
            Self::ContractEventProcessing(config) => config.project_path.clone(),
            Self::FactoryEventProcessing(config) => config.project_path.clone(),
        }
    }
    pub fn to_event_filter(&self) -> Result<RindexerEventFilter, BuildRindexerFilterError> {
        match self {
            Self::ContractEventProcessing(config) => config.to_event_filter(),
            Self::FactoryEventProcessing(config) => config.to_event_filter(),
        }
    }
    pub async fn trigger_event(&self, fn_data: Vec<EventResult>) -> Result<(), String> {
        match self {
            Self::ContractEventProcessing(config) => config.trigger_event(fn_data).await,
            Self::FactoryEventProcessing(config) => config.trigger_event(fn_data).await,
        }
    }
}
#[derive(Clone)]
pub struct TraceProcessingConfig {
    pub id: String,
    pub project_path: PathBuf,
    pub start_block: U64,
    pub end_block: U64,
    pub indexer_name: String,
    pub contract_name: String,
    pub event_name: String,
    pub network: String,
    pub progress: Arc<Mutex<IndexingEventsProgressState>>,
    pub postgres: Option<Arc<PostgresClient>>,
    pub csv_details: Option<CsvDetails>,
    pub registry: Arc<TraceCallbackRegistry>,
    pub method: TraceProcessingMethod,
    pub stream_last_synced_block_file_path: Option<String>,
}
impl TraceProcessingConfig {
    pub async fn trigger_event(&self, fn_data: Vec<TraceResult>) {
        // Trigger events for all registered events in this network's registry
        for event in &self.registry.events {
            let _ = self.registry.trigger_event(&event.id, fn_data.clone()).await;
        }
    }
}
</file>

<file path="core/src/event/contract_setup.rs">
use crate::blockclock::BlockClock;
use crate::helpers::get_full_path;
use crate::manifest::core::Manifest;
use crate::notifications::ChainStateNotification;
use crate::{
    event::callback_registry::Decoder,
    generate_random_id,
    indexer::native_transfer::EVENT_NAME,
    manifest::{
        contract::{Contract, EventInputIndexedFilters},
        native_transfer::{NativeTransfers, TraceProcessingMethod},
    },
    provider::{get_network_provider, CreateNetworkProvider, JsonRpcCachedProvider},
    types::single_or_array::StringOrArray,
};
use alloy::json_abi::{Event, JsonAbi};
use alloy::{
    primitives::{Address, Log, U64},
    rpc::types::ValueOrArray,
};
use serde::{Deserialize, Serialize};
use serde_json::Error;
use std::{any::Any, fs, path::Path, sync::Arc};
use tokio::sync::broadcast::Sender;
#[derive(Clone)]
pub struct NetworkContract {
    pub id: String,
    pub network: String,
    pub indexing_contract_setup: IndexingContractSetup,
    pub cached_provider: Arc<JsonRpcCachedProvider>,
    pub block_clock: BlockClock,
    pub decoder: Decoder,
    pub start_block: Option<U64>,
    pub end_block: Option<U64>,
    pub disable_logs_bloom_checks: bool,
}
impl NetworkContract {
    pub fn decode_log(&self, log: Log) -> Arc<dyn Any + Send + Sync> {
        (self.decoder)(log.topics().to_vec(), log.data.data)
    }
    pub fn is_live_indexing(&self) -> bool {
        self.end_block.is_none()
    }
    pub fn chain_state_notification(&self) -> Option<Sender<ChainStateNotification>> {
        self.cached_provider.chain_state_notification.clone()
    }
}
#[derive(Clone)]
pub struct ContractInformation {
    pub name: String,
    pub details: Vec<NetworkContract>,
    pub abi: StringOrArray,
    pub reorg_safe_distance: bool,
}
#[derive(thiserror::Error, Debug)]
pub enum CreateContractInformationError {
    #[error("Can not find network {0} from providers")]
    CanNotFindNetworkFromProviders(String),
}
impl ContractInformation {
    pub fn create(
        project_path: &Path,
        contract: &Contract,
        network_providers: &[CreateNetworkProvider],
        decoder: Decoder,
        manifest: &Manifest,
    ) -> Result<ContractInformation, CreateContractInformationError> {
        let mut details = vec![];
        for c in &contract.details {
            let provider = get_network_provider(&c.network, network_providers);
            match provider {
                None => {
                    return Err(CreateContractInformationError::CanNotFindNetworkFromProviders(
                        c.network.clone(),
                    ));
                }
                Some(provider) => {
                    let client = Arc::clone(&provider.client);
                    details.push(NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: client.clone(),
                        block_clock: BlockClock::new(
                            manifest.timestamps,
                            manifest.config.timestamp_sample_rate,
                            client,
                        ),
                        decoder: Arc::clone(&decoder),
                        indexing_contract_setup: c.indexing_contract_setup(project_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: provider.disable_logs_bloom_checks,
                    });
                }
            }
        }
        Ok(ContractInformation {
            name: contract.name.clone(),
            details,
            abi: contract.abi.clone(),
            reorg_safe_distance: contract.reorg_safe_distance.unwrap_or_default(),
        })
    }
}
#[derive(Clone)]
pub struct NetworkTrace {
    pub id: String,
    pub network: String,
    pub cached_provider: Arc<JsonRpcCachedProvider>,
    pub start_block: Option<U64>,
    pub end_block: Option<U64>,
    pub method: TraceProcessingMethod,
}
impl NetworkTrace {
    pub fn is_live_indexing(&self) -> bool {
        self.end_block.is_none()
    }
    pub fn chain_state_notification(&self) -> Option<Sender<ChainStateNotification>> {
        self.cached_provider.chain_state_notification.clone()
    }
}
#[derive(Clone)]
pub struct TraceInformation {
    pub name: String,
    pub details: Vec<NetworkTrace>,
    pub reorg_safe_distance: bool,
}
impl TraceInformation {
    pub fn create(
        native_transfers: NativeTransfers,
        network_providers: &[CreateNetworkProvider],
    ) -> Result<TraceInformation, CreateContractInformationError> {
        let mut details = vec![];
        let trace_networks = native_transfers.networks.unwrap_or_default();
        for n in &trace_networks {
            let name = n.network.clone();
            let provider = get_network_provider(&name, network_providers);
            match provider {
                None => {
                    return Err(CreateContractInformationError::CanNotFindNetworkFromProviders(
                        name,
                    ));
                }
                Some(provider) => {
                    details.push(NetworkTrace {
                        id: generate_random_id(10),
                        network: name,
                        cached_provider: Arc::clone(&provider.client),
                        start_block: n.start_block,
                        end_block: n.end_block,
                        method: n.method,
                    });
                }
            }
        }
        Ok(TraceInformation {
            name: EVENT_NAME.to_string(),
            details,
            reorg_safe_distance: native_transfers.reorg_safe_distance.unwrap_or_default(),
        })
    }
}
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub struct ContractEventMapping {
    pub contract_name: String,
    pub event_name: String,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct AddressDetails {
    pub address: ValueOrArray<Address>,
    pub indexed_filters: Option<Vec<EventInputIndexedFilters>>,
}
#[derive(thiserror::Error, Debug)]
pub enum FactoryDetailsFromAbiError {
    #[error(transparent)]
    IOError(#[from] std::io::Error),
    #[error(transparent)]
    ABIParsingError(#[from] Error),
    #[error("Can not find event {0}")]
    EventNotFoundError(String),
}
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FactoryDetails {
    pub contract_name: String,
    pub address: ValueOrArray<Address>,
    pub input_name: ValueOrArray<String>,
    pub event: Event,
    pub indexed_filters: Option<Vec<EventInputIndexedFilters>>,
}
impl FactoryDetails {
    pub fn from_abi(
        project_path: &Path,
        abi: String,
        contract_name: String,
        address: ValueOrArray<Address>,
        event_name: String,
        input_name: ValueOrArray<String>,
        indexed_filters: Option<Vec<EventInputIndexedFilters>>,
    ) -> Result<FactoryDetails, FactoryDetailsFromAbiError> {
        let full_path = get_full_path(project_path, &abi)?;
        let abi_str = fs::read_to_string(full_path)?;
        let abi: JsonAbi = serde_json::from_str(&abi_str)?;
        let event = abi
            .event(&event_name)
            .and_then(|v| v.first())
            .ok_or(FactoryDetailsFromAbiError::EventNotFoundError(event_name.clone()))?
            .clone();
        Ok(FactoryDetails { contract_name, address, input_name, event, indexed_filters })
    }
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct FilterDetails {
    pub events: ValueOrArray<String>,
    pub indexed_filters: Option<EventInputIndexedFilters>,
}
#[derive(Clone)]
pub enum IndexingContractSetup {
    Address(AddressDetails),
    Filter(FilterDetails),
    Factory(FactoryDetails),
}
impl IndexingContractSetup {
    pub fn is_filter(&self) -> bool {
        matches!(self, IndexingContractSetup::Filter(_))
    }
    pub fn factory_details(&self) -> Option<FactoryDetails> {
        match self {
            IndexingContractSetup::Factory(details) => Some(details.clone()),
            _ => None,
        }
    }
}
</file>

<file path="core/src/event/factory_event_filter_sync.rs">
use crate::database::clickhouse::client::ClickhouseError;
use crate::database::generate::generate_internal_factory_event_table_name_no_shorten;
use crate::database::postgres::client::PostgresError;
use crate::database::{
    generate::generate_internal_factory_event_table_name,
    postgres::generate::GenerateInternalFactoryEventTableNameParams,
};
use crate::event::callback_registry::EventResult;
use crate::event::config::FactoryEventProcessingConfig;
use crate::helpers::{get_full_path, parse_log};
use crate::manifest::storage::CsvDetails;
use crate::simple_file_formatters::csv::AsyncCsvReader;
use crate::{AsyncCsvAppender, ClickhouseClient, EthereumSqlTypeWrapper, PostgresClient};
use alloy::primitives::Address;
use mini_moka::sync::Cache;
use serde::Deserialize;
use std::collections::HashSet;
use std::hash::Hash;
use std::path::{Path, PathBuf};
use std::str::FromStr;
use std::sync::{Arc, OnceLock};
#[derive(Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
struct KnownFactoryDeployedAddress {
    factory_address: Address,
    address: Address,
}
#[derive(thiserror::Error, Debug)]
pub enum UpdateKnownFactoryDeployedAddressesError {
    #[error(transparent)]
    IO(#[from] std::io::Error),
    #[error("Could not write addresses to csv: {0}")]
    CsvWrite(#[from] csv::Error),
    #[error("Could not write addresses to postgres: {0}")]
    PostgresWrite(String),
    #[error("Could not write addresses to clickhouse: {0}")]
    ClickhouseWrite(#[from] ClickhouseError),
    #[error("Could not parse logs")]
    LogsParse,
}
#[derive(PartialEq, Eq, Hash)]
struct KnownFactoryDeployedAddressesCacheKey {
    contract_name: String,
    network: String,
    event_name: String,
    input_names: Vec<String>,
}
type FactoryDeployedAddressesCache = Cache<KnownFactoryDeployedAddressesCacheKey, HashSet<Address>>;
static IN_MEMORY_CACHE: OnceLock<Arc<FactoryDeployedAddressesCache>> = OnceLock::new();
fn get_in_memory_cache() -> &'static Arc<FactoryDeployedAddressesCache> {
    IN_MEMORY_CACHE.get_or_init(|| Arc::new(Cache::builder().build()))
}
fn build_known_factory_address_file(
    full_path: &Path,
    contract_name: &str,
    network: &str,
    event_name: &str,
    input_names: &[String],
) -> String {
    let path = full_path.join(contract_name).join("known-factory-addresses").join(format!(
        "{}-{}-{}-{}.csv",
        contract_name.to_lowercase(),
        network.to_lowercase(),
        event_name.to_lowercase(),
        input_names.iter().map(|v| v.to_lowercase()).collect::<Vec<String>>().join("-")
    ));
    path.to_string_lossy().into_owned()
}
fn get_known_factory_deployed_addresses_cache(
    key: &KnownFactoryDeployedAddressesCacheKey,
) -> Option<HashSet<Address>> {
    let cache = get_in_memory_cache();
    cache.get(key)
}
fn set_known_factory_deployed_addresses_cache(
    key: KnownFactoryDeployedAddressesCacheKey,
    value: HashSet<Address>,
) {
    let cache = get_in_memory_cache();
    cache.insert(key, value);
}
fn invalidate_known_factory_deployed_addresses_cache(key: &KnownFactoryDeployedAddressesCacheKey) {
    let cache = get_in_memory_cache();
    cache.invalidate(key);
}
pub async fn update_known_factory_deployed_addresses(
    config: &FactoryEventProcessingConfig,
    events: &[EventResult],
) -> Result<(), UpdateKnownFactoryDeployedAddressesError> {
    let addresses: HashSet<KnownFactoryDeployedAddress> = events
        .iter()
        .map(|event| {
            parse_log(&config.event, &event.log).and_then(|log| {
                config
                    .input_names()
                    .iter()
                    .map(|name| {
                        log.get_param_value(name).and_then(|value| value.as_address()).map(
                            |address| KnownFactoryDeployedAddress {
                                factory_address: event.tx_information.address,
                                address,
                            },
                        )
                    })
                    .collect::<Option<Vec<_>>>()
            })
        })
        .try_fold(HashSet::new(), |mut acc, items| match items {
            Some(items) => {
                acc.extend(items);
                Some(acc)
            }
            None => None,
        })
        .ok_or(UpdateKnownFactoryDeployedAddressesError::LogsParse)?;
    // invalidate in memory cache of factory addresses
    let key = KnownFactoryDeployedAddressesCacheKey {
        contract_name: config.contract_name.clone(),
        network: config.network_contract.network.clone(),
        event_name: config.event.name.clone(),
        input_names: config.input_names(),
    };
    invalidate_known_factory_deployed_addresses_cache(&key);
    if let Some(postgres) = &config.postgres {
        let params = GenerateInternalFactoryEventTableNameParams {
            indexer_name: config.indexer_name.clone(),
            contract_name: config.contract_name.clone(),
            event_name: config.event.name.clone(),
            input_names: config.input_names().clone(),
        };
        let table_name = generate_internal_factory_event_table_name(&params);
        postgres
            .insert_bulk(
                &format!("rindexer_internal.{table_name}"),
                &[
                    "factory_address".to_string(),
                    "factory_deployed_address".to_string(),
                    "network".to_string(),
                ],
                &addresses
                    .clone()
                    .into_iter()
                    .map(|item| {
                        vec![
                            EthereumSqlTypeWrapper::Address(item.factory_address),
                            EthereumSqlTypeWrapper::Address(item.address),
                            EthereumSqlTypeWrapper::String(config.network_contract.network.clone()),
                        ]
                    })
                    .collect::<Vec<_>>(),
            )
            .await
            .map_err(UpdateKnownFactoryDeployedAddressesError::PostgresWrite)?;
        return Ok(());
    }
    if let Some(clickhouse) = &config.clickhouse {
        let params = GenerateInternalFactoryEventTableNameParams {
            indexer_name: config.indexer_name.clone(),
            contract_name: config.contract_name.clone(),
            event_name: config.event.name.clone(),
            input_names: config.input_names().clone(),
        };
        let table_name = generate_internal_factory_event_table_name_no_shorten(&params);
        clickhouse
            .insert_bulk(
                &format!("rindexer_internal.{table_name}"),
                &[
                    "factory_address".to_string(),
                    "factory_deployed_address".to_string(),
                    "network".to_string(),
                ],
                &addresses
                    .clone()
                    .into_iter()
                    .map(|item| {
                        vec![
                            EthereumSqlTypeWrapper::Address(item.factory_address),
                            EthereumSqlTypeWrapper::Address(item.address),
                            EthereumSqlTypeWrapper::String(config.network_contract.network.clone()),
                        ]
                    })
                    .collect::<Vec<_>>(),
            )
            .await?;
        return Ok(());
    }
    if let Some(csv_details) = &config.csv_details {
        let full_path = get_full_path(&config.project_path, &csv_details.path)?;
        let csv_path = build_known_factory_address_file(
            &full_path,
            &config.contract_name,
            &config.network_contract.network,
            &config.event.name,
            &config.input_names(),
        );
        let csv_appender = AsyncCsvAppender::new(&csv_path);
        if !Path::new(&csv_path).exists() {
            csv_appender
                .append_header(vec![
                    "factory_address".to_string(),
                    "factory_deployed_address".to_string(),
                ])
                .await?;
        }
        csv_appender
            .append_bulk(
                addresses
                    .iter()
                    .map(|item| vec![item.factory_address.to_string(), item.address.to_string()])
                    .collect::<Vec<_>>(),
            )
            .await?;
        return Ok(());
    }
    unreachable!("Can't update known factory deployed addresses without database or csv details")
}
#[derive(thiserror::Error, Debug)]
pub enum GetKnownFactoryDeployedAddressesError {
    #[error(transparent)]
    IO(#[from] std::io::Error),
    #[error("Could not read addresses from csv: {0}")]
    CsvRead(#[from] csv::Error),
    #[error("Could not read addresses from postgres: {0}")]
    PostgresRead(#[from] PostgresError),
    #[error("Could not read addresses from clickhouse: {0}")]
    ClickhouseRead(#[from] clickhouse::error::Error),
}
#[derive(Clone)]
pub struct GetKnownFactoryDeployedAddressesParams {
    pub project_path: PathBuf,
    pub indexer_name: String,
    pub contract_name: String,
    pub event_name: String,
    pub input_names: Vec<String>,
    pub network: String,
    pub postgres: Option<Arc<PostgresClient>>,
    pub clickhouse: Option<Arc<ClickhouseClient>>,
    pub csv_details: Option<CsvDetails>,
}
pub async fn get_known_factory_deployed_addresses(
    params: &GetKnownFactoryDeployedAddressesParams,
) -> Result<Option<HashSet<Address>>, GetKnownFactoryDeployedAddressesError> {
    // check cache first
    let key = KnownFactoryDeployedAddressesCacheKey {
        contract_name: params.contract_name.clone(),
        network: params.network.clone(),
        event_name: params.event_name.clone(),
        input_names: params.input_names.clone(),
    };
    if let Some(cache) = get_known_factory_deployed_addresses_cache(&key) {
        return Ok(Some(cache));
    }
    if let Some(database) = &params.postgres {
        let table_params = GenerateInternalFactoryEventTableNameParams {
            indexer_name: params.indexer_name.clone(),
            contract_name: params.contract_name.clone(),
            event_name: params.event_name.clone(),
            input_names: params.input_names.clone(),
        };
        let table_name = generate_internal_factory_event_table_name(&table_params);
        let query = format!(
            "SELECT factory_deployed_address FROM rindexer_internal.{table_name} WHERE network = $1"
        );
        let result = database
            .query(&query, &[&EthereumSqlTypeWrapper::String(params.network.clone())])
            .await?;
        let values = result
            .into_iter()
            .map(|row| {
                Address::from_str(row.get("factory_deployed_address"))
                    .expect("Factory deployed address not a valid ethereum address")
            })
            .collect::<HashSet<_>>();
        set_known_factory_deployed_addresses_cache(key, values.clone());
        return Ok(Some(values));
    }
    if let Some(database) = &params.clickhouse {
        let table_params = GenerateInternalFactoryEventTableNameParams {
            indexer_name: params.indexer_name.clone(),
            contract_name: params.contract_name.clone(),
            event_name: params.event_name.clone(),
            input_names: params.input_names.clone(),
        };
        let table_name = generate_internal_factory_event_table_name_no_shorten(&table_params);
        let query = format!(
            r#"
            SELECT toString(factory_deployed_address) AS factory_deployed_address
            FROM rindexer_internal.{table_name} FINAL
            WHERE network = ?
            "#
        );
        #[derive(Debug, clickhouse::Row, Deserialize)]
        struct FactoryDeployedAddresses {
            factory_deployed_address: String,
        }
        let result: Vec<FactoryDeployedAddresses> =
            database.conn.query(&query).bind(params.network.clone()).fetch_all().await?;
        let values = result
            .into_iter()
            .map(|row| {
                Address::from_str(&row.factory_deployed_address)
                    .expect("Factory deployed address not a valid ethereum address")
            })
            .collect::<HashSet<_>>();
        set_known_factory_deployed_addresses_cache(key, values.clone());
        return Ok(Some(values));
    }
    if let Some(csv_details) = &params.csv_details {
        let full_path = get_full_path(&params.project_path, &csv_details.path)?;
        let csv_path = build_known_factory_address_file(
            &full_path,
            &params.contract_name,
            &params.network,
            &params.event_name,
            &params.input_names,
        );
        if !Path::new(&csv_path).exists() {
            return Ok(None);
        }
        let csv_reader = AsyncCsvReader::new(&csv_path);
        let data = csv_reader.read_all().await?;
        // extracting only 'factory_deployed_address' from the csv row
        let values = data
            .into_iter()
            .map(|row| {
                row[1]
                    .parse::<Address>()
                    .expect("Factory deployed address not a valid ethereum address")
            })
            .collect::<HashSet<_>>();
        set_known_factory_deployed_addresses_cache(key, values.clone());
        return Ok(Some(values));
    }
    unreachable!("Can't get known factory deployed addresses without database or csv details")
}
</file>

<file path="core/src/event/message.rs">
use alloy::primitives::B256;
use serde::{Deserialize, Serialize};
use serde_json::Value;
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct EventMessage {
    pub event_name: String,
    pub event_data: Value,
    pub event_signature_hash: B256,
    pub network: String,
}
</file>

<file path="core/src/event/mod.rs">
pub mod callback_registry;
pub mod config;
pub mod contract_setup;
mod rindexer_event_filter;
pub use rindexer_event_filter::{BuildRindexerFilterError, RindexerEventFilter};
mod message;
pub use message::EventMessage;
mod factory_event_filter_sync;
mod filter;
pub use filter::{filter_by_expression, filter_event_data_by_conditions};
</file>

<file path="core/src/event/rindexer_event_filter.rs">
use crate::event::contract_setup::{AddressDetails, FilterDetails};
use crate::event::factory_event_filter_sync::{
    get_known_factory_deployed_addresses, GetKnownFactoryDeployedAddressesParams,
};
use crate::manifest::storage::CsvDetails;
use crate::{ClickhouseClient, PostgresClient};
use alloy::rpc::types::Topic;
use alloy::{
    primitives::{Address, B256, U64},
    rpc::types::ValueOrArray,
};
use std::collections::HashSet;
use std::path::PathBuf;
use std::sync::Arc;
#[derive(thiserror::Error, Debug)]
pub enum BuildRindexerFilterError {
    #[error("Address is valid format")]
    AddressInvalidFormat,
}
#[derive(Clone, Debug)]
pub struct SimpleEventFilter {
    pub address: Option<ValueOrArray<Address>>,
    pub topic_id: B256,
    pub topics: [Topic; 4],
    pub current_block: U64,
    pub next_block: U64,
}
impl SimpleEventFilter {
    fn set_from_block(mut self, block: U64) -> Self {
        self.current_block = block;
        self
    }
    fn set_to_block(mut self, block: U64) -> Self {
        self.next_block = block;
        self
    }
    fn contract_address(&self) -> Option<HashSet<Address>> {
        self.address.as_ref().map(|address| match address {
            ValueOrArray::Value(address) => HashSet::from([*address]),
            ValueOrArray::Array(addresses) => addresses.iter().copied().collect(),
        })
    }
}
#[derive(Clone)]
pub struct FactoryFilter {
    pub project_path: PathBuf,
    pub indexer_name: String,
    pub factory_address: ValueOrArray<Address>,
    pub factory_contract_name: String,
    pub factory_event_name: String,
    pub factory_input_name: ValueOrArray<String>,
    pub network: String,
    pub topic_id: B256,
    pub topics: [Topic; 4],
    pub clickhouse: Option<Arc<ClickhouseClient>>,
    pub postgres: Option<Arc<PostgresClient>>,
    pub csv_details: Option<CsvDetails>,
    pub current_block: U64,
    pub next_block: U64,
}
impl std::fmt::Debug for FactoryFilter {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("FactoryFilter")
            .field("project_path", &self.project_path)
            .field("indexer_name", &self.indexer_name)
            .field("factory_address", &self.factory_address)
            .field("factory_contract_name", &self.factory_contract_name)
            .field("factory_event_name", &self.factory_event_name)
            .field("factory_input_names", &self.factory_input_name)
            .field("network", &self.network)
            .field("topic_id", &self.topic_id)
            .field("current_block", &self.current_block)
            .field("next_block", &self.next_block)
            .finish()
    }
}
impl FactoryFilter {
    fn set_from_block(mut self, block: U64) -> Self {
        self.current_block = block;
        self
    }
    fn set_to_block(mut self, block: U64) -> Self {
        self.next_block = block;
        self
    }
    async fn contract_address(&self) -> Option<HashSet<Address>> {
        let input_names = match &self.factory_input_name {
            ValueOrArray::Value(name) => vec![name.clone()],
            ValueOrArray::Array(names) => names.clone(),
        };
        get_known_factory_deployed_addresses(&GetKnownFactoryDeployedAddressesParams {
            project_path: self.project_path.clone(),
            indexer_name: self.indexer_name.clone(),
            contract_name: self.factory_contract_name.clone(),
            event_name: self.factory_event_name.clone(),
            input_names,
            network: self.network.clone(),
            clickhouse: self.clickhouse.clone(),
            postgres: self.postgres.clone(),
            csv_details: self.csv_details.clone(),
        })
        .await
        .inspect_err(|e| tracing::error!("Failed to get known factory deployed addresses: {}", e))
        .expect("Failed to get known factory deployed addresses")
    }
}
#[derive(Debug, Clone)]
pub enum RindexerEventFilter {
    Address(SimpleEventFilter),
    Filter(SimpleEventFilter),
    Factory(FactoryFilter),
}
impl RindexerEventFilter {
    pub fn new_address_filter(
        topic_id: &B256,
        event_name: &str,
        address_details: &AddressDetails,
        current_block: U64,
        next_block: U64,
    ) -> Result<RindexerEventFilter, BuildRindexerFilterError> {
        let index_filter = address_details.indexed_filters.iter().find_map(|indexed_filters| {
            indexed_filters.iter().find(|&n| n.event_name == event_name)
        });
        Ok(RindexerEventFilter::Filter(SimpleEventFilter {
            address: Some(address_details.address.clone()),
            topic_id: *topic_id,
            topics: index_filter
                .map(|indexed_filter| indexed_filter.clone().into())
                .unwrap_or_default(),
            current_block,
            next_block,
        }))
    }
    pub fn new_filter(
        topic_id: &B256,
        _: &str,
        filter_details: &FilterDetails,
        current_block: U64,
        next_block: U64,
    ) -> Result<RindexerEventFilter, BuildRindexerFilterError> {
        Ok(RindexerEventFilter::Filter(SimpleEventFilter {
            address: None,
            topic_id: *topic_id,
            topics: filter_details
                .clone()
                .indexed_filters
                .map(|indexed_filter| indexed_filter.clone().into())
                .unwrap_or_default(),
            current_block,
            next_block,
        }))
    }
    pub fn event_signature(&self) -> B256 {
        match self {
            RindexerEventFilter::Address(filter) => filter.topic_id,
            RindexerEventFilter::Filter(filter) => filter.topic_id,
            RindexerEventFilter::Factory(filter) => filter.topic_id,
        }
    }
    pub fn topic1(&self) -> Topic {
        match self {
            RindexerEventFilter::Address(filter) => filter.topics[1].clone(),
            RindexerEventFilter::Filter(filter) => filter.topics[1].clone(),
            RindexerEventFilter::Factory(filter) => filter.topics[1].clone(),
        }
    }
    pub fn topic2(&self) -> Topic {
        match self {
            RindexerEventFilter::Address(filter) => filter.topics[2].clone(),
            RindexerEventFilter::Filter(filter) => filter.topics[2].clone(),
            RindexerEventFilter::Factory(filter) => filter.topics[2].clone(),
        }
    }
    pub fn topic3(&self) -> Topic {
        match self {
            RindexerEventFilter::Address(filter) => filter.topics[3].clone(),
            RindexerEventFilter::Filter(filter) => filter.topics[3].clone(),
            RindexerEventFilter::Factory(filter) => filter.topics[3].clone(),
        }
    }
    pub fn to_block(&self) -> U64 {
        match self {
            RindexerEventFilter::Address(filter) => filter.next_block,
            RindexerEventFilter::Filter(filter) => filter.next_block,
            RindexerEventFilter::Factory(filter) => filter.next_block,
        }
    }
    pub fn from_block(&self) -> U64 {
        match self {
            RindexerEventFilter::Address(filter) => filter.current_block,
            RindexerEventFilter::Filter(filter) => filter.current_block,
            RindexerEventFilter::Factory(filter) => filter.current_block,
        }
    }
    pub fn set_from_block<R: Into<U64>>(self, block: R) -> Self {
        let block = block.into();
        match self {
            Self::Address(filter) => Self::Address(filter.set_from_block(block)),
            Self::Filter(filter) => Self::Filter(filter.set_from_block(block)),
            Self::Factory(filter) => Self::Factory(filter.set_from_block(block)),
        }
    }
    pub fn set_to_block<R: Into<U64>>(self, block: R) -> Self {
        match self {
            RindexerEventFilter::Address(filter) => {
                RindexerEventFilter::Address(filter.set_to_block(block.into()))
            }
            RindexerEventFilter::Filter(filter) => {
                RindexerEventFilter::Filter(filter.set_to_block(block.into()))
            }
            RindexerEventFilter::Factory(filter) => {
                RindexerEventFilter::Factory(filter.set_to_block(block.into()))
            }
        }
    }
    pub async fn contract_addresses(&self) -> Option<HashSet<Address>> {
        match self {
            RindexerEventFilter::Address(filter) => filter.contract_address(),
            RindexerEventFilter::Filter(filter) => filter.contract_address(),
            RindexerEventFilter::Factory(filter) => filter.contract_address().await,
        }
    }
}
</file>

<file path="core/src/generator/build.rs">
use std::{
    fs,
    path::{Path, PathBuf},
};
use super::{
    context_bindings::generate_context_code,
    events_bindings::{
        abigen_contract_file_name, abigen_contract_name, generate_event_bindings,
        generate_event_handlers, GenerateEventBindingsError, GenerateEventHandlersError,
    },
    networks_bindings::generate_networks_code,
};
use crate::manifest::contract::Contract;
use crate::{
    generator::database_bindings::{generate_clickhouse_code, generate_postgres_code},
    generator::trace_bindings::{
        generate_trace_bindings, generate_trace_handlers, trace_abigen_contract_file_name,
        GenerateTraceBindingsError, GenerateTraceHandlersError,
    },
    helpers::{
        camel_to_snake, create_mod_file, format_all_files_for_project, write_file,
        CreateModFileError, WriteFileError,
    },
    indexer::{
        native_transfer::{NATIVE_TRANSFER_ABI, NATIVE_TRANSFER_CONTRACT_NAME},
        Indexer,
    },
    manifest::{
        contract::ParseAbiError,
        core::Manifest,
        network::Network,
        storage::Storage,
        yaml::{read_manifest, ReadManifestError, YAML_CONFIG_NAME},
    },
    types::code::Code,
};
fn generate_file_location(output: &Path, location: &str) -> PathBuf {
    let mut path = PathBuf::from(output);
    path.push(format!("{location}.rs"));
    path
}
#[derive(thiserror::Error, Debug)]
pub enum WriteNetworksError {
    #[error("{0}")]
    CanNotWriteNetworksCode(#[from] WriteFileError),
}
fn write_networks(output: &Path, networks: &[Network]) -> Result<(), WriteNetworksError> {
    let networks_code = generate_networks_code(networks);
    write_file(&generate_file_location(output, "networks"), networks_code.as_str())?;
    Ok(())
}
#[derive(thiserror::Error, Debug)]
pub enum WriteGlobalError {
    #[error("{0}")]
    CanNotWriteGlobalCode(#[from] WriteFileError),
    #[error("{0}")]
    CouldNotDeleteGlobalContractFile(#[from] std::io::Error),
}
fn write_global(
    output: &Path,
    global_contracts: &[Contract],
    networks: &[Network],
) -> Result<(), WriteGlobalError> {
    let global_contract_file_path = generate_file_location(output, "global_contracts");
    if global_contract_file_path.exists() {
        fs::remove_file(&global_contract_file_path)?;
    }
    let context_code = generate_context_code(global_contracts, networks);
    write_file(&global_contract_file_path, context_code.as_str())?;
    Ok(())
}
#[derive(thiserror::Error, Debug)]
pub enum WriteIndexerEvents {
    #[error("Could not write events code: {0}")]
    CouldNotWriteEventsCode(#[from] WriteFileError),
    #[error("Could not read ABI JSON: {0}")]
    CouldNotReadAbiJson(#[from] serde_json::Error),
    #[error("Could not generate Abigen instance")]
    CouldNotCreateAbigenInstance,
    #[error("Could not generate ABI")]
    CouldNotGenerateAbi,
    #[error("Could not write abigen code: {0}")]
    CouldNotWriteAbigenCodeCode(WriteFileError),
    #[error("{0}")]
    GenerateEventBindingCodeError(#[from] GenerateEventBindingsError),
    #[error("{0}")]
    GenerateTraceBindingCodeError(#[from] GenerateTraceBindingsError),
    #[error("Could not parse ABI: {0}")]
    CouldNotParseAbi(#[from] ParseAbiError),
}
fn write_indexer_events(
    project_path: &Path,
    output: &Path,
    indexer: Indexer,
    storage: &Storage,
) -> Result<(), WriteIndexerEvents> {
    for mut contract in indexer.contracts {
        let is_filter = contract.identify_and_modify_filter();
        let events_code =
            generate_event_bindings(project_path, &indexer.name, &contract, is_filter, storage)?;
        let event_path =
            format!("{}/events/{}", camel_to_snake(&indexer.name), camel_to_snake(&contract.name));
        write_file(&generate_file_location(output, &event_path), events_code.as_str())?;
        let abi_string = contract.parse_abi(project_path)?;
        let code = format!(
            r##"
            use alloy::sol;
            sol!(
                #[sol(rpc, all_derives)]
                {contract_name},
                r#"{contract_path}"#
            );
            "##,
            contract_name = abigen_contract_name(&contract),
            contract_path = abi_string,
        );
        let code = Code::new(code);
        write_file(
            &generate_file_location(
                output,
                &format!(
                    "{}/events/{}",
                    camel_to_snake(&indexer.name),
                    abigen_contract_file_name(&contract)
                ),
            ),
            &code.to_string(),
        )
        .map_err(WriteIndexerEvents::CouldNotWriteAbigenCodeCode)?;
    }
    if indexer.native_transfers.enabled {
        let events_code = generate_trace_bindings(
            project_path,
            &indexer.name,
            NATIVE_TRANSFER_CONTRACT_NAME,
            &indexer.native_transfers,
            false,
            storage,
        )?;
        let event_path = format!(
            "{}/events/{}",
            camel_to_snake(&indexer.name),
            camel_to_snake(NATIVE_TRANSFER_CONTRACT_NAME)
        );
        write_file(&generate_file_location(output, &event_path), events_code.as_str())?;
        let abi_string = NATIVE_TRANSFER_ABI;
        let abigen_contract_name = format!("Rindexer{NATIVE_TRANSFER_CONTRACT_NAME}Gen");
        let code = format!(
            r##"
            use alloy::sol;
            sol!(
                #[sol(rpc, all_derives)]
                {abigen_contract_name},
                r#"{abi_string}"#
            );
            "##,
        );
        let code = Code::new(code);
        write_file(
            &generate_file_location(
                output,
                &format!(
                    "{}/events/{}",
                    camel_to_snake(&indexer.name),
                    trace_abigen_contract_file_name(NATIVE_TRANSFER_CONTRACT_NAME)
                ),
            ),
            &code.to_string(),
        )
        .map_err(WriteIndexerEvents::CouldNotWriteAbigenCodeCode)?;
    }
    Ok(())
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateRindexerTypingsError {
    #[error("Manifest location does not have a parent - {0}")]
    ManifestLocationDoesNotHaveAParent(String),
    #[error("Manifest location can not be resolved")]
    ManifestLocationCanNotBeResolved,
    #[error("{0}")]
    WriteNetworksError(#[from] WriteNetworksError),
    #[error("{0}")]
    WriteGlobalError(#[from] WriteGlobalError),
    #[error("{0}")]
    WriteIndexerEventsError(#[from] WriteIndexerEvents),
    #[error("{0}")]
    CreateModFileError(#[from] CreateModFileError),
}
pub fn generate_rindexer_typings(
    manifest: &Manifest,
    manifest_location: &Path,
    format_after_generation: bool,
) -> Result<(), GenerateRindexerTypingsError> {
    let project_path = manifest_location.parent();
    match project_path {
        Some(project_path) => {
            let output = project_path.join("./src/rindexer_lib/typings");
            write_networks(&output, &manifest.networks)?;
            if let Some(global_contracts) = &manifest.global.contracts {
                write_global(&output, global_contracts, &manifest.networks)?;
            }
            if manifest.storage.postgres_enabled() {
                write_file(
                    &generate_file_location(&output, "database"),
                    generate_postgres_code().as_str(),
                )
                .map_err(WriteGlobalError::from)?;
            }
            if manifest.storage.clickhouse_enabled() {
                write_file(
                    &generate_file_location(&output, "database"),
                    generate_clickhouse_code().as_str(),
                )
                .map_err(WriteGlobalError::from)?;
            }
            write_indexer_events(project_path, &output, manifest.to_indexer(), &manifest.storage)?;
            create_mod_file(output.as_path(), true)?;
            if format_after_generation {
                format_all_files_for_project(project_path);
            }
            Ok(())
        }
        None => {
            let manifest_location = manifest_location.to_str();
            match manifest_location {
                Some(manifest_location) => {
                    Err(GenerateRindexerTypingsError::ManifestLocationDoesNotHaveAParent(
                        manifest_location.to_string(),
                    ))
                }
                None => Err(GenerateRindexerTypingsError::ManifestLocationCanNotBeResolved),
            }
        }
    }
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateRindexerHandlersError {
    #[error("Manifest location does not have a parent")]
    ManifestLocationDoesNotHaveAParent,
    #[error("Could not read ABI string: {0}")]
    CouldNotReadAbiString(#[from] std::io::Error),
    #[error("Could not read ABI JSON: {0}")]
    CouldNotReadAbiJson(#[from] serde_json::Error),
    #[error("{0}")]
    GenerateEventBindingCodeError(#[from] GenerateEventHandlersError),
    #[error("{0}")]
    GenerateTraceBindingCodeError(#[from] GenerateTraceHandlersError),
    #[error("Could not write event handler code: {0}")]
    CouldNotWriteEventHandlerCode(#[from] WriteFileError),
    #[error("Could not write event handlers code: {0}")]
    CouldNotWriteEventHandlersCode(WriteFileError),
    #[error("{0}")]
    CreateModFileError(#[from] CreateModFileError),
}
pub fn generate_rindexer_handlers(
    manifest: Manifest,
    manifest_location: &Path,
    format_after_generation: bool,
) -> Result<(), GenerateRindexerHandlersError> {
    let project_path = manifest_location.parent();
    match project_path {
        None => Err(GenerateRindexerHandlersError::ManifestLocationDoesNotHaveAParent),
        Some(project_path) => {
            let output = project_path.join("./src/rindexer_lib");
            let mut handlers = String::new();
            handlers.push_str(
                r#"
            use std::path::PathBuf;
            use rindexer::event::callback_registry::EventCallbackRegistry;
            "#,
            );
            if manifest.native_transfers.enabled {
                handlers.push_str(
                    r#"
                use rindexer::event::callback_registry::TraceCallbackRegistry;
                "#,
                )
            }
            handlers.push_str(
                r#"
            pub async fn register_all_handlers(manifest_path: &PathBuf) -> EventCallbackRegistry {
                 let mut registry = EventCallbackRegistry::new();
            "#,
            );
            let indexer = manifest.to_indexer();
            for mut contract in indexer.contracts {
                let is_filter = contract.identify_and_modify_filter();
                let indexer_name = camel_to_snake(&manifest.name);
                let contract_name = camel_to_snake(&contract.name);
                let handler_fn_name = format!("{contract_name}_handlers");
                handlers.insert_str(
                    0,
                    &format!(r#"use super::{indexer_name}::{contract_name}::{handler_fn_name};"#,),
                );
                handlers.push_str(&format!(
                    r#"{handler_fn_name}(manifest_path, &mut registry).await;"#
                ));
                let handler_path = format!("indexers/{indexer_name}/{contract_name}");
                write_file(
                    &generate_file_location(&output, &handler_path),
                    generate_event_handlers(
                        project_path,
                        &manifest.name,
                        is_filter,
                        &contract,
                        &manifest.storage,
                    )?
                    .as_str(),
                )?;
            }
            if manifest.native_transfers.enabled {
                handlers.push_str("let mut trace_registry = TraceCallbackRegistry::new();");
                let indexer_name = camel_to_snake(&manifest.name);
                let contract_name = camel_to_snake(NATIVE_TRANSFER_CONTRACT_NAME);
                let handler_fn_name = format!("{contract_name}_handlers");
                handlers.insert_str(
                    0,
                    &format!(r#"use super::{indexer_name}::{contract_name}::{handler_fn_name};"#,),
                );
                handlers.push_str(&format!(
                    r#"{handler_fn_name}(manifest_path, &mut registry).await;"#
                ));
                let handler_path = format!("indexers/{indexer_name}/{contract_name}");
                write_file(
                    &generate_file_location(&output, &handler_path),
                    generate_trace_handlers(&indexer_name, &contract_name, &manifest.storage)?
                        .as_str(),
                )?;
            }
            handlers.push_str("registry");
            handlers.push('}');
            write_file(&generate_file_location(&output, "indexers/all_handlers"), &handlers)
                .map_err(GenerateRindexerHandlersError::CouldNotWriteEventHandlersCode)?;
            create_mod_file(output.as_path(), false)?;
            if format_after_generation {
                format_all_files_for_project(project_path);
            }
            Ok(())
        }
    }
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateError {
    #[error("{0}")]
    ReadManifestError(#[from] ReadManifestError),
    #[error("{0}")]
    GenerateRindexerTypingsError(#[from] GenerateRindexerTypingsError),
    #[error("{0}")]
    GenerateRindexerHandlersError(#[from] GenerateRindexerHandlersError),
    #[error("Manifest location does not have a parent - {0}")]
    ManifestLocationDoesNotHaveAParent(String),
    #[error("Manifest location can not be resolved")]
    ManifestLocationCanNotBeResolved,
}
/// Generates all the rindexer project typings and handlers
pub fn generate_rindexer_typings_and_handlers(
    manifest_location: &PathBuf,
) -> Result<(), GenerateError> {
    let manifest = read_manifest(manifest_location)?;
    generate_rindexer_typings(&manifest, manifest_location, false)?;
    generate_rindexer_handlers(manifest, manifest_location, false)?;
    let parent = manifest_location.parent();
    match parent {
        Some(parent) => {
            format_all_files_for_project(parent);
            Ok(())
        }
        None => {
            let manifest_location = manifest_location.to_str();
            match manifest_location {
                Some(manifest_location) => Err(GenerateError::ManifestLocationDoesNotHaveAParent(
                    manifest_location.to_string(),
                )),
                None => Err(GenerateError::ManifestLocationCanNotBeResolved),
            }
        }
    }
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateRustProjectError {
    #[error("{0}")]
    ReadManifestError(#[from] ReadManifestError),
    #[error("Could not create the dir :{0}")]
    CouldNotCreateDir(#[from] std::io::Error),
    #[error("Could not write the file: {0}")]
    WriteFileError(#[from] WriteFileError),
    #[error("{0}")]
    GenerateError(#[from] GenerateError),
}
pub fn generate_rust_project(
    project_path: &Path,
    is_reth_project: bool,
) -> Result<(), GenerateRustProjectError> {
    let manifest_location = project_path.join(YAML_CONFIG_NAME);
    let manifest = read_manifest(&project_path.join(&manifest_location))?;
    let abi_path = project_path.join("abis");
    fs::create_dir_all(abi_path)?;
    let reth_dep = if is_reth_project { ", features = [\"reth\"]" } else { "" };
    let cargo = format!(
        r#"
[package]
name = "{project_name}"
version = "0.1.0"
edition = "2021"
[dependencies]
rindexer = {{ git = "https://github.com/joshstevens19/rindexer", branch = "master" {reth_dep}}}
tokio = {{ version = "1", features = ["full"] }}
alloy = {{ version = "1.0.41", features = ["full"] }}
serde = {{ version = "1.0", features = ["derive"] }}
"#,
        project_name = manifest.name,
        reth_dep = reth_dep,
    );
    let cargo_path = project_path.join("Cargo.toml");
    write_file(&cargo_path, &cargo)?;
    fs::create_dir_all(project_path.join("src"))?;
    let main_code = r#"
            use std::env;
            use self::rindexer_lib::indexers::all_handlers::register_all_handlers;
            use rindexer::{
                event::callback_registry::TraceCallbackRegistry,
                start_rindexer, GraphqlOverrideSettings, IndexingDetails, StartDetails,
            };
            mod rindexer_lib;
            #[tokio::main]
            async fn main() {
                let args: Vec<String> = env::args().collect();
                let mut enable_graphql = false;
                let mut enable_indexer = false;
                let mut port: Option<u16> = None;
                let args = args.iter();
                if args.len() == 1 {
                    enable_graphql = true;
                    enable_indexer = true;
                }
                for arg in args {
                    match arg.as_str() {
                        "--graphql" => enable_graphql = true,
                        "--indexer" => enable_indexer = true,
                        _ if arg.starts_with("--port=") || arg.starts_with("--p") => {
                            if let Some(value) = arg.split('=').nth(1) {
                                let overridden_port = value.parse::<u16>();
                                match overridden_port {
                                    Ok(overridden_port) => port = Some(overridden_port),
                                    Err(_) => {
                                        println!("Invalid port number");
                                        return;
                                    }
                                }
                            }
                        },
                        _ => {}
                    }
                }
                let path = env::current_dir();
                match path {
                    Ok(path) => {
                        let manifest_path = path.join("rindexer.yaml");
                        let result = start_rindexer(StartDetails {
                            manifest_path: &manifest_path,
                            indexing_details: if enable_indexer {
                                Some(IndexingDetails {
                                    registry: register_all_handlers(&manifest_path).await,
                                    trace_registry: TraceCallbackRegistry { events: vec![] },
                                    event_stream: None,
                                })
                            } else {
                                None
                            },
                            graphql_details: GraphqlOverrideSettings {
                                enabled: enable_graphql,
                                override_port: port,
                            }
                        })
                        .await;
                        match result {
                            Ok(_) => {}
                            Err(e) => {
                                println!("Error starting rindexer: {:?}", e);
                            }
                        }
                    }
                    Err(e) => {
                        println!("Error getting current directory: {:?}", e);
                    }
                }
            }
          "#;
    let main_path = project_path.join("src").join("main.rs");
    write_file(&main_path, main_code)?;
    generate_rindexer_typings_and_handlers(&manifest_location)
        .map_err(GenerateRustProjectError::GenerateError)
}
</file>

<file path="core/src/generator/context_bindings.rs">
use alloy::rpc::types::ValueOrArray;
use super::networks_bindings::network_provider_fn_name;
use crate::{
    helpers::{camel_to_snake, to_pascal_case},
    manifest::{
        contract::{Contract, ContractDetails},
        network::Network,
    },
    types::code::Code,
    StringOrArray,
};
fn generate_contract_code(
    contract_name: &str,
    contract_details: &ContractDetails,
    abi_location: &str,
    network: &Network,
) -> Code {
    if let Some(address) = contract_details.address() {
        match address {
            ValueOrArray::Value(address) => {
                let contract_address = format!("{address:?}");
                let code = format!(
                    r#"
                        sol!(
                            #[sol(rpc, all_derives)]
                            {contract_name}{network_suffix},
                            "{contract_path}"
                        );
                        pub async fn {contract_fn_name}_{network_fn_name}_contract() -> {contract_name}{network_suffix}::{contract_name}{network_suffix}Instance<Arc<RindexerProvider>, AnyNetwork> {{
                            let address: Address = "{contract_address}"
                                .parse()
                                .expect("Invalid address");
                            {contract_name}{network_suffix}::new(address, {network_fn_name}().await.clone())
                        }}
                    "#,
                    contract_name = contract_name,
                    network_suffix = to_pascal_case(&network.name),
                    contract_fn_name = camel_to_snake(contract_name),
                    contract_address = contract_address,
                    network_fn_name = network_provider_fn_name(network),
                    contract_path = abi_location
                );
                Code::new(code)
            }
            // let them pass in the address
            ValueOrArray::Array(_) => {
                let code = format!(
                    r#"
                         sol!(
                            #[sol(rpc, all_derives)]
                            {contract_name}{network_suffix},
                            "{contract_path}"
                        );
                        pub fn {contract_fn_name}_contract(address: Address) -> {contract_name}{network_suffix}::{contract_name}{network_suffix}Instance<Arc<RindexerProvider>, AnyNetwork> {{
                            {contract_name}{network_suffix}::new(address, {network_fn_name}().clone())
                        }}
                    "#,
                    contract_name = contract_name,
                    network_suffix = to_pascal_case(&network.name),
                    contract_fn_name = camel_to_snake(contract_name),
                    network_fn_name = network_provider_fn_name(network),
                    contract_path = abi_location
                );
                Code::new(code)
            }
        }
    } else {
        Code::blank()
    }
}
fn generate_contracts_code(contracts: &[Contract], networks: &[Network]) -> Code {
    let network_imports: Vec<String> = networks.iter().map(network_provider_fn_name).collect();
    let mut output = Code::new(format!(
        r#"
        /// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
        ///
        /// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
        /// Any manual changes to this file will be overwritten.
        use super::networks::{{{}}};
        use std::sync::Arc;
        use alloy::network::AnyNetwork;
        use rindexer::provider::RindexerProvider;
        use alloy::primitives::Address;
        use alloy::sol;
        "#,
        network_imports.join(", ")
    ));
    let mut code = Code::blank();
    for contract in contracts {
        for details in &contract.details {
            if let Some(network) = networks.iter().find(|&n| n.name == details.network) {
                if let StringOrArray::Single(abi_path) = &contract.abi {
                    code.push_str(&generate_contract_code(
                        &contract.name,
                        details,
                        abi_path,
                        network,
                    ));
                } else {
                    panic!("Multiple ABIs not supported yet on global contracts");
                }
            }
        }
    }
    output.push_str(&code);
    output
}
pub fn generate_context_code(contracts: &[Contract], networks: &[Network]) -> Code {
    generate_contracts_code(contracts, networks)
}
</file>

<file path="core/src/generator/database_bindings.rs">
use crate::types::code::Code;
pub fn generate_postgres_code() -> Code {
    Code::new(
        r#"
    use std::sync::Arc;
    use rindexer::PostgresClient;
    use tokio::sync::OnceCell;
    static POSTGRES_CLIENT: OnceCell<Arc<PostgresClient>> = OnceCell::const_new();
    pub async fn get_or_init_postgres_client() -> Arc<PostgresClient> {
        POSTGRES_CLIENT
            .get_or_init(|| async {
                Arc::new(PostgresClient::new().await.expect("Failed to connect to Postgres"))
            })
            .await
            .clone()
    }
    "#
        .to_string(),
    )
}
pub fn generate_clickhouse_code() -> Code {
    Code::new(
        r#"
    use std::sync::Arc;
    use rindexer::ClickhouseClient;
    use tokio::sync::OnceCell;
    static CLICKHOUSE_CLIENT: OnceCell<Arc<ClickhouseClient>> = OnceCell::const_new();
    pub async fn get_or_init_clickhouse_client() -> Arc<ClickhouseClient> {
        CLICKHOUSE_CLIENT
            .get_or_init(|| async {
                Arc::new(ClickhouseClient::new().await.expect("Failed to connect to Clickhouse"))
            })
            .await
            .clone()
    }
    "#
        .to_string(),
    )
}
</file>

<file path="core/src/generator/docker.rs">
pub fn generate_docker_file() -> &'static str {
    r#"volumes:
  postgres_data:
    driver: local
services:
  postgresql:
    image: postgres:16
    shm_size: 1g
    restart: always
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - 5440:5432
    env_file:
      - ./.env
 "#
}
</file>

<file path="core/src/generator/events_bindings.rs">
use std::path::{Path, PathBuf};
use alloy::rpc::types::ValueOrArray;
use serde_json::Value;
use crate::abi::AbiProperty;
use crate::helpers::{is_irregular_width_solidity_integer_type, is_solidity_static_bytes_type};
use crate::{
    abi::{
        ABIInput, ABIItem, CreateCsvFileForEvent, EventInfo, GenerateAbiPropertiesType,
        ParamTypeError, ReadAbiError,
    },
    database::{
        generate::generate_event_table_full_name,
        postgres::generate::generate_column_names_only_with_base_properties,
    },
    helpers::camel_to_snake,
    manifest::{
        contract::{Contract, ContractDetails, ParseAbiError},
        storage::{CsvDetails, Storage},
    },
    types::code::Code,
    EthereumSqlTypeWrapper,
};
pub fn abigen_contract_name(contract: &Contract) -> String {
    format!("Rindexer{}Gen", contract.name)
}
pub fn abigen_contract_file_name(contract: &Contract) -> String {
    format!("{}_abi_gen", camel_to_snake(&contract.name))
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateStructsError {
    #[error("Could not read ABI string: {0}")]
    CouldNotReadAbiString(#[from] std::io::Error),
    #[error("Could not read ABI JSON: {0}")]
    CouldNotReadAbiJson(#[from] serde_json::Error),
    #[error("Invalid ABI JSON format")]
    InvalidAbiJsonFormat,
    #[error("{0}")]
    ParseAbiError(#[from] ParseAbiError),
}
fn generate_structs(
    project_path: &Path,
    contract: &Contract,
) -> Result<Code, GenerateStructsError> {
    // TODO - this could be shared with `get_abi_items`
    let abi_str = contract.parse_abi(project_path)?;
    let abi_json: Value = serde_json::from_str(&abi_str)?;
    let mut structs = Code::blank();
    for item in abi_json.as_array().ok_or(GenerateStructsError::InvalidAbiJsonFormat)?.iter() {
        if item["type"] == "event" {
            let event_name = item["name"].as_str().unwrap_or_default();
            let struct_result = format!("{event_name}Result");
            let struct_data = format!("{event_name}Data");
            structs.push_str(&Code::new(format!(
                r#"
                    pub type {struct_data} = {abigen_name}::{event_name};
                    #[derive(Debug, Clone)]
                    pub struct {struct_result} {{
                        pub event_data: {struct_data},
                        pub tx_information: TxInformation
                    }}
                    impl HasTxInformation for {struct_result} {{
                        fn tx_information(&self) -> &TxInformation {{
                            &self.tx_information
                        }}
                    }}
                "#,
                struct_result = struct_result,
                struct_data = struct_data,
                abigen_name = abigen_contract_name(contract),
            )));
        }
    }
    Ok(structs)
}
fn generate_event_enums_code(event_info: &[EventInfo]) -> Code {
    Code::new(
        event_info
            .iter()
            .map(|info| format!("{}({}Event<TExtensions>),", info.name, info.name))
            .collect::<Vec<_>>()
            .join("\n"),
    )
}
fn generate_event_type_name(name: &str) -> String {
    format!("{name}EventType")
}
fn generate_topic_ids_match_arms_code(event_type_name: &str, event_info: &[EventInfo]) -> Code {
    Code::new(
        event_info
            .iter()
            .map(|info| {
                format!(
                    "{}::{}(_) => \"0x{}\",",
                    event_type_name,
                    info.name,
                    info.topic_id_as_hex_string()
                )
            })
            .collect::<Vec<_>>()
            .join("\n"),
    )
}
fn generate_event_names_match_arms_code(event_type_name: &str, event_info: &[EventInfo]) -> Code {
    Code::new(
        event_info
            .iter()
            .map(|info| format!("{}::{}(_) => \"{}\",", event_type_name, info.name, info.name))
            .collect::<Vec<_>>()
            .join("\n"),
    )
}
fn generate_register_match_arms_code(event_type_name: &str, event_info: &[EventInfo]) -> Code {
    Code::new(
        event_info
            .iter()
            .map(|info| {
                format!(
                    r#"
                    {}::{}(event) => {{
                        let event = Arc::new(event);
                        Arc::new(move |result| {{
                            let event = Arc::clone(&event);
                            async move {{ event.call(result).await }}.boxed()
                        }})
                    }},
                "#,
                    event_type_name, info.name
                )
            })
            .collect::<Vec<_>>()
            .join("\n"),
    )
}
fn generate_decoder_match_arms_code(event_type_name: &str, event_info: &[EventInfo]) -> Code {
    Code::new(
        event_info
            .iter()
            .map(|info| {
                format!(
                    r#"
                    {event_type_name}::{event_info_name}(_) => {{
                        Arc::new(move |topics: Vec<B256>, data: Bytes| {{
                            match {event_info_name}Data::decode_raw_log(topics, &data[0..]) {{
                                Ok(event) => {{
                                    let result: {event_info_name}Data = event;
                                    Arc::new(result) as Arc<dyn Any + Send + Sync>
                                }}
                                Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                            }}
                        }})
                    }}
                "#,
                    event_type_name = event_type_name,
                    event_info_name = info.name
                )
            })
            .collect::<Vec<_>>()
            .join("\n"),
    )
}
fn generate_csv_instance(
    project_path: &Path,
    contract: &Contract,
    event_info: &EventInfo,
    csv: &Option<CsvDetails>,
) -> Result<Code, CreateCsvFileForEvent> {
    let mut csv_path = csv.as_ref().map_or(PathBuf::from("generated_csv"), |c| {
        PathBuf::from(c.path.strip_prefix("./").unwrap())
    });
    csv_path = project_path.join(csv_path);
    if !contract.generate_csv.unwrap_or(true) {
        return Ok(Code::new(format!(
            r#"let csv = AsyncCsvAppender::new(r"{}");"#,
            csv_path.display(),
        )));
    }
    let csv_path_str = csv_path.to_str().expect("Failed to convert csv path to string");
    let csv_path =
        event_info.create_csv_file_for_event(project_path, &contract.name, csv_path_str)?;
    let headers: Vec<String> =
        event_info.csv_headers_for_event().iter().map(|h| format!("\"{h}\"")).collect();
    let headers_with_into: Vec<String> = headers.iter().map(|h| format!("{h}.into()")).collect();
    Ok(Code::new(format!(
        r#"
        let csv = AsyncCsvAppender::new(r"{}");
        if !Path::new(r"{}").exists() {{
            csv.append_header(vec![{}].into())
                .await
                .expect("Failed to write CSV header");
        }}
    "#,
        csv_path,
        csv_path,
        headers_with_into.join(", ")
    )))
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateEventCallbackStructsError {
    #[error("{0}")]
    CreateCsvFileForEvent(#[from] CreateCsvFileForEvent),
}
fn generate_event_callback_structs_code(
    project_path: &Path,
    event_info: &[EventInfo],
    contract: &Contract,
    storage: &Storage,
) -> Result<Code, GenerateEventCallbackStructsError> {
    let csv_enabled = storage.csv_enabled();
    let is_filter = contract.is_filter();
    let mut parts = Vec::new();
    for info in event_info {
        let csv_generator = if csv_enabled {
            generate_csv_instance(project_path, contract, info, &storage.csv)?
        } else {
            Code::blank()
        };
        let part = format!(
            r#"
            pub fn {lower_name}_handler<TExtensions, F, Fut>(
                custom_logic: F,
            ) -> {name}EventCallbackType<TExtensions>
            where
                {struct_result}: Clone + 'static,
                F: for<'a> Fn(Vec<{struct_result}>, Arc<EventContext<TExtensions>>) -> Fut
                    + Send
                    + Sync
                    + 'static
                    + Clone,
                Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
                TExtensions: Send + Sync + 'static,
            {{
                Arc::new(move |results, context| {{
                    let custom_logic = custom_logic.clone();
                    let results = results.clone();
                    let context = Arc::clone(&context);
                    async move {{ (custom_logic)(results, context).await }}.boxed()
                }})
            }}
            type {name}EventCallbackType<TExtensions> = Arc<
                dyn for<'a> Fn(&'a Vec<{struct_result}>, Arc<EventContext<TExtensions>>) -> BoxFuture<'a, EventCallbackResult<()>>
                    + Send
                    + Sync,
                >;
            pub struct {name}Event<TExtensions> where TExtensions: Send + Sync + 'static {{
                callback: {name}EventCallbackType<TExtensions>,
                context: Arc<EventContext<TExtensions>>,
            }}
            impl<TExtensions> {name}Event<TExtensions> where TExtensions: Send + Sync + 'static {{
                pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
                where
                    {struct_result}: Clone + 'static,
                    F: for<'a> Fn(Vec<{struct_result}>, Arc<EventContext<TExtensions>>) -> Fut
                        + Send
                        + Sync
                        + 'static
                        + Clone,
                    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
                {{
                    {csv_generator}
                    Self {{
                        callback: {lower_name}_handler(closure),
                        context: Arc::new(EventContext {{
                            {database}
                            {csv}
                            extensions: Arc::new(extensions),
                        }}),
                    }}
                }}
            }}
            #[async_trait]
            impl<TExtensions> EventCallback for {name}Event<TExtensions> where TExtensions: Send + Sync {{
                async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {{
                    {event_callback_events_len}
                    // note some can not downcast because it cant decode
                    // this happens on events which failed decoding due to
                    // not having the right abi for example
                    // transfer events with 2 indexed topics cant decode
                    // transfer events with 3 indexed topics
                    let result: Vec<{struct_result}> = events.into_iter()
                        .filter_map(|item| {{
                            item.decoded_data.downcast::<{struct_data}>()
                                .ok()
                                .map(|arc| {struct_result} {{
                                    event_data: (*arc).clone(),
                                    tx_information: item.tx_information
                                }})
                        }})
                        .collect();
                    {event_callback_return}
                }}
            }}
            "#,
            name = info.name,
            lower_name = info.name.to_lowercase(),
            struct_result = info.struct_result(),
            struct_data = info.struct_data(),
            database = if storage.postgres_enabled() {
                "database: get_or_init_postgres_client().await,"
            } else if storage.clickhouse_enabled() {
                "database: get_or_init_clickhouse_client().await,"
            } else {
                ""
            },
            csv = if csv_enabled { r#"csv: Arc::new(csv),"# } else { "" },
            csv_generator = csv_generator,
            event_callback_events_len =
                if !is_filter { "let events_len = events.len();" } else { "" },
            event_callback_return = if !is_filter {
                format!(
                    r#"
                    if result.len() == events_len {{
                        (self.callback)(&result, Arc::clone(&self.context)).await
                    }} else {{
                        panic!("{name}Event: Unexpected data type - expected: {struct_data}")
                    }}
                    "#,
                    name = info.name,
                    struct_data = info.struct_data()
                )
            } else {
                "(self.callback)(&result, Arc::clone(&self.context)).await".to_string()
            }
        );
        parts.push(part);
    }
    Ok(Code::new(parts.join("\n")))
}
fn decoder_contract_fn(contracts_details: Vec<&ContractDetails>, abi_gen_name: &str) -> Code {
    let mut function = String::new();
    function.push_str(&format!(
        r#"pub async fn decoder_contract(network: &str) -> {abi_gen_name}Instance<Arc<RindexerProvider>, AnyNetwork> {{"#
    ));
    let networks: Vec<&String> = contracts_details.iter().map(|c| &c.network).collect();
    for (index, network) in networks.iter().enumerate() {
        if index == 0 {
            function.push_str("    if ");
        } else {
            function.push_str("    else if ");
        }
        function.push_str(&format!(
            r#"network == "{network}" {{
                {abi_gen_name}::new(
                    // do not care about address here its decoding makes it easier to handle ValueOrArray
                    Address::ZERO,
                    get_provider_cache_for_network(network).await.get_inner_provider(),
                 )
            }}"#
        ));
    }
    // Add a fallback else statement to handle unsupported networks
    function.push_str(
        r#"
        else {
            panic!("Network not supported");
        }
    }"#,
    );
    Code::new(function)
}
fn build_pub_contract_fn(
    contract_name: &str,
    contracts_details: Vec<&ContractDetails>,
    abi_gen_name: &str,
) -> Code {
    let contract_name = camel_to_snake(contract_name);
    let has_array_addresses =
        contracts_details.iter().any(|c| matches!(c.address(), Some(ValueOrArray::Array(_))));
    let no_address = contracts_details.iter().any(|c| c.address().is_none());
    if contracts_details.len() > 1 || has_array_addresses || no_address {
        Code::new(format!(
            r#"pub async fn {contract_name}_contract(network: &str, address: Address) -> {abi_gen_name}Instance<Arc<RindexerProvider>, AnyNetwork> {{
                {abi_gen_name}::new(
                    address,
                    get_provider_cache_for_network(network).await.get_inner_provider(),
                 )
               }}
            "#
        ))
    } else {
        let contract = contracts_details
            .first()
            .expect("Contract details should have at least one contract detail");
        match contract.address() {
            None => {
                panic!("Contract details should have an address");
            }
            Some(value) => match value {
                ValueOrArray::Value(address) => {
                    let address = format!("{address:?}");
                    Code::new(format!(
                        r#"pub async fn {contract_name}_contract(network: &str) -> {abi_gen_name}Instance<Arc<RindexerProvider>, AnyNetwork> {{
                                let address: Address = "{address}".parse().expect("Invalid address");
                                {abi_gen_name}::new(
                                    address,
                                    get_provider_cache_for_network(network).await.get_inner_provider(),
                                 )
                               }}
                            "#,
                    ))
                }
                ValueOrArray::Array(_) => {
                    unreachable!("Contract details should always be an single address");
                }
            },
        }
    }
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateEventBindingCodeError {
    #[error("Could not read ABI string: {0}")]
    CouldNotReadAbiString(#[from] std::io::Error),
    #[error("Could not read ABI JSON: {0}")]
    CouldNotReadAbiJson(#[from] serde_json::Error),
    #[error("{0}")]
    GenerateStructsError(#[from] GenerateStructsError),
    #[error("{0}")]
    GenerateEventCallbackStructsError(#[from] GenerateEventCallbackStructsError),
}
fn generate_event_bindings_code(
    project_path: &Path,
    indexer_name: &str,
    contract: &Contract,
    storage: &Storage,
    event_info: Vec<EventInfo>,
) -> Result<Code, GenerateEventBindingCodeError> {
    let event_type_name = generate_event_type_name(&contract.name);
    let code = Code::new(format!(
        r#"#![allow(non_camel_case_types, clippy::enum_variant_names, clippy::too_many_arguments, clippy::upper_case_acronyms, clippy::type_complexity, dead_code)]
        /// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
        ///
        /// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
        /// Any manual changes to this file will be overwritten.
        use super::{abigen_file_name}::{{
            {abigen_name}::{{self, {abigen_name}Instance, {abigen_name}Events}}
        }};
        use std::{{any::Any, sync::Arc}};
        use std::error::Error;
        use std::future::Future;
        use std::collections::HashMap;
        use std::pin::Pin;
        use std::path::{{Path, PathBuf}};
        use alloy::network::AnyNetwork;
        use alloy::primitives::{{Address, Bytes, B256}};
        use alloy::sol_types::{{SolEvent, SolEventInterface, SolType}};
        use rindexer::{{
            async_trait,
            {csv_import}
            generate_random_id,
            FutureExt,
            blockclock::BlockClock,
            event::{{
                callback_registry::{{
                    EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
                    EventResult, TxInformation, HasTxInformation
                }},
                contract_setup::{{ContractInformation, NetworkContract}},
            }},
            manifest::{{
                contract::{{Contract, ContractDetails}},
                yaml::read_manifest,
            }},
            provider::{{JsonRpcCachedProvider, RindexerProvider}},
            {postgres_client_import}
        }};
        use super::super::super::super::typings::networks::get_provider_cache_for_network;
        {postgres_import}
        {structs}
        type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
        #[async_trait]
        trait EventCallback {{
            async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
        }}
        pub struct EventContext<TExtensions> where TExtensions: Send + Sync {{
            {event_context_database}
            {event_context_csv}
            pub extensions: Arc<TExtensions>,
        }}
        // didn't want to use option or none made harder DX
        // so a blank struct makes interface nice
        pub struct NoExtensions {{}}
        pub fn no_extensions() -> NoExtensions {{
            NoExtensions {{}}
        }}
        {event_callback_structs}
        pub enum {event_type_name}<TExtensions> where TExtensions: 'static + Send + Sync {{
            {event_enums}
        }}
        {build_pub_contract_fn}
        {decoder_contract_fn}
        impl<TExtensions> {event_type_name}<TExtensions> where TExtensions: 'static + Send + Sync {{
            pub fn topic_id(&self) -> &'static str {{
                match self {{
                    {topic_ids_match_arms}
                }}
            }}
            pub fn event_name(&self) -> &'static str {{
                match self {{
                    {event_names_match_arms}
                }}
            }}
            pub fn contract_name(&self) -> String {{
                "{raw_contract_name}".to_string()
            }}
            async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {{
                get_provider_cache_for_network(network).await
            }}
            fn decoder(&self, network: &str) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {{
                let decoder_contract = decoder_contract(network);
                match self {{
                    {decoder_match_arms}
                }}
            }}
            pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {{
                let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
                let topic_id = self.topic_id();
                let contract_name = self.contract_name();
                let event_name = self.event_name();
                let contract_details = rindexer_yaml
                    .all_contracts()
                    .iter()
                    .find(|c| c.name == contract_name)
                    .unwrap_or_else(|| panic!("Contract {{}} not found please make sure its defined in the rindexer.yaml",
                        contract_name))
                    .clone();
                  let index_event_in_order = contract_details
                    .index_event_in_order
                    .as_ref()
                    .map_or(false, |vec| vec.contains(&event_name.to_string()));
                // Expect providers to have been initialized, but it's an async init so this should
                // be fast but for correctness we must await each future.
                let mut providers = HashMap::new();
                for n in contract_details.details.iter() {{
                    let provider = self.get_provider(&n.network).await;
                    providers.insert(n.network.clone(), provider);
                }}
                let contract = ContractInformation {{
                    name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
                    details: contract_details
                        .details
                        .iter()
                        .map(|c| {{
                            let provider = providers.get(&c.network).expect("must have a provider").clone();
                            NetworkContract {{
                                id: generate_random_id(10),
                                network: c.network.clone(),
                                cached_provider: provider.clone(),
                                block_clock: BlockClock::new(
                                    rindexer_yaml.timestamps,
                                    rindexer_yaml.config.timestamp_sample_rate,
                                    provider.clone(),
                                ),
                                decoder: self.decoder(&c.network),
                                indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                                start_block: c.start_block,
                                end_block: c.end_block,
                                disable_logs_bloom_checks: rindexer_yaml
                                                            .networks
                                                            .iter()
                                                            .find(|n| n.name == c.network)
                                                            .map_or(false, |n| n.disable_logs_bloom_checks.unwrap_or_default()),
                            }}
                        }})
                        .collect(),
                    abi: contract_details.abi,
                    reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
                }};
                let callback: Arc<dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync> = match self {{
                    {register_match_arms}
                }};
               registry.register_event(EventCallbackRegistryInformation {{
                    id: generate_random_id(10),
                    indexer_name: "{indexer_name}".to_string(),
                    event_name: event_name.to_string(),
                    index_event_in_order,
                    topic_id: topic_id.parse::<B256>().unwrap(),
                    contract,
                    callback,
                }});
            }}
        }}
        "#,
        postgres_import = if storage.postgres_enabled() {
            "use super::super::super::super::typings::database::get_or_init_postgres_client;"
        } else if storage.clickhouse_enabled() {
            "use super::super::super::super::typings::database::get_or_init_clickhouse_client;"
        } else {
            ""
        },
        postgres_client_import = if storage.postgres_enabled() {
            "PostgresClient,"
        } else if storage.clickhouse_enabled() {
            "ClickhouseClient,"
        } else {
            ""
        },
        csv_import = if storage.csv_enabled() { "AsyncCsvAppender," } else { "" },
        abigen_file_name = abigen_contract_file_name(contract),
        abigen_name = abigen_contract_name(contract),
        structs = generate_structs(project_path, contract)?,
        event_type_name = &event_type_name,
        event_context_database = if storage.postgres_enabled() {
            "pub database: Arc<PostgresClient>,"
        } else if storage.clickhouse_enabled() {
            "pub database: Arc<ClickhouseClient>,"
        } else {
            ""
        },
        event_context_csv =
            if storage.csv_enabled() { "pub csv: Arc<AsyncCsvAppender>," } else { "" },
        event_callback_structs =
            generate_event_callback_structs_code(project_path, &event_info, contract, storage)?,
        event_enums = generate_event_enums_code(&event_info),
        topic_ids_match_arms = generate_topic_ids_match_arms_code(&event_type_name, &event_info),
        event_names_match_arms =
            generate_event_names_match_arms_code(&event_type_name, &event_info),
        raw_contract_name = contract.raw_name(),
        decoder_contract_fn =
            decoder_contract_fn(contract.details.iter().collect(), &abigen_contract_name(contract)),
        build_pub_contract_fn = build_pub_contract_fn(
            &contract.name,
            contract.details.iter().collect(),
            &abigen_contract_name(contract)
        ),
        decoder_match_arms = generate_decoder_match_arms_code(&event_type_name, &event_info),
        register_match_arms = generate_register_match_arms_code(&event_type_name, &event_info)
    ));
    Ok(code)
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateEventBindingsError {
    #[error("{0}")]
    ReadAbi(#[from] ReadAbiError),
    #[error("{0}")]
    GenerateEventBindingCode(#[from] GenerateEventBindingCodeError),
    #[error("{0}")]
    ParamType(#[from] ParamTypeError),
}
pub fn generate_event_bindings(
    project_path: &Path,
    indexer_name: &str,
    contract: &Contract,
    is_filter: bool,
    storage: &Storage,
) -> Result<Code, GenerateEventBindingsError> {
    let abi_items = ABIItem::get_abi_items(project_path, contract, is_filter)?;
    let event_names = ABIItem::extract_event_names_and_signatures_from_abi(abi_items)?;
    generate_event_bindings_code(project_path, indexer_name, contract, storage, event_names)
        .map_err(GenerateEventBindingsError::GenerateEventBindingCode)
}
pub fn generate_event_input_path(property: &AbiProperty) -> (String, bool) {
    let mut path = "result.event_data".to_string();
    let mut is_array = false;
    for current_path in property.path.clone().unwrap_or_default() {
        path = match (current_path.abi_type.ends_with("[]"), is_array) {
            (false, false) => format!("{}.{}", path, current_path.abi_name),
            (true, false) => {
                is_array = true;
                format!("{}.{}.iter().cloned()", path, current_path.abi_name)
            }
            (false, true) => format!("{}.map(|v| v.{})", path, current_path.abi_name),
            (true, true) => format!("{}.flat_map(|v| v.{})", path, current_path.abi_name),
        }
    }
    let full_path = match (property.abi_type.ends_with("[]"), is_array) {
        (_, false) => {
            format!("{}.{}", path, property.abi_name)
        }
        (false, true) => {
            format!("{}.map(|v| v.{})", path, property.abi_name)
        }
        (true, true) => {
            format!("{}.flat_map(|v| v.{})", path, property.abi_name)
        }
    };
    (full_path, is_array)
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateEventHandlersError {
    #[error("{0}")]
    ReadAbiError(#[from] ReadAbiError),
    #[error("{0}")]
    ParamTypeError(#[from] ParamTypeError),
}
pub fn generate_event_handlers(
    project_path: &Path,
    indexer_name: &str,
    is_filter: bool,
    contract: &Contract,
    storage: &Storage,
) -> Result<Code, GenerateEventHandlersError> {
    let abi_items = ABIItem::get_abi_items(project_path, contract, is_filter)?;
    let event_names = ABIItem::extract_event_names_and_signatures_from_abi(abi_items)?;
    let mut imports = String::new();
    imports.push_str(
        r#"#![allow(non_snake_case)]
            use rindexer::{
                event::callback_registry::EventCallbackRegistry,
                EthereumSqlTypeWrapper, PgType, rindexer_error, rindexer_info
            };
        "#,
    );
    imports.push_str("use std::sync::Arc;\n");
    imports.push_str(&format!(
        r#"use std::path::PathBuf;
        use alloy::primitives::{{U64, U256, I256}};
        use super::super::super::typings::{indexer_name_formatted}::events::{handler_registry_name}::{{no_extensions, {event_type_name}"#,
        indexer_name_formatted = camel_to_snake(indexer_name),
        handler_registry_name = camel_to_snake(&contract.name),
        event_type_name = generate_event_type_name(&contract.name)
    ));
    let mut handlers = String::new();
    let mut registry_fn = String::new();
    registry_fn.push_str(&format!(
        r#"pub async fn {handler_registry_fn_name}_handlers(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {{"#,
        handler_registry_fn_name = camel_to_snake(&contract.name),
    ));
    for event in event_names {
        let event_type_name = generate_event_type_name(&contract.name);
        imports.push_str(&format!(r#",{handler_name}Event"#, handler_name = event.name,));
        let abi_name_properties = ABIInput::generate_abi_name_properties(
            &event.inputs,
            &GenerateAbiPropertiesType::Object,
            None,
        );
        let mut csv_write = String::new();
        // this checks storage enabled as well
        if !storage.csv_disable_create_headers() {
            let mut csv_data = String::new();
            csv_data.push_str(r#"result.tx_information.address.to_string(),"#);
            for property in &abi_name_properties {
                let (path, is_array) = generate_event_input_path(property);
                let formatted_path = match is_array {
                    true => {
                        format!(r#"{path}.map(|v| v.to_string()).collect::<Vec<_>>().join(",")"#)
                    }
                    false => format!("{path}.to_string()"),
                };
                csv_data.push_str(&format!(r#"{formatted_path}, "#));
                csv_data.push('\n')
            }
            csv_data.push_str(r#"result.tx_information.transaction_hash.to_string(),"#);
            csv_data.push_str(r#"result.tx_information.block_number.to_string(),"#);
            csv_data.push_str(r#"result.tx_information.block_hash.to_string(),"#);
            csv_data.push_str(r#"result.tx_information.network.to_string(),"#);
            csv_data.push_str(r#"result.tx_information.transaction_index.to_string(),"#);
            csv_data.push_str(r#"result.tx_information.log_index.to_string()"#);
            csv_write = format!(r#"csv_bulk_data.push(vec![{csv_data}]);"#,);
            if storage.postgres_disable_create_tables() {
                csv_write = format!(
                    r#"
                      let mut csv_bulk_data: Vec<Vec<String>> = vec![];
                      for result in &results {{
                        {inner_csv_write}
                      }}
                      if !csv_bulk_data.is_empty() {{
                        let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                        if let Err(e) = csv_result {{
                            rindexer_error!("{event_type_name}::{handler_name} inserting csv data: {{:?}}", e);
                            return Err(e.to_string());
                        }}
                      }}
                    "#,
                    inner_csv_write = csv_write,
                    event_type_name = event_type_name,
                    handler_name = event.name,
                );
            }
        }
        let mut postgres_write = String::new();
        // this checks storage enabled as well
        if !storage.postgres_disable_create_tables() || !storage.clickhouse_disable_create_tables()
        {
            let mut data = "vec![\nEthereumSqlTypeWrapper::Address(result.tx_information.address),"
                .to_string();
            let formatter =
                |path: &str, abi_type: &str, wrapper: &Option<EthereumSqlTypeWrapper>| {
                    format!(
                        "{}{}{}",
                        match wrapper {
                            Some(EthereumSqlTypeWrapper::U256(_)) => "U256::from(",
                            Some(EthereumSqlTypeWrapper::I256(_)) => "I256::from(",
                            _ => "",
                        },
                        path,
                        if is_solidity_static_bytes_type(abi_type) {
                            ".into()"
                        } else if matches!(
                            wrapper,
                            Some(EthereumSqlTypeWrapper::I256(_) | EthereumSqlTypeWrapper::U256(_))
                        ) {
                            ")"
                        } else if abi_type.starts_with("int")
                            && is_irregular_width_solidity_integer_type(abi_type)
                        {
                            ".unchecked_into()"
                        } else if abi_type.starts_with("uint")
                            && is_irregular_width_solidity_integer_type(abi_type)
                        {
                            ".to()"
                        } else if abi_type == "string" || abi_type == "bytes" {
                            ".clone()"
                        } else {
                            ""
                        }
                    )
                };
            for property in &abi_name_properties {
                let (path, is_array) = generate_event_input_path(property);
                let wrapper = property.ethereum_sql_type_wrapper.as_ref().unwrap_or_else(|| {
                    panic!("No EthereumSqlTypeWrapper found for: {:?}", property.abi_type)
                });
                let formatted_path = if is_array {
                    format!(
                        r#"{}.map(|item| {}).collect::<Vec<_>>()"#,
                        path,
                        formatter("item", &property.abi_type, &property.ethereum_sql_type_wrapper)
                    )
                } else {
                    formatter(&path, &property.abi_type, &property.ethereum_sql_type_wrapper)
                };
                data.push_str(&format!(
                    "\nEthereumSqlTypeWrapper::{}({}),",
                    wrapper.raw_name(),
                    formatted_path
                ));
            }
            data.push_str(
                "\nEthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),",
            );
            data.push_str("\nEthereumSqlTypeWrapper::U64(result.tx_information.block_number),");
            data.push_str("\nEthereumSqlTypeWrapper::DateTimeNullable(result.tx_information.block_timestamp_to_datetime()),");
            data.push_str("\nEthereumSqlTypeWrapper::B256(result.tx_information.block_hash),");
            data.push_str(
                "\nEthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),",
            );
            data.push_str(
                "\nEthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),",
            );
            data.push_str("\nEthereumSqlTypeWrapper::U256(result.tx_information.log_index)");
            data.push_str("\n]");
            postgres_write = format!(
                r#"
                    let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
                    {csv_bulk_data}
                    for result in results.iter() {{
                        {csv_write}
                        let data = {data};
                        postgres_bulk_data.push(data);
                    }}
                    {csv_bulk_insert}
                    if postgres_bulk_data.is_empty() {{
                        return Ok(());
                    }}
                    let rows = [{columns_names}];
                    let result = context
                        .database
                        .insert_bulk(
                            "{table_name}",
                            &rows,
                            &postgres_bulk_data,
                        )
                        .await;
                    if let Err(e) = result {{
                        rindexer_error!("{event_type_name}::{handler_name} inserting bulk data: {{:?}}", e);
                        return Err(e.to_string());
                    }}
                "#,
                table_name =
                    generate_event_table_full_name(indexer_name, &contract.name, &event.name),
                handler_name = event.name,
                event_type_name = event_type_name,
                columns_names = generate_column_names_only_with_base_properties(&event.inputs)
                    .iter()
                    .map(|item| format!("\"{item}\".to_string()"))
                    .collect::<Vec<String>>()
                    .join(", "),
                data = data,
                csv_write = csv_write,
                csv_bulk_data = if storage.csv_enabled() {
                    "let mut csv_bulk_data: Vec<Vec<String>> = vec![];"
                } else {
                    ""
                },
                csv_bulk_insert = if storage.csv_enabled() {
                    format!(
                        r#"if !csv_bulk_data.is_empty() {{
                        let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                        if let Err(e) = csv_result {{
                            rindexer_error!("{event_type_name}::{handler_name} inserting csv data: {{:?}}", e);
                            return Err(e.to_string());
                        }}
                    }}"#,
                        event_type_name = event_type_name,
                        handler_name = event.name
                    )
                } else {
                    "".to_string()
                }
            );
        }
        let handler = format!(
            r#"
            async fn {handler_fn_name}_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {{
                let handler = {handler_name}Event::handler(|results, context| async move {{
                                if results.is_empty() {{
                                    return Ok(());
                                }}
                                {csv_write}
                                {postgres_write}
                                rindexer_info!(
                                    "{contract_name}::{handler_name} - INDEXED - {{}} events",
                                    results.len(),
                                );
                                Ok(())
                            }},
                            no_extensions(),
                          )
                          .await;
                {event_type_name}::{handler_name}(handler).register(manifest_path, registry).await;
            }}
        "#,
            handler_fn_name = camel_to_snake(&event.name),
            handler_name = event.name,
            event_type_name = event_type_name,
            contract_name = contract.name,
            csv_write = if !postgres_write.is_empty() { String::new() } else { csv_write },
            postgres_write = postgres_write,
        );
        handlers.push_str(&handler);
        registry_fn.push_str(&format!(
            r#"
                {handler_fn_name}_handler(manifest_path, registry).await;
            "#,
            handler_fn_name = camel_to_snake(&event.name)
        ));
    }
    imports.push_str("};\n");
    registry_fn.push('}');
    let mut code = String::new();
    code.push_str(&imports);
    code.push_str(&handlers);
    code.push_str(&registry_fn);
    Ok(Code::new(code))
}
#[cfg(test)]
mod tests {
    use super::generate_event_input_path;
    use crate::abi::{AbiNamePropertiesPath, AbiProperty};
    #[test]
    fn test_top_level_property() {
        let property = AbiProperty::new("test".to_string(), "key", "uint256", None);
        let (path, is_array) = generate_event_input_path(&property);
        assert_eq!(path, "result.event_data.key");
        assert!(!is_array);
    }
    #[test]
    fn test_one_level_deep() {
        let property = AbiProperty::new(
            "test".to_string(),
            "value",
            "string",
            Some(vec![AbiNamePropertiesPath::new("values", "tuple[]")]),
        );
        let (path, is_array) = generate_event_input_path(&property);
        assert_eq!(path, "result.event_data.values.iter().cloned().map(|v| v.value)");
        assert!(is_array);
    }
    #[test]
    fn test_nested_arrays() {
        let property = AbiProperty::new(
            "test".to_string(),
            "value",
            "string",
            Some(vec![
                AbiNamePropertiesPath::new("result", "tuple[]"),
                AbiNamePropertiesPath::new("values", "tuple[]"),
            ]),
        );
        let (path, is_array) = generate_event_input_path(&property);
        assert_eq!(
            path,
            "result.event_data.result.iter().cloned().flat_map(|v| v.values).map(|v| v.value)"
        );
        assert!(is_array);
    }
    #[test]
    fn test_complex_path() {
        let property = AbiProperty::new(
            "test".to_string(),
            "name",
            "string",
            Some(vec![
                AbiNamePropertiesPath::new("result", "tuple"),
                AbiNamePropertiesPath::new("data", "tuple[]"),
                AbiNamePropertiesPath::new("values", "tuple[]"),
                AbiNamePropertiesPath::new("value", "tuple"),
            ]),
        );
        let (path, is_array) = generate_event_input_path(&property);
        assert_eq!(
            path,
            "result.event_data.result.data.iter().cloned().flat_map(|v| v.values).map(|v| v.value).map(|v| v.name)"
        );
        assert!(is_array);
    }
}
</file>

<file path="core/src/generator/mod.rs">
pub mod build;
mod context_bindings;
mod database_bindings;
mod docker;
mod events_bindings;
mod networks_bindings;
mod trace_bindings;
pub use docker::generate_docker_file;
</file>

<file path="core/src/generator/networks_bindings.rs">
use crate::{manifest::network::Network, types::code::Code};
fn network_provider_name(network: &Network) -> String {
    network_provider_name_from_name(&network.name)
}
fn network_provider_name_from_name(network_name: &str) -> String {
    format!("{network_name}_PROVIDER", network_name = network_name.to_uppercase())
}
pub fn network_provider_fn_name(network: &Network) -> String {
    format!("get_{fn_name}", fn_name = network_provider_name(network).to_lowercase())
}
#[cfg(not(feature = "reth"))]
fn generate_reth_init_fn(_network: &Network) -> Code {
    Code::new(
        r#"
            let chain_state_notification = None;
            "#
        .to_string(),
    )
}
#[cfg(feature = "reth")]
fn generate_reth_init_fn(network: &Network) -> Code {
    if network.is_reth_enabled() {
        let reth_cli_args = network.reth.as_ref().unwrap().to_cli_args();
        Code::new(format!(
            r#"
            use rindexer::reth::node::start_reth_node_with_exex;
            use rindexer::reth::Cli;
            let cli = Cli::try_parse_args_from({reth_cli_args:?}).unwrap();
            let chain_state_notification = start_reth_node_with_exex(cli).unwrap();
            let chain_state_notification = Some(chain_state_notification);
            "#
        ))
    } else {
        Code::new(
            r#"
            let chain_state_notification = None;
            "#
            .to_string(),
        )
    }
}
fn get_network_url(network: &Network) -> String {
    #[cfg(feature = "reth")]
    if network.is_reth_enabled() {
        network.get_reth_ipc_path().unwrap()
    } else {
        network.rpc.clone()
    }
    #[cfg(not(feature = "reth"))]
    network.rpc.clone()
}
fn generate_network_lazy_provider_code(network: &Network) -> Code {
    Code::new(format!(
        r#"
        {network_name}
            .get_or_init(|| async {{
                {reth_init_fn}
                {client_fn}(&public_read_env_value("{network_url}").unwrap_or("{network_url}".to_string()), {chain_id}, {compute_units_per_second}, {max_block_range}, {block_poll_frq} {placeholder_headers}, {get_logs_settings}, {chain_state_notification})
                .await
                .expect("Error creating provider")
            }})
            .await
            .clone()
        "#,
        network_name = network_provider_name(network),
        network_url = get_network_url(network),
        chain_id = network.chain_id,
        compute_units_per_second =
            if let Some(compute_units_per_second) = network.compute_units_per_second {
                format!("Some({compute_units_per_second})")
            } else {
                "None".to_string()
            },
        max_block_range = if let Some(max_block_range) = network.max_block_range {
            format!("Some(U64::from({max_block_range}))")
        } else {
            "None".to_string()
        },
        block_poll_frq = if let Some(block_frq) = network.block_poll_frequency {
            format!("Some(BlockPollFrequency::{block_frq:?})")
        } else {
            "None".to_string()
        },
        get_logs_settings = if let Some(settings) = &network.get_logs_settings {
            format!("Some(AddressFiltering::{:?})", settings.address_filtering)
        } else {
            "None".to_string()
        },
        client_fn =
            if network.rpc.contains("shadow") { "create_shadow_client" } else { "create_client" },
        placeholder_headers =
            if network.rpc.contains("shadow") { "" } else { ", HeaderMap::new()" },
        chain_state_notification = "chain_state_notification",
        reth_init_fn = generate_reth_init_fn(network),
    ))
}
fn generate_network_provider_code(network: &Network) -> Code {
    Code::new(format!(
        r#"
            pub async fn {fn_name}_cache() -> Arc<JsonRpcCachedProvider> {{
                {provider_init_fn}
            }}
            pub async fn {fn_name}() -> Arc<RindexerProvider> {{
                {fn_name}_cache().await.get_inner_provider()
            }}
        "#,
        fn_name = network_provider_fn_name(network),
        provider_init_fn = generate_network_lazy_provider_code(network),
    ))
}
fn generate_provider_cache_for_network_fn(networks: &[Network]) -> Code {
    let mut if_code = Code::blank();
    for network in networks {
        let network_if = format!(
            r#"
            if network == "{network_name}" {{
                return get_{network_name}_provider_cache().await;
            }}
        "#,
            network_name = network.name
        );
        if_code.push_str(&Code::new(network_if));
    }
    if_code.push_str(&Code::new(r#"panic!("Network not supported")"#.to_string()));
    let provider_cache_for_network_fn = format!(
        r#"
        pub async fn get_provider_cache_for_network(network: &str) -> Arc<JsonRpcCachedProvider>  {{
            {if_code}
        }}
    "#
    );
    Code::new(provider_cache_for_network_fn)
}
pub fn generate_networks_code(networks: &[Network]) -> Code {
    let mut output = Code::new(
        r#"
    /// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
    ///
    /// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
    /// Any manual changes to this file will be overwritten.
    use alloy::{primitives::U64, transports::http::reqwest::header::HeaderMap};
    use rindexer::{
        lazy_static,
        manifest::network::{AddressFiltering, BlockPollFrequency},
        provider::{RindexerProvider, create_client, JsonRpcCachedProvider, RetryClientError},
        notifications::ChainStateNotification,
        public_read_env_value
    };
    use std::sync::Arc;
    use tokio::sync::OnceCell;
    use tokio::sync::broadcast::Sender;
    #[allow(dead_code)]
    async fn create_shadow_client(
        rpc_url: &str,
        chain_id: u64,
        compute_units_per_second: Option<u64>,
        block_poll_frequency: Option<BlockPollFrequency>,
        max_block_range: Option<U64>,
        address_filtering: Option<AddressFiltering>,
        chain_state_notification: Option<Sender<ChainStateNotification>>,
    ) -> Result<Arc<JsonRpcCachedProvider>, RetryClientError> {
        let mut header = HeaderMap::new();
        header.insert(
            "X-SHADOW-API-KEY",
            public_read_env_value("RINDEXER_PHANTOM_API_KEY").unwrap().parse().unwrap(),
        );
        create_client(rpc_url, chain_id, compute_units_per_second, max_block_range, block_poll_frequency, header, address_filtering, chain_state_notification).await
    }
        "#
        .to_string(),
    );
    for network in networks {
        output.push_str(&Code::new(format!(
            r#"
            static {network_name}: OnceCell<Arc<JsonRpcCachedProvider>> = OnceCell::const_new();
            "#,
            network_name = network_provider_name(network),
        )));
    }
    for network in networks {
        output.push_str(&generate_network_provider_code(network));
    }
    output.push_str(&generate_provider_cache_for_network_fn(networks));
    output
}
</file>

<file path="core/src/generator/trace_bindings.rs">
use std::path::{Path, PathBuf};
use serde_json::Value;
use crate::{
    abi::{
        ABIInput, ABIItem, CreateCsvFileForEvent, EventInfo, GenerateAbiPropertiesType,
        ParamTypeError, ReadAbiError,
    },
    database::{
        generate::generate_event_table_full_name,
        postgres::generate::generate_column_names_only_with_base_properties,
    },
    helpers::camel_to_snake,
    indexer::native_transfer::{NATIVE_TRANSFER_ABI, NATIVE_TRANSFER_CONTRACT_NAME},
    manifest::{
        contract::ParseAbiError,
        native_transfer::{NativeTransferDetails, NativeTransfers},
        storage::{CsvDetails, Storage},
    },
    types::code::Code,
};
pub fn trace_abigen_contract_name(contract_name: &str) -> String {
    format!("Rindexer{contract_name}Gen")
}
pub fn trace_abigen_contract_file_name(contract_name: &str) -> String {
    format!("{}_abi_gen", camel_to_snake(contract_name))
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateStructsError {
    #[error("Could not read ABI string: {0}")]
    CouldNotReadAbiString(#[from] std::io::Error),
    #[error("Could not read ABI JSON: {0}")]
    CouldNotReadAbiJson(#[from] serde_json::Error),
    #[error("Invalid ABI JSON format")]
    InvalidAbiJsonFormat,
    #[error("{0}")]
    ParseAbiError(#[from] ParseAbiError),
}
fn trace_generate_structs(contract_name: &str) -> Result<Code, GenerateStructsError> {
    let abi_str = NATIVE_TRANSFER_ABI;
    let abi_json: Value = serde_json::from_str(abi_str)?;
    let mut structs = Code::blank();
    for item in abi_json.as_array().ok_or(GenerateStructsError::InvalidAbiJsonFormat)?.iter() {
        if item["type"] == "event" {
            let event_name = item["name"].as_str().unwrap_or_default();
            let struct_result = format!("{event_name}Result");
            let struct_data = format!("{event_name}Data");
            structs.push_str(&Code::new(format!(
                r#"
                    pub type {struct_data} = {abigen_mod_name}::{event_name};
                    #[derive(Debug, Clone)]
                    pub struct {struct_result} {{
                        pub event_data: {struct_data},
                        pub tx_information: TxInformation
                    }}
                    impl HasTxInformation for {struct_result} {{
                        fn tx_information(&self) -> &TxInformation {{
                            &self.tx_information
                        }}
                    }}
                "#,
                struct_result = struct_result,
                struct_data = struct_data,
                abigen_mod_name = trace_abigen_contract_name(contract_name),
            )));
        }
    }
    // Add Block event support
    structs.push_str(&Code::new(
        r#"
        pub type BlockData = alloy::network::AnyRpcBlock;
        #[derive(Debug, Clone)]
        pub struct BlockResult {
            pub block: Box<BlockData>,
            pub tx_information: TxInformation
        }
        impl HasTxInformation for BlockResult {
            fn tx_information(&self) -> &TxInformation {
                &self.tx_information
            }
        }
        "#
        .to_string(),
    ));
    Ok(structs)
}
fn generate_event_enums_code(event_info: &[EventInfo]) -> Code {
    let mut events = event_info
        .iter()
        .map(|info| format!("{}({}Event<TExtensions>),", info.name, info.name))
        .collect::<Vec<_>>();
    // Add Block event to the enum
    events.push("Block(BlockEvent<TExtensions>),".to_string());
    Code::new(events.join("\n"))
}
fn generate_event_type_name(name: &str) -> String {
    format!("{name}EventType")
}
fn generate_event_names_match_arms_code(event_type_name: &str, event_info: &[EventInfo]) -> Code {
    let mut arms = event_info
        .iter()
        .map(|info| format!("{}::{}(_) => \"{}\",", event_type_name, info.name, info.name))
        .collect::<Vec<_>>();
    // Add Block event match arm
    arms.push(format!("{}::Block(_) => \"Block\",", event_type_name));
    Code::new(arms.join("\n"))
}
fn generate_register_match_arms_code(event_type_name: &str, event_info: &[EventInfo]) -> Code {
    let mut arms = event_info
        .iter()
        .map(|info| {
            format!(
                r#"
                {}::{}(event) => {{
                    let event = Arc::new(event);
                    Arc::new(move |result| {{
                        let event = Arc::clone(&event);
                        async move {{ event.call(result).await }}.boxed()
                    }})
                }},
            "#,
                event_type_name, info.name
            )
        })
        .collect::<Vec<_>>();
    // Add Block event register match arm
    arms.push(format!(
        r#"
        {}::Block(event) => {{
            let event = Arc::new(event);
            Arc::new(move |result| {{
                let event = Arc::clone(&event);
                async move {{ event.call(result).await }}.boxed()
            }})
        }},
    "#,
        event_type_name
    ));
    Code::new(arms.join("\n"))
}
fn generate_decoder_match_arms_code(event_type_name: &str, event_info: &[EventInfo]) -> Code {
    let mut arms = event_info
        .iter()
        .map(|info| {
            format!(
                r#"
                {event_type_name}::{event_info_name}(_) => {{
                    Arc::new(move |topics: Vec<B256>, data: Bytes| {{
                        match {event_info_name}Data::decode_raw_log(topics, &data[0..]) {{
                            Ok(event) => {{
                                let result: {event_info_name}Data = event;
                                Arc::new(result) as Arc<dyn Any + Send + Sync>
                            }}
                            Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                        }}
                    }})
                }}
            "#,
                event_type_name = event_type_name,
                event_info_name = info.name
            )
        })
        .collect::<Vec<_>>();
    // Add Block event decoder match arm
    arms.push(format!(
        r#"
        {event_type_name}::Block(_) => {{
            Arc::new(move |_topics: Vec<B256>, _data: Bytes| {{
                // Block events don't need decoding, they're passed as-is
                Arc::new(()) as Arc<dyn Any + Send + Sync>
            }})
        }}
    "#,
        event_type_name = event_type_name
    ));
    Code::new(arms.join("\n"))
}
fn generate_csv_instance(
    project_path: &Path,
    contract: &NativeTransfers,
    event_info: &EventInfo,
    csv: &Option<CsvDetails>,
) -> Result<Code, CreateCsvFileForEvent> {
    let mut csv_path = csv.as_ref().map_or(PathBuf::from("generated_csv"), |c| {
        PathBuf::from(c.path.strip_prefix("./").unwrap())
    });
    csv_path = project_path.join(csv_path);
    if !contract.generate_csv.unwrap_or(true) {
        return Ok(Code::new(format!(
            r#"let csv = AsyncCsvAppender::new(r"{}");"#,
            csv_path.display(),
        )));
    }
    let csv_path_str = csv_path.to_str().expect("Failed to convert csv path to string");
    let csv_path = event_info.create_csv_file_for_event(
        project_path,
        NATIVE_TRANSFER_CONTRACT_NAME,
        csv_path_str,
    )?;
    let headers: Vec<String> =
        event_info.csv_headers_for_event().iter().map(|h| format!("\"{h}\"")).collect();
    let headers_with_into: Vec<String> = headers.iter().map(|h| format!("{h}.into()")).collect();
    Ok(Code::new(format!(
        r#"
        let csv = AsyncCsvAppender::new(r"{}");
        if !Path::new(r"{}").exists() {{
            csv.append_header(vec![{}].into())
                .await
                .expect("Failed to write CSV header");
        }}
    "#,
        csv_path,
        csv_path,
        headers_with_into.join(", ")
    )))
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateTraceCallbackStructsError {
    #[error("{0}")]
    CreateCsvFileForEvent(#[from] CreateCsvFileForEvent),
}
fn generate_trace_callback_structs_code(
    project_path: &Path,
    event_info: &[EventInfo],
    contract: &NativeTransfers,
    storage: &Storage,
) -> Result<Code, GenerateTraceCallbackStructsError> {
    let databases_enabled = storage.postgres_enabled();
    let csv_enabled = storage.csv_enabled();
    let is_filter = false; // TODO: Hardcoded for now
    let mut parts = Vec::new();
    for info in event_info {
        let csv_generator = if csv_enabled {
            generate_csv_instance(project_path, contract, info, &storage.csv)?
        } else {
            Code::blank()
        };
        let part = format!(
            r#"
            pub fn {lower_name}_handler<TExtensions, F, Fut>(
                custom_logic: F,
            ) -> {name}TraceCallbackType<TExtensions>
            where
                {struct_result}: Clone + 'static,
                F: for<'a> Fn(Vec<{struct_result}>, Arc<TraceContext<TExtensions>>) -> Fut
                    + Send
                    + Sync
                    + 'static
                    + Clone,
                Fut: Future<Output = TraceCallbackResult<()>> + Send + 'static,
                TExtensions: Send + Sync + 'static,
            {{
                Arc::new(move |results, context| {{
                    let custom_logic = custom_logic.clone();
                    let results = results.clone();
                    let context = Arc::clone(&context);
                    async move {{ (custom_logic)(results, context).await }}.boxed()
                }})
            }}
            type {name}TraceCallbackType<TExtensions> = Arc<
                dyn for<'a> Fn(&'a Vec<{struct_result}>, Arc<TraceContext<TExtensions>>) -> BoxFuture<'a, TraceCallbackResult<()>>
                    + Send
                    + Sync,
                >;
            pub struct {name}Event<TExtensions> where TExtensions: Send + Sync + 'static {{
                callback: {name}TraceCallbackType<TExtensions>,
                context: Arc<TraceContext<TExtensions>>,
            }}
            impl<TExtensions> {name}Event<TExtensions> where TExtensions: Send + Sync + 'static {{
                pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
                where
                    {struct_result}: Clone + 'static,
                    F: for<'a> Fn(Vec<{struct_result}>, Arc<TraceContext<TExtensions>>) -> Fut
                        + Send
                        + Sync
                        + 'static
                        + Clone,
                    Fut: Future<Output = TraceCallbackResult<()>> + Send + 'static,
                {{
                    {csv_generator}
                    Self {{
                        callback: {lower_name}_handler(closure),
                        context: Arc::new(TraceContext {{
                            {database}
                            {csv}
                            extensions: Arc::new(extensions),
                        }}),
                    }}
                }}
            }}
            #[async_trait]
            impl<TExtensions> TraceCallback for {name}Event<TExtensions> where TExtensions: Send + Sync {{
                async fn call(&self, events: Vec<TraceResult>) -> TraceCallbackResult<()> {{
                    {event_callback_events_len}
                    // note some can not downcast because it cant decode
                    // this happens on events which failed decoding due to
                    // not having the right abi for example
                    // transfer events with 2 indexed topics cant decode
                    // transfer events with 3 indexed topics
                    let result: Vec<{name}Result> = events
                        .into_iter()
                        .filter_map(|item| {{
                            match item {{
                                TraceResult::NativeTransfer {{ from, to, value, tx_information, .. }} => {{
                                    Some({name}Result {{
                                        event_data: {name}Data {{
                                            from,
                                            to,
                                            value,
                                        }},
                                        tx_information,
                                    }})
                                }}
                                _ => None,
                            }}
                        }})
                        .collect();
                    {event_callback_return}
                }}
            }}
            "#,
            name = info.name,
            lower_name = info.name.to_lowercase(),
            struct_result = info.struct_result(),
            database = if databases_enabled {
                "database: get_or_init_postgres_client().await,"
            } else {
                ""
            },
            csv = if csv_enabled { r#"csv: Arc::new(csv),"# } else { "" },
            csv_generator = csv_generator,
            event_callback_events_len =
                if !is_filter { "let _events_len = events.len();" } else { "" },
            event_callback_return = if !is_filter {
                r#"
                        (self.callback)(&result, Arc::clone(&self.context)).await
                    "#
                .to_string()
            } else {
                "(self.callback)(&result, Arc::clone(&self.context)).await".to_string()
            }
        );
        parts.push(part);
    }
    // Add Block event callback struct
    let block_part = format!(
        r#"
        pub fn block_handler<TExtensions, F, Fut>(
            custom_logic: F,
        ) -> BlockTraceCallbackType<TExtensions>
        where
            BlockResult: Clone + 'static,
            F: for<'a> Fn(Vec<BlockResult>, Arc<TraceContext<TExtensions>>) -> Fut
                + Send
                + Sync
                + 'static
                + Clone,
            Fut: Future<Output = TraceCallbackResult<()>> + Send + 'static,
            TExtensions: Send + Sync + 'static,
        {{
            Arc::new(move |results, context| {{
                let custom_logic = custom_logic.clone();
                let results = results.clone();
                let context = Arc::clone(&context);
                async move {{ (custom_logic)(results, context).await }}.boxed()
            }})
        }}
        type BlockTraceCallbackType<TExtensions> = Arc<
            dyn for<'a> Fn(&'a Vec<BlockResult>, Arc<TraceContext<TExtensions>>) -> BoxFuture<'a, TraceCallbackResult<()>>
                + Send
                + Sync,
            >;
        pub struct BlockEvent<TExtensions> where TExtensions: Send + Sync + 'static {{
            callback: BlockTraceCallbackType<TExtensions>,
            context: Arc<TraceContext<TExtensions>>,
        }}
        impl<TExtensions> BlockEvent<TExtensions> where TExtensions: Send + Sync + 'static {{
            pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
            where
                BlockResult: Clone + 'static,
                F: for<'a> Fn(Vec<BlockResult>, Arc<TraceContext<TExtensions>>) -> Fut
                    + Send
                    + Sync
                    + 'static
                    + Clone,
                Fut: Future<Output = TraceCallbackResult<()>> + Send + 'static,
            {{
                Self {{
                    callback: block_handler(closure),
                    context: Arc::new(TraceContext {{
                        {database}
                        extensions: Arc::new(extensions),
                    }}),
                }}
            }}
        }}
        #[async_trait]
        impl<TExtensions> TraceCallback for BlockEvent<TExtensions> where TExtensions: Send + Sync {{
            async fn call(&self, events: Vec<TraceResult>) -> TraceCallbackResult<()> {{
                // Block events are passed as-is, no decoding needed
                let result: Vec<BlockResult> = events
                    .into_iter()
                    .filter_map(|item| {{
                        match item {{
                            TraceResult::Block {{ block, tx_information, .. }} => {{
                                Some(BlockResult {{ block, tx_information }})
                            }}
                            _ => None,
                        }}
                    }})
                    .collect();
                if !result.is_empty() {{
                    (self.callback)(&result, Arc::clone(&self.context)).await
                }} else {{
                    Ok(())
                }}
            }}
        }}
        "#,
        database =
            if databases_enabled { "database: get_or_init_postgres_client().await," } else { "" },
    );
    parts.push(block_part);
    Ok(Code::new(parts.join("\n")))
}
fn decoder_contract_fn(contracts_details: Vec<NativeTransferDetails>, abi_gen_name: &str) -> Code {
    let mut function = String::new();
    function.push_str(&format!(
        r#"pub async fn decoder_contract(network: &str) -> {abi_gen_name}Instance<Arc<RindexerProvider>, AnyNetwork> {{"#
    ));
    let networks: Vec<&String> = contracts_details.iter().map(|c| &c.network).collect();
    for (index, network) in networks.iter().enumerate() {
        if index == 0 {
            function.push_str("    if ");
        } else {
            function.push_str("    else if ");
        }
        function.push_str(&format!(
            r#"network == "{network}" {{
                {abi_gen_name}::new(
                    // do not care about address here its decoding makes it easier to handle ValueOrArray
                    Address::ZERO,
                    get_provider_cache_for_network(network).await.get_inner_provider(),
                 )
            }}"#
        ));
    }
    // Add a fallback else statement to handle unsupported networks
    function.push_str(
        r#"
        else {
            panic!("Network not supported");
        }
    }"#,
    );
    Code::new(function)
}
fn build_pub_contract_fn(
    contract_name: &str,
    _contracts_details: Vec<NativeTransferDetails>,
    abi_gen_name: &str,
) -> Code {
    let contract_name = camel_to_snake(contract_name);
    Code::new(format!(
        r#"pub async fn {contract_name}_contract(network: &str, address: Address) -> {abi_gen_name}Instance<Arc<RindexerProvider>, AnyNetwork> {{
                {abi_gen_name}::new(
                    address,
                    get_provider_cache_for_network(network).await.get_inner_provider(),
                 )
               }}
            "#
    ))
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateTraceBindingCodeError {
    #[error("Could not read ABI string: {0}")]
    CouldNotReadAbiString(#[from] std::io::Error),
    #[error("Could not read ABI JSON: {0}")]
    CouldNotReadAbiJson(#[from] serde_json::Error),
    #[error("{0}")]
    GenerateStructsError(#[from] GenerateStructsError),
    #[error("{0}")]
    GenerateTraceCallbackStructsError(#[from] GenerateTraceCallbackStructsError),
}
fn generate_trace_bindings_code(
    project_path: &Path,
    indexer_name: &str,
    contract_name: &str,
    contract: &NativeTransfers,
    storage: &Storage,
    event_info: Vec<EventInfo>,
) -> Result<Code, GenerateTraceBindingCodeError> {
    let event_type_name = generate_event_type_name(contract_name);
    let code = Code::new(format!(
        r#"#![allow(non_camel_case_types, clippy::enum_variant_names, clippy::too_many_arguments, clippy::upper_case_acronyms, clippy::type_complexity, dead_code)]
        /// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
        ///
        /// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
        /// Any manual changes to this file will be overwritten.
        use super::{abigen_file_name}::{{
            {abigen_name}::{{self, {abigen_name}Instance, {abigen_name}Events}}
        }};
        use std::{{any::Any, sync::Arc}};
        use std::error::Error;
        use std::future::Future;
        use std::collections::HashMap;
        use std::pin::Pin;
        use std::path::{{Path, PathBuf}};
        use alloy::network::AnyNetwork;
        use alloy::primitives::{{Address, Bytes, B256}};
        use alloy::sol_types::{{SolEvent, SolEventInterface, SolType}};
        use rindexer::{{
            async_trait,
            {csv_import}
            generate_random_id,
            FutureExt,
            event::{{
                callback_registry::{{
                    TraceCallbackRegistry, TraceCallbackRegistryInformation, TraceCallbackResult,
                    TraceResult, TxInformation, HasTxInformation
                }},
                contract_setup::{{TraceInformation, NetworkTrace}},
            }},
            manifest::{{
                contract::{{Contract, ContractDetails}},
                yaml::read_manifest,
            }},
            provider::{{JsonRpcCachedProvider, RindexerProvider}},
            {postgres_client_import}
        }};
        use super::super::super::super::typings::networks::get_provider_cache_for_network;
        {postgres_import}
        {structs}
        type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
        #[async_trait]
        trait TraceCallback {{
            async fn call(&self, events: Vec<TraceResult>) -> TraceCallbackResult<()>;
        }}
        pub struct TraceContext<TExtensions> where TExtensions: Send + Sync {{
            {event_context_database}
            {event_context_csv}
            pub extensions: Arc<TExtensions>,
        }}
        // didn't want to use option or none made harder DX
        // so a blank struct makes interface nice
        pub struct NoExtensions {{}}
        pub fn no_extensions() -> NoExtensions {{
            NoExtensions {{}}
        }}
        {event_callback_structs}
        pub enum {event_type_name}<TExtensions> where TExtensions: 'static + Send + Sync {{
            {event_enums}
        }}
        {build_pub_contract_fn}
        {decoder_contract_fn}
        impl<TExtensions> {event_type_name}<TExtensions> where TExtensions: 'static + Send + Sync {{
            pub fn event_name(&self) -> &'static str {{
                match self {{
                    {event_names_match_arms}
                }}
            }}
            pub fn contract_name(&self) -> String {{
                "{raw_contract_name}".to_string()
            }}
            async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {{
                get_provider_cache_for_network(network).await
            }}
            fn decoder(&self, network: &str) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {{
                let decoder_contract = decoder_contract(network);
                match self {{
                    {decoder_match_arms}
                }}
            }}
            pub async fn register(self, manifest_path: &PathBuf, registry: &mut TraceCallbackRegistry) {{
                 let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
                 let contract_name = self.contract_name();
                 let event_name = self.event_name();
                 let contract_details = rindexer_yaml
                    .native_transfers.networks.unwrap_or_default();
                // Expect providers to have been initialized, but it's an async init so this should
                // be fast but for correctness we must await each future.
                let mut providers = HashMap::new();
                for n in contract_details.iter() {{
                    let provider = self.get_provider(&n.network).await;
                    providers.insert(n.network.clone(), provider);
                }}
                let trace_information = TraceInformation {{
                    name: event_name.to_string(),
                    details: contract_details
                        .iter()
                        .map(|c| NetworkTrace {{
                            id: generate_random_id(10),
                            network: c.network.clone(),
                            cached_provider: providers
                                .get(&c.network)
                                .expect("must have a provider")
                                .clone(),
                            start_block: c.start_block,
                            end_block: c.end_block,
                            method: c.method,
                        }})
                        .collect(),
                    reorg_safe_distance: rindexer_yaml
                        .native_transfers.reorg_safe_distance.unwrap_or_default(),
                }};
                let callback: Arc<dyn Fn(Vec<TraceResult>) -> BoxFuture<'static, TraceCallbackResult<()>> + Send + Sync> = match self {{
                    {register_match_arms}
                }};
               registry.register_event(TraceCallbackRegistryInformation {{
                    id: generate_random_id(10),
                    indexer_name: "{indexer_name}".to_string(),
                    event_name: event_name.to_string(),
                    contract_name: contract_name.to_string(),
                    trace_information: trace_information,
                    callback,
                }});
            }}
        }}
        "#,
        postgres_import = if storage.postgres_enabled() {
            "use super::super::super::super::typings::database::get_or_init_postgres_client;"
        } else {
            ""
        },
        postgres_client_import = if storage.postgres_enabled() { "PostgresClient," } else { "" },
        csv_import = if storage.csv_enabled() { "AsyncCsvAppender," } else { "" },
        abigen_file_name = trace_abigen_contract_file_name(contract_name),
        abigen_name = trace_abigen_contract_name(contract_name),
        structs = trace_generate_structs(contract_name)?,
        event_type_name = &event_type_name,
        event_context_database =
            if storage.postgres_enabled() { "pub database: Arc<PostgresClient>," } else { "" },
        event_context_csv =
            if storage.csv_enabled() { "pub csv: Arc<AsyncCsvAppender>," } else { "" },
        event_callback_structs =
            generate_trace_callback_structs_code(project_path, &event_info, contract, storage)?,
        event_enums = generate_event_enums_code(&event_info),
        event_names_match_arms =
            generate_event_names_match_arms_code(&event_type_name, &event_info),
        raw_contract_name = contract_name,
        decoder_contract_fn = decoder_contract_fn(
            contract.networks.clone().unwrap_or_default(),
            &trace_abigen_contract_name(contract_name)
        ),
        build_pub_contract_fn = build_pub_contract_fn(
            contract_name,
            contract.networks.clone().unwrap_or_default(),
            &trace_abigen_contract_name(contract_name)
        ),
        decoder_match_arms = generate_decoder_match_arms_code(&event_type_name, &event_info),
        register_match_arms = generate_register_match_arms_code(&event_type_name, &event_info)
    ));
    Ok(code)
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateTraceBindingsError {
    #[error("{0}")]
    ReadAbi(#[from] ReadAbiError),
    #[error("{0}")]
    GenerateEventBindingCode(#[from] GenerateTraceBindingCodeError),
    #[error("{0}")]
    ParamType(#[from] ParamTypeError),
}
/// Minimal changes by hardcoding the "mocked" native transfer Event abi as per erc20 standard.
fn get_native_transfer_abi_items() -> Vec<ABIItem> {
    vec![ABIItem {
        inputs: vec![
            ABIInput {
                indexed: Some(true),
                name: "from".to_string(),
                type_: "address".to_string(),
                components: None,
            },
            ABIInput {
                indexed: Some(true),
                name: "to".to_string(),
                type_: "address".to_string(),
                components: None,
            },
            ABIInput {
                indexed: Some(false),
                name: "value".to_string(),
                type_: "uint256".to_string(),
                components: None,
            },
        ],
        name: "NativeTransfer".to_string(),
        type_: "event".to_string(),
    }]
}
pub fn generate_trace_bindings(
    project_path: &Path,
    indexer_name: &str,
    contract_name: &str,
    contract: &NativeTransfers,
    _is_filter: bool,
    storage: &Storage,
) -> Result<Code, GenerateTraceBindingsError> {
    let abi_items = get_native_transfer_abi_items();
    let event_names = ABIItem::extract_event_names_and_signatures_from_abi(abi_items)?;
    generate_trace_bindings_code(
        project_path,
        indexer_name,
        contract_name,
        contract,
        storage,
        event_names,
    )
    .map_err(GenerateTraceBindingsError::GenerateEventBindingCode)
}
#[derive(thiserror::Error, Debug)]
pub enum GenerateTraceHandlersError {
    #[error("{0}")]
    ReadAbiError(#[from] ReadAbiError),
    #[error("{0}")]
    ParamTypeError(#[from] ParamTypeError),
}
pub fn generate_trace_handlers(
    indexer_name: &str,
    contract_name: &str,
    storage: &Storage,
) -> Result<Code, GenerateTraceHandlersError> {
    let abi_items = get_native_transfer_abi_items();
    let event_names = ABIItem::extract_event_names_and_signatures_from_abi(abi_items)?;
    let mut imports = String::new();
    imports.push_str(
        r#"#![allow(non_snake_case)]
            use rindexer::{
                event::callback_registry::TraceCallbackRegistry,
                EthereumSqlTypeWrapper, PgType, rindexer_error, rindexer_info
            };
        "#,
    );
    imports.push_str("use std::sync::Arc;\n");
    imports.push_str(&format!(
        r#"use std::path::PathBuf;
        use super::super::super::typings::{indexer_name_formatted}::events::{handler_registry_name}::{{no_extensions, {event_type_name}"#,
        indexer_name_formatted = camel_to_snake(indexer_name),
        handler_registry_name = camel_to_snake(contract_name),
        event_type_name = generate_event_type_name(contract_name)
    ));
    let mut handlers = String::new();
    let mut registry_fn = String::new();
    registry_fn.push_str(&format!(
        r#"pub async fn {handler_registry_fn_name}_handlers(manifest_path: &PathBuf, registry: &mut TraceCallbackRegistry) {{"#,
        handler_registry_fn_name = camel_to_snake(contract_name),
    ));
    for event in event_names {
        let event_type_name = generate_event_type_name(contract_name);
        imports.push_str(&format!(r#",{handler_name}Event"#, handler_name = event.name,));
        let abi_name_properties = ABIInput::generate_abi_name_properties(
            &event.inputs,
            &GenerateAbiPropertiesType::Object,
            None,
        );
        let mut csv_write = String::new();
        // this checks storage enabled as well
        if !storage.csv_disable_create_headers() {
            let mut csv_data = String::new();
            csv_data.push_str(r#"format!("{:?}", result.tx_information.address),"#);
            for item in &abi_name_properties {
                // note: tracing uses a custom NativeTransfer event
                // so only address and uint256 are used
                csv_data.push_str(&format!("result.event_data.{}.to_string(),", item.value));
            }
            csv_data.push_str(r#"format!("{:?}", result.tx_information.transaction_hash),"#);
            csv_data.push_str(r#"result.tx_information.block_number.to_string(),"#);
            csv_data.push_str(r#"result.tx_information.block_hash.to_string(),"#);
            csv_data.push_str(r#"result.tx_information.network.to_string(),"#);
            csv_data.push_str(r#"result.tx_information.transaction_index.to_string(),"#);
            csv_data.push_str(r#"result.tx_information.log_index.to_string()"#);
            csv_write = format!(r#"csv_bulk_data.push(vec![{csv_data}]);"#,);
            if storage.postgres_disable_create_tables() {
                csv_write = format!(
                    r#"
                      let mut csv_bulk_data: Vec<Vec<String>> = vec![];
                      for result in &results {{
                        {inner_csv_write}
                      }}
                      if !csv_bulk_data.is_empty() {{
                        let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                        if let Err(e) = csv_result {{
                            rindexer_error!("{event_type_name}::{handler_name} inserting csv data: {{:?}}", e);
                            return Err(e.to_string());
                        }}
                      }}
                    "#,
                    inner_csv_write = csv_write,
                    event_type_name = event_type_name,
                    handler_name = event.name,
                );
            }
        }
        let mut postgres_write = String::new();
        // this checks storage enabled as well
        if !storage.postgres_disable_create_tables() {
            let mut data =
                "vec![EthereumSqlTypeWrapper::Address(result.tx_information.address),".to_string();
            for item in &abi_name_properties {
                if let Some(wrapper) = &item.ethereum_sql_type_wrapper {
                    data.push_str(&format!(
                        "EthereumSqlTypeWrapper::{}(result.event_data.{}),",
                        wrapper.raw_name(),
                        item.value,
                    ));
                } else {
                    // data.push_str(&format!("result.event_data.{},", item.value));
                    panic!("No EthereumSqlTypeWrapper found for: {:?}", item.abi_type);
                }
            }
            data.push_str(
                "\nEthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),",
            );
            data.push_str("\nEthereumSqlTypeWrapper::U64(result.tx_information.block_number),");
            data.push_str("\nEthereumSqlTypeWrapper::DateTimeNullable(result.tx_information.block_timestamp_to_datetime()),");
            data.push_str("\nEthereumSqlTypeWrapper::B256(result.tx_information.block_hash),");
            data.push_str(
                "EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),",
            );
            data.push_str(
                "\nEthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),",
            );
            data.push_str("\nEthereumSqlTypeWrapper::U256(result.tx_information.log_index)");
            data.push(']');
            postgres_write = format!(
                r#"
                    let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
                    {csv_bulk_data}
                    for result in results.iter() {{
                        {csv_write}
                        let data = {data};
                        postgres_bulk_data.push(data);
                    }}
                    {csv_bulk_insert}
                    if postgres_bulk_data.is_empty() {{
                        return Ok(());
                    }}
                    let result = context
                        .database
                        .insert_bulk(
                            "{table_name}",
                            &[{columns_names}],
                            &postgres_bulk_data,
                        )
                        .await;
                    if let Err(e) = result {{
                        rindexer_error!("{event_type_name}::{handler_name} inserting bulk data: {{:?}}", e);
                        return Err(e.to_string());
                    }}
                "#,
                table_name =
                    generate_event_table_full_name(indexer_name, contract_name, &event.name),
                handler_name = event.name,
                event_type_name = event_type_name,
                columns_names = generate_column_names_only_with_base_properties(&event.inputs)
                    .iter()
                    .map(|item| format!("\"{item}\".to_string()"))
                    .collect::<Vec<String>>()
                    .join(", "),
                data = data,
                csv_write = csv_write,
                csv_bulk_data = if storage.csv_enabled() {
                    "let mut csv_bulk_data: Vec<Vec<String>> = vec![];"
                } else {
                    ""
                },
                csv_bulk_insert = if storage.csv_enabled() {
                    format!(
                        r#"if !csv_bulk_data.is_empty() {{
                        let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                        if let Err(e) = csv_result {{
                            rindexer_error!("{event_type_name}::{handler_name} inserting csv data: {{:?}}", e);
                            return Err(e.to_string());
                        }}
                    }}"#,
                        event_type_name = event_type_name,
                        handler_name = event.name
                    )
                } else {
                    "".to_string()
                }
            );
        }
        let handler = format!(
            r#"
            async fn {handler_fn_name}_handler(manifest_path: &PathBuf, registry: &mut TraceCallbackRegistry) {{
                {event_type_name}::{handler_name}(
                    {handler_name}Event::handler(|results, context| async move {{
                            if results.is_empty() {{
                                return Ok(());
                            }}
                            {csv_write}
                            {postgres_write}
                            rindexer_info!(
                                "{contract_name}::{handler_name} - INDEXED - {{}} events",
                                results.len(),
                            );
                            Ok(())
                        }},
                        no_extensions(),
                      )
                      .await,
                )
                .register(manifest_path, registry).await;
            }}
        "#,
            handler_fn_name = camel_to_snake(&event.name),
            handler_name = event.name,
            event_type_name = event_type_name,
            contract_name = contract_name,
            csv_write = if !postgres_write.is_empty() { String::new() } else { csv_write },
            postgres_write = postgres_write,
        );
        handlers.push_str(&handler);
        registry_fn.push_str(&format!(
            r#"
                {handler_fn_name}_handler(manifest_path, registry).await;
            "#,
            handler_fn_name = camel_to_snake(&event.name)
        ));
    }
    // Add Block event handler
    imports.push_str(",BlockEvent};\n");
    let block_handler = format!(
        r#"
        async fn block_handler(manifest_path: &PathBuf, registry: &mut TraceCallbackRegistry) {{
            BlockEvent::handler(|results, context| async move {{
                if results.is_empty() {{
                    return Ok(());
                }}
                rindexer_info!(
                    "{contract_name}::Block - INDEXED - {{}} blocks",
                    results.len(),
                );
                Ok(())
            }},
            no_extensions(),
          )
          .await,
        }}
        "#,
        contract_name = contract_name,
    );
    handlers.push_str(&block_handler);
    registry_fn.push_str("block_handler(manifest_path, registry).await;");
    registry_fn.push('}');
    let mut code = String::new();
    code.push_str(&imports);
    code.push_str(&handlers);
    code.push_str(&registry_fn);
    Ok(Code::new(code))
}
</file>

<file path="core/src/helpers/array.rs">
use std::collections::HashSet;
pub fn chunk_hashset<T: Clone + Eq + std::hash::Hash>(
    set: HashSet<T>,
    chunk_size: usize,
) -> Vec<HashSet<T>> {
    let mut chunks = Vec::with_capacity(set.len().div_ceil(chunk_size));
    let mut current_chunk = HashSet::with_capacity(chunk_size);
    for item in set {
        current_chunk.insert(item);
        if current_chunk.len() == chunk_size {
            chunks.push(std::mem::replace(&mut current_chunk, HashSet::with_capacity(chunk_size)));
        }
    }
    if !current_chunk.is_empty() {
        chunks.push(current_chunk);
    }
    chunks
}
</file>

<file path="core/src/helpers/duration.rs">
use std::time::Duration;
pub fn format_duration(duration: Duration) -> String {
    let secs = duration.as_secs();
    let hours = secs / 3600;
    let minutes = (secs % 3600) / 60;
    let seconds = secs % 60;
    match (hours, minutes) {
        (h, m) if h > 0 => format!("{}h {}m {}s", h, m, seconds),
        (0, m) if m > 0 => format!("{}m {}s", m, seconds),
        _ => format!("{}s", seconds),
    }
}
</file>

<file path="core/src/helpers/evm_log.rs">
use std::collections::HashSet;
use std::str::FromStr;
use alloy::network::AnyRpcBlock;
use alloy::{
    dyn_abi::{DynSolValue, EventExt},
    json_abi::Event,
    primitives::{keccak256, Address, Bloom, B256, U256, U64},
    rpc::types::{FilterSet, FilteredParams, Log, ValueOrArray},
};
use tracing::error;
use crate::types::core::{LogParam, ParsedLog};
pub fn parse_log(event: &Event, log: &Log) -> Option<ParsedLog> {
    // as topic[0] is the event signature
    let topics_length = log.topics().len() - 1;
    let indexed_inputs_abi_length = event.inputs.iter().filter(|param| param.indexed).count();
    // check if topics and data match the event
    if topics_length == indexed_inputs_abi_length {
        if let Ok(decoded) = event.decode_log(&log.inner) {
            let mut indexed_iter = decoded.indexed.into_iter();
            let mut body_iter = decoded.body.into_iter();
            let params = event
                .inputs
                .iter()
                .map(|input| {
                    let value = if input.indexed {
                        indexed_iter.next().expect("Not enough indexed values")
                    } else {
                        body_iter.next().expect("Not enough body values")
                    };
                    LogParam {
                        name: input.name.clone(),
                        value,
                        components: input.components.clone(),
                    }
                })
                .collect();
            return Some(ParsedLog { params });
        }
    }
    None
}
fn map_token_to_raw_values(token: &DynSolValue) -> Vec<String> {
    match token {
        DynSolValue::Address(addr) => vec![format!("{:?}", addr)],
        DynSolValue::FixedBytes(bytes, _) => vec![format!("{:?}", bytes)],
        DynSolValue::Bytes(bytes) => vec![format!("{:?}", bytes)],
        DynSolValue::Int(int, _) => {
            vec![int.to_string()]
        }
        DynSolValue::Uint(uint, _) => {
            vec![uint.to_string()]
        }
        DynSolValue::Bool(b) => vec![b.to_string()],
        DynSolValue::String(s) => vec![s.clone()],
        DynSolValue::FixedArray(tokens) | DynSolValue::Array(tokens) => {
            let values: Vec<String> = tokens.iter().flat_map(map_token_to_raw_values).collect();
            vec![format!("[{}]", values.join(", "))]
        }
        DynSolValue::Tuple(tokens) => {
            let mut values = vec![];
            for token in tokens {
                values.extend(map_token_to_raw_values(token));
            }
            values
        }
        _ => {
            error!("Error parsing unsupported token {:?}", token);
            unimplemented!(
                "Functions and CustomStruct are not supported yet for `map_token_to_raw_values`"
            );
        }
    }
}
pub fn map_log_params_to_raw_values(params: &[LogParam]) -> Vec<String> {
    let mut raw_values = vec![];
    for param in params {
        raw_values.extend(map_token_to_raw_values(&param.value));
    }
    raw_values
}
pub fn parse_topic(input: &str) -> B256 {
    match input.to_lowercase().as_str() {
        "true" => B256::from(U256::from(1)),
        "false" => B256::ZERO,
        _ => {
            if let Ok(address) = Address::from_str(input) {
                B256::from(address.into_word())
            } else if let Ok(num) = U256::from_str(input) {
                B256::from(num)
            } else {
                B256::from(keccak256(input))
            }
        }
    }
}
pub fn contract_in_bloom(contract_address: Address, logs_bloom: Bloom) -> bool {
    let filter = FilterSet::from(ValueOrArray::Value(contract_address));
    let address_filter = FilteredParams::address_filter(&filter);
    FilteredParams::matches_address(logs_bloom, &address_filter)
}
pub fn topic_in_bloom(topic_id: B256, logs_bloom: Bloom) -> bool {
    let filter = FilterSet::from(ValueOrArray::Value(Some(topic_id)));
    let topic_filter = FilteredParams::topics_filter(&[filter]);
    FilteredParams::matches_topics(logs_bloom, &topic_filter)
}
pub fn is_relevant_block(
    contract_address: &Option<HashSet<Address>>,
    topic_id: &B256,
    latest_block: &AnyRpcBlock,
) -> bool {
    let logs_bloom = latest_block.header.logs_bloom;
    if let Some(contract_address) = contract_address {
        if contract_address.iter().all(|addr| !contract_in_bloom(*addr, logs_bloom)) {
            return false;
        }
    }
    if !topic_in_bloom(*topic_id, logs_bloom) {
        return false;
    }
    true
}
/// Take either the halved block range, or 2 blocks from the current. This is to prevent possible
/// stalling risk and ensure we always make progress.
pub fn halved_block_number(to_block: U64, from_block: U64) -> U64 {
    let halved_range = (to_block - from_block) / U64::from(2);
    (from_block + halved_range).max(from_block + U64::from(2))
}
</file>

<file path="core/src/helpers/file.rs">
use std::{fs, fs::File, io::Write, path::Path, process::Command};
use dotenv::{dotenv, from_path};
/// Formats all Rust source files in the given folder using `cargo fmt`.
pub fn format_all_files_for_project<P: AsRef<Path>>(project_path: P) {
    let manifest_path = project_path.as_ref().join("Cargo.toml");
    let status = Command::new("cargo")
        .arg("fmt")
        .arg("--manifest-path")
        .arg(manifest_path)
        .status()
        .expect("Failed to execute cargo fmt.");
    if !status.success() {
        panic!("cargo fmt failed with status: {status:?}");
    }
}
#[derive(thiserror::Error, Debug)]
pub enum WriteFileError {
    #[error("Could not create dir: {0}")]
    CouldNotCreateDir(std::io::Error),
    #[error("Could not convert string to bytes: {0}")]
    CouldNotConvertToBytes(std::io::Error),
    #[error("Could not create the file: {0}")]
    CouldNotCreateFile(std::io::Error),
}
pub fn write_file(path: &Path, contents: &str) -> Result<(), WriteFileError> {
    if let Some(dir) = path.parent() {
        fs::create_dir_all(dir).map_err(WriteFileError::CouldNotCreateDir)?;
    }
    let cleaned_contents: String =
        contents.lines().map(|line| line.trim_end()).collect::<Vec<&str>>().join("\n");
    let mut file = File::create(path).map_err(WriteFileError::CouldNotCreateFile)?;
    file.write_all(cleaned_contents.as_bytes()).map_err(WriteFileError::CouldNotConvertToBytes)?;
    Ok(())
}
#[derive(thiserror::Error, Debug)]
pub enum CreateModFileError {
    #[error("Could not read path: {0}")]
    ReadPath(std::io::Error),
    #[error("Could not read dir entries: {0}")]
    ReadDirEntries(std::io::Error),
    #[error("Could not create file: {0}")]
    CreateFile(std::io::Error),
    #[error("Could not write extra lines to file: {0}")]
    WriteExtraLines(std::io::Error),
}
/// Creates a `mod.rs` file for a given directory, including submodules for all Rust files and
/// directories.
pub fn create_mod_file(
    path: &Path,
    code_generated_comment: bool,
) -> Result<(), CreateModFileError> {
    let entries = fs::read_dir(path).map_err(CreateModFileError::ReadPath)?;
    let mut mods = Vec::new();
    let mut dirs = Vec::new();
    for entry in entries {
        let entry = entry.map_err(CreateModFileError::ReadDirEntries)?;
        let path = entry.path();
        if path.is_dir() {
            if let Some(dir_name) = path.file_name().and_then(|n| n.to_str()) {
                dirs.push(dir_name.to_owned());
                create_mod_file(&path, code_generated_comment)?;
            }
        } else if let Some(ext) = path.extension() {
            #[allow(clippy::unnecessary_map_or)]
            if ext == "rs" && path.file_stem().map_or(true, |s| s != "mod") {
                if let Some(file_stem) = path.file_stem().and_then(|s| s.to_str()) {
                    mods.push(file_stem.to_owned());
                }
            }
        }
    }
    if !mods.is_empty() || !dirs.is_empty() {
        let mod_path = path.join("mod.rs");
        let mut mod_file = File::create(mod_path).map_err(CreateModFileError::CreateFile)?;
        writeln!(mod_file, "#![allow(dead_code, unused)]")
            .map_err(CreateModFileError::WriteExtraLines)?;
        if code_generated_comment {
            write!(
                mod_file,
                r#"
            /// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
            ///
            /// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer
            /// Any manual changes to this file will be overwritten.
            "#
            )
            .map_err(CreateModFileError::WriteExtraLines)?;
        }
        for item in mods.iter().chain(dirs.iter()) {
            if item.contains("_abi_gen") {
                writeln!(mod_file, "mod {item};").map_err(CreateModFileError::WriteExtraLines)?;
            } else {
                writeln!(mod_file, "pub mod {item};")
                    .map_err(CreateModFileError::WriteExtraLines)?;
            }
        }
    }
    Ok(())
}
pub fn load_env_from_project_path(project_path: &Path) {
    if from_path(project_path.join(".env")).is_err() {
        dotenv().ok();
    }
}
pub fn load_env_from_full_path(path: &Path) {
    if from_path(path).is_err() {
        dotenv().ok();
    }
}
</file>

<file path="core/src/helpers/mod.rs">
mod thread;
pub use thread::set_thread_no_logging;
mod array;
mod evm_log;
mod file;
mod solidity;
pub use array::chunk_hashset;
pub use evm_log::{
    halved_block_number, is_relevant_block, map_log_params_to_raw_values, parse_log, parse_topic,
};
pub use solidity::{
    is_irregular_width_solidity_integer_type, is_solidity_static_bytes_type,
    parse_solidity_integer_type,
};
use std::{
    env,
    env::VarError,
    path::{Path, PathBuf},
    str,
};
use dotenv::dotenv;
pub use file::{
    create_mod_file, format_all_files_for_project, load_env_from_full_path,
    load_env_from_project_path, write_file, CreateModFileError, WriteFileError,
};
use rand::{distr::Alphanumeric, Rng};
mod duration;
pub use duration::format_duration;
pub fn camel_to_snake(s: &str) -> String {
    camel_to_snake_advanced(s, false)
}
pub fn camel_to_snake_advanced(s: &str, numbers_attach_to_last_word: bool) -> String {
    let mut snake_case = String::new();
    let mut previous_was_uppercase = false;
    let mut previous_was_digit = false;
    let mut uppercase_sequence_length = 0;
    for (i, c) in s.chars().enumerate() {
        if c.is_alphanumeric() || c == '_' {
            if c.is_uppercase() {
                if i > 0
                    && (!previous_was_uppercase
                        || (i + 1 < s.len()
                            && s.chars().nth(i + 1).expect("Failed to get char").is_lowercase()))
                {
                    snake_case.push('_');
                }
                snake_case.push(c.to_ascii_lowercase());
                previous_was_uppercase = true;
                previous_was_digit = false;
                uppercase_sequence_length += 1;
            } else if c.is_ascii_digit() {
                if !numbers_attach_to_last_word
                    && i > 0
                    && !previous_was_digit
                    && !snake_case.ends_with('_')
                    && uppercase_sequence_length != 1
                {
                    snake_case.push('_');
                }
                snake_case.push(c);
                previous_was_uppercase = false;
                previous_was_digit = true;
                uppercase_sequence_length = 0;
            } else {
                snake_case.push(c);
                previous_was_uppercase = false;
                previous_was_digit = false;
                uppercase_sequence_length = 0;
            }
        }
    }
    snake_case
}
pub fn to_pascal_case(input: &str) -> String {
    if input.is_empty() {
        return String::new();
    }
    // Events like Erc1155 "URI" Event are capitalized and need to remain that way. Return it as it
    // is.
    if !input.contains("_") && input.chars().filter(|c| c.is_ascii_lowercase()).count() == 0 {
        return input.to_string();
    }
    let words: Vec<&str> = input.split('_').filter(|s| !s.is_empty()).collect();
    let mut result = String::with_capacity(input.len());
    for (i, word) in words.iter().enumerate() {
        if i > 0 {
            result.push('_');
        }
        result.push_str(&capitalize_word(word, i == 0 && words.len() == 1));
    }
    result.replace('_', "")
}
fn capitalize_word(word: &str, is_single_word: bool) -> String {
    if word.chars().all(|c| c.is_ascii_uppercase()) {
        if is_single_word {
            // Convert single all-uppercase word to Pascal case
            let mut chars = word.chars();
            return chars.next().unwrap().to_string() + &chars.as_str().to_lowercase();
        } else {
            // Preserve acronyms in compound words
            return word.to_string();
        }
    }
    let mut result = String::with_capacity(word.len());
    let mut chars = word.chars();
    // Capitalize the first character
    if let Some(first) = chars.next() {
        result.extend(first.to_uppercase());
    }
    let mut prev_is_upper = false;
    for c in chars {
        if c.is_ascii_uppercase() {
            if !prev_is_upper {
                result.push('_');
            }
            result.extend(c.to_uppercase());
            prev_is_upper = true;
        } else {
            result.extend(c.to_lowercase());
            prev_is_upper = false;
        }
    }
    result
}
pub fn generate_random_id(len: usize) -> String {
    rand::rng().sample_iter(&Alphanumeric).take(len).map(char::from).collect()
}
pub fn get_full_path(project_path: &Path, file_path: &str) -> Result<PathBuf, std::io::Error> {
    let path = PathBuf::from(file_path);
    if let Ok(canonical_path) = path.canonicalize() {
        Ok(canonical_path)
    } else {
        let joined_path = project_path.join(file_path);
        joined_path.canonicalize()
    }
}
pub fn kill_process_on_port(port: u16) -> Result<(), String> {
    port_killer::kill(port).map(|_| ()).map_err(|e| e.to_string())
}
pub fn public_read_env_value(var_name: &str) -> Result<String, VarError> {
    dotenv().ok();
    env::var(var_name)
}
pub fn replace_env_variable_to_raw_name(rpc: &str) -> String {
    if rpc.starts_with("${") && rpc.ends_with('}') {
        rpc[2..rpc.len() - 1].to_string()
    } else {
        rpc.to_string()
    }
}
// Remove this if we're sure we no longer need it after the Alloy migration
//
// pub fn u256_to_i256(value: U256) -> I256 {
//     let max_i256_as_u256 = U256::from_str_radix(
//         "7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff",
//         16,
//     )
//     .unwrap();
//
//     if value <= max_i256_as_u256 {
//         // If the value is less than or equal to I256::MAX, it's a positive number
//         I256::from_raw(value)
//     } else {
//         // If it's larger, it represents a negative number in two's complement
//         let twos_complement = (!value).overflowing_add(U256::ONE).0;
//         I256::from_raw(twos_complement).wrapping_neg()
//     }
// }
#[cfg(test)]
mod tests {
    use super::*;
    #[test]
    fn test_camel_to_snake() {
        assert_eq!(camel_to_snake("CamelCase"), "camel_case");
        assert_eq!(camel_to_snake("Camel-Case"), "camel_case");
        assert_eq!(camel_to_snake("camelCase"), "camel_case");
        assert_eq!(camel_to_snake("camel_case"), "camel_case");
        assert_eq!(camel_to_snake("Camel"), "camel");
        assert_eq!(camel_to_snake("camel"), "camel");
        assert_eq!(camel_to_snake("collectNFTId"), "collect_nft_id");
        assert_eq!(camel_to_snake("ERC20"), "erc_20");
        assert_eq!(camel_to_snake("arg1"), "arg_1");
        assert_eq!(camel_to_snake("sqrtPriceX96"), "sqrt_price_x96");
        assert_eq!(camel_to_snake_advanced("ERC20", false), "erc_20");
        assert_eq!(camel_to_snake_advanced("ERC20", true), "erc20");
    }
    #[test]
    fn test_underscore_separated() {
        assert_eq!(to_pascal_case("user_profile_update"), "UserProfileUpdate");
        assert_eq!(to_pascal_case("get_user_by_id"), "GetUserById");
    }
    #[test]
    fn test_already_pascal_case() {
        assert_eq!(to_pascal_case("UserProfile"), "UserProfile");
        assert_eq!(to_pascal_case("GetUserById"), "GetUserById");
    }
    #[test]
    fn test_mixed_case() {
        assert_eq!(to_pascal_case("getUserProfile"), "GetUserProfile");
        assert_eq!(to_pascal_case("userProfileUpdate"), "UserProfileUpdate");
    }
    #[test]
    fn test_with_numbers() {
        assert_eq!(to_pascal_case("user123_profile"), "User123Profile");
        assert_eq!(to_pascal_case("get_user_2_by_id"), "GetUser2ById");
    }
    #[test]
    fn test_with_acronyms() {
        assert_eq!(to_pascal_case("ETH_USD_price"), "ETHUSDPrice");
        assert_eq!(to_pascal_case("http_request_handler"), "HttpRequestHandler");
    }
    #[test]
    fn test_single_word() {
        assert_eq!(to_pascal_case("user"), "User");
        assert_eq!(to_pascal_case("CONSTANT"), "CONSTANT");
        assert_eq!(to_pascal_case("URI"), "URI");
    }
    #[test]
    fn test_empty_string() {
        assert_eq!(to_pascal_case(""), "");
    }
    #[test]
    fn test_multiple_underscores() {
        assert_eq!(to_pascal_case("user__profile___update"), "UserProfileUpdate");
    }
}
</file>

<file path="core/src/helpers/solidity.rs">
/// Extracts the integer type and its size (bit width) from a Solidity type string.
/// Panics if the type is not a valid Solidity integer type.
pub fn parse_solidity_integer_type(solidity_type: &str) -> (&str, usize) {
    match solidity_type {
        t if t.starts_with("int") => ("int", t[3..].parse().expect("Invalid intN type")),
        t if t.starts_with("uint") => ("uint", t[4..].parse().expect("Invalid uintN type")),
        _ => panic!("Invalid Solidity type: {solidity_type}"),
    }
}
const fn is_power_of_two(n: usize) -> bool {
    n != 0 && (n & (n - 1)) == 0
}
/// Checks if a Solidity type is an irregular integer type (with byte width that is not power of two).
/// Panics if the type is not a valid Solidity integer type.
pub fn is_irregular_width_solidity_integer_type(solidity_type: &str) -> bool {
    let (_, size) = parse_solidity_integer_type(solidity_type);
    !is_power_of_two(size)
}
/// Checks if a Solidity type is a static bytes type (e.g., `bytes32`, `bytes64`, etc.).
pub fn is_solidity_static_bytes_type(solidity_type: &str) -> bool {
    solidity_type.starts_with("bytes")
        && solidity_type.len() > 5
        && solidity_type[5..].chars().all(char::is_numeric)
}
</file>

<file path="core/src/helpers/thread.rs">
use std::{
    cell::RefCell,
    io::{self, Write},
};
pub struct NullWriter;
impl Write for NullWriter {
    fn write(&mut self, buf: &[u8]) -> io::Result<usize> {
        Ok(buf.len())
    }
    fn flush(&mut self) -> io::Result<()> {
        Ok(())
    }
}
thread_local! {
    static THREAD_STDOUT: RefCell<Option<Box<dyn Write + Send>>> = RefCell::new(None);
    static THREAD_STDERR: RefCell<Option<Box<dyn Write + Send>>> = RefCell::new(None);
}
pub fn set_thread_no_logging() {
    THREAD_STDOUT.with(|thread_stdout| {
        *thread_stdout.borrow_mut() = Some(Box::new(NullWriter));
    });
    THREAD_STDERR.with(|thread_stderr| {
        *thread_stderr.borrow_mut() = Some(Box::new(NullWriter));
    });
}
#[cfg(test)]
mod tests {
    use std::io::Write;
    use super::*;
    #[test]
    fn test_null_writer_write() {
        let mut writer = NullWriter;
        let data = b"hello";
        let result = writer.write(data);
        assert!(result.is_ok());
        assert_eq!(result.expect("Failed to write"), data.len());
    }
    #[test]
    fn test_null_writer_flush() {
        let mut writer = NullWriter;
        let result = writer.flush();
        assert!(result.is_ok());
    }
    #[test]
    fn test_thread_no_logging_stdout() {
        set_thread_no_logging();
        THREAD_STDOUT.with(|thread_stdout| {
            let mut thread_stdout = thread_stdout.borrow_mut();
            let writer = thread_stdout.as_mut().expect("Failed to get thread stdout");
            let data = b"hello";
            let result = writer.write(data);
            assert!(result.is_ok());
            assert_eq!(result.expect("Failed to write"), data.len());
        });
    }
    #[test]
    fn test_thread_no_logging_stderr() {
        set_thread_no_logging();
        THREAD_STDERR.with(|thread_stderr| {
            let mut thread_stderr = thread_stderr.borrow_mut();
            let writer = thread_stderr.as_mut().expect("Failed to get thread stderr");
            let data = b"error";
            let result = writer.write(data);
            assert!(result.is_ok());
            assert_eq!(result.expect("Failed to write"), data.len());
        });
    }
}
</file>

<file path="core/src/indexer/dependency.rs">
use std::{
    collections::{HashMap, HashSet},
    sync::Arc,
};
use crate::{
    database::postgres::relationship::Relationship,
    event::{config::EventProcessingConfig, contract_setup::ContractEventMapping},
    manifest::{contract::DependencyEventTree, core::Manifest},
};
#[derive(Debug, Clone)]
pub struct EventsDependencyTree {
    pub contract_events: Vec<ContractEventMapping>,
    pub then: Box<Option<Arc<EventsDependencyTree>>>,
}
impl EventsDependencyTree {
    pub fn new(events: Vec<ContractEventMapping>) -> Self {
        EventsDependencyTree { contract_events: events, then: Box::new(None) }
    }
    #[allow(clippy::replace_box)]
    pub fn add_then(&mut self, tree: EventsDependencyTree) {
        self.then = Box::new(Some(Arc::new(tree)));
    }
}
impl EventsDependencyTree {
    pub fn from_dependency_event_tree(event_tree: &DependencyEventTree) -> Self {
        Self {
            contract_events: event_tree.contract_events.clone(),
            then: match event_tree.then.clone() {
                Some(children) => Box::new(Some(Arc::new(
                    EventsDependencyTree::from_dependency_event_tree(&children),
                ))),
                _ => Box::new(None),
            },
        }
    }
}
#[derive(Debug, Clone)]
pub struct EventDependencies {
    pub tree: Arc<EventsDependencyTree>,
    pub dependency_events: Vec<ContractEventMapping>,
}
impl EventDependencies {
    pub fn has_dependency(&self, contract_event: &ContractEventMapping) -> bool {
        self.dependency_events.contains(contract_event)
    }
}
#[derive(Debug, Clone)]
pub struct ContractEventDependencies {
    pub contract_name: String,
    pub event_dependencies: EventDependencies,
}
#[derive(thiserror::Error, Debug)]
pub enum ContractEventDependenciesMapFromRelationshipsError {
    #[error("Cross contract relationships are need manually mapping in the dependency_events, https://rindexer.xyz/docs/start-building/yaml-config/contracts#dependency_events")]
    CrossContractRelationshipsNotDefinedInDependencyEvents,
}
impl ContractEventDependencies {
    pub fn map_from_relationships(
        relationships: &[Relationship],
    ) -> Result<Vec<ContractEventDependencies>, ContractEventDependenciesMapFromRelationshipsError>
    {
        if Relationship::has_cross_contract_dependency(relationships) {
            return Err(ContractEventDependenciesMapFromRelationshipsError::CrossContractRelationshipsNotDefinedInDependencyEvents);
        }
        Ok(ContractEventDependencies::map_all_dependencies(relationships))
    }
    fn map_all_dependencies(relationships: &[Relationship]) -> Vec<ContractEventDependencies> {
        let relationships_map =
            ContractEventDependencies::generate_relationships_map(relationships);
        let mut result_map = HashMap::new();
        let mut visited = HashSet::new();
        for event in relationships_map.keys() {
            let tree = ContractEventDependencies::build_dependency_tree(
                event,
                &relationships_map,
                &mut visited,
            );
            let dependency_events = ContractEventDependencies::collect_dependency_events(&tree);
            result_map
                .entry(event.contract_name.clone())
                .and_modify(|e: &mut EventDependencies| {
                    e.tree = Arc::new(ContractEventDependencies::merge_trees(&e.tree, &tree));
                    e.dependency_events.extend(dependency_events.clone());
                })
                .or_insert(EventDependencies { tree: Arc::clone(&tree), dependency_events });
        }
        result_map
            .into_iter()
            .map(|(contract_name, event_dependencies)| ContractEventDependencies {
                contract_name,
                event_dependencies,
            })
            .collect()
    }
    fn merge_trees(
        tree1: &EventsDependencyTree,
        tree2: &EventsDependencyTree,
    ) -> EventsDependencyTree {
        let mut contract_events = tree1.contract_events.clone();
        contract_events.extend(tree2.contract_events.clone());
        contract_events.sort_by(|a, b| a.event_name.cmp(&b.event_name));
        contract_events.dedup();
        EventsDependencyTree {
            contract_events,
            then: if tree1.then.is_none() && tree2.then.is_none() {
                Box::new(None)
            } else {
                Box::new(Some(Arc::new(ContractEventDependencies::merge_trees(
                    tree1.then.as_ref().as_ref().unwrap_or(&Arc::new(EventsDependencyTree {
                        contract_events: vec![],
                        then: Box::new(None),
                    })),
                    tree2.then.as_ref().as_ref().unwrap_or(&Arc::new(EventsDependencyTree {
                        contract_events: vec![],
                        then: Box::new(None),
                    })),
                ))))
            },
        }
    }
    fn build_dependency_tree(
        event: &ContractEventMapping,
        relationships_map: &HashMap<ContractEventMapping, Vec<ContractEventMapping>>,
        visited: &mut HashSet<ContractEventMapping>,
    ) -> Arc<EventsDependencyTree> {
        if visited.contains(event) {
            return Arc::new(EventsDependencyTree {
                contract_events: vec![],
                then: Box::new(None),
            });
        }
        visited.insert(event.clone());
        let contract_events = vec![event.clone()];
        let mut next_tree: Option<Arc<EventsDependencyTree>> = None;
        if let Some(linked_events) = relationships_map.get(event) {
            for linked_event in linked_events {
                let tree = ContractEventDependencies::build_dependency_tree(
                    linked_event,
                    relationships_map,
                    visited,
                );
                match next_tree {
                    None => {
                        next_tree = Some(tree);
                    }
                    Some(next_tree_value) => {
                        next_tree = Some(Arc::new(ContractEventDependencies::merge_trees(
                            &next_tree_value,
                            &tree,
                        )));
                    }
                }
            }
        }
        Arc::new(EventsDependencyTree { contract_events, then: Box::new(next_tree) })
    }
    fn generate_relationships_map(
        relationships: &[Relationship],
    ) -> HashMap<ContractEventMapping, Vec<ContractEventMapping>> {
        let mut relationships_map = HashMap::new();
        for relationship in relationships {
            let event = ContractEventMapping {
                contract_name: relationship.contract_name.clone(),
                event_name: relationship.event.clone(),
            };
            let linked_event = ContractEventMapping {
                contract_name: relationship.linked_to.contract_name.clone(),
                event_name: relationship.linked_to.event.clone(),
            };
            relationships_map.entry(linked_event).or_insert_with(Vec::new).push(event);
        }
        relationships_map
    }
    fn collect_dependency_events(tree: &EventsDependencyTree) -> Vec<ContractEventMapping> {
        let mut events = tree.contract_events.clone();
        if let Some(ref then_tree) = *tree.then {
            events.extend(ContractEventDependencies::collect_dependency_events(then_tree));
        }
        events
    }
}
#[derive(Debug)]
pub struct DependencyStatus {
    pub has_dependency_in_own_contract: bool,
    pub dependencies_in_other_contracts: Vec<String>,
}
impl DependencyStatus {
    pub fn has_dependency_in_other_contracts_multiple_times(&self) -> bool {
        self.dependencies_in_other_contracts.len() > 1
    }
    pub fn has_dependencies(&self) -> bool {
        self.has_dependency_in_own_contract || !self.dependencies_in_other_contracts.is_empty()
    }
    pub fn get_first_dependencies_in_other_contracts(&self) -> Option<String> {
        self.dependencies_in_other_contracts.first().cloned()
    }
}
impl ContractEventDependencies {
    pub fn parse(manifest: &Manifest) -> Vec<ContractEventDependencies> {
        let mut dependencies: Vec<ContractEventDependencies> = vec![];
        for contract in &manifest.all_contracts() {
            let contract_dependencies_tree = contract
                .dependency_events
                .clone()
                .map(|dependency| contract.convert_dependency_event_tree_yaml(dependency));
            if let Some(dependency_event_tree) = contract_dependencies_tree {
                let dependency_tree =
                    EventsDependencyTree::from_dependency_event_tree(&dependency_event_tree);
                dependencies.push(ContractEventDependencies {
                    contract_name: contract.name.clone(),
                    event_dependencies: EventDependencies {
                        tree: Arc::new(dependency_tree),
                        dependency_events: dependency_event_tree.collect_dependency_events(),
                    },
                });
            }
        }
        dependencies
    }
    pub fn dependencies_status(
        contract_name: &str,
        event_name: &str,
        dependencies: &[ContractEventDependencies],
    ) -> DependencyStatus {
        let has_dependency_in_own_contract =
            dependencies.iter().find(|d| d.contract_name == contract_name).is_some_and(|deps| {
                deps.event_dependencies.has_dependency(&ContractEventMapping {
                    contract_name: deps.contract_name.clone(),
                    event_name: event_name.to_string(),
                })
            });
        let dependencies_in_other_contracts: Vec<String> = dependencies
            .iter()
            .filter_map(|d| {
                if d.contract_name != contract_name {
                    let has_dependency =
                        d.event_dependencies.has_dependency(&ContractEventMapping {
                            contract_name: contract_name.to_string(),
                            event_name: event_name.to_string(),
                        });
                    if has_dependency {
                        return Some(d.contract_name.to_string());
                    }
                    // check if it's a filter event
                    let has_dependency =
                        d.event_dependencies.has_dependency(&ContractEventMapping {
                            // TODO - this is a hacky way to check if it's a filter event
                            contract_name: contract_name.to_string().replace("Filter", ""),
                            event_name: event_name.to_string(),
                        });
                    if has_dependency {
                        return Some(d.contract_name.to_string());
                    }
                }
                None
            })
            .collect();
        DependencyStatus { has_dependency_in_own_contract, dependencies_in_other_contracts }
    }
}
pub struct ContractEventsDependenciesConfig {
    pub contract_name: String,
    pub event_dependencies: EventDependencies,
    pub events_config: Vec<Arc<EventProcessingConfig>>,
}
impl ContractEventsDependenciesConfig {
    fn add_event_config(&mut self, config: Arc<EventProcessingConfig>) {
        self.events_config.push(config);
    }
    pub fn add_to_event_or_new_entry(
        dependency_event_processing_configs: &mut Vec<ContractEventsDependenciesConfig>,
        event_processing_config: Arc<EventProcessingConfig>,
        dependencies: &[ContractEventDependencies],
    ) {
        match dependency_event_processing_configs
            .iter_mut()
            .find(|c| c.contract_name == event_processing_config.contract_name())
        {
            Some(contract_events_config) => {
                contract_events_config.add_event_config(event_processing_config)
            }
            None => {
                dependency_event_processing_configs.push(ContractEventsDependenciesConfig {
                    contract_name: event_processing_config.contract_name().clone(),
                    event_dependencies: dependencies
                        .iter()
                        .find(|d| d.contract_name == event_processing_config.contract_name())
                        .expect("Failed to find contract dependencies")
                        .event_dependencies
                        .clone(),
                    events_config: vec![event_processing_config],
                });
            }
        }
    }
    pub fn add_to_event_or_panic(
        contract_name: &str,
        dependency_event_processing_configs: &mut [ContractEventsDependenciesConfig],
        event_processing_config: Arc<EventProcessingConfig>,
    ) {
        match dependency_event_processing_configs
            .iter_mut()
            .find(|c| c.contract_name == contract_name)
        {
            Some(contract_events_config) => {
                contract_events_config.add_event_config(event_processing_config)
            }
            None => {
                panic!("Contract events config not found for {} dependency event processing config make sure it registered - trying to add to it - contract {} - event {}",
                       contract_name,
                       event_processing_config.contract_name(),
                       event_processing_config.event_name()
                );
            }
        }
    }
}
</file>

<file path="core/src/indexer/fetch_logs.rs">
use crate::blockclock::BlockClock;
use crate::helpers::{halved_block_number, is_relevant_block};
use crate::indexer::reorg::reorg_safe_distance_for_chain;
use crate::{
    event::{config::EventProcessingConfig, RindexerEventFilter},
    indexer::{reorg::handle_chain_notification, IndexingEventProgressStatus},
    is_running,
    provider::{JsonRpcCachedProvider, ProviderError},
};
use alloy::{
    primitives::{B256, U64},
    rpc::types::Log,
};
use lru::LruCache;
use rand::{random_bool, random_ratio};
use regex::Regex;
use std::num::NonZeroUsize;
use std::{error::Error, str::FromStr, sync::Arc, time::Duration};
use tokio::{sync::mpsc, time::Instant};
use tokio_stream::wrappers::ReceiverStream;
use tracing::{debug, error, info, warn};
pub struct FetchLogsResult {
    pub logs: Vec<Log>,
    pub from_block: U64,
    pub to_block: U64,
}
pub fn fetch_logs_stream(
    config: Arc<EventProcessingConfig>,
    force_no_live_indexing: bool,
) -> impl tokio_stream::Stream<Item = Result<FetchLogsResult, Box<dyn Error + Send>>> + Send + Unpin
{
    // If the sink is slower than the producer it can lead to unbounded memory growth and
    // a system OOM kill.
    //
    // To prevent this, we maintain a memory bound to give the system time to catch up and
    // backpressure the producer. Many RPC responses are large, so this is important.
    //
    // This is per network contract-event, so it should be relatively small.
    let channel_size = config.config().buffer.unwrap_or(4);
    debug!("{} Configured with {} event buffer", config.info_log_name(), channel_size);
    let (tx, rx) = mpsc::channel(channel_size);
    tokio::spawn(async move {
        let mut current_filter = config.to_event_filter().unwrap();
        let snapshot_to_block = current_filter.to_block();
        let from_block = current_filter.from_block();
        // add any max block range limitation before we start processing
        let original_max_limit = config.network_contract().cached_provider.max_block_range;
        let mut max_block_range_limitation =
            config.network_contract().cached_provider.max_block_range;
        #[allow(clippy::unnecessary_unwrap)]
        if max_block_range_limitation.is_some() {
            current_filter = current_filter.set_to_block(calculate_process_historic_log_to_block(
                &from_block,
                &snapshot_to_block,
                &max_block_range_limitation,
            ));
            if random_ratio(1, 20) {
                warn!(
                    "{} - {} - max block range of {} applied - indexing will be slower than providers supplying the optimal ranges - https://rindexer.xyz/docs/references/rpc-node-providers#rpc-node-providers",
                    config.info_log_name(),
                    IndexingEventProgressStatus::Syncing.log(),
                    max_block_range_limitation.unwrap()
                );
            }
        }
        while current_filter.from_block() <= snapshot_to_block {
            if !is_running() {
                break;
            }
            let result = fetch_historic_logs_stream(
                config.timestamps(),
                config.network_contract().block_clock.clone(),
                &config.network_contract().cached_provider,
                &tx,
                &config.topic_id(),
                current_filter.clone(),
                max_block_range_limitation,
                snapshot_to_block,
                &config.info_log_name(),
            )
            .await;
            // This check can be very noisy. We want to only sample this warning to notify
            // the user, rather than warn on every log fetch.
            if let Some(range) = max_block_range_limitation {
                if range.to::<u64>() < 5000 && random_ratio(1, 20) {
                    warn!(
                        "{} - RPC PROVIDER IS SLOW - Slow indexing mode enabled, max block range limitation: {} blocks - we advise using a faster provider who can predict the next block ranges.",
                        &config.info_log_name(),
                        range
                    );
                }
            }
            if let Some(result) = result {
                // Useful for occasionally breaking out of temporary limitations or parsing errors
                // that lock down to a `1` block limitation. Returns back to the original
                let new_max_block_range_limitation = if random_bool(0.10) {
                    original_max_limit
                } else {
                    result.max_block_range_limitation
                };
                current_filter = result.next;
                max_block_range_limitation = new_max_block_range_limitation;
            } else {
                break;
            }
        }
        info!(
            "{} - {} - Finished indexing historic events",
            &config.info_log_name(),
            IndexingEventProgressStatus::Completed.log()
        );
        // Live indexing mode
        if config.live_indexing() && !force_no_live_indexing {
            live_indexing_stream(
                config.timestamps(),
                config.network_contract().block_clock.clone(),
                &config.network_contract().cached_provider,
                &tx,
                snapshot_to_block,
                &config.topic_id(),
                &config.indexing_distance_from_head(),
                current_filter,
                &config.info_log_name(),
                config.network_contract().disable_logs_bloom_checks,
                original_max_limit,
            )
            .await;
        }
    });
    ReceiverStream::new(rx)
}
struct ProcessHistoricLogsStreamResult {
    pub next: RindexerEventFilter,
    pub max_block_range_limitation: Option<U64>,
}
#[allow(clippy::too_many_arguments)]
async fn fetch_historic_logs_stream(
    timestamps: bool,
    block_clock: BlockClock,
    cached_provider: &Arc<JsonRpcCachedProvider>,
    tx: &mpsc::Sender<Result<FetchLogsResult, Box<dyn Error + Send>>>,
    topic_id: &B256,
    current_filter: RindexerEventFilter,
    max_block_range_limitation: Option<U64>,
    snapshot_to_block: U64,
    info_log_name: &str,
) -> Option<ProcessHistoricLogsStreamResult> {
    let from_block = current_filter.from_block();
    let to_block = current_filter.to_block();
    debug!(
        "{} - {} - Process historic events - blocks: {} - {}",
        info_log_name,
        IndexingEventProgressStatus::Syncing.log(),
        from_block,
        to_block
    );
    if from_block > to_block {
        warn!(
            "{} - {} - from_block {:?} > to_block {:?}",
            info_log_name,
            IndexingEventProgressStatus::Syncing.log(),
            from_block,
            to_block
        );
        return Some(ProcessHistoricLogsStreamResult {
            next: current_filter.set_from_block(to_block).set_to_block(to_block + U64::from(1)),
            max_block_range_limitation,
        });
    }
    debug!(
        "{} - {} - Processing filter: {:?}",
        info_log_name,
        IndexingEventProgressStatus::Syncing.log(),
        current_filter
    );
    let sender = tx.reserve().await.ok()?;
    if tx.capacity() == 0 {
        debug!(
            "{} - {} - Log channel full, waiting for events to be processed.",
            info_log_name,
            IndexingEventProgressStatus::Syncing.log(),
        );
    }
    match cached_provider.get_logs(&current_filter).await {
        Ok(logs) => {
            debug!(
                "{} - {} - topic_id {}, Logs: {} from {} to {}",
                info_log_name,
                IndexingEventProgressStatus::Syncing.log(),
                topic_id,
                logs.len(),
                from_block,
                to_block
            );
            let logs_empty = logs.is_empty();
            // clone here over the full logs way less overhead
            let last_log = logs.last().cloned();
            if !logs_empty {
                info!(
                    "{} - {} - Fetched {} logs between: {} - {}",
                    info_log_name,
                    IndexingEventProgressStatus::Syncing.log(),
                    logs.len(),
                    from_block,
                    to_block
                );
            }
            if timestamps {
                if let Ok(logs) = block_clock.attach_log_timestamps(logs).await {
                    sender.send(Ok(FetchLogsResult { logs, from_block, to_block }));
                } else {
                    return Some(ProcessHistoricLogsStreamResult {
                        next: current_filter
                            .set_from_block(from_block)
                            .set_to_block(halved_block_number(to_block, from_block)),
                        max_block_range_limitation,
                    });
                }
            } else {
                sender.send(Ok(FetchLogsResult { logs, from_block, to_block }));
            }
            if logs_empty {
                let next_from_block = to_block + U64::from(1);
                return if next_from_block > snapshot_to_block {
                    None
                } else {
                    let new_to_block = calculate_process_historic_log_to_block(
                        &next_from_block,
                        &snapshot_to_block,
                        &max_block_range_limitation,
                    );
                    debug!(
                        "{} - No events between {} - {}. Searching next {} blocks.",
                        info_log_name,
                        from_block,
                        to_block,
                        new_to_block - next_from_block
                    );
                    debug!(
                        "{} - {} - new_from_block {:?} new_to_block {:?}",
                        info_log_name,
                        IndexingEventProgressStatus::Syncing.log(),
                        next_from_block,
                        new_to_block
                    );
                    Some(ProcessHistoricLogsStreamResult {
                        next: current_filter
                            .set_from_block(next_from_block)
                            .set_to_block(new_to_block),
                        max_block_range_limitation,
                    })
                };
            }
            if let Some(last_log) = last_log {
                let next_from_block = U64::from(
                    last_log.block_number.expect("block number should always be present in a log")
                        + 1,
                );
                debug!(
                    "{} - {} - next_block {:?}",
                    info_log_name,
                    IndexingEventProgressStatus::Syncing.log(),
                    next_from_block
                );
                return if next_from_block > snapshot_to_block {
                    None
                } else {
                    let new_to_block = calculate_process_historic_log_to_block(
                        &next_from_block,
                        &snapshot_to_block,
                        &max_block_range_limitation,
                    );
                    debug!(
                        "{} - {} - new_from_block {:?} new_to_block {:?}",
                        info_log_name,
                        IndexingEventProgressStatus::Syncing.log(),
                        next_from_block,
                        new_to_block
                    );
                    Some(ProcessHistoricLogsStreamResult {
                        next: current_filter
                            .set_from_block(next_from_block)
                            .set_to_block(new_to_block),
                        max_block_range_limitation,
                    })
                };
            }
        }
        Err(err) => {
            // This is fundamental to the rindexer flow. We intentionally fetch a large block range
            // to get information on what the ideal block range should be.
            if let Some(retry_result) = retry_with_block_range(
                info_log_name,
                &err,
                from_block,
                to_block,
                max_block_range_limitation,
            )
            .await
            {
                // Log if we "overshrink"
                if retry_result.to - retry_result.from < U64::from(1000) {
                    debug!(
                        "{} - {} - Over-fetched {} to {}. Shrunk ({}): {} to {}{}",
                        info_log_name,
                        IndexingEventProgressStatus::Syncing.log(),
                        from_block,
                        to_block,
                        retry_result.to - retry_result.from,
                        retry_result.from,
                        retry_result.to,
                        retry_result
                            .max_block_range
                            .map(|m| format!(" (max {m})"))
                            .unwrap_or("".to_owned()),
                    );
                }
                return Some(ProcessHistoricLogsStreamResult {
                    next: current_filter
                        .set_from_block(U64::from(retry_result.from))
                        .set_to_block(U64::from(retry_result.to)),
                    max_block_range_limitation: retry_result.max_block_range,
                });
            }
            let halved_to_block = halved_block_number(to_block, from_block);
            // Handle deserialization, networking, and other non-rpc related errors.
            error!(
                "{} - {} - Unexpected error fetching logs in range {} - {}. Retry fetching {} - {}: {:?}",
                info_log_name,
                IndexingEventProgressStatus::Syncing.log(),
                from_block,
                to_block,
                from_block,
                halved_to_block,
                err
            );
            return Some(ProcessHistoricLogsStreamResult {
                next: current_filter.set_from_block(from_block).set_to_block(halved_to_block),
                max_block_range_limitation,
            });
        }
    }
    None
}
/// Handles live indexing mode, continuously checking for new blocks, ensuring they are
/// within a safe range, updating the filter, and sending the logs to the provided channel.
#[allow(clippy::too_many_arguments)]
async fn live_indexing_stream(
    timestamps: bool,
    block_clock: BlockClock,
    cached_provider: &Arc<JsonRpcCachedProvider>,
    tx: &mpsc::Sender<Result<FetchLogsResult, Box<dyn Error + Send>>>,
    last_seen_block_number: U64,
    topic_id: &B256,
    reorg_safe_distance: &U64,
    mut current_filter: RindexerEventFilter,
    info_log_name: &str,
    disable_logs_bloom_checks: bool,
    original_max_limit: Option<U64>,
) {
    let mut last_seen_block_number = last_seen_block_number;
    let mut log_response_to_large_to_block: Option<U64> = None;
    let mut last_no_new_block_log_time = Instant::now();
    let log_no_new_block_interval = Duration::from_secs(300);
    let target_iteration_duration = Duration::from_millis(200);
    let chain_state_notification = cached_provider.get_chain_state_notification();
    // Spawn a separate task to handle notifications
    if let Some(notifications) = chain_state_notification {
        let info_log_name = info_log_name.to_string();
        tokio::spawn(async move {
            let mut notifications_clone = notifications.subscribe();
            while let Ok(notification) = notifications_clone.recv().await {
                handle_chain_notification(notification, &info_log_name);
            }
        });
    }
    // This is a local cache of the last blocks we've crawled and their timestamps.
    //
    // It allows us to cheaply persist and fetch timestamps for blocks in any log range
    // fetch for a recent period. It is about 16-bytes per entry.
    //
    // 500 should cover any block-lag we could reasonably encounter at near-zer memory cost.
    let mut block_times: LruCache<u64, u64> = LruCache::new(NonZeroUsize::new(50).unwrap());
    loop {
        let iteration_start = Instant::now();
        if !is_running() {
            break;
        }
        let latest_block = cached_provider.get_latest_block().await;
        match latest_block {
            Ok(latest_block) => {
                if let Some(latest_block) = latest_block {
                    block_times.put(latest_block.header.number, latest_block.header.timestamp);
                    let latest_block_number = log_response_to_large_to_block
                        .unwrap_or(U64::from(latest_block.header.number));
                    if last_seen_block_number == latest_block_number {
                        debug!(
                            "{} - {} - No new blocks to process...",
                            info_log_name,
                            IndexingEventProgressStatus::Live.log()
                        );
                        if last_no_new_block_log_time.elapsed() >= log_no_new_block_interval {
                            info!(
                                    "{} - {} - No new blocks published in the last 5 minutes - latest block number {}",
                                    info_log_name,
                                    IndexingEventProgressStatus::Live.log(),
                                    last_seen_block_number,
                                );
                            last_no_new_block_log_time = Instant::now();
                        }
                    } else {
                        debug!(
                            "{} - {} - New block seen {} - Last seen block {}",
                            info_log_name,
                            IndexingEventProgressStatus::Live.log(),
                            latest_block_number,
                            last_seen_block_number
                        );
                        let safe_block_number = latest_block_number - reorg_safe_distance;
                        let from_block = current_filter.from_block();
                        if from_block > safe_block_number {
                            if reorg_safe_distance.is_zero() {
                                let block_distance = from_block - latest_block_number;
                                let is_outside_reorg_range = block_distance
                                    > reorg_safe_distance_for_chain(cached_provider.chain.id());
                                // it should never get under normal conditions outside the reorg range,
                                // therefore, we log an error as means RCP state is not in sync with the blockchain
                                if is_outside_reorg_range {
                                    error!(
                                        "{} - {} - LIVE INDEXING STREAM - RPC has gone back on latest block: rpc returned {}, last seen: {}",
                                        info_log_name,
                                        IndexingEventProgressStatus::Live.log(),
                                        latest_block_number,
                                        from_block
                                    );
                                } else {
                                    info!(
                                        "{} - {} - LIVE INDEXING STREAM - RPC has gone back on latest block: rpc returned {}, last seen: {}",
                                        info_log_name,
                                        IndexingEventProgressStatus::Live.log(),
                                        latest_block_number,
                                        from_block
                                    );
                                }
                            } else {
                                info!(
                                    "{} - {} - LIVE INDEXING STREAM - not in safe reorg block range yet block: {} > range: {}",
                                    info_log_name,
                                    IndexingEventProgressStatus::Live.log(),
                                    from_block,
                                    safe_block_number
                                );
                            }
                        } else {
                            let contract_address = current_filter.contract_addresses().await;
                            let to_block = safe_block_number;
                            if from_block == to_block
                                && !disable_logs_bloom_checks
                                && !is_relevant_block(&contract_address, topic_id, &latest_block)
                            {
                                debug!(
                                    "{} - {} - Skipping block {} as it's not relevant",
                                    info_log_name,
                                    IndexingEventProgressStatus::Live.log(),
                                    from_block
                                );
                                debug!(
                                        "{} - {} - Did not need to hit RPC as no events in {} block - LogsBloom for block checked",
                                        info_log_name,
                                        IndexingEventProgressStatus::Live.log(),
                                        from_block
                                    );
                                current_filter =
                                    current_filter.set_from_block(to_block + U64::from(1));
                                last_seen_block_number = to_block;
                            } else {
                                current_filter = current_filter.set_to_block(to_block);
                                debug!(
                                    "{} - {} - Processing live filter: {:?}",
                                    info_log_name,
                                    IndexingEventProgressStatus::Live.log(),
                                    current_filter
                                );
                                match cached_provider.get_logs(&current_filter).await {
                                    Ok(logs) => {
                                        debug!(
                                            "{} - {} - Live topic_id {}, Logs: {} from {} to {}",
                                            info_log_name,
                                            IndexingEventProgressStatus::Live.log(),
                                            topic_id,
                                            logs.len(),
                                            from_block,
                                            to_block
                                        );
                                        debug!(
                                            "{} - {} - Fetched {} event logs - blocks: {} - {}",
                                            info_log_name,
                                            IndexingEventProgressStatus::Live.log(),
                                            logs.len(),
                                            from_block,
                                            to_block
                                        );
                                        last_seen_block_number = to_block;
                                        let logs_empty = logs.is_empty();
                                        let last_log = logs.last().cloned();
                                        // Attach timestamp from current latest_block to the logs
                                        // to prevent any further fetches.
                                        let logs = logs
                                            .into_iter()
                                            .map(|mut log| {
                                                if let Some(n) = log.block_number {
                                                    if let Some(time) = block_times.get(&n) {
                                                        log.block_timestamp = Some(*time);
                                                    }
                                                }
                                                log
                                            })
                                            .collect::<Vec<_>>();
                                        if tx.capacity() == 0 {
                                            warn!(
                                                "{} - {} - Log channel full, live indexer will wait for events to be processed.",
                                                info_log_name,
                                                IndexingEventProgressStatus::Live.log(),
                                            );
                                        }
                                        let logs = if timestamps {
                                            if let Ok(logs_with_ts) =
                                                block_clock.attach_log_timestamps(logs).await
                                            {
                                                logs_with_ts
                                            } else {
                                                error!(
                                                    "Error getting blocktime, will try again in 1s"
                                                );
                                                tokio::time::sleep(Duration::from_secs(1)).await;
                                                continue;
                                            }
                                        } else {
                                            logs
                                        };
                                        if let Err(e) = tx
                                            .send(Ok(FetchLogsResult {
                                                logs,
                                                from_block,
                                                to_block,
                                            }))
                                            .await
                                        {
                                            error!(
                                                "{} - {} - Failed to send logs to stream consumer! Err: {}",
                                                info_log_name,
                                                IndexingEventProgressStatus::Live.log(),
                                                e
                                            );
                                            break;
                                        }
                                        // Clear any remaining references to reduce memory pressure
                                        log_response_to_large_to_block = None;
                                        if logs_empty {
                                            current_filter = current_filter
                                                .set_from_block(to_block + U64::from(1));
                                            debug!(
                                                "{} - {} - No events found between blocks {} - {}",
                                                info_log_name,
                                                IndexingEventProgressStatus::Live.log(),
                                                from_block,
                                                to_block,
                                            );
                                        } else if let Some(last_log) = last_log {
                                            if let Some(last_log_block_number) =
                                                last_log.block_number
                                            {
                                                current_filter = current_filter.set_from_block(
                                                    U64::from(last_log_block_number + 1),
                                                );
                                            } else {
                                                error!("Failed to get last log block number the provider returned null (should never happen) - try again in 200ms");
                                            }
                                        }
                                    }
                                    Err(err) => {
                                        if let Some(retry_result) = retry_with_block_range(
                                            info_log_name,
                                            &err,
                                            from_block,
                                            to_block,
                                            original_max_limit,
                                        )
                                        .await
                                        {
                                            debug!(
                                                    "{} - {} - Overfetched from {} to {} - shrinking to block range: from {} to {}",
                                                    info_log_name,
                                                    IndexingEventProgressStatus::Live.log(),
                                                    from_block,
                                                    to_block,
                                                    from_block,
                                                    retry_result.to
                                                    );
                                            log_response_to_large_to_block = Some(retry_result.to);
                                        } else {
                                            let halved_to_block =
                                                halved_block_number(to_block, from_block);
                                            error!(
                                                    "{} - {} - Unexpected error fetching logs in range {} - {}. Retry fetching {} - {}: {:?}",
                                                    info_log_name,
                                                    IndexingEventProgressStatus::Live.log(),
                                                    from_block,
                                                    to_block,
                                                    from_block,
                                                    halved_to_block,
                                                    err
                                                );
                                            log_response_to_large_to_block = Some(halved_to_block);
                                        }
                                    }
                                }
                            }
                        }
                    }
                } else {
                    info!("WARNING - empty latest block returned from provider, will try again in 200ms");
                }
            }
            Err(e) => {
                error!(
                    "Error getting latest block, will try again in 1 second - err: {}",
                    e.to_string()
                );
                tokio::time::sleep(Duration::from_secs(1)).await;
                continue;
            }
        }
        let elapsed = iteration_start.elapsed();
        if elapsed < target_iteration_duration {
            tokio::time::sleep(target_iteration_duration - elapsed).await;
        }
    }
}
#[derive(Debug)]
struct RetryWithBlockRangeResult {
    from: U64,
    to: U64,
    // This is only populated if you are using an RPC provider
    // who doesn't give block ranges, this tends to be providers
    // which are a lot slower than others, expect these providers
    // to be slow
    max_block_range: Option<U64>,
}
/// Attempts to retry with a new block range based on the error message.
async fn retry_with_block_range(
    info_log_name: &str,
    error: &ProviderError,
    from_block: U64,
    to_block: U64,
    max_block_range_limitation: Option<U64>,
) -> Option<RetryWithBlockRangeResult> {
    let error_struct = match error {
        ProviderError::RequestFailed(json_rpc_err) => json_rpc_err.as_error_resp(),
        _ => None,
    };
    let (error_message, error_data) = if let Some(error) = error_struct {
        let error_message = error.message.to_string();
        let error_data_binding = error.data.as_ref().map(|data| data.to_string());
        let empty_string = String::from("");
        let error_data = error_data_binding.unwrap_or(empty_string);
        let trimmed = error_message.chars().take(5000).collect::<String>();
        (trimmed.to_lowercase(), error_data.to_lowercase())
    } else {
        let str_err = error.to_string();
        let trimmed = str_err.chars().take(5000).collect::<String>();
        debug!("Failed to parse structured error, trying with raw string: {}", &str_err);
        (trimmed.to_lowercase(), "".to_string())
    };
    // Thanks Ponder for the regex patterns - https://github.com/ponder-sh/ponder/blob/889096a3ef5f54a0c....ts#L34
    // Alchemy
    if let Ok(re) =
        Regex::new(r"this block range should work: \[0x([0-9a-fA-F]+),\s*0x([0-9a-fA-F]+)\]")
    {
        if let Some(captures) = re.captures(&error_message).or_else(|| re.captures(&error_data)) {
            if let (Some(start_block), Some(end_block)) = (captures.get(1), captures.get(2)) {
                let start_block_str = start_block.as_str();
                let end_block_str = end_block.as_str();
                if let (Ok(from), Ok(to)) = (
                    u64::from_str_radix(start_block_str, 16),
                    u64::from_str_radix(end_block_str, 16),
                ) {
                    if from > to {
                        warn!(
                            "{} Alchemy returned a negative block range {} to {}. Inverting.",
                            info_log_name, from, to
                        );
                        // Negative range fixed by inverting.
                        let to = U64::from(from);
                        return Some(RetryWithBlockRangeResult {
                            from: from_block,
                            to,
                            max_block_range: max_block_range_limitation,
                        });
                    }
                    return Some(RetryWithBlockRangeResult {
                        from: U64::from(from),
                        to: U64::from(to),
                        max_block_range: max_block_range_limitation,
                    });
                } else {
                    info!(
                        "{} Failed to parse block numbers {} and {}",
                        info_log_name, start_block_str, end_block_str
                    );
                }
            }
        }
    }
    // Infura, Thirdweb, zkSync, Tenderly
    if let Ok(re) =
        Regex::new(r"try with this block range \[0x([0-9a-fA-F]+),\s*0x([0-9a-fA-F]+)\]")
    {
        if let Some(captures) = re.captures(&error_message).or_else(|| re.captures(&error_data)) {
            if let (Some(start_block), Some(end_block)) = (captures.get(1), captures.get(2)) {
                if let (Ok(from), Ok(to)) = (
                    u64::from_str_radix(start_block.as_str(), 16),
                    u64::from_str_radix(end_block.as_str(), 16),
                ) {
                    return Some(RetryWithBlockRangeResult {
                        from: U64::from(from),
                        to: U64::from(to),
                        max_block_range: max_block_range_limitation,
                    });
                }
            }
        }
    }
    // Ankr
    if error_message.contains("block range is too wide") {
        // Use the minimum of original config or 3000
        let suggested_range = max_block_range_limitation
            .map(|original| std::cmp::min(original, U64::from(3000)))
            .unwrap_or(U64::from(3000));
        return Some(RetryWithBlockRangeResult {
            from: from_block,
            to: from_block + suggested_range,
            max_block_range: Some(suggested_range),
        });
    }
    // QuickNode, 1RPC, zkEVM, Blast, BlockPI
    if let Ok(re) = Regex::new(r"limited to a ([\d,.]+)") {
        if let Some(captures) = re.captures(&error_message).or_else(|| re.captures(&error_data)) {
            if let Some(range_str_match) = captures.get(1) {
                let range_str = range_str_match.as_str().replace(&['.', ','][..], "");
                if let Ok(range) = U64::from_str(&range_str) {
                    // Use the minimum of original config or provider suggestion
                    let suggested_range = max_block_range_limitation
                        .map(|original| std::cmp::min(original, range))
                        .unwrap_or(range);
                    return Some(RetryWithBlockRangeResult {
                        from: from_block,
                        to: from_block + suggested_range,
                        max_block_range: Some(suggested_range),
                    });
                }
            }
        }
    }
    // Base
    if error_message.contains("block range too large") {
        // Use the minimum of original config or 2000
        let suggested_range = max_block_range_limitation
            .map(|original| std::cmp::min(original, U64::from(2000)))
            .unwrap_or(U64::from(2000));
        return Some(RetryWithBlockRangeResult {
            from: from_block,
            to: from_block + suggested_range,
            max_block_range: Some(suggested_range),
        });
    }
    // Transient response errors, likely solved by halving the range or just retrying
    if error_message.contains("response is too big")
        || error_message.contains("error decoding response body")
    {
        let halved_to_block = halved_block_number(to_block, from_block);
        return Some(RetryWithBlockRangeResult {
            from: from_block,
            to: halved_to_block,
            max_block_range: max_block_range_limitation,
        });
    }
    // We can't keep up with our own sending rate. This is rare, but we must backoff throughput.
    if error_message.contains("error sending request") {
        tokio::time::sleep(Duration::from_secs(1)).await;
        return Some(RetryWithBlockRangeResult {
            from: from_block,
            to: halved_block_number(to_block, from_block),
            max_block_range: max_block_range_limitation,
        });
    }
    // Fallback range
    if to_block > from_block {
        let diff = to_block - from_block;
        let mut block_range = FallbackBlockRange::from_diff(diff);
        let mut next_to_block = from_block + block_range.value();
        warn!(
            "{} Computed a fallback block range {:?}. Provider did not provide information in error: {:?}",
            info_log_name, block_range, error_message
        );
        if next_to_block == to_block {
            block_range = block_range.lower();
            next_to_block = from_block + block_range.value();
        }
        if next_to_block < from_block {
            error!(
                "{} Computed a negative fallback block range. Overriding to single block fetch.",
                info_log_name
            );
            return Some(RetryWithBlockRangeResult {
                from: from_block,
                to: halved_block_number(to_block, from_block),
                max_block_range: max_block_range_limitation,
            });
        }
        // Use the minimum of original config or fallback range
        let fallback_range = U64::from(block_range.value());
        let suggested_range = max_block_range_limitation
            .map(|original| std::cmp::min(original, fallback_range))
            .unwrap_or(fallback_range);
        return Some(RetryWithBlockRangeResult {
            from: from_block,
            to: from_block + suggested_range,
            max_block_range: Some(suggested_range),
        });
    }
    None
}
#[derive(Debug, PartialEq)]
enum FallbackBlockRange {
    Range5000,
    Range500,
    Range75,
    Range50,
    Range45,
    Range40,
    Range35,
    Range30,
    Range25,
    Range20,
    Range15,
    Range10,
    Range5,
    Range1,
}
impl FallbackBlockRange {
    fn value(&self) -> U64 {
        match self {
            FallbackBlockRange::Range5000 => U64::from(5000),
            FallbackBlockRange::Range500 => U64::from(500),
            FallbackBlockRange::Range75 => U64::from(75),
            FallbackBlockRange::Range50 => U64::from(50),
            FallbackBlockRange::Range45 => U64::from(45),
            FallbackBlockRange::Range40 => U64::from(40),
            FallbackBlockRange::Range35 => U64::from(35),
            FallbackBlockRange::Range30 => U64::from(30),
            FallbackBlockRange::Range25 => U64::from(25),
            FallbackBlockRange::Range20 => U64::from(20),
            FallbackBlockRange::Range15 => U64::from(15),
            FallbackBlockRange::Range10 => U64::from(10),
            FallbackBlockRange::Range5 => U64::from(5),
            FallbackBlockRange::Range1 => U64::from(1),
        }
    }
    fn lower(&self) -> FallbackBlockRange {
        match self {
            FallbackBlockRange::Range5000 => FallbackBlockRange::Range500,
            FallbackBlockRange::Range500 => FallbackBlockRange::Range75,
            FallbackBlockRange::Range75 => FallbackBlockRange::Range50,
            FallbackBlockRange::Range50 => FallbackBlockRange::Range45,
            FallbackBlockRange::Range45 => FallbackBlockRange::Range40,
            FallbackBlockRange::Range40 => FallbackBlockRange::Range35,
            FallbackBlockRange::Range35 => FallbackBlockRange::Range30,
            FallbackBlockRange::Range30 => FallbackBlockRange::Range25,
            FallbackBlockRange::Range25 => FallbackBlockRange::Range20,
            FallbackBlockRange::Range20 => FallbackBlockRange::Range15,
            FallbackBlockRange::Range15 => FallbackBlockRange::Range10,
            FallbackBlockRange::Range10 => FallbackBlockRange::Range5,
            FallbackBlockRange::Range5 => FallbackBlockRange::Range1,
            FallbackBlockRange::Range1 => FallbackBlockRange::Range1,
        }
    }
    fn from_diff(diff: U64) -> FallbackBlockRange {
        let diff = diff.as_limbs()[0];
        if diff >= 5000 {
            FallbackBlockRange::Range5000
        } else if diff >= 500 {
            FallbackBlockRange::Range500
        } else if diff >= 75 {
            FallbackBlockRange::Range75
        } else if diff >= 50 {
            FallbackBlockRange::Range50
        } else if diff >= 45 {
            FallbackBlockRange::Range45
        } else if diff >= 40 {
            FallbackBlockRange::Range40
        } else if diff >= 35 {
            FallbackBlockRange::Range35
        } else if diff >= 30 {
            FallbackBlockRange::Range30
        } else if diff >= 25 {
            FallbackBlockRange::Range25
        } else if diff >= 20 {
            FallbackBlockRange::Range20
        } else if diff >= 15 {
            FallbackBlockRange::Range15
        } else if diff >= 10 {
            FallbackBlockRange::Range10
        } else if diff >= 5 {
            FallbackBlockRange::Range5
        } else {
            FallbackBlockRange::Range1
        }
    }
}
fn calculate_process_historic_log_to_block(
    new_from_block: &U64,
    snapshot_to_block: &U64,
    max_block_range_limitation: &Option<U64>,
) -> U64 {
    if let Some(max_block_range_limitation) = max_block_range_limitation {
        let to_block = new_from_block + max_block_range_limitation;
        if to_block > *snapshot_to_block {
            *snapshot_to_block
        } else {
            to_block
        }
    } else {
        *snapshot_to_block
    }
}
</file>

<file path="core/src/indexer/last_synced.rs">
use alloy::primitives::U64;
use clickhouse::Row;
use rust_decimal::Decimal;
use serde::Deserialize;
use std::time::Duration;
use std::{path::Path, str::FromStr, sync::Arc};
use tokio::{
    fs,
    fs::File,
    io::{AsyncBufReadExt, AsyncWriteExt, BufReader},
};
use tracing::{debug, error};
use crate::database::clickhouse::client::ClickhouseClient;
use crate::database::postgres::generate::generate_internal_event_table_name_no_shorten;
use crate::{
    database::{
        generate::generate_indexer_contract_schema_name,
        postgres::generate::generate_internal_event_table_name,
    },
    event::config::{EventProcessingConfig, TraceProcessingConfig},
    helpers::get_full_path,
    manifest::{storage::CsvDetails, stream::StreamsConfig},
    EthereumSqlTypeWrapper, PostgresClient,
};
async fn get_last_synced_block_number_file(
    full_path: &Path,
    contract_name: &str,
    network: &str,
    event_name: &str,
) -> Result<Option<U64>, UpdateLastSyncedBlockNumberFile> {
    let file_path =
        build_last_synced_block_number_file(full_path, contract_name, network, event_name);
    let path = Path::new(&file_path);
    if !path.exists() {
        return Ok(None);
    }
    let file = File::open(path).await?;
    let mut reader = BufReader::new(file);
    let mut line = String::new();
    if reader.read_line(&mut line).await? > 0 {
        let value = line.trim();
        let parse = U64::from_str(value);
        return match parse {
            Ok(value) => Ok(Some(value)),
            Err(e) => {
                Err(UpdateLastSyncedBlockNumberFile::ParseError(value.to_string(), e.to_string()))
            }
        };
    }
    Ok(None)
}
fn build_last_synced_block_number_file(
    full_path: &Path,
    contract_name: &str,
    network: &str,
    event_name: &str,
) -> String {
    let path = full_path.join(contract_name).join("last-synced-blocks").join(format!(
        "{}-{}-{}.txt",
        contract_name.to_lowercase(),
        network.to_lowercase(),
        event_name.to_lowercase()
    ));
    path.to_string_lossy().into_owned()
}
pub struct SyncConfig<'a> {
    pub project_path: &'a Path,
    pub postgres: &'a Option<Arc<PostgresClient>>,
    pub clickhouse: &'a Option<Arc<ClickhouseClient>>,
    pub csv_details: &'a Option<CsvDetails>,
    pub stream_details: &'a Option<&'a StreamsConfig>,
    pub contract_csv_enabled: bool,
    pub indexer_name: &'a str,
    pub contract_name: &'a str,
    pub event_name: &'a str,
    pub network: &'a str,
}
pub async fn get_last_synced_block_number(config: SyncConfig<'_>) -> Option<U64> {
    // Check CSV file for last seen block as no database enabled
    if config.postgres.is_none() && config.contract_csv_enabled {
        if let Some(csv_details) = config.csv_details {
            return if let Ok(result) = get_last_synced_block_number_file(
                &get_full_path(config.project_path, &csv_details.path).unwrap_or_else(|_| {
                    panic!("failed to get full path {}", config.project_path.display())
                }),
                config.contract_name,
                config.network,
                config.event_name,
            )
            .await
            {
                if let Some(value) = result {
                    if value.is_zero() {
                        return None;
                    }
                }
                result
            } else {
                error!("Error fetching last synced block from CSV");
                None
            };
        }
    }
    // Then check streams if no csv or database to find out last synced block
    if config.postgres.is_none() && !config.contract_csv_enabled && config.stream_details.is_some()
    {
        let stream_details = config.stream_details.as_ref().unwrap();
        // create the path if it does not exist
        stream_details
            .create_full_streams_last_synced_block_path(config.project_path, config.contract_name)
            .await;
        return if let Ok(result) = get_last_synced_block_number_file(
            &config
                .project_path
                .join(stream_details.get_streams_last_synced_block_path())
                .canonicalize()
                .expect("Failed to canonicalize path"),
            config.contract_name,
            config.network,
            config.event_name,
        )
        .await
        {
            if let Some(value) = result {
                if value.is_zero() {
                    return None;
                }
            }
            result
        } else {
            error!("Error fetching last synced block from stream");
            None
        };
    }
    // Query database for last synced block
    if let Some(postgres) = config.postgres {
        let schema =
            generate_indexer_contract_schema_name(config.indexer_name, config.contract_name);
        let table_name = generate_internal_event_table_name(&schema, config.event_name);
        let query = format!(
            "SELECT last_synced_block FROM rindexer_internal.{table_name} WHERE network = $1"
        );
        return match postgres.query_one(&query, &[&config.network]).await {
            Ok(row) => {
                let result: Decimal = row.get("last_synced_block");
                let parsed =
                    U64::from_str(&result.to_string()).expect("Failed to parse last_synced_block");
                if parsed.is_zero() {
                    None
                } else {
                    Some(parsed)
                }
            }
            Err(e) => {
                error!("Error fetching last synced block: {:?}", e);
                None
            }
        };
    }
    // Query database for last synced block
    if let Some(clickhouse) = config.clickhouse {
        #[derive(Row, Deserialize)]
        struct LastBlock {
            last_synced_block: u64,
        }
        let schema =
            generate_indexer_contract_schema_name(config.indexer_name, config.contract_name);
        let table_name = generate_internal_event_table_name_no_shorten(&schema, config.event_name);
        let query = format!(
            "SELECT last_synced_block FROM rindexer_internal.{table_name} FINAL WHERE network = '{}'",
            config.network
        );
        let row = clickhouse.query_one::<LastBlock>(&query).await;
        return match row {
            Ok(row) => {
                let result = row.last_synced_block.to_string();
                let parsed =
                    U64::from_str(&result.to_string()).expect("Failed to parse last_synced_block");
                if parsed.is_zero() {
                    None
                } else {
                    Some(parsed)
                }
            }
            Err(e) => {
                error!("Error fetching last synced block: {:?}", e);
                None
            }
        };
    }
    None
}
#[derive(thiserror::Error, Debug)]
pub enum UpdateLastSyncedBlockNumberFile {
    #[error("File IO error: {0}")]
    FileIo(#[from] std::io::Error),
    #[error("Failed to parse block number: {0} err: {0}")]
    ParseError(String, String),
}
async fn update_last_synced_block_number_for_file(
    contract_name: &str,
    network: &str,
    event_name: &str,
    full_path: &Path,
    to_block: U64,
) -> Result<(), UpdateLastSyncedBlockNumberFile> {
    let file_path =
        build_last_synced_block_number_file(full_path, contract_name, network, event_name);
    let last_block =
        get_last_synced_block_number_file(full_path, contract_name, network, event_name).await?;
    let to_block_higher_then_last_block =
        if let Some(last_block_value) = last_block { to_block > last_block_value } else { true };
    if last_block.is_none() || to_block_higher_then_last_block {
        let temp_file_path = format!("{file_path}.tmp");
        let mut file = File::create(&temp_file_path).await?;
        file.write_all(to_block.to_string().as_bytes()).await?;
        file.sync_all().await?;
        fs::rename(temp_file_path, file_path).await?;
    }
    Ok(())
}
/// Update the last indexed block.
///
/// Note: this is an async task and should be awaited rather than spawned in the background
/// to prevent overloading concurrent "update" statements on the database which will increase
/// lock contention and slow down the system.
pub async fn update_progress_and_last_synced_task(
    config: Arc<EventProcessingConfig>,
    to_block: U64,
    on_complete: impl FnOnce() + Send + 'static,
) {
    let update_last_synced_block_result = tokio::time::timeout(Duration::from_millis(100), async {
        config
            .progress()
            .lock()
            .await
            .update_last_synced_block(&config.network_contract().id, to_block)
    })
    .await;
    // We don't want the in-memory progress reporter to hold up processing. Under high-ingest
    // workloads, contention can be high enough to hang here.
    match update_last_synced_block_result {
        Ok(Err(e)) => error!("Error updating in-mem last synced block result: {:?}", e),
        Err(_) => debug!("Timeout in update_last_synced_block_result, completing early"),
        _ => {}
    };
    let latest = config
        .network_contract()
        .cached_provider
        .get_latest_block()
        .await
        .ok()
        .flatten()
        .map(|b| b.header.number)
        .unwrap_or(0);
    if let Some(postgres) = &config.postgres() {
        let schema =
            generate_indexer_contract_schema_name(&config.indexer_name(), &config.contract_name());
        let table_name = generate_internal_event_table_name(&schema, &config.event_name());
        let network = &config.network_contract().network;
        let query = format!(
            "UPDATE rindexer_internal.{table_name} SET last_synced_block = {to_block} WHERE network = '{network}' AND {to_block} > last_synced_block;
             UPDATE rindexer_internal.latest_block SET block = {latest} WHERE network = '{network}' AND {latest} > block;"
        );
        let result = postgres.batch_execute(&query).await;
        if let Err(e) = result {
            error!("Error updating db last synced block: {:?}", e);
        }
    } else if let Some(clickhouse) = &config.clickhouse() {
        let schema =
            generate_indexer_contract_schema_name(&config.indexer_name(), &config.contract_name());
        let table_name =
            generate_internal_event_table_name_no_shorten(&schema, &config.event_name());
        let network = &config.network_contract().network;
        let query = format!(
            r#"
            INSERT INTO rindexer_internal.{table_name} (network, last_synced_block) VALUES ('{network}', {to_block});
            INSERT INTO rindexer_internal.latest_block (network, block) VALUES ('{network}', {latest});
            "#
        );
        let result = clickhouse.execute_batch(&query).await;
        if let Err(e) = result {
            error!("Error updating clickhouse last synced block: {:?}", e);
        }
    } else if let Some(csv_details) = &config.csv_details() {
        if let Err(e) = update_last_synced_block_number_for_file(
            &config.contract_name(),
            &config.network_contract().network,
            &config.event_name(),
            &get_full_path(&config.project_path(), &csv_details.path).unwrap_or_else(|_| {
                panic!("failed to get full path {}", config.project_path().display())
            }),
            to_block,
        )
        .await
        {
            error!(
                "Error updating last synced block to CSV - path - {} error - {:?}",
                csv_details.path, e
            );
        }
    } else if let Some(stream_last_synced_block_file_path) =
        &config.stream_last_synced_block_file_path()
    {
        if let Err(e) = update_last_synced_block_number_for_file(
            &config.contract_name(),
            &config.network_contract().network,
            &config.event_name(),
            &config
                .project_path()
                .join(stream_last_synced_block_file_path)
                .canonicalize()
                .expect("Failed to canonicalize path"),
            to_block,
        )
        .await
        {
            error!(
                "Error updating last synced block to stream - path - {} error - {:?}",
                stream_last_synced_block_file_path, e
            );
        }
    }
    on_complete();
}
pub async fn evm_trace_update_progress_and_last_synced_task(
    config: Arc<TraceProcessingConfig>,
    to_block: U64,
    on_complete: impl FnOnce() + Send + 'static,
) {
    let update_last_synced_block_result = tokio::time::timeout(Duration::from_millis(100), async {
        config.progress.lock().await.update_last_synced_block(&config.id, to_block)
    })
    .await;
    // We don't want the in-memory progress reporter to hold up processing. Under high-ingest
    // workloads, contention can be high enough to hang here.
    match update_last_synced_block_result {
        Ok(Err(e)) => error!("Error updating in-mem last synced trace result: {:?}", e),
        Err(_) => debug!("Timeout in update_last_synced_block_result, completing early"),
        _ => {}
    }
    if let Some(postgres) = &config.postgres {
        // Use the native_transfer table for all trace events since they share the same pipeline
        let schema =
            generate_indexer_contract_schema_name(&config.indexer_name, &config.contract_name);
        let table_name = generate_internal_event_table_name(&schema, "native_transfer");
        let query = format!(
                "UPDATE rindexer_internal.{table_name} SET last_synced_block = $1 WHERE network = $2 AND $1 > last_synced_block"
            );
        let result = postgres
            .execute(&query, &[&EthereumSqlTypeWrapper::U64(to_block.to()), &config.network])
            .await;
        if let Err(e) = result {
            error!("Error updating last synced trace block db: {:?}", e);
        }
    }
    if let Some(csv_details) = &config.csv_details {
        if let Err(e) = update_last_synced_block_number_for_file(
            &config.contract_name,
            &config.network,
            &config.event_name,
            &get_full_path(&config.project_path, &csv_details.path).unwrap_or_else(|_| {
                panic!("failed to get full path {}", config.project_path.display())
            }),
            to_block,
        )
        .await
        {
            error!(
                "Error updating last synced block to CSV - path - {} error - {:?}",
                csv_details.path, e
            );
        }
    } else if let Some(stream_last_synced_block_file_path) =
        &config.stream_last_synced_block_file_path
    {
        if let Err(e) = update_last_synced_block_number_for_file(
            &config.contract_name,
            &config.network,
            &config.event_name,
            &config
                .project_path
                .join(stream_last_synced_block_file_path)
                .canonicalize()
                .expect("Failed to canonicalize path"),
            to_block,
        )
        .await
        {
            error!(
                "Error updating last synced block to stream - path - {} error - {:?}",
                stream_last_synced_block_file_path, e
            );
        }
    }
    on_complete();
}
</file>

<file path="core/src/indexer/mod.rs">
mod process;
mod progress;
pub use progress::{IndexingEventProgressStatus, IndexingEventsProgressState};
use serde::{Deserialize, Serialize};
mod dependency;
pub use dependency::ContractEventDependenciesMapFromRelationshipsError;
mod fetch_logs;
pub use fetch_logs::FetchLogsResult;
mod last_synced;
pub mod native_transfer;
pub mod no_code;
mod reorg;
pub mod start;
pub mod task_tracker;
pub use dependency::{ContractEventDependencies, EventDependencies, EventsDependencyTree};
use crate::manifest::{contract::Contract, native_transfer::NativeTransfers};
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Indexer {
    pub name: String,
    pub contracts: Vec<Contract>,
    pub native_transfers: NativeTransfers,
}
</file>

<file path="core/src/indexer/native_transfer.rs">
use std::{cmp, collections::VecDeque, ops::RangeInclusive, sync::Arc, time::Duration};
use alloy::consensus::Transaction;
use alloy::transports::{RpcError, TransportErrorKind};
use alloy::{
    primitives::{Address, Bytes, U256, U64},
    rpc::types::trace::parity::{Action, LocalizedTransactionTrace},
};
use futures::future::try_join_all;
use serde::Serialize;
use tokio::{sync::mpsc, time::sleep};
use tracing::{debug, error, info, warn};
use crate::is_running;
use crate::provider::RECOMMENDED_RPC_CHUNK_SIZE;
use crate::{
    event::{
        callback_registry::{TraceResult, TxInformation},
        config::TraceProcessingConfig,
    },
    indexer::{
        last_synced::evm_trace_update_progress_and_last_synced_task,
        process::ProcessEventError,
        reorg::handle_chain_notification,
        task_tracker::{indexing_event_processed, indexing_event_processing},
    },
    manifest::native_transfer::TraceProcessingMethod,
    provider::{JsonRpcCachedProvider, ProviderError},
};
/// An imaginary contract name to ensure native transfer "debug trace" indexing is compatible
/// with the streams and sinks to which rindexer writes.
pub const NATIVE_TRANSFER_CONTRACT_NAME: &str = "EvmTraces";
/// An imaginary contract name to ensure native transfer "debug trace" indexing is compatible
/// with the streams and sinks to which rindexer writes.
pub const EVENT_NAME: &str = "NativeTransfer";
/// Invent an ABI to mimic an ERC20 Transfer.
///
/// This will allow indexer consumers, which will typically be configured to consume contract events
/// to simply ingest an ERC20 compatible event for the native tokens.
///
/// In order to reduce name conflicts we will call the Transfer event `NativeTransfer`.
pub const NATIVE_TRANSFER_ABI: &str = r#"[{
    "anonymous": false,
    "inputs": [
        {
            "indexed": true,
            "name": "from",
            "type": "address"
        },
        {
            "indexed": true,
            "name": "to",
            "type": "address"
        },
        {
            "indexed": false,
            "name": "value",
            "type": "uint256"
        }
    ],
    "name": "NativeTransfer",
    "type": "event"
}]"#;
/// Refer to [`NATIVE_TRANSFER_ABI`] as an imaginary associated ABI for this Native Transfer
/// "event" struct.
#[derive(Debug, Clone, Serialize)]
pub struct NativeTransfer {
    pub from: Address,
    pub to: Address,
    pub value: U256,
    pub transaction_information: TxInformation,
}
/// Push a range of blocks to the back-pressured channel and block producer when full.
async fn push_range(block_tx: &mpsc::Sender<U64>, last: U64, latest: U64) {
    let range: RangeInclusive<u64> =
        last.try_into().expect("U64 fits u64")..=latest.try_into().expect("U64 fits u64");
    let mut range = range.collect::<VecDeque<_>>();
    while let Some(block) = range.pop_front() {
        if let Err(e) = block_tx.send(U64::from(block)).await {
            if block_tx.is_closed() {
                // Log error if not shutting down
                if is_running() {
                    error!("Failed to send block via channel: {}", e.to_string());
                }
                break;
            }
            error!("Failed to send block via channel. Re-queuing: {}", e.to_string());
            range.push_front(block);
        }
    }
}
/// Block publisher.
///
/// This is a long-running process designed to accept a [`Sender`] handle and publish blocks
/// in an efficient manner which respects the user defined manifest block ranges.
///
/// This process respects channel backpressure and will only complete once the `end_block` is
/// reached.
pub async fn native_transfer_block_fetch(
    publisher: Arc<JsonRpcCachedProvider>,
    block_tx: mpsc::Sender<U64>,
    start_block: U64,
    end_block: Option<U64>,
    indexing_distance_from_head: U64,
    network: String,
) -> Result<(), ProcessEventError> {
    let mut last_seen_block = start_block;
    let chain_state_notification = publisher.get_chain_state_notification();
    // Spawn a separate task to handle notifications
    if let Some(notifications) = chain_state_notification {
        // Subscribe to notifications for this network
        let mut notifications_clone = notifications.subscribe();
        tokio::spawn(async move {
            while let Ok(notification) = notifications_clone.recv().await {
                handle_chain_notification(notification, "NativeTransfer");
            }
        });
    }
    loop {
        if !is_running() {
            info!("Exiting native transfer indexing block processor!");
            break Ok(());
        }
        let latest_block = publisher.get_latest_block().await;
        match latest_block {
            Ok(Some(latest_block)) => {
                let block = U64::from(latest_block.header.number);
                // Always trim back to the safe indexing threshold (which is zero if disabled)
                let block = block - indexing_distance_from_head;
                if block > last_seen_block {
                    let to_block = end_block.map(|end| block.min(end)).unwrap_or(block);
                    let from_block = block.min(last_seen_block + U64::from(1));
                    debug!("Pushing trace blocks {} - {}", from_block, to_block);
                    push_range(&block_tx, from_block, to_block).await;
                    last_seen_block = to_block;
                }
                if end_block.is_some() && block > end_block.expect("must have block") {
                    info!("Finished HISTORICAL INDEXING for {} NativeEvmTraces. No more blocks to push.", network);
                    debug!("Dropping {} 'NativeEvmTraces' block Sender handle", network);
                    drop(block_tx);
                    return Ok(());
                }
            }
            Ok(None) => {}
            Err(e) => {
                error!("Error fetching '{}' blocks: {}", network, e.to_string());
                sleep(Duration::from_secs(1)).await;
            }
        }
    }
}
pub async fn native_transfer_block_processor(
    network_name: String,
    provider: Arc<JsonRpcCachedProvider>,
    config: Arc<TraceProcessingConfig>,
    mut block_rx: mpsc::Receiver<U64>,
) -> Result<(), ProcessEventError> {
    // Set the concurrency used to make requests based on the method.
    //
    // Currently, `eth_getBlockByNumber` is a single JSON-RPC batch, and others are individual
    // network calls so can be treated differently.
    let (initial_concurrent_requests, limit_concurrent_requests) = (5, RECOMMENDED_RPC_CHUNK_SIZE);
    let mut concurrent_requests: usize = initial_concurrent_requests;
    let mut buffer: Vec<U64> = Vec::with_capacity(limit_concurrent_requests);
    loop {
        if !is_running() {
            info!("Exiting native transfer indexing block processor!");
            break Ok(());
        }
        // Fetch more only if buffer was processed ok last time and cleared.
        let recv = if buffer.is_empty() {
            block_rx.recv_many(&mut buffer, concurrent_requests).await
        } else {
            buffer.len()
        };
        if recv == 0 {
            sleep(Duration::from_secs(1)).await;
            continue;
        }
        let processed_block = native_transfer_block_consumer(
            provider.clone(),
            &buffer[..recv],
            &network_name,
            &config,
        )
        .await;
        // If this has an error, we need to not and reconsume the blocks. We don't have
        // to worry about double-publish because the failure point is on the provider
        // call itself, which is before publish.
        if let Err(e) = processed_block {
            // On error, drop the block query range. We want a slow increase in concurrency and a
            // relatively aggressive backoff.
            concurrent_requests = cmp::max(1, (concurrent_requests as f64 * 0.8) as usize);
            let is_rate_limit_error = matches!(&e, ProcessEventError::ProviderCallError(
                ProviderError::RequestFailed(
                    RpcError::Transport(
                        TransportErrorKind::HttpError(http_err)
                    )
                )) if http_err.status == 429
            );
            if is_rate_limit_error {
                warn!(
                    "Rate-limited 429 '{}' block requests. Retrying in 2s: {}",
                    network_name,
                    e.to_string(),
                );
                sleep(Duration::from_secs(2)).await;
                continue;
            } else {
                warn!(
                    "Could not process '{}' block requests for {}..{}, Retrying in 500ms: {}",
                    network_name,
                    &buffer.first().map(|n| n.as_limbs()[0]).unwrap_or_else(|| 0),
                    &buffer.last().map(|n| n.as_limbs()[0]).unwrap_or_else(|| 0),
                    e.to_string(),
                );
            }
            sleep(Duration::from_secs(1)).await;
            continue;
        } else {
            buffer.clear();
            // A random chance of increasing the request count helps us not overload
            // the ratelimit too rapidly across multi-network trace indexing and have a
            // slow ramp-up time (if rpc batching isn't available)
            if rand::random_bool(0.1) {
                concurrent_requests =
                    ((concurrent_requests * 20) / 10).min(limit_concurrent_requests);
            }
            sleep(Duration::from_millis(50)).await;
        };
    }
}
async fn provider_trace_call(
    provider: Arc<JsonRpcCachedProvider>,
    config: &TraceProcessingConfig,
    block: U64,
) -> Result<Vec<LocalizedTransactionTrace>, ProviderError> {
    match config.method {
        TraceProcessingMethod::TraceBlock => provider.trace_block(block).await,
        TraceProcessingMethod::DebugTraceBlockByNumber => {
            provider.debug_trace_block_by_number(block).await
        }
        _ => unimplemented!("Unsupported trace method"),
    }
}
/// Index native transfers via batched rpc block call method
pub async fn native_transfer_block_consumer(
    provider: Arc<JsonRpcCachedProvider>,
    block_numbers: &[U64],
    network_name: &str,
    config: &Arc<TraceProcessingConfig>,
) -> Result<(), ProcessEventError> {
    let blocks = provider.get_block_by_number_batch(block_numbers, true).await?;
    let (from_block, to_block) = block_numbers
        .iter()
        .fold((U64::MAX, U64::ZERO), |(min, max), &num| (cmp::min(min, num), cmp::max(max, num)));
    let native_transfers = blocks
        .clone()
        .into_iter()
        .flat_map(|b| {
            b.transactions.clone().into_transactions().map(move |tx| (b.header.timestamp, tx))
        })
        .filter_map(|(ts, tx)| {
            let is_empty_input = tx.input().is_empty();
            let is_value_zero = tx.value().is_zero();
            let has_to_address = tx.to().is_some();
            if has_to_address && is_empty_input && !is_value_zero {
                let to = tx.to().unwrap();
                Some(TraceResult::new_native_transfer(
                    tx,
                    ts,
                    to,
                    network_name,
                    provider.chain.id(),
                    from_block,
                    to_block,
                ))
            } else {
                None
            }
        })
        .collect::<Vec<_>>();
    // Important that we call this for every event even if there are no logs.
    // This is because we need to sync the last seen block number still.
    indexing_event_processing();
    let blocks = blocks
        .into_iter()
        .map(|b| TraceResult::new_block(b, network_name, provider.chain.id(), from_block, to_block))
        .collect::<Vec<_>>();
    config.trigger_event(blocks).await;
    if !native_transfers.is_empty() {
        config.trigger_event(native_transfers).await;
    }
    evm_trace_update_progress_and_last_synced_task(
        config.clone(),
        to_block,
        indexing_event_processed,
    )
    .await;
    Ok(())
}
// Indexing native transfers if debug or trace indexing is enabled.
///
/// NOTE: This is currently unused as we have temporarily migrated to exclusively using
/// `eth_getBlockByNumber` calls instead, this is being retained for posterity should we
/// choose to continue to support `debug` and `trace` based native transfer indexing.
#[allow(unused)]
pub async fn native_transfer_block_consumer_debug(
    provider: Arc<JsonRpcCachedProvider>,
    block_numbers: &[U64],
    network_name: &str,
    config: Arc<TraceProcessingConfig>,
) -> Result<(), ProcessEventError> {
    let trace_futures: Vec<_> =
        block_numbers.iter().map(|n| provider_trace_call(provider.clone(), &config, *n)).collect();
    let trace_calls = try_join_all(trace_futures).await?;
    let (from_block, to_block) = block_numbers
        .iter()
        .fold((U64::MAX, U64::ZERO), |(min, max), &num| (cmp::min(min, num), cmp::max(max, num)));
    // We're not ready to support complete "trace" indexing for zksync chains. So we can
    // effectively only get what we need for native transfers by removing calls to "system
    // contracts".
    //
    // As an example, a Zksync ETH transfer will have a complex set of interactions with system
    // contracts. But, there will be only one deeply-nested "transfer" call for the actual two EOAs,
    // so filtering everything else will allow us to grab that without noise.
    //
    // Read more:
    // - https://docs.zksync.io/zksync-protocol/contracts/system-contracts#l2basetoken-msgvaluesimulator
    // - https://github.com/matter-labs/zksync-era/blob/7f36ed98fc6066c1224ff07...-upgrade.ts#L24
    let zksync_system_contracts: [Address; 13] = [
        "0x0000000000000000000000000000000000008001".parse().unwrap(), // Native Token
        "0x0000000000000000000000000000000000008002".parse().unwrap(), // AccountCodeStorage
        "0x0000000000000000000000000000000000008003".parse().unwrap(), // NonceHolder
        "0x0000000000000000000000000000000000008004".parse().unwrap(), // KnownCodesStorage
        "0x0000000000000000000000000000000000008005".parse().unwrap(), // ImmutableSimulator
        "0x0000000000000000000000000000000000008006".parse().unwrap(), // ContractDeployer
        "0x0000000000000000000000000000000000008008".parse().unwrap(), // L1Messenger
        "0x0000000000000000000000000000000000008009".parse().unwrap(), // MsgValueSimulator
        "0x000000000000000000000000000000000000800a".parse().unwrap(), // L2BaseToken
        "0x000000000000000000000000000000000000800b".parse().unwrap(), // SystemContext
        "0x000000000000000000000000000000000000800c".parse().unwrap(), // BootloaderUtilities
        "0x000000000000000000000000000000000000800e".parse().unwrap(), // BytecodeCompressor
        "0x000000000000000000000000000000000000800f".parse().unwrap(), // ComplexUpgrader
    ];
    let native_transfers = trace_calls
        .into_iter()
        .flatten()
        .filter_map(|trace| {
            let action = match &trace.trace.action {
                Action::Call(call) => Some(call),
                _ => None,
            }?;
            let no_input = action.input == Bytes::new();
            let has_value = !action.value.is_zero();
            let is_zksync_system_transfer = zksync_system_contracts.contains(&action.from)
                || zksync_system_contracts.contains(&action.to);
            let is_native_transfer = has_value && no_input && !is_zksync_system_transfer;
            if is_native_transfer {
                Some(TraceResult::new_debug_native_transfer(
                    action,
                    &trace,
                    network_name,
                    provider.chain.id(),
                    from_block,
                    to_block,
                ))
            } else {
                None
            }
        })
        .collect::<Vec<_>>();
    if native_transfers.is_empty() {
        return Ok(());
    }
    indexing_event_processing();
    if !native_transfers.is_empty() {
        config.trigger_event(native_transfers).await;
    }
    evm_trace_update_progress_and_last_synced_task(config, to_block, indexing_event_processed)
        .await;
    Ok(())
}
</file>

<file path="core/src/indexer/no_code.rs">
use std::{
    io,
    path::{Path, PathBuf},
    sync::Arc,
};
use alloy::{
    dyn_abi::DynSolValue,
    json_abi::{Event, JsonAbi},
};
use serde_json::Value;
use tokio_postgres::types::Type as PgType;
use tracing::{debug, error, info, warn};
use super::native_transfer::{NATIVE_TRANSFER_ABI, NATIVE_TRANSFER_CONTRACT_NAME};
use crate::database::clickhouse::client::ClickhouseClient;
use crate::database::clickhouse::setup::{setup_clickhouse, SetupClickhouseError};
use crate::database::generate::generate_event_table_full_name;
use crate::database::sql_type_wrapper::{
    map_ethereum_wrapper_to_json, map_log_params_to_ethereum_wrapper, EthereumSqlTypeWrapper,
};
use crate::manifest::contract::Contract;
use crate::{
    abi::{ABIItem, CreateCsvFileForEvent, EventInfo, ParamTypeError, ReadAbiError},
    chat::ChatClients,
    database::postgres::{
        client::PostgresClient,
        generate::generate_column_names_only_with_base_properties,
        setup::{setup_postgres, SetupPostgresError},
    },
    event::{
        callback_registry::{
            noop_decoder, CallbackResult, EventCallbackRegistry, EventCallbackRegistryInformation,
            EventCallbackType, TraceCallbackRegistry, TraceCallbackRegistryInformation,
            TraceCallbackType, TxInformation,
        },
        contract_setup::{ContractInformation, CreateContractInformationError, TraceInformation},
        EventMessage,
    },
    generate_random_id,
    manifest::{
        contract::ParseAbiError,
        core::Manifest,
        yaml::{read_manifest, ReadManifestError},
    },
    provider::{CreateNetworkProvider, RetryClientError},
    setup_info_logger,
    streams::StreamsClients,
    types::core::LogParam,
    AsyncCsvAppender, FutureExt, IndexingDetails, StartDetails, StartNoCodeDetails,
};
use crate::{
    event::callback_registry::TraceResult,
    helpers::{map_log_params_to_raw_values, parse_log},
};
#[derive(thiserror::Error, Debug)]
pub enum SetupNoCodeError {
    #[error("Could not work out project path from the parent of the manifest")]
    NoProjectPathFoundUsingParentOfManifestPath,
    #[error("Could not read manifest: {0}")]
    CouldNotReadManifest(#[from] ReadManifestError),
    #[error("Could not setup postgres: {0}")]
    SetupPostgresError(#[from] SetupPostgresError),
    #[error("{0}")]
    RetryClientError(#[from] RetryClientError),
    #[error("Could not process indexers: {0}")]
    ProcessIndexersError(#[from] ProcessIndexersError),
    #[error("Could not setup clickhouse: {0}")]
    SetupClickhouseError(#[from] SetupClickhouseError),
    #[error("You have graphql disabled as well as indexer so nothing can startup")]
    NothingToStartNoCode,
}
pub async fn setup_no_code(
    details: StartNoCodeDetails<'_>,
) -> Result<StartDetails<'_>, SetupNoCodeError> {
    if !details.indexing_details.enabled && !details.graphql_details.enabled {
        return Err(SetupNoCodeError::NothingToStartNoCode);
    }
    let project_path = details.manifest_path.parent();
    match project_path {
        Some(project_path) => {
            let mut manifest = read_manifest(details.manifest_path)?;
            setup_info_logger();
            info!("Starting rindexer no code");
            let mut postgres: Option<Arc<PostgresClient>> = None;
            if manifest.storage.postgres_enabled() {
                postgres = Some(Arc::new(setup_postgres(project_path, &manifest).await?));
            }
            let mut clickhouse: Option<Arc<ClickhouseClient>> = None;
            if manifest.storage.clickhouse_enabled() {
                clickhouse = Some(Arc::new(setup_clickhouse(project_path, &manifest).await?));
            }
            if !details.indexing_details.enabled {
                return Ok(StartDetails {
                    manifest_path: details.manifest_path,
                    indexing_details: None,
                    graphql_details: details.graphql_details,
                });
            }
            let network_providers = CreateNetworkProvider::create(&manifest).await?;
            info!(
                "Networks enabled: {}",
                network_providers
                    .iter()
                    .map(|result| result.network_name.as_str())
                    .collect::<Vec<&str>>()
                    .join(", ")
            );
            let events = process_events(
                project_path,
                &manifest,
                postgres.clone(),
                clickhouse.clone(),
                &network_providers,
            )
            .await?;
            let registry = EventCallbackRegistry { events };
            info!(
                "Events registered to index:{}",
                registry
                    .events
                    .iter()
                    .map(|event| event.info_log_name())
                    .collect::<Vec<String>>()
                    .join(", ")
            );
            let trace_events = process_trace_events(
                project_path,
                &mut manifest,
                postgres,
                clickhouse,
                &network_providers,
            )
            .await?;
            let trace_registry = TraceCallbackRegistry { events: trace_events };
            if manifest.has_enabled_native_transfers() {
                info!(
                    "Native token transfers to index: {}",
                    manifest
                        .native_transfers
                        .networks
                        .unwrap_or_default()
                        .iter()
                        .map(|network| network.network.clone())
                        .collect::<Vec<String>>()
                        .join(", ")
                );
            }
            Ok(StartDetails {
                manifest_path: details.manifest_path,
                indexing_details: Some(IndexingDetails {
                    registry,
                    trace_registry,
                    event_stream: None,
                }),
                graphql_details: details.graphql_details,
            })
        }
        None => Err(SetupNoCodeError::NoProjectPathFoundUsingParentOfManifestPath),
    }
}
#[derive(Clone)]
struct NoCodeCallbackParams {
    event_info: EventInfo,
    indexer_name: String,
    contract_name: String,
    event: Event,
    index_event_in_order: bool,
    csv: Option<Arc<AsyncCsvAppender>>,
    postgres: Option<Arc<PostgresClient>>,
    sql_event_table_name: String,
    sql_column_names: Vec<String>,
    clickhouse: Option<Arc<ClickhouseClient>>,
    streams_clients: Arc<Option<StreamsClients>>,
    chat_clients: Arc<Option<ChatClients>>,
}
struct EventCallbacks {
    event_callback: EventCallbackType,
    trace_callback: TraceCallbackType,
}
fn no_code_callback(params: Arc<NoCodeCallbackParams>) -> EventCallbacks {
    let shared_callback = Arc::new(move |results| {
        let params = Arc::clone(&params);
        async move {
            let event_length = match &results {
                CallbackResult::Event(event) => event.len(),
                CallbackResult::Trace(event) => event.len(),
            };
            if event_length == 0 {
                debug!(
                    "{} {}: {} - {}",
                    params.indexer_name, params.contract_name, params.event_info.name, "NO EVENTS"
                );
                return Ok(());
            }
            // TODO
            // Remove unwrap
            let (from_block, to_block) = match &results {
                CallbackResult::Event(event) => (
                    event.first().unwrap().found_in_request.from_block,
                    event.first().unwrap().found_in_request.to_block,
                ),
                CallbackResult::Trace(event) => {
                    // Filter to only NativeTransfer events and get the first one
                    let native_transfer = event
                        .iter()
                        .filter_map(|result| match result {
                            TraceResult::NativeTransfer { found_in_request, .. } => {
                                Some(found_in_request)
                            }
                            TraceResult::Block { .. } => None,
                        })
                        .next()
                        .unwrap();
                    (native_transfer.from_block, native_transfer.to_block)
                }
            };
            let network = match &results {
                CallbackResult::Event(event) => {
                    event.first().unwrap().tx_information.network.clone()
                }
                CallbackResult::Trace(event) => {
                    // Filter to only NativeTransfer events and get the first one
                    event
                        .iter()
                        .filter_map(|result| match result {
                            TraceResult::NativeTransfer { tx_information, .. } => {
                                Some(&tx_information.network)
                            }
                            TraceResult::Block { .. } => None,
                        })
                        .next()
                        .unwrap()
                        .clone()
                }
            };
            let mut indexed_count = 0;
            let mut sql_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = Vec::new();
            let mut sql_bulk_column_types: Vec<PgType> = Vec::new();
            let mut csv_bulk_data: Vec<Vec<String>> = Vec::new();
            // stream and chat info
            let mut event_message_data: Vec<Value> = Vec::new();
            let owned_results = match &results {
                CallbackResult::Event(events) => events
                    .iter()
                    .filter_map(|result| {
                        let log = parse_log(&params.event, &result.log)?;
                        let address = result.tx_information.address;
                        let transaction_hash = result.tx_information.transaction_hash;
                        let block_number = result.tx_information.block_number;
                        let block_timestamp = result
                            .tx_information
                            .block_timestamp
                            .and_then(|ts| chrono::DateTime::from_timestamp(ts.to(), 0));
                        let block_hash = result.tx_information.block_hash;
                        let network = result.tx_information.network.to_string();
                        let chain_id = result.tx_information.chain_id;
                        let transaction_index = result.tx_information.transaction_index;
                        let log_index = result.tx_information.log_index;
                        let event_parameters: Vec<EthereumSqlTypeWrapper> =
                            map_log_params_to_ethereum_wrapper(
                                &params.event_info.inputs,
                                &log.params,
                            );
                        let contract_address = EthereumSqlTypeWrapper::Address(address);
                        let end_global_parameters = vec![
                            EthereumSqlTypeWrapper::B256(transaction_hash),
                            EthereumSqlTypeWrapper::U64(block_number),
                            EthereumSqlTypeWrapper::DateTimeNullable(block_timestamp),
                            EthereumSqlTypeWrapper::B256(block_hash),
                            EthereumSqlTypeWrapper::String(network.to_string()),
                            EthereumSqlTypeWrapper::U64(transaction_index),
                            EthereumSqlTypeWrapper::U256(log_index),
                        ];
                        Some((
                            log.params,
                            address,
                            transaction_hash,
                            log_index,
                            transaction_index,
                            block_number,
                            result.tx_information.block_timestamp,
                            block_hash,
                            network,
                            chain_id,
                            contract_address,
                            event_parameters,
                            end_global_parameters,
                        ))
                    })
                    .collect::<Vec<_>>(),
                CallbackResult::Trace(events) => events
                    .iter()
                    .filter_map(|result| {
                        match result {
                            TraceResult::NativeTransfer {
                                from, to, value, tx_information, ..
                            } => {
                                let log_params = vec![
                                    LogParam::new("from".to_string(), DynSolValue::Address(*from)),
                                    LogParam::new("to".to_string(), DynSolValue::Address(*to)),
                                    LogParam::new(
                                        "value".to_string(),
                                        DynSolValue::Uint(*value, 256),
                                    ),
                                ];
                                let address = tx_information.address;
                                let transaction_hash = tx_information.transaction_hash;
                                let block_number = tx_information.block_number;
                                let block_timestamp = tx_information
                                    .block_timestamp
                                    .and_then(|ts| chrono::DateTime::from_timestamp(ts.to(), 0));
                                let block_hash = tx_information.block_hash;
                                let network = tx_information.network.to_string();
                                let chain_id = tx_information.chain_id;
                                let transaction_index = tx_information.transaction_index;
                                let log_index = tx_information.log_index;
                                let event_parameters: Vec<EthereumSqlTypeWrapper> =
                                    map_log_params_to_ethereum_wrapper(
                                        &params.event_info.inputs,
                                        &log_params,
                                    );
                                let contract_address = EthereumSqlTypeWrapper::Address(address);
                                let end_global_parameters = vec![
                                    EthereumSqlTypeWrapper::B256(transaction_hash),
                                    EthereumSqlTypeWrapper::U64(block_number),
                                    EthereumSqlTypeWrapper::DateTimeNullable(block_timestamp),
                                    EthereumSqlTypeWrapper::B256(block_hash),
                                    EthereumSqlTypeWrapper::String(network.to_string()),
                                    EthereumSqlTypeWrapper::U64(transaction_index),
                                    EthereumSqlTypeWrapper::U256(log_index),
                                ];
                                Some((
                                    log_params,
                                    address,
                                    transaction_hash,
                                    log_index,
                                    transaction_index,
                                    block_number,
                                    tx_information.block_timestamp,
                                    block_hash,
                                    network,
                                    chain_id,
                                    contract_address,
                                    event_parameters,
                                    end_global_parameters,
                                ))
                            }
                            TraceResult::Block { .. } => None, // Skip block events in no-code mode
                        }
                    })
                    .collect::<Vec<_>>(),
            };
            for (
                log_params,
                address,
                transaction_hash,
                log_index,
                transaction_index,
                block_number,
                block_timestamp,
                block_hash,
                network,
                chain_id,
                contract_address,
                event_parameters,
                end_global_parameters,
            ) in owned_results
            {
                if params.streams_clients.is_some() || params.chat_clients.is_some() {
                    let event_result = map_ethereum_wrapper_to_json(
                        &params.event_info.inputs,
                        &event_parameters,
                        &TxInformation {
                            network: network.clone(),
                            chain_id,
                            address,
                            block_hash,
                            block_number,
                            transaction_hash,
                            block_timestamp,
                            log_index,
                            transaction_index,
                        },
                        false,
                    );
                    event_message_data.push(event_result);
                }
                let mut all_params: Vec<EthereumSqlTypeWrapper> = vec![contract_address];
                all_params.extend(event_parameters);
                all_params.extend(end_global_parameters);
                // Set column types dynamically based on first result
                if sql_bulk_column_types.is_empty() {
                    sql_bulk_column_types =
                        all_params.iter().map(|param| param.to_type()).collect();
                }
                if params.postgres.is_some() || params.clickhouse.is_some() {
                    sql_bulk_data.push(all_params);
                }
                if params.csv.is_some() {
                    let mut csv_data: Vec<String> = vec![format!("{:?}", address)];
                    let raw_values = map_log_params_to_raw_values(&log_params);
                    for param in raw_values {
                        csv_data.push(param);
                    }
                    csv_data.push(format!("{transaction_hash:?}"));
                    csv_data.push(format!("{block_number:?}"));
                    csv_data.push(format!("{block_hash:?}"));
                    csv_data.push(network);
                    csv_bulk_data.push(csv_data);
                }
                indexed_count += 1;
            }
            if let Some(postgres) = &params.postgres {
                if !sql_bulk_data.is_empty() {
                    if let Err(e) = postgres
                        .insert_bulk(
                            &params.sql_event_table_name,
                            &params.sql_column_names,
                            &sql_bulk_data,
                        )
                        .await
                    {
                        error!(
                            "{}::{} - Error performing postgres bulk insert: {}",
                            params.contract_name, params.event_info.name, e
                        );
                        return Err(e.to_string());
                    }
                }
            }
            if let Some(clickhouse) = &params.clickhouse {
                if !sql_bulk_data.is_empty() {
                    if let Err(e) = clickhouse
                        .insert_bulk(
                            &params.sql_event_table_name,
                            &params.sql_column_names,
                            &sql_bulk_data,
                        )
                        .await
                    {
                        error!(
                            "{}::{} - Error performing clickhouse bulk insert: {}",
                            params.contract_name, params.event_info.name, e
                        );
                        return Err(e.to_string());
                    };
                }
            }
            if let Some(csv) = &params.csv {
                if !csv_bulk_data.is_empty() {
                    if let Err(e) = csv.append_bulk(csv_bulk_data).await {
                        return Err(e.to_string());
                    }
                }
            }
            let event_message = EventMessage {
                event_name: params.event_info.name.clone(),
                event_data: Value::Array(event_message_data),
                event_signature_hash: params.event.selector(),
                network: network.clone(),
            };
            if let Some(streams_clients) = params.streams_clients.as_ref() {
                let stream_id = format!(
                    "{}-{}-{}-{}-{}",
                    params.contract_name, params.event_info.name, network, from_block, to_block
                );
                let is_trace_event = match results {
                    CallbackResult::Event(_) => false,
                    CallbackResult::Trace(_) => true,
                };
                match streams_clients
                    .stream(stream_id, &event_message, params.index_event_in_order, is_trace_event)
                    .await
                {
                    Ok(streamed) => {
                        if streamed > 0 {
                            info!(
                                "{}::{} - {} - {} events {}",
                                params.contract_name,
                                params.event_info.name,
                                "STREAMED",
                                streamed,
                                format!(
                                    "- blocks: {} - {} - network: {}",
                                    from_block, to_block, network
                                )
                            );
                        }
                    }
                    Err(e) => {
                        error!("Error streaming event: {}", e);
                        return Err(e.to_string());
                    }
                }
            }
            if let Some(chat_clients) = params.chat_clients.as_ref() {
                if !chat_clients.is_in_block_range_to_send(&from_block, &to_block) {
                    warn!(
                        "{}::{} - {} - messages has a max 10 block range due the rate limits - {}",
                        params.contract_name,
                        params.event_info.name,
                        "CHAT_MESSAGES_DISABLED",
                        format!("- blocks: {} - {} - network: {}", from_block, to_block, network)
                    );
                } else {
                    match chat_clients
                        .send_message(
                            &event_message,
                            params.index_event_in_order,
                            &from_block,
                            &to_block,
                        )
                        .await
                    {
                        Ok(messages_sent) => {
                            if messages_sent > 0 {
                                info!(
                                    "{}::{} - {} - {} events {}",
                                    params.contract_name,
                                    params.event_info.name,
                                    "CHAT_MESSAGES_SENT",
                                    messages_sent,
                                    format!(
                                        "- blocks: {} - {} - network: {}",
                                        from_block, to_block, network
                                    )
                                );
                            }
                        }
                        Err(e) => {
                            error!("Error sending chat messages: {}", e);
                            return Err(e.to_string());
                        }
                    }
                }
            }
            info!(
                "{}::{} - {} - {} events {}",
                params.contract_name,
                params.event_info.name,
                "INDEXED",
                indexed_count,
                format!("- blocks: {} - {} - network: {}", from_block, to_block, network)
            );
            Ok(())
        }
        .boxed()
    });
    let callback = Arc::clone(&shared_callback);
    let event_callback: EventCallbackType =
        Arc::new(move |events| callback(CallbackResult::Event(events)));
    let callback = Arc::clone(&shared_callback);
    let trace_callback: TraceCallbackType =
        Arc::new(move |traces| callback(CallbackResult::Trace(traces)));
    EventCallbacks { trace_callback, event_callback }
}
#[derive(thiserror::Error, Debug)]
pub enum ProcessIndexersError {
    #[error("Could not read ABI string: {0}")]
    CouldNotReadAbiString(#[from] io::Error),
    #[error("Could not read ABI JSON: {0}")]
    CouldNotReadAbiJson(#[from] serde_json::Error),
    #[error("Could not read ABI items: {0}")]
    CouldNotReadAbiItems(#[from] ReadAbiError),
    #[error("Could not append headers to csv: {0}")]
    CsvHeadersAppendError(#[from] csv::Error),
    #[error("{0}")]
    CreateContractInformationError(#[from] CreateContractInformationError),
    #[error("{0}")]
    CreateCsvFileForEventError(#[from] CreateCsvFileForEvent),
    #[error("{0}")]
    ParamTypeError(#[from] ParamTypeError),
    #[error("Event name not found in ABI for contract: {0} - event: {1}")]
    EventNameNotFoundInAbi(String, String),
    #[error("Contract name is reserved: {0}. Please use another contract name.")]
    ContractNameConflict(String),
    #[error("{0}")]
    ParseAbiError(#[from] ParseAbiError),
}
pub async fn process_events(
    project_path: &Path,
    manifest: &Manifest,
    postgres: Option<Arc<PostgresClient>>,
    clickhouse: Option<Arc<ClickhouseClient>>,
    network_providers: &[CreateNetworkProvider],
) -> Result<Vec<EventCallbackRegistryInformation>, ProcessIndexersError> {
    let mut events: Vec<EventCallbackRegistryInformation> = vec![];
    for mut contract in manifest.all_contracts().clone() {
        let contract_events = process_contract(
            project_path,
            manifest,
            postgres.clone(),
            clickhouse.clone(),
            network_providers,
            &mut contract,
        )
        .await?;
        events.extend(contract_events);
    }
    Ok(events)
}
async fn process_contract(
    project_path: &Path,
    manifest: &Manifest,
    postgres: Option<Arc<PostgresClient>>,
    clickhouse: Option<Arc<ClickhouseClient>>,
    network_providers: &[CreateNetworkProvider],
    contract: &mut Contract,
) -> Result<Vec<EventCallbackRegistryInformation>, ProcessIndexersError> {
    if contract.name.to_lowercase() == NATIVE_TRANSFER_CONTRACT_NAME.to_lowercase() {
        return Err(ProcessIndexersError::ContractNameConflict(contract.name.to_string()));
    }
    // TODO - this could be shared with `get_abi_items`
    let abi_str = contract.parse_abi(project_path)?;
    let abi: JsonAbi = serde_json::from_str(&abi_str)?;
    let is_filter = contract.identify_and_modify_filter();
    let abi_items = ABIItem::get_abi_items(project_path, contract, is_filter)?;
    let event_names = ABIItem::extract_event_names_and_signatures_from_abi(abi_items)?;
    let mut events: Vec<EventCallbackRegistryInformation> = vec![];
    for event_info in event_names {
        let event_name = event_info.name.clone();
        let event = abi
            .events
            .get(&event_name)
            .and_then(|events| events.first())
            .ok_or_else(|| {
                ProcessIndexersError::EventNameNotFoundInAbi(
                    contract.name.clone(),
                    event_name.clone(),
                )
            })?
            .clone();
        let contract_information = ContractInformation::create(
            project_path,
            contract,
            network_providers,
            noop_decoder(),
            manifest,
        )?;
        let mut csv: Option<Arc<AsyncCsvAppender>> = None;
        if contract.generate_csv.unwrap_or(true) && manifest.storage.csv_enabled() {
            let csv_path =
                manifest.storage.csv.as_ref().map_or(PathBuf::from("generated_csv"), |c| {
                    PathBuf::from(c.path.strip_prefix("./").unwrap())
                });
            let headers: Vec<String> = event_info.csv_headers_for_event();
            let csv_path_str = csv_path.to_str().expect("Failed to convert csv path to string");
            let csv_path =
                event_info.create_csv_file_for_event(project_path, &contract.name, csv_path_str)?;
            let csv_appender = AsyncCsvAppender::new(&csv_path);
            if !Path::new(&csv_path).exists() {
                csv_appender.append_header(headers).await?;
            }
            csv = Some(Arc::new(csv_appender));
        }
        let sql_column_names = generate_column_names_only_with_base_properties(&event_info.inputs);
        let sql_event_table_name =
            generate_event_table_full_name(&manifest.name, &contract.name, &event_info.name);
        let streams_client = if let Some(streams) = &contract.streams {
            Some(StreamsClients::new(streams.clone()).await)
        } else {
            None
        };
        let chat_clients = if let Some(chats) = &contract.chat {
            Some(ChatClients::new(chats.clone()).await)
        } else {
            None
        };
        let index_event_in_order = contract
            .index_event_in_order
            .as_ref()
            .is_some_and(|vec| vec.contains(&event_info.name));
        let event = EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: manifest.name.clone(),
            event_name: event_info.name.clone(),
            index_event_in_order,
            topic_id: event_info.topic_id(),
            contract: contract_information,
            callback: no_code_callback(Arc::new(NoCodeCallbackParams {
                event_info,
                indexer_name: manifest.name.clone(),
                contract_name: contract.name.clone(),
                event: event.clone(),
                index_event_in_order,
                csv,
                postgres: postgres.clone(),
                clickhouse: clickhouse.clone(),
                sql_event_table_name,
                sql_column_names,
                streams_clients: Arc::new(streams_client),
                chat_clients: Arc::new(chat_clients),
            }))
            .event_callback,
        };
        events.push(event);
    }
    Ok(events)
}
pub async fn process_trace_events(
    project_path: &Path,
    manifest: &mut Manifest,
    postgres: Option<Arc<PostgresClient>>,
    clickhouse: Option<Arc<ClickhouseClient>>,
    network_providers: &[CreateNetworkProvider],
) -> Result<Vec<TraceCallbackRegistryInformation>, ProcessIndexersError> {
    let mut events: Vec<TraceCallbackRegistryInformation> = vec![];
    if !manifest.has_enabled_native_transfers() {
        return Ok(events);
    }
    let abi_str = NATIVE_TRANSFER_ABI;
    let abi: JsonAbi = serde_json::from_str(abi_str)?;
    #[allow(clippy::useless_conversion)]
    let abi_items: Vec<ABIItem> = serde_json::from_str(abi_str)?;
    let event_names = ABIItem::extract_event_names_and_signatures_from_abi(abi_items)?;
    let contract = &manifest.native_transfers;
    let contract_name = NATIVE_TRANSFER_CONTRACT_NAME.to_string();
    for event_info in event_names {
        let event_name = event_info.name.clone();
        let event = &abi
            .events
            .iter()
            .find(|(name, _)| *name == &event_name)
            .map(|(_, event)| event)
            .ok_or_else(|| {
                ProcessIndexersError::EventNameNotFoundInAbi(
                    contract_name.clone(),
                    event_name.clone(),
                )
            })?
            .first()
            .ok_or_else(|| {
                ProcessIndexersError::EventNameNotFoundInAbi(
                    contract_name.clone(),
                    event_name.clone(),
                )
            })?
            .clone();
        let trace_information =
            TraceInformation::create(manifest.native_transfers.clone(), network_providers)?;
        let mut csv: Option<Arc<AsyncCsvAppender>> = None;
        if contract.generate_csv.unwrap_or(true) && manifest.storage.csv_enabled() {
            let csv_path =
                manifest.storage.csv.as_ref().map_or(PathBuf::from("generated_csv"), |c| {
                    PathBuf::from(c.path.strip_prefix("./").unwrap())
                });
            let headers: Vec<String> = event_info.csv_headers_for_event();
            let csv_path_str = csv_path.to_str().expect("Failed to convert csv path to string");
            let csv_path =
                event_info.create_csv_file_for_event(project_path, &contract_name, csv_path_str)?;
            let csv_appender = AsyncCsvAppender::new(&csv_path);
            if !Path::new(&csv_path).exists() {
                csv_appender.append_header(headers).await?;
            }
            csv = Some(Arc::new(csv_appender));
        }
        let sql_column_names = generate_column_names_only_with_base_properties(&event_info.inputs);
        let sql_event_table_name =
            generate_event_table_full_name(&manifest.name, &contract_name, &event_info.name);
        let streams_client = if let Some(streams) = &contract.streams {
            Some(StreamsClients::new(streams.clone()).await)
        } else {
            None
        };
        let chat_clients = if let Some(chats) = &contract.chat {
            Some(ChatClients::new(chats.clone()).await)
        } else {
            None
        };
        let callback_params = Arc::new(NoCodeCallbackParams {
            event_info: event_info.clone(),
            indexer_name: manifest.name.clone(),
            contract_name: contract_name.clone(),
            event: event.clone(),
            index_event_in_order: false,
            csv,
            postgres: postgres.clone(),
            clickhouse: clickhouse.clone(),
            sql_event_table_name,
            sql_column_names,
            streams_clients: Arc::new(streams_client),
            chat_clients: Arc::new(chat_clients),
        });
        let event = TraceCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: manifest.name.clone(),
            event_name: event_info.name.clone(),
            contract_name: contract_name.clone(),
            trace_information: trace_information.clone(),
            callback: no_code_callback(callback_params).trace_callback,
        };
        events.push(event);
    }
    Ok(events)
}
</file>

<file path="core/src/indexer/process.rs">
use alloy::primitives::{B256, U64};
use futures::future::join_all;
use futures::StreamExt;
use std::{collections::HashMap, sync::Arc, time::Duration};
use tokio::sync::Semaphore;
use tokio::{
    sync::Mutex,
    task::{JoinError, JoinHandle},
    time::Instant,
};
use tracing::{debug, error, info};
use crate::helpers::is_relevant_block;
use crate::indexer::reorg::reorg_safe_distance_for_chain;
use crate::provider::JsonRpcCachedProvider;
use crate::{
    event::{
        callback_registry::EventResult, config::EventProcessingConfig, BuildRindexerFilterError,
        RindexerEventFilter,
    },
    indexer::{
        dependency::{ContractEventsDependenciesConfig, EventDependencies},
        fetch_logs::{fetch_logs_stream, FetchLogsResult},
        last_synced::update_progress_and_last_synced_task,
        progress::IndexingEventProgressStatus,
        task_tracker::{indexing_event_processed, indexing_event_processing},
    },
    is_running,
    provider::ProviderError,
};
#[derive(thiserror::Error, Debug)]
pub enum ProcessEventError {
    #[error("Could not process logs: {0}")]
    ProcessLogs(#[from] Box<ProviderError>),
    #[error("Could not build filter: {0}")]
    BuildFilterError(#[from] BuildRindexerFilterError),
    #[error("Could not get block number from provider: {0}")]
    ProviderCallError(#[from] ProviderError),
}
/// Processes an event that doesn't have dependencies.
/// First processes historical logs, then starts live indexing if the event is configured for live indexing.
/// This function returns immediately without waiting for the indexing to complete.
pub async fn process_non_blocking_event(
    config: EventProcessingConfig,
) -> Result<(), ProcessEventError> {
    debug!("{} - Processing non blocking event", config.info_log_name());
    process_event_logs(Arc::new(config), false, false).await?;
    Ok(())
}
/// Processes historical logs for a blocking event that has dependencies.
/// This function waits for the indexing to complete before returning.
pub async fn process_blocking_event_historical_data(
    config: Arc<EventProcessingConfig>,
) -> Result<(), Box<ProviderError>> {
    debug!("{} - Processing blocking event historical data", config.info_log_name());
    process_event_logs(config, true, true).await?;
    Ok(())
}
/// note block_until_indexed:
/// Whether to wait for all indexing tasks to complete for an event before returning
//  (needed for dependency indexing)
async fn process_event_logs(
    config: Arc<EventProcessingConfig>,
    force_no_live_indexing: bool,
    block_until_indexed: bool,
) -> Result<(), Box<ProviderError>> {
    // The concurrency with which we can call the trigger. If the indexer is running in-order
    // we can only call one at a time, otherwise we can call multiple in parallel based on what is
    // best for the application.
    //
    // We default to `2`, but the user will ideally override this based on the logic in the handler.
    // TODO: this feature is not safe need to review it
    let callback_concurrency = if config.index_event_in_order() {
        1usize
    } else {
        config.config().callback_concurrency.unwrap_or(1)
    };
    let callback_permits = Arc::new(Semaphore::new(callback_concurrency));
    let mut logs_stream = fetch_logs_stream(Arc::clone(&config), force_no_live_indexing);
    let mut tasks = Vec::new();
    while let Some(result) = logs_stream.next().await {
        let task = handle_logs_result(Arc::clone(&config), callback_permits.clone(), result)
            .await
            .map_err(|e| Box::new(ProviderError::CustomError(e.to_string())))?;
        if block_until_indexed {
            task.await.map_err(|e| Box::new(ProviderError::CustomError(e.to_string())))?;
        } else {
            tasks.push(task);
        }
    }
    // Wait for all remaining tasks to complete
    if !tasks.is_empty() {
        futures::future::try_join_all(tasks)
            .await
            .map_err(|e| Box::new(ProviderError::CustomError(e.to_string())))?;
    }
    Ok(())
}
#[derive(thiserror::Error, Debug)]
pub enum ProcessContractsEventsWithDependenciesError {
    #[error("{0}")]
    ProcessContractEventsWithDependenciesError(#[from] ProcessContractEventsWithDependenciesError),
    #[error("{0}")]
    JoinError(#[from] JoinError),
}
pub async fn process_contracts_events_with_dependencies(
    contracts_events_config: Vec<ContractEventsDependenciesConfig>,
) -> Result<(), ProcessContractsEventsWithDependenciesError> {
    let mut handles: Vec<JoinHandle<Result<(), ProcessContractEventsWithDependenciesError>>> =
        Vec::new();
    for contract_events in contracts_events_config {
        let handle = tokio::spawn(async move {
            process_contract_events_with_dependencies(
                contract_events.event_dependencies,
                Arc::new(contract_events.events_config),
            )
            .await
        });
        handles.push(handle);
    }
    let results = join_all(handles).await;
    for result in results {
        match result {
            Ok(inner_result) => inner_result?,
            Err(join_error) => {
                return Err(ProcessContractsEventsWithDependenciesError::JoinError(join_error))
            }
        }
    }
    Ok(())
}
#[derive(thiserror::Error, Debug)]
pub enum ProcessContractEventsWithDependenciesError {
    #[error("Could not process logs: {0}")]
    ProcessLogs(#[from] Box<ProviderError>),
    #[error("Could not build filter: {0}")]
    BuildFilterError(#[from] BuildRindexerFilterError),
    #[error("Event config not found for contract: {0} and event: {1}")]
    EventConfigNotFound(String, String),
    #[error("Could not run all the logs processes {0}")]
    JoinError(#[from] JoinError),
}
#[derive(Clone)]
pub struct OrderedLiveIndexingDetails {
    pub filter: RindexerEventFilter,
    pub last_seen_block_number: U64,
    pub last_no_new_block_log_time: Instant,
}
async fn process_contract_events_with_dependencies(
    dependencies: EventDependencies,
    events_processing_config: Arc<Vec<Arc<EventProcessingConfig>>>,
) -> Result<(), ProcessContractEventsWithDependenciesError> {
    let mut stack = vec![dependencies.tree];
    let live_indexing_events =
        Arc::new(Mutex::new(HashMap::<String, EventDependenciesIndexingConfig>::new()));
    while let Some(current_tree) = stack.pop() {
        let mut tasks = vec![];
        for dependency in &current_tree.contract_events {
            // multi network can have many of the same event names so we need to get them all
            let event_processing_configs = events_processing_config
                .iter()
                .filter(|e| {
                    // TODO - this is a hacky way to check if it's a filter event
                    (e.contract_name() == dependency.contract_name
                        || e.contract_name().replace("Filter", "") == dependency.contract_name)
                        && e.event_name() == dependency.event_name
                })
                .cloned()
                .collect::<Vec<Arc<EventProcessingConfig>>>();
            for event_processing_config in event_processing_configs {
                let task = tokio::spawn({
                    let live_indexing_events = Arc::clone(&live_indexing_events);
                    async move {
                        process_blocking_event_historical_data(Arc::clone(
                            &event_processing_config,
                        ))
                        .await?;
                        if event_processing_config.live_indexing() {
                            let network_contract = event_processing_config.network_contract();
                            let mut live_indexing_events = live_indexing_events.lock().await;
                            let entry = live_indexing_events
                                .entry(network_contract.network.clone())
                                .or_insert_with(|| EventDependenciesIndexingConfig {
                                    network: network_contract.network.clone(),
                                    cached_provider: network_contract.cached_provider.clone(),
                                    events: Vec::new(),
                                });
                            let rindexer_event_filter =
                                event_processing_config.to_event_filter()?;
                            entry.events.push((
                                Arc::clone(&event_processing_config),
                                rindexer_event_filter,
                            ));
                        }
                        Ok::<(), ProcessContractEventsWithDependenciesError>(())
                    }
                });
                tasks.push(task);
            }
        }
        let results = join_all(tasks).await;
        for result in results {
            match result {
                Ok(result) => match result {
                    Ok(_) => {}
                    Err(e) => {
                        error!("Error processing logs due to dependencies error: {:?}", e);
                        return Err(e);
                    }
                },
                Err(e) => {
                    error!("Error processing logs: {:?}", e);
                    return Err(ProcessContractEventsWithDependenciesError::JoinError(e));
                }
            }
        }
        // If there are more dependencies to process, push the next level onto the stack
        if let Some(next_tree) = &*current_tree.then {
            stack.push(Arc::clone(next_tree));
        }
    }
    let live_indexing_events = live_indexing_events.lock().await;
    if live_indexing_events.is_empty() {
        return Ok(());
    }
    let live_indexing_tasks = live_indexing_events
        .values()
        .map(|config| tokio::spawn(live_indexing_for_contract_event_dependencies(config.clone())))
        .collect::<Vec<_>>();
    futures::future::try_join_all(live_indexing_tasks).await?;
    Ok(())
}
#[derive(Clone)]
pub struct EventDependenciesIndexingConfig {
    pub network: String,
    pub cached_provider: Arc<JsonRpcCachedProvider>,
    pub events: Vec<(Arc<EventProcessingConfig>, RindexerEventFilter)>,
}
// TODO - this is a similar to live_indexing_stream but has to be a bit different we should merge
// code
#[allow(clippy::type_complexity)]
async fn live_indexing_for_contract_event_dependencies(
    EventDependenciesIndexingConfig { cached_provider, events, network }: EventDependenciesIndexingConfig,
) {
    debug!(
        "Live indexing events on {} in order: {}",
        network,
        events
            .iter()
            .map(|(config, _)| format!("{}::{}", config.contract_name(), config.event_name()))
            .collect::<Vec<_>>()
            .join(", ")
    );
    let mut ordering_live_indexing_details_map: HashMap<
        B256,
        Arc<Mutex<OrderedLiveIndexingDetails>>,
    > = HashMap::with_capacity(events.len());
    for (config, event_filter) in events.iter() {
        let mut filter = event_filter.clone();
        let last_seen_block_number = filter.to_block();
        let next_block_number = last_seen_block_number + U64::from(1);
        filter = filter.set_from_block(next_block_number).set_to_block(next_block_number);
        ordering_live_indexing_details_map.insert(
            config.id(),
            Arc::new(Mutex::new(OrderedLiveIndexingDetails {
                filter,
                last_seen_block_number,
                last_no_new_block_log_time: Instant::now(),
            })),
        );
    }
    // this is used for less busy chains to make sure they know rindexer is still alive
    let log_no_new_block_interval = Duration::from_secs(300);
    let target_iteration_duration = Duration::from_millis(200);
    let callback_permits = Arc::new(Semaphore::new(1));
    loop {
        if !is_running() {
            break;
        }
        let iteration_start = Instant::now();
        // a consistent latest block number across all events in the batch is required to avoid race conditions
        let latest_block = match cached_provider.get_latest_block().await {
            Ok(Some(block)) => block,
            Ok(None) => {
                error!("Empty latest block returned from provider, will try again in 200ms");
                tokio::time::sleep(Duration::from_millis(200)).await;
                continue;
            }
            Err(error) => {
                error!(
                    "Failed to get latest block, will try again in 1 second - error: {}",
                    error.to_string()
                );
                tokio::time::sleep(Duration::from_secs(1)).await;
                continue;
            }
        };
        let latest_block_number = U64::from(latest_block.header.number);
        for (config, _) in events.iter() {
            let mut ordering_live_indexing_details = ordering_live_indexing_details_map
                .get(&config.id())
                .expect("Failed to get ordering_live_indexing_details_map")
                .lock()
                .await
                .clone();
            if ordering_live_indexing_details.last_seen_block_number == latest_block_number {
                debug!(
                    "{} - {} - No new blocks to process...",
                    &config.info_log_name(),
                    IndexingEventProgressStatus::Live.log()
                );
                if ordering_live_indexing_details.last_no_new_block_log_time.elapsed()
                    >= log_no_new_block_interval
                {
                    info!(
                        "{} - {} - No new blocks published in the last 5 minutes - latest block number {}",
                        &config.info_log_name(),
                        IndexingEventProgressStatus::Live.log(),
                        latest_block_number
                    );
                    ordering_live_indexing_details.last_no_new_block_log_time = Instant::now();
                    *ordering_live_indexing_details_map
                        .get(&config.id())
                        .expect("Failed to get ordering_live_indexing_details_map")
                        .lock()
                        .await = ordering_live_indexing_details;
                }
                continue;
            }
            debug!(
                "{} - {} - New block seen {} - Last seen block {}",
                &config.info_log_name(),
                IndexingEventProgressStatus::Live.log(),
                latest_block_number,
                ordering_live_indexing_details.last_seen_block_number
            );
            let reorg_safe_distance = &config.indexing_distance_from_head();
            let safe_block_number = latest_block_number - reorg_safe_distance;
            let from_block = ordering_live_indexing_details.filter.from_block();
            // check reorg distance and skip if not safe
            if from_block > safe_block_number {
                if reorg_safe_distance.is_zero() {
                    let block_distance = from_block - latest_block_number;
                    let is_outside_reorg_range =
                        block_distance > reorg_safe_distance_for_chain(cached_provider.chain.id());
                    // it should never get under normal conditions outside the reorg range,
                    // therefore, we log an error as means RCP state is not in sync with the blockchain
                    if is_outside_reorg_range {
                        error!(
                            "{} - {} - RPC has gone back on latest block: rpc returned {}, last seen: {}",
                            &config.info_log_name(),
                            IndexingEventProgressStatus::Live.log(),
                            latest_block_number,
                            from_block
                        );
                    } else {
                        info!(
                            "{} - {} - RPC has gone back on latest block: rpc returned {}, last seen: {}",
                            &config.info_log_name(),
                            IndexingEventProgressStatus::Live.log(),
                            latest_block_number,
                            from_block
                        );
                    }
                    continue;
                } else {
                    info!(
                        "{} - {} - not in safe reorg block range yet block: {} > range: {}",
                        &config.info_log_name(),
                        IndexingEventProgressStatus::Live.log(),
                        from_block,
                        safe_block_number
                    );
                    continue;
                }
            }
            let to_block = safe_block_number;
            if from_block == to_block
                && !config.network_contract().disable_logs_bloom_checks
                && !is_relevant_block(
                    &ordering_live_indexing_details.filter.contract_addresses().await,
                    &config.topic_id(),
                    &latest_block,
                )
            {
                debug!(
                    "{} - {} - Skipping block {} as it's not relevant",
                    &config.info_log_name(),
                    IndexingEventProgressStatus::Live.log(),
                    from_block
                );
                debug!(
                    "{} - {} - Did not need to hit RPC as no events in {} block - LogsBloom for block checked",
                    &config.info_log_name(),
                    IndexingEventProgressStatus::Live.log(),
                    from_block
                );
                ordering_live_indexing_details.filter =
                    ordering_live_indexing_details.filter.set_from_block(to_block + U64::from(1));
                ordering_live_indexing_details.last_seen_block_number = to_block;
                *ordering_live_indexing_details_map
                    .get(&config.id())
                    .expect("Failed to get ordering_live_indexing_details_map")
                    .lock()
                    .await = ordering_live_indexing_details;
                continue;
            }
            ordering_live_indexing_details.filter =
                ordering_live_indexing_details.filter.set_to_block(to_block);
            debug!(
                "{} - {} - Processing live filter: {:?}",
                &config.info_log_name(),
                IndexingEventProgressStatus::Live.log(),
                ordering_live_indexing_details.filter
            );
            match cached_provider.get_logs(&ordering_live_indexing_details.filter).await {
                Ok(logs) => {
                    debug!(
                        "{} - {} - Live id {} topic_id {}, Logs: {} from {} to {}",
                        &config.info_log_name(),
                        IndexingEventProgressStatus::Live.log(),
                        &config.id(),
                        &config.topic_id(),
                        logs.len(),
                        from_block,
                        to_block
                    );
                    debug!(
                        "{} - {} - Fetched {} event logs - blocks: {} - {}",
                        &config.info_log_name(),
                        IndexingEventProgressStatus::Live.log(),
                        logs.len(),
                        from_block,
                        to_block
                    );
                    let logs_empty = logs.is_empty();
                    // clone here over the full logs way less overhead
                    let last_log = logs.last().cloned();
                    let fetched_logs = Ok(FetchLogsResult { logs, from_block, to_block });
                    let result = handle_logs_result(
                        Arc::clone(config),
                        callback_permits.clone(),
                        fetched_logs,
                    )
                    .await;
                    match result {
                        Ok(task) => {
                            let complete = task.await;
                            if let Err(e) = complete {
                                error!(
                                    "{} - {} - Error indexing task: {} - will try again in 200ms",
                                    &config.info_log_name(),
                                    IndexingEventProgressStatus::Live.log(),
                                    e
                                );
                                break;
                            }
                            ordering_live_indexing_details.last_seen_block_number = to_block;
                            if logs_empty {
                                ordering_live_indexing_details.filter =
                                    ordering_live_indexing_details
                                        .filter
                                        .set_from_block(to_block + U64::from(1));
                                debug!(
                                    "{} - {} - No events found between blocks {} - {}",
                                    &config.info_log_name(),
                                    IndexingEventProgressStatus::Live.log(),
                                    from_block,
                                    to_block
                                );
                            } else if let Some(last_log) = last_log {
                                if let Some(last_log_block_number) = last_log.block_number {
                                    ordering_live_indexing_details.filter =
                                        ordering_live_indexing_details
                                            .filter
                                            .set_from_block(U64::from(last_log_block_number + 1));
                                } else {
                                    error!("Failed to get last log block number the provider returned null (should never happen) - try again in 200ms");
                                }
                            }
                            *ordering_live_indexing_details_map
                                .get(&config.id())
                                .expect("Failed to get ordering_live_indexing_details_map")
                                .lock()
                                .await = ordering_live_indexing_details;
                        }
                        Err(err) => {
                            error!(
                                "{} - {} - Error fetching logs: {} - will try again in 200ms",
                                &config.info_log_name(),
                                IndexingEventProgressStatus::Live.log(),
                                err
                            );
                            break;
                        }
                    }
                }
                Err(err) => {
                    error!(
                        "{} - {} - Error fetching logs: {} - will try again in 200ms",
                        &config.info_log_name(),
                        IndexingEventProgressStatus::Live.log(),
                        err
                    );
                    break;
                }
            }
        }
        let elapsed = iteration_start.elapsed();
        if elapsed < target_iteration_duration {
            tokio::time::sleep(target_iteration_duration - elapsed).await;
        }
    }
}
async fn trigger_event(
    config: Arc<EventProcessingConfig>,
    fn_data: Vec<EventResult>,
    to_block: U64,
) {
    indexing_event_processing();
    let should_update_progress = if fn_data.is_empty() {
        #[allow(clippy::needless_bool)]
        if !is_running() {
            false
        } else {
            true
        }
    } else {
        config.trigger_event(fn_data.clone()).await.is_ok()
    };
    if should_update_progress {
        // TODO: There is a double-index race condition here. If we get a crash or failure between
        //       triggering the event and syncing the last updated block, we may double index.
        update_progress_and_last_synced_task(config, to_block, indexing_event_processed).await;
    } else {
        indexing_event_processed();
    }
}
async fn handle_logs_result(
    config: Arc<EventProcessingConfig>,
    callback_permits: Arc<Semaphore>,
    result: Result<FetchLogsResult, Box<dyn std::error::Error + Send>>,
) -> Result<JoinHandle<()>, Box<dyn std::error::Error + Send>> {
    match result {
        Ok(result) => {
            debug!("{} - Processing {} logs", config.info_log_name(), result.logs.len());
            let fn_data = result
                .logs
                .into_iter()
                .map(|log| {
                    EventResult::new(
                        Arc::clone(&config.network_contract()),
                        log,
                        result.from_block,
                        result.to_block,
                    )
                })
                .collect::<Vec<_>>();
            if let Ok(permit) = callback_permits.clone().acquire_owned().await {
                let task = tokio::spawn(async move {
                    trigger_event(config, fn_data, result.to_block).await;
                    drop(permit)
                });
                Ok(task)
            } else {
                trigger_event(config, fn_data, result.to_block).await;
                Ok(tokio::spawn(async {}))
            }
        }
        Err(e) => {
            error!(
                "[{}] - {} - {} - Error fetching logs: {}",
                config.network_contract().network,
                config.event_name(),
                IndexingEventProgressStatus::Live.log(),
                e
            );
            Err(e)
        }
    }
}
</file>

<file path="core/src/indexer/progress.rs">
use alloy::primitives::U64;
use std::collections::HashMap;
use std::{
    hash::{Hash, Hasher},
    sync::Arc,
};
use tokio::sync::Mutex;
use tracing::{debug, error, info};
use crate::event::callback_registry::{
    EventCallbackRegistryInformation, TraceCallbackRegistryInformation,
};
#[derive(Clone, Debug, Hash)]
pub enum IndexingEventProgressStatus {
    Syncing,
    Live,
    Completed,
    Failed,
}
impl IndexingEventProgressStatus {
    fn as_str(&self) -> &str {
        match self {
            Self::Syncing => "SYNCING",
            Self::Live => "LIVE",
            Self::Completed => "COMPLETED",
            Self::Failed => "FAILED",
        }
    }
    pub fn log(&self) -> &str {
        self.as_str()
    }
}
#[derive(Clone, Debug)]
pub struct IndexingEventProgress {
    pub id: String,
    pub contract_name: String,
    pub event_name: String,
    pub starting_block: U64,
    pub last_synced_block: U64,
    pub syncing_to_block: U64,
    pub network: String,
    pub live_indexing: bool,
    pub status: IndexingEventProgressStatus,
    pub progress: f64,
    pub info_log: String,
}
impl Hash for IndexingEventProgress {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.contract_name.hash(state);
        self.event_name.hash(state);
        self.last_synced_block.hash(state);
        self.syncing_to_block.hash(state);
        self.network.hash(state);
        self.live_indexing.hash(state);
        self.status.hash(state);
        let progress_int = (self.progress * 1_000.0) as u64;
        progress_int.hash(state);
    }
}
impl IndexingEventProgress {
    #[allow(clippy::too_many_arguments)]
    fn running(
        id: String,
        contract_name: String,
        event_name: String,
        starting_block: U64,
        last_synced_block: U64,
        syncing_to_block: U64,
        network: String,
        live_indexing: bool,
        info_log: String,
    ) -> Self {
        Self {
            id,
            contract_name,
            event_name,
            starting_block,
            last_synced_block,
            syncing_to_block,
            network,
            live_indexing,
            status: IndexingEventProgressStatus::Syncing,
            progress: 0.0,
            info_log,
        }
    }
}
pub struct IndexingEventsProgressState {
    pub events: Vec<IndexingEventProgress>,
}
#[derive(thiserror::Error, Debug)]
pub enum SyncError {
    #[error("Event with id {0} not found")]
    EventNotFound(String),
    #[error("Block number conversion error for total blocks: from {0} to {1}")]
    BlockNumberConversionTotalBlocksError(U64, U64),
    #[error("Block number conversion error for synced blocks: from {0} to {1}")]
    BlockNumberConversionSyncedBlocksError(U64, U64),
}
impl IndexingEventsProgressState {
    pub async fn monitor(
        event_information: &Vec<EventCallbackRegistryInformation>,
    ) -> Arc<Mutex<IndexingEventsProgressState>> {
        let mut events = Vec::new();
        let mut network_latest_cache: HashMap<String, U64> = HashMap::new();
        for event_info in event_information {
            for network_contract in &event_info.contract.details {
                let network = network_contract.network.clone();
                let latest_block_cached = network_latest_cache.get(&network);
                let latest_block = match latest_block_cached {
                    Some(b) => {
                        debug!("Got block for {} from cache", &network);
                        Ok(*b)
                    }
                    None => {
                        let block = network_contract.cached_provider.get_block_number().await;
                        if let Ok(b) = block {
                            network_latest_cache.insert(network, b);
                        }
                        block
                    }
                };
                match latest_block {
                    Ok(latest_block) => {
                        let start_block = network_contract.start_block.unwrap_or(latest_block);
                        let end_block = network_contract.end_block.unwrap_or(latest_block);
                        events.push(IndexingEventProgress::running(
                            network_contract.id.to_string(),
                            event_info.contract.name.clone(),
                            event_info.event_name.to_string(),
                            start_block,
                            start_block,
                            if latest_block > end_block { end_block } else { latest_block },
                            network_contract.network.clone(),
                            network_contract.end_block.is_none(),
                            event_info.info_log_name(),
                        ));
                    }
                    Err(e) => {
                        error!(
                            "Failed to get latest block for network {}: {}",
                            network_contract.network, e
                        );
                    }
                }
            }
        }
        Arc::new(Mutex::new(Self { events }))
    }
    pub async fn monitor_traces(
        event_information: &Vec<TraceCallbackRegistryInformation>,
    ) -> Arc<Mutex<IndexingEventsProgressState>> {
        let mut events = Vec::new();
        for event_info in event_information {
            for network_traces in &event_info.trace_information.details {
                let latest_block = network_traces.cached_provider.get_block_number().await;
                match latest_block {
                    Ok(latest_block) => {
                        let start_block = network_traces.start_block.unwrap_or(latest_block);
                        let end_block = network_traces.end_block.unwrap_or(latest_block);
                        events.push(IndexingEventProgress::running(
                            event_info.id.to_string(),
                            event_info.contract_name.clone(),
                            event_info.event_name.to_string(),
                            start_block,
                            start_block,
                            if latest_block > end_block { end_block } else { latest_block },
                            network_traces.network.clone(),
                            network_traces.end_block.is_none(),
                            event_info.info_log_name(),
                        ));
                    }
                    Err(e) => {
                        error!(
                            "Failed to get latest block for tracing network {}: {}",
                            network_traces.network, e
                        );
                    }
                }
            }
        }
        Arc::new(Mutex::new(Self { events }))
    }
    pub fn update_last_synced_block(
        &mut self,
        id: &str,
        new_last_synced_block: U64,
    ) -> Result<(), SyncError> {
        for event in &mut self.events {
            if event.id == id {
                if event.progress < 1.0 {
                    if event.syncing_to_block > event.last_synced_block {
                        let total_blocks: u64 = event
                            .syncing_to_block
                            .checked_sub(event.starting_block)
                            .ok_or(SyncError::BlockNumberConversionTotalBlocksError(
                                event.syncing_to_block,
                                event.starting_block,
                            ))?
                            .try_into()
                            .map_err(|_| {
                                SyncError::BlockNumberConversionTotalBlocksError(
                                    event.syncing_to_block,
                                    event.starting_block,
                                )
                            })?;
                        let blocks_synced: u64 = new_last_synced_block
                            .checked_sub(event.starting_block)
                            .ok_or(SyncError::BlockNumberConversionSyncedBlocksError(
                                new_last_synced_block,
                                event.starting_block,
                            ))?
                            .try_into()
                            .map_err(|_| {
                                SyncError::BlockNumberConversionSyncedBlocksError(
                                    new_last_synced_block,
                                    event.starting_block,
                                )
                            })?;
                        // Calculate progress based on the proportion of total blocks synced so far
                        event.progress = (blocks_synced as f64) / (total_blocks as f64);
                        event.progress = event.progress.clamp(0.0, 1.0);
                    }
                    if new_last_synced_block >= event.syncing_to_block {
                        event.progress = 1.0;
                        info!(
                            "{}::{} - {:.2}% progress",
                            event.info_log,
                            event.network,
                            event.progress * 100.0
                        );
                        event.status = if event.live_indexing {
                            IndexingEventProgressStatus::Live
                        } else {
                            IndexingEventProgressStatus::Completed
                        };
                    }
                    if event.progress != 1.0 {
                        info!(
                            "{}::{} - {:.2}% progress",
                            event.info_log,
                            event.network,
                            event.progress * 100.0
                        );
                    }
                }
                event.last_synced_block = new_last_synced_block;
                return Ok(());
            }
        }
        Err(SyncError::EventNotFound(id.to_string()))
    }
}
</file>

<file path="core/src/indexer/reorg.rs">
use alloy::primitives::U64;
use tracing::{debug, warn};
use crate::notifications::ChainStateNotification;
/// Handles chain state notifications (reorgs, reverts, commits)
pub fn handle_chain_notification(notification: ChainStateNotification, info_log_name: &str) {
    match notification {
        ChainStateNotification::Reorged {
            revert_from_block,
            revert_to_block,
            new_from_block,
            new_to_block,
            new_tip_hash,
        } => {
            warn!(
                "{} - REORG DETECTED! Need to revert blocks {} to {} and re-index {} to {} (new tip: {})",
                info_log_name,
                revert_from_block, revert_to_block,
                new_from_block, new_to_block,
                new_tip_hash
            );
            // TODO: In future PR, actually handle the reorg by reverting and re-indexing
        }
        ChainStateNotification::Reverted { from_block, to_block } => {
            warn!(
                "{} - CHAIN REVERTED! Blocks {} to {} have been reverted",
                info_log_name, from_block, to_block
            );
            // TODO: In future PR, mark affected logs as removed in the database
        }
        ChainStateNotification::Committed { from_block, to_block, tip_hash } => {
            debug!(
                "{} - Chain committed: blocks {} to {} (tip: {})",
                info_log_name, from_block, to_block, tip_hash
            );
        }
    }
}
pub fn reorg_safe_distance_for_chain(chain_id: u64) -> U64 {
    if chain_id == 1 {
        U64::from(12)
    } else {
        U64::from(64)
    }
}
#[cfg(test)]
mod tests {
    use super::*;
    #[test]
    fn test_reorg_safe_distance_for_chain() {
        let mainnet_chain_id = 1;
        assert_eq!(reorg_safe_distance_for_chain(mainnet_chain_id), U64::from(12));
        let testnet_chain_id = 3;
        assert_eq!(reorg_safe_distance_for_chain(testnet_chain_id), U64::from(64));
        let other_chain_id = 42;
        assert_eq!(reorg_safe_distance_for_chain(other_chain_id), U64::from(64));
    }
}
</file>

<file path="core/src/indexer/start.rs">
use std::{path::Path, sync::Arc};
use alloy::primitives::U64;
use futures::future::try_join_all;
use futures::stream::FuturesUnordered;
use futures::StreamExt;
use tokio::{
    join,
    task::{JoinError, JoinHandle},
    time::Instant,
};
use tracing::{error, info};
use crate::database::clickhouse::client::{ClickhouseClient, ClickhouseConnectionError};
use crate::event::config::{ContractEventProcessingConfig, FactoryEventProcessingConfig};
use crate::events::RindexerEventEmitter;
use crate::helpers::format_duration;
use crate::indexer::native_transfer::native_transfer_block_processor;
use crate::indexer::Indexer;
use crate::{
    database::postgres::client::PostgresConnectionError,
    event::{
        callback_registry::{EventCallbackRegistry, TraceCallbackRegistry},
        config::{EventProcessingConfig, TraceProcessingConfig},
    },
    indexer::{
        dependency::ContractEventsDependenciesConfig,
        last_synced::{get_last_synced_block_number, SyncConfig},
        native_transfer::{native_transfer_block_fetch, NATIVE_TRANSFER_CONTRACT_NAME},
        process::{
            process_contracts_events_with_dependencies, process_non_blocking_event,
            ProcessContractsEventsWithDependenciesError, ProcessEventError,
        },
        progress::IndexingEventsProgressState,
        reorg::reorg_safe_distance_for_chain,
        ContractEventDependencies,
    },
    manifest::core::Manifest,
    provider::{JsonRpcCachedProvider, ProviderError},
    PostgresClient, RindexerEvent,
};
#[derive(thiserror::Error, Debug)]
pub enum CombinedLogEventProcessingError {
    #[error("{0}")]
    DependencyError(#[from] ProcessContractsEventsWithDependenciesError),
    #[error("{0}")]
    NonBlockingError(#[from] ProcessEventError),
    #[error("{0}")]
    JoinError(#[from] JoinError),
}
#[derive(thiserror::Error, Debug)]
pub enum StartIndexingError {
    #[error("Could not run all index handlers join error: {0}")]
    CouldNotRunAllIndexHandlersJoin(#[from] JoinError),
    #[error("Could not run all index handlers {0}")]
    CouldNotRunAllIndexHandlers(#[from] ProcessEventError),
    #[error("{0}")]
    PostgresConnectionError(#[from] PostgresConnectionError),
    #[error("{0}")]
    ClickhouseConnectionError(#[from] ClickhouseConnectionError),
    #[error("Could not get block number from provider: {0}")]
    GetBlockNumberError(#[from] ProviderError),
    #[error("Could not get chain id from provider: {0}")]
    GetChainIdError(ProviderError),
    #[error("Could not process event sequentially: {0}")]
    ProcessEventSequentiallyError(ProcessEventError),
    #[error("{0}")]
    CombinedError(#[from] CombinedLogEventProcessingError),
    #[error("The start block set for {0} is higher than the latest block: {1} - start block: {2}")]
    StartBlockIsHigherThanLatestBlockError(String, U64, U64),
    #[error("The end block set for {0} is higher than the latest block: {1} - end block: {2}")]
    EndBlockIsHigherThanLatestBlockError(String, U64, U64),
    #[error("Encountered unknown error: {0}")]
    UnknownError(String),
}
#[derive(Clone)]
pub struct ProcessedNetworkContract {
    pub id: String,
    pub processed_up_to: U64,
}
async fn get_start_end_block(
    provider: Arc<JsonRpcCachedProvider>,
    manifest_start_block: Option<U64>,
    manifest_end_block: Option<U64>,
    config: SyncConfig<'_>,
    event_name: &str,
    network: &str,
    reorg_safe_distance: bool,
) -> Result<(U64, U64, U64), StartIndexingError> {
    let latest_block = provider.get_block_number().await?;
    if let Some(start_block) = manifest_start_block {
        if start_block > latest_block {
            error!(
                "{} - start_block supplied in yaml - {} {} is higher then latest block number - {}",
                event_name, network, start_block, latest_block
            );
            return Err(StartIndexingError::StartBlockIsHigherThanLatestBlockError(
                event_name.to_string(),
                start_block,
                latest_block,
            ));
        }
    }
    if let Some(end_block) = manifest_end_block {
        if end_block > latest_block {
            error!(
                "{} - end_block supplied in yaml - {} {} is higher then latest block number - {}",
                event_name, network, end_block, latest_block
            );
            return Err(StartIndexingError::EndBlockIsHigherThanLatestBlockError(
                event_name.to_string(),
                end_block,
                latest_block,
            ));
        }
    }
    let last_known_start_block = if manifest_start_block.is_some() {
        let last_synced_block = get_last_synced_block_number(config).await;
        if let Some(value) = last_synced_block {
            let start_from = value + U64::from(1);
            info!(
                "{} Found last synced block number - {:?} rindexer will start up from {:?}",
                event_name, value, start_from
            );
            Some(start_from)
        } else {
            None
        }
    } else {
        None
    };
    let start_block =
        last_known_start_block.unwrap_or(manifest_start_block.unwrap_or(latest_block));
    let end_block = std::cmp::min(manifest_end_block.unwrap_or(latest_block), latest_block);
    info!("{}::{} Starting block number - {}", event_name, network, start_block);
    if let Some(end_block) = manifest_end_block {
        if end_block > latest_block {
            error!("{} - end_block supplied in yaml - {} is higher then latest - {} - end_block now will be {}", event_name, end_block, latest_block, latest_block);
        }
    }
    let (end_block, indexing_distance_from_head) =
        calculate_safe_block_number(reorg_safe_distance, &provider, latest_block, end_block);
    Ok((start_block, end_block, indexing_distance_from_head))
}
async fn start_indexing_traces(
    manifest: &Manifest,
    project_path: &Path,
    postgres: Option<Arc<PostgresClient>>,
    clickhouse: Option<Arc<ClickhouseClient>>,
    indexer: &Indexer,
    trace_registry: Arc<TraceCallbackRegistry>,
) -> Result<Vec<JoinHandle<Result<(), ProcessEventError>>>, StartIndexingError> {
    if !manifest.native_transfers.enabled {
        info!("Native transfer indexing disabled!");
        return Ok(vec![]);
    }
    let mut non_blocking_process_events = Vec::new();
    let trace_progress_state =
        IndexingEventsProgressState::monitor_traces(&trace_registry.events).await;
    // Group events by network to create one pipeline per network
    let mut network_events: std::collections::HashMap<
        String,
        Vec<&crate::event::callback_registry::TraceCallbackRegistryInformation>,
    > = std::collections::HashMap::new();
    for event in trace_registry.events.iter() {
        for network in event.trace_information.details.iter() {
            network_events.entry(network.network.clone()).or_default().push(event);
        }
    }
    // Create one pipeline per network
    for (network_name, events) in network_events {
        // Get the first event's network details (they should all be the same for a given network)
        let first_event = events.first().unwrap();
        let network_details = first_event
            .trace_information
            .details
            .iter()
            .find(|n| n.network == network_name)
            .unwrap();
        let stream_details = indexer
            .contracts
            .iter()
            .find(|c| c.name == first_event.contract_name)
            .and_then(|c| c.streams.as_ref());
        let sync_config = SyncConfig {
            project_path,
            postgres: &postgres,
            clickhouse: &clickhouse,
            csv_details: &manifest.storage.csv,
            contract_csv_enabled: manifest.contract_csv_enabled(&first_event.contract_name),
            stream_details: &stream_details,
            indexer_name: &first_event.indexer_name,
            contract_name: &first_event.contract_name,
            event_name: &first_event.event_name,
            network: &network_name,
        };
        let (block_tx, block_rx) = tokio::sync::mpsc::channel(4096);
        let (start_block, end_block, indexing_distance_from_head) = get_start_end_block(
            network_details.cached_provider.clone(),
            network_details.start_block,
            network_details.end_block,
            sync_config,
            &format!("TraceEvents[{}]", network_name),
            &network_name,
            first_event.trace_information.reorg_safe_distance,
        )
        .await?;
        // Create a shared registry for this network's events
        let network_registry = Arc::new(TraceCallbackRegistry {
            events: events.iter().map(|e| (*e).clone()).collect(),
        });
        let config = Arc::new(TraceProcessingConfig {
            id: first_event.id.clone(), // Use the first event's ID for progress tracking
            project_path: project_path.to_path_buf(),
            start_block,
            end_block,
            indexer_name: first_event.indexer_name.clone(),
            contract_name: NATIVE_TRANSFER_CONTRACT_NAME.to_string(),
            event_name: "TraceEvents".to_string(),
            network: network_name.clone(),
            progress: trace_progress_state.clone(),
            postgres: postgres.clone(),
            csv_details: None,
            registry: network_registry,
            method: network_details.method,
            stream_last_synced_block_file_path: None,
        });
        let block_fetch_handle = tokio::spawn(native_transfer_block_fetch(
            network_details.cached_provider.clone(),
            block_tx,
            start_block,
            network_details.end_block,
            indexing_distance_from_head,
            network_name.clone(),
        ));
        non_blocking_process_events.push(block_fetch_handle);
        let provider = network_details.cached_provider.clone();
        let config = config.clone();
        let block_processor_handle =
            tokio::spawn(native_transfer_block_processor(network_name, provider, config, block_rx));
        non_blocking_process_events.push(block_processor_handle);
    }
    Ok(non_blocking_process_events)
}
#[allow(clippy::too_many_arguments)]
async fn start_indexing_contract_events(
    manifest: &Manifest,
    project_path: &Path,
    postgres: Option<Arc<PostgresClient>>,
    clickhouse: Option<Arc<ClickhouseClient>>,
    indexer: &Indexer,
    registry: Arc<EventCallbackRegistry>,
    dependencies: &[ContractEventDependencies],
    no_live_indexing_forced: bool,
) -> Result<
    (
        Vec<JoinHandle<Result<(), ProcessEventError>>>,
        Vec<ProcessedNetworkContract>,
        Vec<(String, Arc<EventProcessingConfig>)>,
        Vec<ContractEventsDependenciesConfig>,
    ),
    StartIndexingError,
> {
    let event_progress_state = IndexingEventsProgressState::monitor(&registry.events).await;
    let mut apply_cross_contract_dependency_events_config_after_processing = Vec::new();
    let mut non_blocking_process_events = Vec::new();
    let mut processed_network_contracts: Vec<ProcessedNetworkContract> = Vec::new();
    let mut dependency_event_processing_configs: Vec<ContractEventsDependenciesConfig> = Vec::new();
    let mut block_tasks = FuturesUnordered::new();
    if let Some(true) = manifest.timestamps {
        info!("Block timestamps enabled globally!");
    }
    for event in registry.events.iter() {
        let stream_details = indexer
            .contracts
            .iter()
            .find(|c| c.name == event.contract.name)
            .and_then(|c| c.streams.as_ref());
        for network_contract in event.contract.details.iter() {
            let event = event.clone();
            let network_contract = network_contract.clone();
            let project_path = project_path.to_path_buf();
            let postgres = postgres.clone();
            let clickhouse = clickhouse.clone();
            let manifest_csv_details = manifest.storage.csv.clone();
            let registry = Arc::clone(&registry);
            let event_progress_state = Arc::clone(&event_progress_state);
            let dependencies = dependencies.to_vec();
            block_tasks.push(async move {
                let config = SyncConfig {
                    project_path: &project_path,
                    postgres: &postgres,
                    clickhouse: &clickhouse,
                    csv_details: &manifest_csv_details,
                    contract_csv_enabled: manifest.contract_csv_enabled(&event.contract.name),
                    stream_details: &stream_details,
                    indexer_name: &event.indexer_name,
                    contract_name: &event.contract.name,
                    event_name: &event.event_name,
                    network: &network_contract.network,
                };
                let result = get_start_end_block(
                    network_contract.cached_provider.clone(),
                    network_contract.start_block,
                    network_contract.end_block,
                    config,
                    &event.info_log_name(),
                    &network_contract.network,
                    event.contract.reorg_safe_distance,
                )
                .await;
                result.map(|blocks| {
                    (
                        event,
                        network_contract,
                        stream_details,
                        blocks,
                        project_path,
                        postgres,
                        clickhouse,
                        manifest_csv_details,
                        registry,
                        event_progress_state,
                        no_live_indexing_forced,
                        dependencies,
                    )
                })
            });
        }
    }
    while let Some(res) = block_tasks.next().await {
        let (
            event,
            network_contract,
            stream_details,
            (start_block, end_block, indexing_distance_from_head),
            project_path,
            postgres,
            clickhouse,
            manifest_csv_details,
            registry,
            event_progress_state,
            no_live_indexing_forced,
            dependencies,
        ) = res?;
        processed_network_contracts.push(ProcessedNetworkContract {
            id: network_contract.id.clone(),
            processed_up_to: end_block,
        });
        // TODO: doesnt work with factory atm so leave overrides to fix later as breaks the world
        // let contract = manifest
        //     .contracts
        //     .iter()
        //     .find(|c| {
        //         format!("{}Filter", c.name) == event.contract.name || c.name == event.contract.name
        //     })
        //     .unwrap();
        // let timestamp_enabled_for_event = contract
        //     .include_events
        //     .iter()
        //     .flatten()
        //     .find(|a| a.name == event.event_name)
        //     .unwrap()
        //     .timestamps;
        // match timestamp_enabled_for_event {
        //     Some(true) => info!("Timestamps enabled for event: {}", event.event_name),
        //     Some(false) => info!("Timestamps disabled for event: {}", event.event_name),
        //     None => {}
        // };
        let event_processing_config: EventProcessingConfig = match event.is_factory_filter_event() {
            true => {
                let factory_details = network_contract
                    .indexing_contract_setup
                    .factory_details()
                    .expect("Factory event contract must have a factory details");
                FactoryEventProcessingConfig {
                    id: event.id.clone(),
                    address: factory_details.address.clone(),
                    input_name: factory_details.input_name.clone(),
                    contract_name: factory_details.contract_name.clone(),
                    project_path: project_path.clone(),
                    indexer_name: event.indexer_name.clone(),
                    event: factory_details.event.clone(),
                    network_contract: Arc::new(network_contract.clone()),
                    start_block,
                    end_block,
                    registry: Arc::clone(&registry),
                    progress: Arc::clone(&event_progress_state),
                    clickhouse: clickhouse.clone(),
                    postgres: postgres.clone(),
                    config: manifest.config.clone(),
                    csv_details: manifest_csv_details.clone(),
                    // timestamps: timestamp_enabled_for_event
                    //     .unwrap_or(manifest.timestamps.unwrap_or(false)),
                    timestamps: manifest.timestamps.unwrap_or(false),
                    stream_last_synced_block_file_path: stream_details
                        .as_ref()
                        .map(|s| s.get_streams_last_synced_block_path()),
                    live_indexing: if no_live_indexing_forced {
                        false
                    } else {
                        network_contract.is_live_indexing()
                    },
                    index_event_in_order: event.index_event_in_order,
                    indexing_distance_from_head,
                }
                .into()
            }
            false => ContractEventProcessingConfig {
                id: event.id.clone(),
                project_path: project_path.clone(),
                indexer_name: event.indexer_name.clone(),
                contract_name: event.contract.name.clone(),
                topic_id: event.topic_id,
                event_name: event.event_name.clone(),
                network_contract: Arc::new(network_contract.clone()),
                start_block,
                end_block,
                registry: Arc::clone(&registry),
                progress: Arc::clone(&event_progress_state),
                postgres: postgres.clone(),
                clickhouse: clickhouse.clone(),
                csv_details: manifest_csv_details.clone(),
                config: manifest.config.clone(),
                // timestamps: timestamp_enabled_for_event
                //     .unwrap_or(manifest.timestamps.unwrap_or(false)),
                timestamps: manifest.timestamps.unwrap_or(false),
                stream_last_synced_block_file_path: stream_details
                    .as_ref()
                    .map(|s| s.get_streams_last_synced_block_path()),
                live_indexing: if no_live_indexing_forced {
                    false
                } else {
                    network_contract.is_live_indexing()
                },
                index_event_in_order: event.index_event_in_order,
                indexing_distance_from_head,
            }
            .into(),
        };
        let dependencies_status = ContractEventDependencies::dependencies_status(
            &event_processing_config.contract_name(),
            &event_processing_config.event_name(),
            &dependencies,
        );
        if dependencies_status.has_dependency_in_other_contracts_multiple_times() {
            panic!("Multiple dependencies of the same event on different contracts not supported yet - please raise an issue if you need this feature");
        }
        if dependencies_status.has_dependencies() {
            if let Some(dependency_in_other_contract) =
                dependencies_status.get_first_dependencies_in_other_contracts()
            {
                apply_cross_contract_dependency_events_config_after_processing
                    .push((dependency_in_other_contract, Arc::new(event_processing_config)));
                continue;
            }
            ContractEventsDependenciesConfig::add_to_event_or_new_entry(
                &mut dependency_event_processing_configs,
                Arc::new(event_processing_config),
                &dependencies,
            );
        } else {
            let process_event = tokio::spawn(process_non_blocking_event(event_processing_config));
            non_blocking_process_events.push(process_event);
        }
    }
    Ok((
        non_blocking_process_events,
        processed_network_contracts,
        apply_cross_contract_dependency_events_config_after_processing,
        dependency_event_processing_configs,
    ))
}
pub async fn start_historical_indexing(
    manifest: &Manifest,
    project_path: &Path,
    dependencies: &[ContractEventDependencies],
    registry: Arc<EventCallbackRegistry>,
    trace_registry: Arc<TraceCallbackRegistry>,
    event_emitter: Option<RindexerEventEmitter>,
) -> Result<Vec<ProcessedNetworkContract>, StartIndexingError> {
    info!("Historical indexing started");
    let start = Instant::now();
    let result =
        start_indexing(manifest, project_path, dependencies, true, registry, trace_registry)
            .await?;
    let duration = start.elapsed();
    info!("Historical indexing completed - time taken: {}", format_duration(duration));
    if let Some(ref emitter) = event_emitter {
        emitter.emit(RindexerEvent::HistoricalIndexingCompleted);
    }
    Ok(result)
}
pub async fn start_live_indexing(
    manifest: &Manifest,
    project_path: &Path,
    dependencies: &[ContractEventDependencies],
    registry: Arc<EventCallbackRegistry>,
    trace_registry: Arc<TraceCallbackRegistry>,
) -> Result<Vec<ProcessedNetworkContract>, StartIndexingError> {
    info!("Live indexing started");
    start_indexing(manifest, project_path, dependencies, false, registry, trace_registry).await
}
async fn start_indexing(
    manifest: &Manifest,
    project_path: &Path,
    dependencies: &[ContractEventDependencies],
    no_live_indexing_forced: bool,
    registry: Arc<EventCallbackRegistry>,
    trace_registry: Arc<TraceCallbackRegistry>,
) -> Result<Vec<ProcessedNetworkContract>, StartIndexingError> {
    let database = initialize_database(manifest).await?;
    let clickhouse = initialize_clickhouse(manifest).await?;
    // any events which are non-blocking and can be fired in parallel
    let mut non_blocking_process_events = Vec::new();
    let indexer = manifest.to_indexer();
    // Start the sub-indexers concurrently to ensure fast startup times
    let (trace_indexer_handles, contract_events_indexer) = join!(
        start_indexing_traces(
            manifest,
            project_path,
            database.clone(),
            clickhouse.clone(),
            &indexer,
            trace_registry.clone()
        ),
        start_indexing_contract_events(
            manifest,
            project_path,
            database.clone(),
            clickhouse.clone(),
            &indexer,
            registry.clone(),
            dependencies,
            no_live_indexing_forced,
        )
    );
    let (
        non_blocking_contract_handles,
        processed_network_contracts,
        apply_cross_contract_dependency_events_config_after_processing,
        mut dependency_event_processing_configs,
    ) = contract_events_indexer?;
    non_blocking_process_events.extend(trace_indexer_handles?);
    non_blocking_process_events.extend(non_blocking_contract_handles);
    // apply dependency events config after processing to avoid ordering issues
    for apply in apply_cross_contract_dependency_events_config_after_processing {
        let (dependency_in_other_contract, event_processing_config) = apply;
        ContractEventsDependenciesConfig::add_to_event_or_panic(
            &dependency_in_other_contract,
            &mut dependency_event_processing_configs,
            event_processing_config,
        );
    }
    let dependency_handle: JoinHandle<Result<(), ProcessContractsEventsWithDependenciesError>> =
        tokio::spawn(process_contracts_events_with_dependencies(
            dependency_event_processing_configs,
        ));
    let mut handles: Vec<JoinHandle<Result<(), CombinedLogEventProcessingError>>> = Vec::new();
    handles.push(tokio::spawn(async {
        dependency_handle
            .await
            .map_err(CombinedLogEventProcessingError::from)
            .and_then(|res| res.map_err(CombinedLogEventProcessingError::from))
    }));
    for handle in non_blocking_process_events {
        handles.push(tokio::spawn(async {
            handle
                .await
                .map_err(CombinedLogEventProcessingError::from)
                .and_then(|res| res.map_err(CombinedLogEventProcessingError::from))
        }));
    }
    let results = try_join_all(handles).await?;
    for result in results {
        match result {
            Ok(()) => {}
            Err(e) => return Err(StartIndexingError::CombinedError(e)),
        }
    }
    Ok(processed_network_contracts)
}
pub async fn initialize_database(
    manifest: &Manifest,
) -> Result<Option<Arc<PostgresClient>>, StartIndexingError> {
    if manifest.storage.postgres_enabled() {
        match PostgresClient::new().await {
            Ok(postgres) => Ok(Some(Arc::new(postgres))),
            Err(e) => {
                error!("Error connecting to Postgres: {:?}", e);
                Err(StartIndexingError::PostgresConnectionError(e))
            }
        }
    } else {
        Ok(None)
    }
}
pub async fn initialize_clickhouse(
    manifest: &Manifest,
) -> Result<Option<Arc<ClickhouseClient>>, StartIndexingError> {
    if manifest.storage.clickhouse_enabled() {
        match ClickhouseClient::new().await {
            Ok(clickhouse) => Ok(Some(Arc::new(clickhouse))),
            Err(e) => {
                error!("Error connecting to Clickhouse: {:?}", e);
                Err(StartIndexingError::ClickhouseConnectionError(e))
            }
        }
    } else {
        Ok(None)
    }
}
pub fn calculate_safe_block_number(
    reorg_safe_distance: bool,
    provider: &Arc<JsonRpcCachedProvider>,
    latest_block: U64,
    mut end_block: U64,
) -> (U64, U64) {
    let mut indexing_distance_from_head = U64::ZERO;
    if reorg_safe_distance {
        let chain_id = provider.chain.id();
        let reorg_safe_distance = reorg_safe_distance_for_chain(chain_id);
        let safe_block_number = latest_block - reorg_safe_distance;
        if end_block > safe_block_number {
            end_block = safe_block_number;
        }
        indexing_distance_from_head = reorg_safe_distance;
    }
    (end_block, indexing_distance_from_head)
}
</file>

<file path="core/src/indexer/task_tracker.rs">
use std::sync::atomic::{AtomicUsize, Ordering};
use once_cell::sync::Lazy;
static INDEXING_TASKS: Lazy<AtomicUsize> = Lazy::new(|| AtomicUsize::new(0));
pub fn indexing_event_processing() {
    INDEXING_TASKS.fetch_add(1, Ordering::SeqCst);
}
pub fn indexing_event_processed() {
    INDEXING_TASKS.fetch_sub(1, Ordering::SeqCst);
}
pub fn active_indexing_count() -> usize {
    INDEXING_TASKS.load(Ordering::SeqCst)
}
</file>

<file path="core/src/manifest/chat.rs">
use serde::{Deserialize, Serialize};
use serde_json::{Map, Value};
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct TelegramConfig {
    pub bot_token: String,
    pub chat_id: i64,
    pub networks: Vec<String>,
    pub messages: Vec<TelegramEvent>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct TelegramEvent {
    pub event_name: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Map<String, Value>>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub filter_expression: Option<String>,
    pub template_inline: String,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct DiscordConfig {
    pub bot_token: String,
    pub channel_id: u64,
    pub networks: Vec<String>,
    pub messages: Vec<DiscordEvent>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct DiscordEvent {
    pub event_name: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Map<String, Value>>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub filter_expression: Option<String>,
    pub template_inline: String,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct SlackConfig {
    pub bot_token: String,
    pub channel: String,
    pub networks: Vec<String>,
    pub messages: Vec<SlackEvent>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct SlackEvent {
    pub event_name: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Map<String, Value>>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub filter_expression: Option<String>,
    pub template_inline: String,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ChatConfig {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub telegram: Option<Vec<TelegramConfig>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub discord: Option<Vec<DiscordConfig>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub slack: Option<Vec<SlackConfig>>,
}
</file>

<file path="core/src/manifest/config.rs">
use serde::{Deserialize, Serialize};
/// Advanced config options for tuning rindexer beyond its default settings.
#[derive(Debug, Default, Serialize, Deserialize, Clone)]
pub struct Config {
    /// Sets the buffer of events we hold in memory per "network-event". Useful for balancing
    /// memory with event throughput on large scale backfill operations.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub buffer: Option<usize>,
    /// Sets the per "network-event" handler callback rate, which will increase/decrease throughput
    /// depending on the specific logic found in the handler.
    ///
    /// If `index_event_in_order` is used, this option will always be set as `1` (sequential).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub callback_concurrency: Option<usize>,
    /// Optionally configure a worst case sample rate.
    ///
    /// In cases where a batch of logs includes thousands of blocks we will not call every block for
    /// a timestamp, but will instead sample blocks and interpolate the remaining timestamps.
    ///
    /// In other cases where we are requesting a small handful of blocks in a single batch rpc this
    /// sample rate will not be applied. The sample rate should be considered a "worst case"
    /// acceptable rate.
    ///
    /// For many applications this will be `1.0` or no error tolerance. Where only loose-time ordering
    /// is required this can provide considerable speedup and RPC CU reduction at minimal accuracy loss.
    ///
    /// The default is `1.0`, which represents no sampling. A high sample rate would be `0.1` and a
    /// reasonable one would be `0.01` or below. Modern chains are suprisingly consistent in their
    /// block times so often no accuracy loss occurs.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timestamp_sample_rate: Option<f32>,
}
#[cfg(test)]
mod tests {
    use serde_yaml;
    use super::*;
    #[test]
    fn test_config_simple() {
        let yaml = r#"
          buffer: 4
          callback_concurrency: 2
        "#;
        let transfer: Config = serde_yaml::from_str(yaml).unwrap();
        assert_eq!(transfer.buffer, Some(4));
        assert_eq!(transfer.callback_concurrency, Some(2));
    }
    #[test]
    fn test_config_optional() {
        let yaml = r#"
          buffer: 4
        "#;
        let transfer: Config = serde_yaml::from_str(yaml).unwrap();
        assert_eq!(transfer.buffer, Some(4));
        assert_eq!(transfer.callback_concurrency, None);
    }
}
</file>

<file path="core/src/manifest/contract.rs">
use std::{borrow::Cow, collections::HashSet, fs, path::Path};
use alloy::rpc::types::Topic;
use alloy::{
    primitives::{Address, U64},
    rpc::types::ValueOrArray,
};
use serde::{Deserialize, Serialize};
use serde_json::{json, Value};
use super::core::{deserialize_option_u64_from_string, serialize_option_u64_as_string};
use crate::event::contract_setup::FactoryDetails;
use crate::helpers::parse_topic;
use crate::{
    event::contract_setup::{
        AddressDetails, ContractEventMapping, FilterDetails, IndexingContractSetup,
    },
    helpers::get_full_path,
    manifest::{chat::ChatConfig, stream::StreamsConfig},
    types::single_or_array::StringOrArray,
};
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct EventInputIndexedFilters {
    pub event_name: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub indexed_1: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub indexed_2: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub indexed_3: Option<Vec<String>>,
}
impl From<EventInputIndexedFilters> for [Topic; 4] {
    fn from(input: EventInputIndexedFilters) -> Self {
        let mut topics: [Topic; 4] = Default::default();
        if let Some(indexed_1) = &input.indexed_1 {
            topics[1] = indexed_1.iter().map(|i| parse_topic(i)).collect::<Vec<_>>().into();
        }
        if let Some(indexed_2) = &input.indexed_2 {
            topics[2] = indexed_2.iter().map(|i| parse_topic(i)).collect::<Vec<_>>().into();
        }
        if let Some(indexed_3) = &input.indexed_3 {
            topics[3] = indexed_3.iter().map(|i| parse_topic(i)).collect::<Vec<_>>().into();
        }
        topics
    }
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct FilterDetailsYaml {
    pub event_name: String,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct FactoryDetailsYaml {
    pub name: String,
    pub address: ValueOrArray<Address>,
    pub event_name: String,
    pub input_name: ValueOrArray<String>,
    pub abi: String,
}
impl FactoryDetailsYaml {
    pub fn input_names(&self) -> Vec<String> {
        match &self.input_name {
            ValueOrArray::Value(name) => vec![name.clone()],
            ValueOrArray::Array(names) => names.clone(),
        }
    }
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ContractDetails {
    pub network: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub address: Option<ValueOrArray<Address>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub filter: Option<ValueOrArray<FilterDetailsYaml>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub indexed_filters: Option<Vec<EventInputIndexedFilters>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub factory: Option<FactoryDetailsYaml>,
    #[serde(
        default,
        skip_serializing_if = "Option::is_none",
        deserialize_with = "deserialize_option_u64_from_string",
        serialize_with = "serialize_option_u64_as_string"
    )]
    pub start_block: Option<U64>,
    #[serde(
        default,
        skip_serializing_if = "Option::is_none",
        deserialize_with = "deserialize_option_u64_from_string",
        serialize_with = "serialize_option_u64_as_string"
    )]
    pub end_block: Option<U64>,
}
impl ContractDetails {
    pub fn indexing_contract_setup(&self, project_path: &Path) -> IndexingContractSetup {
        if let Some(address) = &self.address {
            IndexingContractSetup::Address(AddressDetails {
                address: address.clone(),
                indexed_filters: self.indexed_filters.clone(),
            })
        } else if let Some(factory) = &self.factory {
            IndexingContractSetup::Factory(
                FactoryDetails::from_abi(
                    project_path,
                    factory.abi.clone(),
                    factory.name.clone(),
                    factory.address.clone(),
                    factory.event_name.clone(),
                    factory.input_name.clone(),
                    self.indexed_filters.clone(),
                )
                .unwrap_or_else(|_| panic!("Could not parse ABI from path: {}", factory.abi)),
            )
        } else if let Some(filter) = &self.filter {
            match filter {
                ValueOrArray::Value(filter) => IndexingContractSetup::Filter(FilterDetails {
                    events: ValueOrArray::Value(filter.event_name.clone()),
                    indexed_filters: self.indexed_filters.as_ref().and_then(|f| f.first().cloned()),
                }),
                ValueOrArray::Array(filters) => IndexingContractSetup::Filter(FilterDetails {
                    events: ValueOrArray::Array(
                        filters.iter().map(|f| f.event_name.clone()).collect(),
                    ),
                    indexed_filters: self.indexed_filters.as_ref().and_then(|f| f.first().cloned()),
                }),
            }
        } else {
            panic!("Contract details must have an address, factory or filter");
        }
    }
    pub fn address(&self) -> Option<&ValueOrArray<Address>> {
        if let Some(address) = &self.address {
            return Some(address);
        }
        None
    }
    pub fn new_with_address(
        network: String,
        address: ValueOrArray<Address>,
        indexed_filters: Option<Vec<EventInputIndexedFilters>>,
        start_block: Option<U64>,
        end_block: Option<U64>,
    ) -> Self {
        Self {
            network,
            address: Some(address),
            filter: None,
            indexed_filters,
            factory: None,
            start_block,
            end_block,
        }
    }
}
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum SimpleEventOrContractEvent {
    SimpleEvent(String),
    ContractEvent(ContractEventMapping),
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct DependencyEventTreeYaml {
    pub events: Vec<SimpleEventOrContractEvent>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub then: Option<Box<DependencyEventTreeYaml>>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct DependencyEventTree {
    pub contract_events: Vec<ContractEventMapping>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub then: Option<Box<DependencyEventTree>>,
}
impl DependencyEventTree {
    pub fn collect_dependency_events(&self) -> Vec<ContractEventMapping> {
        let mut dependencies = Vec::new();
        dependencies.extend(self.contract_events.clone());
        if let Some(children) = &self.then {
            dependencies.extend(children.collect_dependency_events());
        }
        dependencies
    }
}
#[derive(Debug, Deserialize)]
#[serde(untagged)]
enum ContractEventDeserializer {
    String(String),
    Struct(ContractEvent),
}
fn deserialize_events<'de, D>(deserializer: D) -> Result<Option<Vec<ContractEvent>>, D::Error>
where
    D: serde::Deserializer<'de>,
{
    let defs = Vec::<ContractEventDeserializer>::deserialize(deserializer)?;
    Ok(Some(
        defs.into_iter()
            .map(|def| match def {
                ContractEventDeserializer::String(s) => ContractEvent { name: s, timestamps: None },
                ContractEventDeserializer::Struct(ev) => ev,
            })
            .collect(),
    ))
}
#[derive(Debug, Serialize, Deserialize, Clone, PartialEq, Eq)]
pub struct ContractEvent {
    /// The name of the event.
    pub name: String,
    /// Enable or disable timestamps for the event. This will override the global timestamp
    /// setting with either the true or false state if set.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timestamps: Option<bool>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Contract {
    pub name: String,
    pub details: Vec<ContractDetails>,
    pub abi: StringOrArray,
    #[serde(
        default,
        deserialize_with = "deserialize_events",
        skip_serializing_if = "Option::is_none"
    )]
    pub include_events: Option<Vec<ContractEvent>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub index_event_in_order: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub dependency_events: Option<DependencyEventTreeYaml>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub reorg_safe_distance: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub generate_csv: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub streams: Option<StreamsConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub chat: Option<ChatConfig>,
}
#[derive(thiserror::Error, Debug)]
pub enum ParseAbiError {
    #[error("Could not read ABI string: {0}")]
    CouldNotReadAbiString(String),
    #[error("Could not get full path: {0}")]
    CouldNotGetFullPath(#[from] std::io::Error),
    #[error("Invalid ABI format: {0}")]
    InvalidAbiFormat(String),
    #[error("Could not merge ABI: {0}")]
    CouldNotMergeAbis(#[from] serde_json::Error),
}
impl Contract {
    pub fn override_name(&mut self, name: String) {
        self.name = name;
    }
    pub fn parse_abi(&self, project_path: &Path) -> Result<String, ParseAbiError> {
        match &self.abi {
            StringOrArray::Single(abi_path) => {
                let full_path = get_full_path(project_path, abi_path)?;
                let abi_str = fs::read_to_string(full_path)?;
                Ok(abi_str)
            }
            StringOrArray::Multiple(abis) => {
                let mut unique_entries = HashSet::new();
                let mut merged_abi_value = Vec::new();
                for abi_path in abis {
                    let full_path = get_full_path(project_path, abi_path)?;
                    let abi_str = fs::read_to_string(full_path)?;
                    let abi_value: Value = serde_json::from_str(&abi_str)?;
                    if let Value::Array(abi_arr) = abi_value {
                        for entry in abi_arr {
                            let entry_str = serde_json::to_string(&entry)?;
                            if unique_entries.insert(entry_str) {
                                merged_abi_value.push(entry);
                            }
                        }
                    } else {
                        return Err(ParseAbiError::InvalidAbiFormat(format!(
                            "Expected an array but got a single value: {abi_value}"
                        )));
                    }
                }
                let merged_abi_str = serde_json::to_string(&json!(merged_abi_value))?;
                Ok(merged_abi_str)
            }
        }
    }
    pub fn convert_dependency_event_tree_yaml(
        &self,
        yaml: DependencyEventTreeYaml,
    ) -> DependencyEventTree {
        DependencyEventTree {
            contract_events: yaml
                .events
                .into_iter()
                .map(|event| match event {
                    SimpleEventOrContractEvent::ContractEvent(contract_event) => contract_event,
                    SimpleEventOrContractEvent::SimpleEvent(event_name) => {
                        ContractEventMapping { contract_name: self.name.clone(), event_name }
                    }
                })
                .collect(),
            then: yaml
                .then
                .map(|then_event| Box::new(self.convert_dependency_event_tree_yaml(*then_event))),
        }
    }
    pub fn is_filter(&self) -> bool {
        let filter_count = self.details.iter().filter(|details| details.filter.is_some()).count();
        if filter_count > 0 && filter_count != self.details.len() {
            // panic as this should never happen as validation has already happened
            panic!("Cannot mix and match address and filter for the same contract definition.");
        }
        filter_count > 0
    }
    fn contract_name_to_filter_name(&self) -> String {
        format!("{}Filter", self.name)
    }
    pub fn raw_name(&self) -> String {
        if self.is_filter() {
            self.name.split("Filter").collect::<Vec<&str>>()[0].to_string()
        } else {
            self.name.clone()
        }
    }
    pub fn before_modify_name_if_filter_readonly(&'_ self) -> Cow<'_, str> {
        if self.is_filter() {
            Cow::Owned(self.contract_name_to_filter_name())
        } else {
            Cow::Borrowed(&self.name)
        }
    }
    pub fn identify_and_modify_filter(&mut self) -> bool {
        if self.is_filter() {
            self.override_name(self.contract_name_to_filter_name());
            true
        } else {
            false
        }
    }
}
#[cfg(test)]
mod tests {
    use serde_yaml;
    use super::*;
    #[test]
    fn test_contract_include_events_simple() {
        let yaml = r#"
            name: ERC20
            abi: ./abis/ERC20.abi.json
            details:
              - network: ethereum
                start_block: 20090000
                filter:
                  - event_name: Transfer
                  - event_name: Approval
            include_events:
              - Transfer
              - Approval
        "#;
        let contract: Contract = serde_yaml::from_str(yaml).unwrap();
        assert_eq!(
            contract.include_events,
            Some(vec![
                ContractEvent { name: "Transfer".to_string(), timestamps: None },
                ContractEvent { name: "Approval".to_string(), timestamps: None }
            ])
        );
    }
    #[test]
    fn test_contract_include_events_complex() {
        let yaml = r#"
            name: ERC20
            abi: ./abis/ERC20.abi.json
            details:
              - network: ethereum
                start_block: 20090000
                filter:
                  - event_name: Transfer
                  - event_name: Approval
            include_events:
              - name: Transfer
                timestamps: true
              - name: Approval
                timestamps: false
        "#;
        let contract: Contract = serde_yaml::from_str(yaml).unwrap();
        assert_eq!(
            contract.include_events,
            Some(vec![
                ContractEvent { name: "Transfer".to_string(), timestamps: Some(true) },
                ContractEvent { name: "Approval".to_string(), timestamps: Some(false) }
            ])
        );
    }
}
</file>

<file path="core/src/manifest/core.rs">
use std::str::FromStr;
use alloy::{primitives::U64, transports::http::reqwest::header::HeaderMap};
use serde::{Deserialize, Deserializer, Serialize, Serializer};
use serde_yaml::Value;
use crate::event::contract_setup::ContractEventMapping;
use crate::helpers::to_pascal_case;
use crate::manifest::config::Config;
use crate::manifest::contract::{
    ContractDetails, ContractEvent, DependencyEventTreeYaml, FactoryDetailsYaml,
    SimpleEventOrContractEvent,
};
use crate::{
    indexer::Indexer,
    manifest::{
        contract::Contract,
        global::Global,
        graphql::GraphQLSettings,
        native_transfer::{deserialize_native_transfers, NativeTransferDetails, NativeTransfers},
        network::Network,
        phantom::Phantom,
        storage::Storage,
    },
};
fn deserialize_project_type<'de, D>(deserializer: D) -> Result<ProjectType, D::Error>
where
    D: Deserializer<'de>,
{
    let value: Value = Deserialize::deserialize(deserializer)?;
    match value {
        Value::String(s) => match s.as_str() {
            "rust" => Ok(ProjectType::Rust),
            "no-code" => Ok(ProjectType::NoCode),
            _ => Err(serde::de::Error::custom(format!("Unknown project type: {s}"))),
        },
        _ => Err(serde::de::Error::custom("Invalid project type format")),
    }
}
fn serialize_project_type<S>(value: &ProjectType, serializer: S) -> Result<S::Ok, S::Error>
where
    S: Serializer,
{
    let string_value = match value {
        ProjectType::Rust => "rust",
        ProjectType::NoCode => "no-code",
    };
    serializer.serialize_str(string_value)
}
#[derive(Debug, Serialize, Deserialize, PartialEq, Clone)]
#[serde(untagged)]
pub enum ProjectType {
    Rust,
    NoCode,
}
fn default_storage() -> Storage {
    Storage::default()
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Manifest {
    pub name: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub repository: Option<String>,
    #[serde(deserialize_with = "deserialize_project_type")]
    #[serde(serialize_with = "serialize_project_type")]
    pub project_type: ProjectType,
    #[serde(default)]
    pub config: Config,
    #[serde(default)]
    pub timestamps: Option<bool>,
    pub networks: Vec<Network>,
    #[serde(default = "default_storage")]
    pub storage: Storage,
    #[serde(default)]
    #[serde(deserialize_with = "deserialize_native_transfers")]
    pub native_transfers: NativeTransfers,
    pub contracts: Vec<Contract>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub phantom: Option<Phantom>,
    #[serde(default)]
    pub global: Global,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub graphql: Option<GraphQLSettings>,
}
impl Manifest {
    /// Includes both user-defined contracts and factory filter contracts
    pub fn all_contracts(&self) -> Vec<Contract> {
        self.contracts.clone().into_iter().flat_map(|contract| {
            let factory_filter_details = contract.details.iter()
                .filter_map(|detail| {
                    detail.factory.as_ref().map(|factory| (
                        factory.clone(),
                        detail.network.clone(),
                        detail.start_block,
                        detail.end_block
                    ))
                }).collect::<Vec<_>>();
            let first_factory = factory_filter_details.first().cloned();
            match first_factory {
                Some((first_factory, ..)) => {
                    let has_factory_mismatch = factory_filter_details.iter().any(|(detail, ..)| detail.name != first_factory.name || detail.abi != first_factory.abi || detail.event_name != first_factory.event_name);
                    if has_factory_mismatch {
                        panic!("Contract using factory filter must use same factory across all networks. Please raise issue in github if you need different factories across networks");
                    }
                    // suffix with factory filter details to allow having the same contract name at the `contracts` level in yaml
                    let overridden_factory_contract_name = format!("{}{}{}", first_factory.name, to_pascal_case(&first_factory.event_name), first_factory.input_names().iter().map(|v| to_pascal_case(v)).collect::<Vec<_>>().join(""));
                    let factory_contract = Contract {
                        name: overridden_factory_contract_name.clone(),
                        details: factory_filter_details.into_iter().map(|(factory, network, start_block, end_block)| ContractDetails {
                            network,
                            start_block,
                            end_block,
                            factory: Some(FactoryDetailsYaml {
                                name: overridden_factory_contract_name.clone(),
                                ..factory
                            }),
                            address: None,
                            filter: None,
                            indexed_filters: None,
                        }).collect::<Vec<_>>(),
                        abi: first_factory.abi.clone().into(),
                        dependency_events: None,
                        include_events: Some(vec![ContractEvent { name: first_factory.event_name.clone(), timestamps: None }]),
                        index_event_in_order: contract.index_event_in_order.clone(),
                        reorg_safe_distance: contract.reorg_safe_distance,
                        generate_csv: contract.generate_csv,
                        streams: None,
                        chat: None,
                    };
                    let dependency_contract = Contract {
                        dependency_events: Some(DependencyEventTreeYaml {
                            events: vec![SimpleEventOrContractEvent::ContractEvent(ContractEventMapping {
                                contract_name: factory_contract.name.clone(),
                                event_name: first_factory.event_name,
                            })],
                            then: contract.dependency_events.or_else(|| {
                                let events = contract
                                    .include_events
                                    .clone()
                                    .expect("Contract using factory filter must specify `include_events`.");
                                Some(DependencyEventTreeYaml {
                                    events: events
                                        .into_iter()
                                        .map(|e|SimpleEventOrContractEvent::SimpleEvent(e.name))
                                        .collect::<Vec<_>>(),
                                    then: None,
                                })
                            }).map(Box::new),
                        }),
                        details: contract.details.into_iter().map(|detail| ContractDetails {
                            factory: Some(FactoryDetailsYaml {
                                name: overridden_factory_contract_name.clone(),
                                ..detail.factory.expect("Factory details must be present")
                            }),
                            ..detail
                        }).collect(),
                        ..contract
                    };
                    vec![factory_contract, dependency_contract]
                }
                None => vec![contract]
            }
        }).collect()
    }
    pub fn to_indexer(&self) -> Indexer {
        Indexer {
            name: self.name.clone(),
            contracts: self.all_contracts().clone(),
            native_transfers: self.native_transfers.clone(),
        }
    }
    pub fn has_any_contracts_live_indexing(&self) -> bool {
        self.all_contracts().iter().any(|c| c.details.iter().any(|p| p.end_block.is_none()))
    }
    /// Check if the manifest has opted-in to indexing native transfers. It is off by default.
    pub fn has_enabled_native_transfers(&self) -> bool {
        self.native_transfers.enabled
    }
    /// We allow `networks` to be None for native transfers. Which has a special semantic of meaning
    /// opt in to "all supported networks" for live indexing only.
    ///
    /// This means we can map the root `networks` object into the `native_transfers.networks` for
    /// simplicity when constructing the manifest.
    ///
    /// If the user defines a `networks` list this will take priority.
    ///
    /// # Example
    ///
    /// ```yaml
    /// project_type: no-code
    /// native_transfers: true
    /// networks:
    ///   - name: ethereum
    //      chain_id: 1
    //      rpc: https://example.rpc.org
    /// ```
    pub fn set_native_transfer_networks(&mut self) {
        if self.native_transfers.networks.is_none() {
            let root_networks = self
                .networks
                .iter()
                .cloned()
                .map(|n| NativeTransferDetails {
                    network: n.name,
                    start_block: None,
                    end_block: None,
                    method: Default::default(),
                })
                .collect::<Vec<_>>();
            self.native_transfers.networks = Some(root_networks);
        }
    }
    pub fn contract_csv_enabled(&self, contract_name: &str) -> bool {
        let contract_csv_enabled = self
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .is_some_and(|c| c.generate_csv.unwrap_or(true));
        self.storage.csv_enabled() && contract_csv_enabled
    }
    pub fn get_custom_headers(&self) -> HeaderMap {
        let mut headers = HeaderMap::new();
        if let Some(phantom) = &self.phantom {
            if let Some(shadow) = &phantom.shadow {
                headers.insert("X-SHADOW-API-KEY", shadow.api_key.parse().unwrap());
            }
        }
        headers
    }
}
pub fn deserialize_option_u64_from_string<'de, D>(deserializer: D) -> Result<Option<U64>, D::Error>
where
    D: Deserializer<'de>,
{
    let s: Option<String> = Option::deserialize(deserializer)?;
    match s {
        Some(string) => U64::from_str(&string).map(Some).map_err(serde::de::Error::custom),
        None => Ok(None),
    }
}
pub fn serialize_option_u64_as_string<S>(
    value: &Option<U64>,
    serializer: S,
) -> Result<S::Ok, S::Error>
where
    S: Serializer,
{
    match *value {
        Some(ref u64_value) => serializer.serialize_some(&u64_value.as_limbs()[0].to_string()),
        None => serializer.serialize_none(),
    }
}
#[cfg(test)]
mod tests {
    use serde_yaml;
    use super::*;
    #[test]
    fn test_conflicting_storage() {
        let yaml = r#"
        name: test
        project_type: no-code
        networks: []
        contracts: []
        storage:
          clickhouse:
            enabled: true
          postgres:
            enabled: true
        "#;
        let manifest: Result<Manifest, _> = serde_yaml::from_str(yaml);
        assert_eq!(manifest.is_err(), true);
        assert!(manifest
            .unwrap_err()
            .to_string()
            .contains("cannot specify both `postgres` and `clickhouse` at the same time"));
    }
    #[test]
    fn test_clickhouse_storage() {
        let yaml = r#"
        name: test
        project_type: no-code
        networks: []
        contracts: []
        storage:
          clickhouse:
            enabled: true
        "#;
        let manifest: Manifest = serde_yaml::from_str(yaml).unwrap();
        assert!(manifest.storage.clickhouse.is_some());
    }
    #[test]
    fn test_native_transfers_complex() {
        let yaml = r#"
        name: test
        project_type: no-code
        networks: []
        contracts: []
        storage:
          csv:
            enabled: true
          postgres:
            enabled: true
        native_transfers:
          networks:
            - network: ethereum
              start_block: 100
              end_block: 200
            - network: base
          streams:
            sns:
              aws_config:
                region: us-east-1
                access_key: test
                secret_key: test
                endpoint_url: http://localhost:8000
              topics:
                - topic_arn: arn:aws:sns:us-east-1:000000000000:native-transfers-test
                  networks:
                    - ethereum
                  events:
                    - event_name: NativeTransfer
        "#;
        let manifest: Manifest = serde_yaml::from_str(yaml).unwrap();
        assert!(manifest.native_transfers.enabled);
        assert_eq!(
            manifest.native_transfers.streams.unwrap().sns.unwrap().topics[0].events[0].event_name,
            "NativeTransfer"
        );
        let networks = manifest.native_transfers.networks.unwrap();
        assert_eq!(networks[0].network, "ethereum");
        assert_eq!(networks[0].start_block.unwrap().as_limbs()[0], 100);
        assert_eq!(networks[0].end_block.unwrap().as_limbs()[0], 200);
        assert_eq!(networks[1].network, "base");
        assert_eq!(networks[1].start_block, None);
        assert_eq!(networks[1].end_block, None);
    }
    #[test]
    fn test_native_transfers_simple() {
        let yaml = r#"
        name: test
        project_type: no-code
        networks: []
        contracts: []
        native_transfers: true
        "#;
        let manifest: Manifest = serde_yaml::from_str(yaml).unwrap();
        assert!(manifest.native_transfers.enabled);
        let yaml = r#"
        name: test
        project_type: no-code
        networks: []
        contracts: []
        native_transfers: false
        "#;
        let manifest: Manifest = serde_yaml::from_str(yaml).unwrap();
        assert!(!manifest.native_transfers.enabled);
        let yaml = r#"
        name: test
        project_type: no-code
        networks: []
        contracts: []
        "#;
        let manifest: Manifest = serde_yaml::from_str(yaml).unwrap();
        assert!(!manifest.native_transfers.enabled);
    }
    #[test]
    fn test_config_simple() {
        let yaml = r#"
        name: test
        project_type: no-code
        networks: []
        contracts: []
        "#;
        let manifest: Manifest = serde_yaml::from_str(yaml).unwrap();
        assert_eq!(manifest.config.callback_concurrency, None);
        assert_eq!(manifest.config.buffer, None);
    }
    #[test]
    fn test_timestamps_simple() {
        let yaml = r#"
        name: test
        project_type: no-code
        timestamps: true
        networks: []
        contracts: []
        "#;
        let manifest: Manifest = serde_yaml::from_str(yaml).unwrap();
        assert_eq!(manifest.timestamps, Some(true));
    }
}
</file>

<file path="core/src/manifest/global.rs">
use serde::{Deserialize, Serialize};
use crate::manifest::contract::Contract;
pub fn default_health_port() -> u16 {
    8080
}
#[derive(Debug, Serialize, Deserialize, Default, Clone)]
pub struct Global {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub contracts: Option<Vec<Contract>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub etherscan_api_key: Option<String>,
    #[serde(default = "default_health_port")]
    pub health_port: u16,
}
</file>

<file path="core/src/manifest/graphql.rs">
use serde::{Deserialize, Serialize};
pub fn default_graphql_port() -> u16 {
    3001
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct GraphQLSettings {
    #[serde(default = "default_graphql_port")]
    pub port: u16,
    #[serde(default)]
    pub disable_advanced_filters: bool,
    #[serde(default)]
    pub filter_only_on_indexed_columns: bool,
}
impl Default for GraphQLSettings {
    fn default() -> Self {
        Self {
            port: default_graphql_port(),
            disable_advanced_filters: false,
            filter_only_on_indexed_columns: false,
        }
    }
}
impl GraphQLSettings {
    pub fn set_port(&mut self, port: u16) {
        self.port = port;
    }
}
</file>

<file path="core/src/manifest/mod.rs">
pub mod chat;
pub mod config;
pub mod contract;
pub mod core;
pub mod global;
pub mod graphql;
pub mod native_transfer;
pub mod network;
pub mod phantom;
pub mod reth;
pub mod storage;
pub mod stream;
pub mod yaml;
</file>

<file path="core/src/manifest/native_transfer.rs">
use std::str::FromStr;
use alloy::primitives::U64;
use serde::{Deserialize, Deserializer, Serialize};
use super::core::serialize_option_u64_as_string;
use crate::manifest::{chat::ChatConfig, stream::StreamsConfig};
#[derive(Serialize, Deserialize)]
#[serde(untagged)]
enum StringOrNum {
    String(String),
    Num(u64),
}
#[derive(Clone, Copy, Debug, Hash, PartialEq, Eq, Default, Deserialize, Serialize)]
pub enum TraceProcessingMethod {
    #[serde(rename = "trace_block")]
    TraceBlock,
    #[serde(rename = "debug_traceBlockByNumber")]
    DebugTraceBlockByNumber,
    #[default]
    #[serde(rename = "eth_getBlockByNumber")]
    EthGetBlockByNumber,
}
/// Deserialize a number or string into a U64. This is required for the untagged deserialize of
/// native transfers to succeed.
fn deserialize_option_u64_from_string_or_num<'de, D>(
    deserializer: D,
) -> Result<Option<U64>, D::Error>
where
    D: Deserializer<'de>,
{
    let s: Option<StringOrNum> = Option::deserialize(deserializer)?;
    match s {
        Some(StringOrNum::String(string)) => {
            U64::from_str(&string).map(Some).map_err(serde::de::Error::custom)
        }
        Some(StringOrNum::Num(num)) => Ok(Some(U64::from(num))),
        None => Ok(None),
    }
}
#[derive(Debug, Hash, PartialEq, Eq, Serialize, Deserialize, Clone)]
pub struct NativeTransferDetails {
    pub network: String,
    #[serde(default)]
    pub method: TraceProcessingMethod,
    #[serde(
        default,
        skip_serializing_if = "Option::is_none",
        deserialize_with = "deserialize_option_u64_from_string_or_num",
        serialize_with = "serialize_option_u64_as_string"
    )]
    pub start_block: Option<U64>,
    #[serde(
        default,
        skip_serializing_if = "Option::is_none",
        deserialize_with = "deserialize_option_u64_from_string_or_num",
        serialize_with = "serialize_option_u64_as_string"
    )]
    pub end_block: Option<U64>,
}
fn default_enabled() -> bool {
    true
}
/// The normalized 'Native Transfers' config.
#[derive(Debug, Default, Serialize, Deserialize, Clone)]
pub struct NativeTransfers {
    #[serde(default = "default_enabled")]
    pub enabled: bool,
    /// None has a special meaning of "All" networks for this case. This is because we want the
    /// functionality to be as simple to opt in to as possible.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub networks: Option<Vec<NativeTransferDetails>>,
    /// Define any stream provider, you can manually override the event name by using the Stream
    /// `alias` option, or by default we ensure the event is included in the list of `events`
    /// on the stream if none are provided to the native_transfers stream config..
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub streams: Option<StreamsConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub chat: Option<ChatConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub generate_csv: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub reorg_safe_distance: Option<bool>,
}
/// The config to enable native transfers. This can be either a "simple" opinionated enable-all, or
/// a detailed "full" option configuration. The most common live-index setup will be "simple".
///
/// # Example
///
/// ```yaml
/// # Simple opt-in to all native transfer live indexing
/// native_transfers: true
/// ```
#[allow(clippy::large_enum_variant)]
#[derive(Debug, Serialize, Deserialize, Clone)]
#[serde(untagged)]
pub enum NativeTransferFullOrSimple {
    Simple(bool),
    Full(NativeTransfers),
}
pub fn deserialize_native_transfers<'de, D>(deserializer: D) -> Result<NativeTransfers, D::Error>
where
    D: Deserializer<'de>,
{
    let value = NativeTransferFullOrSimple::deserialize(deserializer)?;
    let native = match value {
        NativeTransferFullOrSimple::Simple(enabled) => NativeTransfers {
            enabled,
            networks: None,
            streams: None,
            chat: None,
            generate_csv: None,
            reorg_safe_distance: None,
        },
        NativeTransferFullOrSimple::Full(transfers) => transfers,
    };
    Ok(native)
}
#[cfg(test)]
mod tests {
    use serde_yaml;
    use super::*;
    #[test]
    fn test_native_transfer_full_string() {
        let yaml = r#"
          networks:
            - network: ethereum
              start_block: "100"
              end_block: "200"
        "#;
        let transfer: NativeTransfers = serde_yaml::from_str(yaml).unwrap();
        let networks: Vec<NativeTransferDetails> = transfer.networks.unwrap().into_iter().collect();
        assert!(transfer.enabled);
        assert_eq!(networks[0].network, "ethereum");
        assert_eq!(networks[0].start_block.unwrap().as_limbs()[0], 100);
        assert_eq!(networks[0].end_block.unwrap().as_limbs()[0], 200);
        assert_eq!(networks[0].method, TraceProcessingMethod::EthGetBlockByNumber);
    }
    #[test]
    fn test_native_transfer_full_u64() {
        let yaml = r#"
          networks:
            - network: base
              start_block: 100
        "#;
        let transfer: NativeTransfers = serde_yaml::from_str(yaml).unwrap();
        let networks: Vec<NativeTransferDetails> = transfer.networks.unwrap().into_iter().collect();
        assert!(transfer.enabled);
        assert_eq!(networks[0].network, "base");
        assert_eq!(networks[0].start_block.unwrap().as_limbs()[0], 100);
        assert_eq!(networks[0].end_block, None);
        assert_eq!(networks[0].method, TraceProcessingMethod::EthGetBlockByNumber);
    }
    #[test]
    fn test_native_transfer_full_method() {
        let yaml = r#"
          networks:
            - network: base
              start_block: 100
              method: "trace_block"
            - network: ethereum
              method: "debug_traceBlockByNumber"
            - network: blast
              method: debug_traceBlockByNumber
            - network: optimism
        "#;
        let transfer: NativeTransfers = serde_yaml::from_str(yaml).unwrap();
        let networks: Vec<NativeTransferDetails> = transfer.networks.unwrap().into_iter().collect();
        assert!(transfer.enabled);
        assert_eq!(networks[0].network, "base");
        assert_eq!(networks[0].method, TraceProcessingMethod::TraceBlock);
        assert!(transfer.enabled);
        assert_eq!(networks[1].network, "ethereum");
        assert_eq!(networks[1].method, TraceProcessingMethod::DebugTraceBlockByNumber);
        assert!(transfer.enabled);
        assert_eq!(networks[2].network, "blast");
        assert_eq!(networks[2].method, TraceProcessingMethod::DebugTraceBlockByNumber);
        assert!(transfer.enabled);
        assert_eq!(networks[3].network, "optimism");
        assert_eq!(networks[3].method, TraceProcessingMethod::EthGetBlockByNumber);
    }
}
</file>

<file path="core/src/manifest/network.rs">
use std::fmt;
use std::time::Duration;
use alloy::primitives::U64;
use serde::de::Visitor;
use serde::{de, Deserialize, Deserializer, Serialize};
#[cfg(feature = "reth")]
use tokio::sync::broadcast::Sender;
use tokio::time::sleep;
use super::core::{deserialize_option_u64_from_string, serialize_option_u64_as_string};
#[cfg(feature = "reth")]
use super::reth::RethConfig;
#[cfg(feature = "reth")]
use crate::notifications::ChainStateNotification;
#[cfg(feature = "reth")]
use crate::reth::node::start_reth_node_with_exex;
#[cfg(feature = "reth")]
use reth::cli::Commands;
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Network {
    pub name: String,
    pub chain_id: u64,
    pub rpc: String,
    /// Poll the latest block at a defined frequency. It is recommended that this frequency be a
    /// multiple faster than the networks block time to ensure fast indexing.
    ///
    /// If RPC use is a concern, this can be reduced at the cost of slower indexing of new logs.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub block_poll_frequency: Option<BlockPollFrequency>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub compute_units_per_second: Option<u64>,
    #[serde(
        default,
        skip_serializing_if = "Option::is_none",
        deserialize_with = "deserialize_option_u64_from_string",
        serialize_with = "serialize_option_u64_as_string"
    )]
    pub max_block_range: Option<U64>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub get_logs_settings: Option<GetLogsSettings>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub disable_logs_bloom_checks: Option<bool>,
    /// Reth configuration for this network
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[cfg(feature = "reth")]
    pub reth: Option<RethConfig>,
    #[cfg(not(feature = "reth"))]
    pub reth: Option<()>,
}
#[cfg(feature = "reth")]
impl Network {
    /// Get the IPC path for the Reth node
    pub fn get_reth_ipc_path(&self) -> Option<String> {
        let reth = self.reth.as_ref()?;
        let cli = reth.to_cli().ok()?;
        match &cli.command {
            Commands::Node(node_cmd) => Some(node_cmd.rpc.ipcpath.clone()),
            _ => None,
        }
    }
    /// Check if Reth is enabled for this network
    pub fn is_reth_enabled(&self) -> bool {
        self.reth.is_some()
    }
    /// Try to start the Reth node for this network
    ///
    /// Returns a Sender for the Reth node if it was started successfully.
    ///
    /// If Reth is not enabled, the function will return None.
    ///
    /// If the Reth node fails to start, the function will return an error.
    pub async fn try_start_reth_node(
        &self,
    ) -> Result<Option<Sender<ChainStateNotification>>, eyre::Error> {
        if !self.is_reth_enabled() {
            return Ok(None);
        }
        let reth_cli = self.reth.as_ref().unwrap().to_cli().map_err(|e| eyre::eyre!(e))?;
        let reth_tx = start_reth_node_with_exex(reth_cli)?;
        // Wait for IPC path to be ready if specified
        if let Some(ipc_path) = self.get_reth_ipc_path() {
            wait_for_ipc_ready(&ipc_path).await?;
        }
        println!("started reth node");
        Ok(Some(reth_tx))
    }
}
#[derive(Debug, Serialize, Clone)]
pub enum AddressFiltering {
    InMemory,
    MaxAddressPerGetLogsRequest(usize),
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct AddressFilteringConfig {
    pub max_address_per_get_logs_request: usize,
}
impl<'de> Deserialize<'de> for AddressFiltering {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        use serde::de::Error;
        let value = serde_yaml::Value::deserialize(deserializer)?;
        if let Some(s) = value.as_str() {
            if s == "in-memory" {
                return Ok(AddressFiltering::InMemory);
            }
        }
        // Try to deserialize as AddressFilteringConfig
        match AddressFilteringConfig::deserialize(value) {
            Ok(config) => Ok(AddressFiltering::MaxAddressPerGetLogsRequest(
                config.max_address_per_get_logs_request,
            )),
            Err(_) => Err(Error::custom("Invalid AddressFiltering format")),
        }
    }
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct GetLogsSettings {
    pub address_filtering: AddressFiltering,
}
#[derive(Debug, Serialize, Clone, Copy, PartialEq, Eq)]
pub enum BlockPollFrequency {
    /// Poll the latest block a defined rate for a network. E.g. 500ms.
    PollRateMs { millis: u64 },
    /// Poll the latest block at factor of the networks block time. E.g. 1/5th of a 2s block time.
    Division { divisor: u32 },
    /// Use very fast block polling designed to index blocks as close to realtime as possible.
    /// This will be at the expense of more RPC calls.
    Rapid,
    /// This will use an RPC balanced / optimised poll rate, it is currently defined as 1/3 of the
    /// networks block rate and is shorthand for `/3`, however may change in the future to represent
    /// use cases where we want to balance speed of indexing with rpc use automatically.
    RpcOptimized,
}
impl<'de> Deserialize<'de> for BlockPollFrequency {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        struct BlockPollFrequencyVisitor;
        impl<'de> Visitor<'de> for BlockPollFrequencyVisitor {
            type Value = BlockPollFrequency;
            fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                formatter.write_str("a string representing block poll frequency")
            }
            fn visit_str<E>(self, value: &str) -> Result<BlockPollFrequency, E>
            where
                E: de::Error,
            {
                match value {
                    "rapid" => Ok(BlockPollFrequency::Rapid),
                    "optimized" => Ok(BlockPollFrequency::RpcOptimized),
                    _ if value.starts_with('/') => {
                        let divisor = value[1..].parse::<u32>().map_err(E::custom)?;
                        Ok(BlockPollFrequency::Division { divisor })
                    }
                    _ => {
                        let millis = value.parse::<u64>().map_err(E::custom)?;
                        Ok(BlockPollFrequency::PollRateMs { millis })
                    }
                }
            }
        }
        deserializer.deserialize_str(BlockPollFrequencyVisitor)
    }
}
/// Wait for IPC socket file to be ready
pub async fn wait_for_ipc_ready(ipc_path: &str) -> Result<(), eyre::Error> {
    use alloy::providers::{IpcConnect, Provider, ProviderBuilder};
    let max_retries = 60; // 60 seconds max wait
    let mut last_error = None;
    for i in 0..max_retries {
        // Try to connect to the IPC socket
        let ipc = IpcConnect::new(ipc_path.to_string());
        match ProviderBuilder::new().connect_ipc(ipc).await {
            Ok(provider) => {
                // Try a simple call to ensure it's really ready
                match provider.get_chain_id().await {
                    Ok(_) => {
                        tracing::info!("IPC socket at {} is ready", ipc_path);
                        return Ok(());
                    }
                    Err(e) => {
                        last_error = Some(format!("Connected but get_chain_id failed: {e}"));
                    }
                }
            }
            Err(e) => {
                last_error = Some(format!("Connection failed: {e}"));
            }
        }
        if i == 0 {
            tracing::info!("Waiting for IPC socket at {} to be ready...", ipc_path);
        } else if i % 5 == 0 {
            tracing::info!(
                "Still waiting for IPC socket at {} to be ready... ({}/{})",
                ipc_path,
                i,
                max_retries
            );
        }
        sleep(Duration::from_secs(1)).await;
    }
    Err(eyre::eyre!(
        "IPC socket at {} did not become ready after {} seconds. Last error: {:?}",
        ipc_path,
        max_retries,
        last_error
    ))
}
#[cfg(test)]
mod tests {
    #[cfg(feature = "reth")]
    use reth::cli::Commands;
    use serde_yaml;
    use super::*;
    #[test]
    fn test_network_defaults() {
        let network: Network = serde_yaml::from_str(
            r#"
            name: ethereum
            chain_id: 1
            rpc: https://mainnet.gateway.tenderly.co
            "#,
        )
        .unwrap();
        assert_eq!(network.name, "ethereum");
        assert_eq!(network.chain_id, 1);
        assert_eq!(network.rpc, "https://mainnet.gateway.tenderly.co");
        assert_eq!(network.max_block_range, None);
        assert_eq!(network.compute_units_per_second, None);
        assert_eq!(network.block_poll_frequency, None);
    }
    #[test]
    fn test_network_block_poll_frequency() {
        let network: Network = serde_yaml::from_str(
            r#"
            name: ethereum
            chain_id: 1
            rpc: https://mainnet.gateway.tenderly.co
            block_poll_frequency: "rapid"
            "#,
        )
        .unwrap();
        assert_eq!(network.block_poll_frequency, Some(BlockPollFrequency::Rapid));
        let network: Network = serde_yaml::from_str(
            r#"
            name: ethereum
            chain_id: 1
            rpc: https://mainnet.gateway.tenderly.co
            block_poll_frequency: "/3"
            "#,
        )
        .unwrap();
        assert_eq!(network.block_poll_frequency, Some(BlockPollFrequency::Division { divisor: 3 }));
        let network: Network = serde_yaml::from_str(
            r#"
            name: ethereum
            chain_id: 1
            rpc: https://mainnet.gateway.tenderly.co
            block_poll_frequency: "1500"
            "#,
        )
        .unwrap();
        assert_eq!(
            network.block_poll_frequency,
            Some(BlockPollFrequency::PollRateMs { millis: 1500 })
        );
        let network: Network = serde_yaml::from_str(
            r#"
            name: ethereum
            chain_id: 1
            rpc: https://mainnet.gateway.tenderly.co
            block_poll_frequency: 100
            "#,
        )
        .unwrap();
        assert_eq!(
            network.block_poll_frequency,
            Some(BlockPollFrequency::PollRateMs { millis: 100 })
        );
    }
    #[cfg(feature = "reth")]
    #[test]
    fn test_network_with_reth_config() {
        let network: Network = serde_yaml::from_str(
            r#"
            name: ethereum
            chain_id: 1
            rpc: https://mainnet.gateway.tenderly.co
            reth:
                enabled: true
                cli_args:
                    - --ipcpath /custom/reth.ipc
            "#,
        )
        .unwrap();
        assert!(network.reth.is_some());
        let reth = network.reth.as_ref().unwrap();
        assert!(reth.enabled);
        assert_eq!(reth.cli_args, vec!["--ipcpath /custom/reth.ipc"]);
        // Test get_reth_ipc_path
        let ipc_path = network.get_reth_ipc_path();
        assert_eq!(ipc_path, Some("/custom/reth.ipc".to_string()));
    }
    #[cfg(feature = "reth")]
    #[test]
    fn test_network_with_reth_config_no_ipc_path() {
        let network: Network = serde_yaml::from_str(
            r#"
            name: ethereum
            chain_id: 1
            rpc: https://mainnet.gateway.tenderly.co
            reth:
                enabled: true
                cli_args:
                    - --authrpc.jwtsecret /Users/skanda/secrets/jwt.hex
                    - --authrpc.addr 127.0.0.1
                    - --authrpc.port 8551
                    - --datadir /Volumes/T9/reth
                    - --metrics 127.0.0.1:9001
                    - --chain sepolia
                    - --http
            "#,
        )
        .unwrap();
        assert!(network.reth.is_some());
        let reth = network.reth.as_ref().unwrap();
        assert!(reth.enabled);
        // Test get_reth_ipc_path
        let cli = reth.to_cli().unwrap();
        match &cli.command {
            Commands::Node(node_cmd) => {
                println!("node_cmd: {:?}", node_cmd.rpc.ipcpath);
                // Default IPC path when not specified
                assert!(!node_cmd.rpc.ipcdisable);
            }
            _ => panic!("Expected Node command"),
        }
    }
    #[cfg(feature = "reth")]
    #[test]
    fn test_network_reth_ipc_disabled() {
        let network: Network = serde_yaml::from_str(
            r#"
            name: ethereum
            chain_id: 1
            rpc: https://mainnet.gateway.tenderly.co
            reth:
                enabled: true
                cli_args:
                    - --ipcdisable
            "#,
        )
        .unwrap();
        // Should return None when IPC is disabled
        let cli = network.reth.as_ref().unwrap().to_cli().unwrap();
        if let Commands::Node(node_cmd) = &cli.command {
            assert!(node_cmd.rpc.ipcdisable);
        }
        // Note: get_reth_ipc_path might still return a default path even with ipcdisable
    }
    #[cfg(feature = "reth")]
    #[test]
    fn test_network_no_reth_config() {
        let network: Network = serde_yaml::from_str(
            r#"
            name: ethereum
            chain_id: 1
            rpc: https://mainnet.gateway.tenderly.co
            "#,
        )
        .unwrap();
        assert!(network.reth.is_none());
        assert_eq!(network.get_reth_ipc_path(), None);
    }
}
</file>

<file path="core/src/manifest/phantom.rs">
use serde::{Deserialize, Serialize};
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct PhantomShadow {
    pub api_key: String,
    pub fork_id: String,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct PhantomDyrpc {
    pub api_key: String,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Phantom {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub dyrpc: Option<PhantomDyrpc>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub shadow: Option<PhantomShadow>,
}
impl Phantom {
    pub fn dyrpc_enabled(&self) -> bool {
        self.dyrpc.is_some()
    }
    pub fn shadow_enabled(&self) -> bool {
        self.shadow.is_some()
    }
}
</file>

<file path="core/src/manifest/reth.rs">
#![cfg(feature = "reth")]
use reth::cli::Cli;
use serde::{Deserialize, Serialize};
/// Default value for logging field
fn default_true() -> bool {
    true
}
/// Configuration for Reth node and ExEx
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RethConfig {
    /// Whether to enable Reth integration
    pub enabled: bool,
    /// Whether to show reth logs in stdout
    #[serde(default = "default_true")]
    pub logging: bool,
    /// CLI args as "key value" strings (e.g., "--datadir /path/to/data")
    #[serde(default)]
    pub cli_args: Vec<String>,
}
impl RethConfig {
    /// Create RethConfig from CLI arguments, validating them through reth's CLI parser
    pub fn from_cli_args(args: Vec<String>) -> Result<Self, String> {
        // Validate args by parsing through reth's CLI
        let mut full_args = vec!["reth".to_string(), "node".to_string()];
        full_args.extend(args.clone());
        // This will error if args are invalid
        let _cli = Cli::try_parse_args_from(&full_args)
            .map_err(|e| format!("Failed to parse reth CLI args: {e}"))?;
        // Store args in space-separated format for clean YAML
        let cli_args = Self::combine_args_with_values(&args);
        Ok(Self { enabled: true, logging: true, cli_args })
    }
    /// Combine CLI flags with their values into space-separated strings
    /// e.g., ["--http", "--datadir", "/path"] -> ["--http", "--datadir /path"]
    fn combine_args_with_values(args: &[String]) -> Vec<String> {
        let mut combined = Vec::new();
        let mut i = 0;
        while i < args.len() {
            let arg = &args[i];
            if arg.starts_with("--") {
                if i + 1 < args.len() && !args[i + 1].starts_with("--") {
                    // Combine flag and value
                    combined.push(format!("{} {}", arg, args[i + 1]));
                    i += 2;
                } else {
                    // Just the flag
                    combined.push(arg.to_string());
                    i += 1;
                }
            } else {
                // Skip non-flag arguments
                i += 1;
            }
        }
        combined
    }
    /// Convert to reth CLI, parsing the stored args
    pub fn to_cli(&self) -> Result<reth::cli::Cli, String> {
        let args = self.to_cli_args();
        Cli::try_parse_args_from(&args).map_err(|e| format!("Failed to parse reth CLI args: {e}"))
    }
    /// Convert to reth CLI args
    pub fn to_cli_args(&self) -> Vec<String> {
        let mut args = vec!["reth".to_string()];
        // Add node subcommand
        args.push("node".to_string());
        // Add --quiet if logging is disabled
        if !self.logging {
            println!("adding --quiet");
            args.push("--quiet".to_string());
        }
        // Split space-separated args back into individual arguments
        for arg in &self.cli_args {
            if let Some(space_idx) = arg.find(' ') {
                let (flag, value) = arg.split_at(space_idx);
                args.push(flag.to_string());
                args.push(value.trim().to_string());
            } else {
                args.push(arg.to_string());
            }
        }
        args
    }
}
#[cfg(test)]
mod tests {
    use super::*;
    #[test]
    fn test_deserialize_basic_config() {
        let yaml = r#"
            enabled: true
            cli_args: []
        "#;
        let config: RethConfig = serde_yaml::from_str(yaml).unwrap();
        assert!(config.enabled);
        assert!(config.cli_args.is_empty());
    }
    #[test]
    fn test_deserialize_with_custom_ipc_path() {
        let yaml = r#"
            enabled: true
            cli_args:
                - --ipcpath /custom/path.ipc
        "#;
        let config: RethConfig = serde_yaml::from_str(yaml).unwrap();
        assert!(config.enabled);
        assert_eq!(config.cli_args, vec!["--ipcpath /custom/path.ipc"]);
        // Test that to_cli works and extracts the IPC path
        let cli = config.to_cli().unwrap();
        if let reth::cli::Commands::Node(node_cmd) = &cli.command {
            assert_eq!(node_cmd.rpc.ipcpath, "/custom/path.ipc");
        } else {
            panic!("Expected node command");
        }
    }
    #[test]
    fn test_deserialize_with_http_enabled() {
        let yaml = r#"
            enabled: true
            cli_args:
                - --http
                - --http.addr 0.0.0.0
        "#;
        let config: RethConfig = serde_yaml::from_str(yaml).unwrap();
        assert!(config.enabled);
        assert_eq!(config.cli_args, vec!["--http", "--http.addr 0.0.0.0"]);
        // Build CLI to verify HTTP settings
        let cli = config.to_cli().unwrap();
        if let reth::cli::Commands::Node(node_cmd) = &cli.command {
            assert!(node_cmd.rpc.http);
            assert_eq!(node_cmd.rpc.http_addr.to_string(), "0.0.0.0");
        } else {
            panic!("Expected node command");
        }
    }
    #[test]
    fn test_ipc_disabled() {
        let yaml = r#"
            enabled: true
            cli_args:
                - --ipcdisable
        "#;
        let config: RethConfig = serde_yaml::from_str(yaml).unwrap();
        assert!(config.enabled);
        assert_eq!(config.cli_args, vec!["--ipcdisable"]);
        // Should parse successfully but IPC should be disabled
        let cli = config.to_cli().unwrap();
        if let reth::cli::Commands::Node(node_cmd) = &cli.command {
            assert!(node_cmd.rpc.ipcdisable);
        } else {
            panic!("Expected node command");
        }
    }
    #[test]
    fn test_serialize_includes_config() {
        let yaml = r#"
            enabled: true
            cli_args:
                - --http
        "#;
        let config: RethConfig = serde_yaml::from_str(yaml).unwrap();
        let serialized = serde_yaml::to_string(&config).unwrap();
        // Should contain both fields
        assert!(serialized.contains("enabled: true"));
        assert!(serialized.contains("cli_args:"));
        assert!(serialized.contains("--http"));
    }
    #[test]
    fn test_complex_cli_args() {
        let yaml = r#"
            enabled: true
            cli_args:
                - --authrpc.jwtsecret /Users/skanda/secrets/jwt.hex
                - --authrpc.addr 127.0.0.1
                - --authrpc.port 8551
                - --datadir /Volumes/T9/reth
                - --metrics 127.0.0.1:9001
                - --chain sepolia
                - --http
        "#;
        let config: RethConfig = serde_yaml::from_str(yaml).unwrap();
        assert!(config.enabled);
        assert_eq!(config.cli_args.len(), 7);
        assert!(config
            .cli_args
            .contains(&"--authrpc.jwtsecret /Users/skanda/secrets/jwt.hex".to_string()));
        assert!(config.cli_args.contains(&"--http".to_string()));
        // Test that to_cli_args produces correct output
        let args = config.to_cli_args();
        assert!(args.contains(&"--authrpc.jwtsecret".to_string()));
        assert!(args.contains(&"/Users/skanda/secrets/jwt.hex".to_string()));
        assert!(args.contains(&"--http".to_string()));
    }
    #[test]
    fn test_from_cli_args() {
        let args = vec![
            "--http".to_string(),
            "--datadir".to_string(),
            "/path/to/data".to_string(),
            "--chain".to_string(),
            "sepolia".to_string(),
            "--ipcdisable".to_string(),
        ];
        let config = RethConfig::from_cli_args(args).unwrap();
        assert!(config.enabled);
        assert_eq!(
            config.cli_args,
            vec!["--http", "--datadir /path/to/data", "--chain sepolia", "--ipcdisable"]
        );
        // Verify it can be converted back to CLI
        let cli = config.to_cli().unwrap();
        assert!(matches!(cli.command, reth::cli::Commands::Node(_)));
    }
    #[test]
    fn test_logging_config() {
        // Test with logging enabled (default)
        let yaml = r#"
            enabled: true
            cli_args:
                - --http
        "#;
        let config: RethConfig = serde_yaml::from_str(yaml).unwrap();
        assert!(config.enabled);
        assert!(config.logging); // Should default to true
        let args = config.to_cli_args();
        assert!(!args.contains(&"--quiet".to_string()));
        // Test with logging disabled
        let yaml = r#"
            enabled: true
            logging: false
            cli_args:
                - --http
        "#;
        let config: RethConfig = serde_yaml::from_str(yaml).unwrap();
        assert!(config.enabled);
        assert!(!config.logging);
        let args = config.to_cli_args();
        assert!(args.contains(&"--quiet".to_string()));
        // --quiet should come before other args
        let quiet_index = args.iter().position(|arg| arg == "--quiet").unwrap();
        let http_index = args.iter().position(|arg| arg == "--http").unwrap();
        assert!(quiet_index < http_index);
    }
}
</file>

<file path="core/src/manifest/storage.rs">
use std::path::Path;
use serde::de::Error;
use serde::{Deserialize, Deserializer, Serialize};
use tracing::info;
use crate::{
    database::postgres::{
        indexes::{
            drop_last_known_indexes, prepare_indexes, DropLastKnownIndexesError,
            PostgresIndexResult, PrepareIndexesError,
        },
        relationship::{
            create_relationships, drop_last_known_relationships, CreateRelationshipError,
            DropLastKnownRelationshipsError, Relationship,
        },
    },
    manifest::contract::Contract,
};
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ForeignKey {
    pub contract_name: String,
    pub event_name: String,
    pub event_input_name: String,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ForeignKeys {
    pub contract_name: String,
    pub event_name: String,
    pub event_input_name: String,
    #[serde(rename = "linked_to")]
    pub foreign_keys: Vec<ForeignKey>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct EventIndex {
    pub event_input_names: Vec<String>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ContractEventsIndexes {
    pub name: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub injected_parameters: Option<Vec<String>>,
    pub events: Vec<EventIndexes>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct EventIndexes {
    pub name: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub injected_parameters: Option<Vec<String>>,
    pub indexes: Vec<EventIndex>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct PostgresIndexes {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub global_injected_parameters: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub contracts: Option<Vec<ContractEventsIndexes>>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct PostgresDetails {
    pub enabled: bool,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop_each_run: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub relationships: Option<Vec<ForeignKeys>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub indexes: Option<PostgresIndexes>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub disable_create_tables: Option<bool>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ClickhouseDetails {
    pub enabled: bool,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub drop_each_run: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub disable_create_tables: Option<bool>,
}
fn default_csv_path() -> String {
    "./generated_csv".to_string()
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CsvDetails {
    pub enabled: bool,
    #[serde(default = "default_csv_path")]
    pub path: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub disable_create_headers: Option<bool>,
}
#[derive(Debug, Serialize, Default, Clone)]
pub struct Storage {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub postgres: Option<PostgresDetails>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub clickhouse: Option<ClickhouseDetails>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub csv: Option<CsvDetails>,
}
impl<'de> Deserialize<'de> for Storage {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        #[derive(Deserialize)]
        struct StorageRaw {
            #[serde(default)]
            postgres: Option<PostgresDetails>,
            #[serde(default)]
            clickhouse: Option<ClickhouseDetails>,
            #[serde(default)]
            csv: Option<CsvDetails>,
        }
        let raw = StorageRaw::deserialize(deserializer)?;
        if raw.postgres.is_some() && raw.clickhouse.is_some() {
            return Err(Error::custom(
                "cannot specify both `postgres` and `clickhouse` at the same time",
            ));
        }
        Ok(Storage { postgres: raw.postgres, clickhouse: raw.clickhouse, csv: raw.csv })
    }
}
#[derive(thiserror::Error, Debug)]
pub enum RelationshipsAndIndexersError {
    #[error("{0}")]
    DropLastKnownRelationshipsError(#[from] DropLastKnownRelationshipsError),
    #[error("Yaml relationship error: {0}")]
    RelationshipError(#[from] CreateRelationshipError),
    #[error("{0}")]
    DropLastKnownIndexesError(#[from] DropLastKnownIndexesError),
    #[error("Could not prepare and drop indexes: {0}")]
    FailedToPrepareAndDropIndexes(#[from] PrepareIndexesError),
}
impl Storage {
    pub fn postgres_enabled(&self) -> bool {
        match &self.postgres {
            Some(details) => details.enabled,
            None => false,
        }
    }
    pub fn postgres_disable_create_tables(&self) -> bool {
        let enabled = self.postgres_enabled();
        if !enabled {
            return true;
        }
        self.postgres
            .as_ref()
            .is_some_and(|details| details.disable_create_tables.unwrap_or_default())
    }
    pub fn postgres_drop_each_run(&self) -> bool {
        let enabled = self.postgres_enabled();
        if !enabled {
            return false;
        }
        self.postgres.as_ref().is_some_and(|details| details.drop_each_run.unwrap_or_default())
    }
    pub fn clickhouse_enabled(&self) -> bool {
        match &self.clickhouse {
            Some(details) => details.enabled,
            None => false,
        }
    }
    pub fn clickhouse_disable_create_tables(&self) -> bool {
        if !self.clickhouse_enabled() {
            return true;
        }
        self.clickhouse
            .as_ref()
            .is_some_and(|details| details.disable_create_tables.unwrap_or_default())
    }
    pub fn clickhouse_drop_each_run(&self) -> bool {
        if !self.clickhouse_enabled() {
            return false;
        }
        self.clickhouse.as_ref().is_some_and(|details| details.drop_each_run.unwrap_or_default())
    }
    pub fn csv_enabled(&self) -> bool {
        match &self.csv {
            Some(details) => details.enabled,
            None => false,
        }
    }
    pub fn csv_disable_create_headers(&self) -> bool {
        let enabled = self.csv_enabled();
        if !enabled {
            return true;
        }
        self.csv.as_ref().is_some_and(|details| details.disable_create_headers.unwrap_or_default())
    }
    pub async fn create_relationships_and_indexes(
        &self,
        project_path: &Path,
        manifest_name: &str,
        contracts: &[Contract],
    ) -> Result<(Vec<Relationship>, Vec<PostgresIndexResult>), RelationshipsAndIndexersError> {
        if self.postgres_enabled() && !self.postgres_disable_create_tables() {
            if let Some(storage) = &self.postgres {
                // setup relationships
                let mut relationships: Vec<Relationship> = vec![];
                // setup postgres indexes
                let mut postgres_indexes: Vec<PostgresIndexResult> = vec![];
                info!("Temp dropping constraints relationships from the database for historic indexing for speed reasons");
                drop_last_known_relationships(manifest_name).await?;
                let mapped_relationships = &storage.relationships;
                if let Some(mapped_relationships) = mapped_relationships {
                    let relationships_result = create_relationships(
                        project_path,
                        manifest_name,
                        contracts,
                        mapped_relationships,
                    )
                    .await;
                    match relationships_result {
                        Ok(result) => {
                            relationships = result;
                        }
                        Err(e) => {
                            return Err(RelationshipsAndIndexersError::RelationshipError(e));
                        }
                    }
                }
                info!("Temp dropping indexes from the database for historic indexing for speed reasons");
                drop_last_known_indexes(manifest_name).await?;
                if let Some(indexes) = &storage.indexes {
                    let indexes_result =
                        prepare_indexes(project_path, manifest_name, indexes, contracts).await;
                    match indexes_result {
                        Ok(result) => {
                            postgres_indexes = result;
                        }
                        Err(e) => {
                            return Err(
                                RelationshipsAndIndexersError::FailedToPrepareAndDropIndexes(e),
                            );
                        }
                    }
                }
                return Ok((relationships, postgres_indexes));
            }
        }
        Ok((vec![], vec![]))
    }
}
</file>

<file path="core/src/manifest/stream.rs">
use std::path::Path;
use lapin::ExchangeKind;
use serde::{Deserialize, Deserializer, Serialize};
use serde_json::{Map, Value};
use tokio::fs;
use crate::types::aws_config::AwsConfig;
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct StreamEvent {
    pub event_name: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Map<String, Value>>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub alias: Option<String>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct SNSStreamTopicConfig {
    pub prefix_id: Option<String>,
    pub topic_arn: String,
    pub networks: Vec<String>,
    #[serde(default)]
    pub events: Vec<StreamEvent>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct SNSStreamConfig {
    pub aws_config: AwsConfig,
    pub topics: Vec<SNSStreamTopicConfig>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct WebhookStreamConfig {
    pub endpoint: String,
    pub shared_secret: String,
    pub networks: Vec<String>,
    #[serde(default)]
    pub events: Vec<StreamEvent>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RedisStreamConfig {
    pub connection_uri: String,
    #[serde(default = "default_pool_size")]
    pub max_pool_size: u32,
    pub streams: Vec<RedisStreamStreamConfig>,
}
fn default_pool_size() -> u32 {
    50
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RedisStreamStreamConfig {
    pub stream_name: String,
    pub networks: Vec<String>,
    #[serde(default)]
    pub events: Vec<StreamEvent>,
}
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct ExchangeKindWrapper(pub ExchangeKind);
impl Serialize for ExchangeKindWrapper {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        let kind = match &self.0 {
            ExchangeKind::Direct => "direct",
            ExchangeKind::Fanout => "fanout",
            ExchangeKind::Headers => "headers",
            ExchangeKind::Topic => "topic",
            ExchangeKind::Custom(s) => s,
        };
        serializer.serialize_str(kind)
    }
}
impl<'de> Deserialize<'de> for ExchangeKindWrapper {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        let s: String = Deserialize::deserialize(deserializer)?;
        let kind = match s.to_lowercase().as_str() {
            "direct" => ExchangeKind::Direct,
            "fanout" => ExchangeKind::Fanout,
            "headers" => ExchangeKind::Headers,
            "topic" => ExchangeKind::Topic,
            _ => ExchangeKind::Custom(s),
        };
        Ok(ExchangeKindWrapper(kind))
    }
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RabbitMQStreamQueueConfig {
    pub exchange: String,
    pub exchange_type: ExchangeKindWrapper,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub routing_key: Option<String>,
    pub networks: Vec<String>,
    #[serde(default)]
    pub events: Vec<StreamEvent>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RabbitMQStreamConfig {
    pub url: String,
    pub exchanges: Vec<RabbitMQStreamQueueConfig>,
}
impl RabbitMQStreamConfig {
    pub fn validate(&self) -> Result<(), String> {
        if self.exchanges.is_empty() {
            return Err("No exchanges defined in RabbitMQ config".to_string());
        }
        for config in &self.exchanges {
            if config.exchange_type.0 != ExchangeKind::Direct
                && config.exchange_type.0 != ExchangeKind::Fanout
                && config.exchange_type.0 != ExchangeKind::Topic
            {
                return Err("Only direct, topic and fanout exchanges are supported".to_string());
            }
            if config.exchange_type.0 == ExchangeKind::Fanout && config.routing_key.is_some() {
                return Err("Fanout exchanges do not support routing keys".to_string());
            }
            if config.exchange_type.0 == ExchangeKind::Topic && config.routing_key.is_none() {
                return Err("Topic exchanges require a routing key".to_string());
            }
            if config.exchange_type.0 == ExchangeKind::Direct && config.routing_key.is_none() {
                return Err("Direct exchanges require a routing keys".to_string());
            }
        }
        Ok(())
    }
}
#[cfg(feature = "kafka")]
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct KafkaStreamQueueConfig {
    pub topic: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    pub networks: Vec<String>,
    #[serde(default)]
    pub events: Vec<StreamEvent>,
}
#[cfg(feature = "kafka")]
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct KafkaStreamConfig {
    pub brokers: Vec<String>,
    pub security_protocol: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sasl_mechanisms: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sasl_username: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sasl_password: Option<String>,
    pub acks: String,
    pub topics: Vec<KafkaStreamQueueConfig>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CloudflareQueuesStreamQueueConfig {
    pub queue_id: String,
    pub networks: Vec<String>,
    #[serde(default)]
    pub events: Vec<StreamEvent>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CloudflareQueuesStreamConfig {
    pub api_token: String,
    pub account_id: String,
    pub queues: Vec<CloudflareQueuesStreamQueueConfig>,
}
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct StreamsConfig {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sns: Option<SNSStreamConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub webhooks: Option<Vec<WebhookStreamConfig>>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub rabbitmq: Option<RabbitMQStreamConfig>,
    #[cfg(feature = "kafka")]
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub kafka: Option<KafkaStreamConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub redis: Option<RedisStreamConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cloudflare_queues: Option<CloudflareQueuesStreamConfig>,
}
impl StreamsConfig {
    pub fn validate(&self) -> Result<(), String> {
        if let Some(rabbitmq) = &self.rabbitmq {
            return rabbitmq.validate();
        }
        Ok(())
    }
    pub fn get_streams_last_synced_block_path(&self) -> String {
        let mut path = ".rindexer/".to_string();
        #[allow(clippy::blocks_in_conditions)]
        if self.rabbitmq.is_some() {
            path.push_str("rabbitmq_");
        } else if self.sns.is_some() {
            path.push_str("sns_");
        } else if self.webhooks.is_some() {
            path.push_str("webhooks_");
        } else if {
            #[cfg(feature = "kafka")]
            {
                self.kafka.is_some()
            }
            #[cfg(not(feature = "kafka"))]
            {
                false
            }
        } {
            path.push_str("kafka_");
        } else if self.redis.is_some() {
            path.push_str("redis_");
        } else if self.cloudflare_queues.is_some() {
            path.push_str("cloudflare_queues_");
        }
        path.trim_end_matches('_').to_string()
    }
    pub async fn create_full_streams_last_synced_block_path(
        &self,
        project_path: &Path,
        contract_name: &str,
    ) {
        let streams_last_synced_block_path = self.get_streams_last_synced_block_path();
        let base_path = Path::new(&streams_last_synced_block_path);
        let path = base_path.join(contract_name).join("last-synced-blocks");
        let full_path = project_path.join(path);
        if !full_path.exists() {
            fs::create_dir_all(&full_path).await.expect("Failed to create directory for stream");
        }
    }
}
</file>

<file path="core/src/manifest/yaml.rs">
use std::{
    collections::HashSet,
    env,
    fs::File,
    io::{Read, Write},
    path::{Path, PathBuf},
};
use alloy::rpc::types::ValueOrArray;
use regex::{Captures, Regex};
use serde::{Deserialize, Serialize};
use tracing::error;
use crate::{
    abi::ABIItem,
    helpers::{load_env_from_full_path, replace_env_variable_to_raw_name},
    manifest::{
        core::{Manifest, ProjectType},
        network::Network,
    },
    StringOrArray,
};
pub const YAML_CONFIG_NAME: &str = "rindexer.yaml";
fn substitute_env_variables(contents: &str) -> Result<String, regex::Error> {
    let re = Regex::new(r"\$\{([^}]+)\}")?;
    let result = re.replace_all(contents, |caps: &Captures| {
        let var_name = &caps[1];
        match env::var(var_name) {
            Ok(val) => val,
            Err(_) => {
                error!("Environment variable {} not found", var_name);
                panic!("Environment variable {var_name} not found")
            }
        }
    });
    Ok(result.into_owned())
}
#[derive(thiserror::Error, Debug)]
pub enum ValidateManifestError {
    #[error("Contract names {0} must be unique")]
    ContractNameMustBeUnique(String),
    #[error("Contract name {0} can not include 'Filter' in the name as it is a reserved word")]
    ContractNameCanNotIncludeFilter(String),
    #[error("Invalid network mapped to contract: network - {0} contract - {1}")]
    InvalidNetworkMappedToContract(String, String),
    #[error("Invalid filter event name {0} for contract {1} does not exist in ABI")]
    InvalidFilterEventNameDoesntExistInABI(String, String),
    #[error("Could not read or parse ABI for contract {0} with path {1}")]
    InvalidABI(String, String),
    #[error("Event {0} included in include_events for contract {1} but not found in ABI - it must be an event type and match the name exactly")]
    EventIncludedNotFoundInABI(String, String),
    #[error("Event {0} not found in ABI for contract {1}")]
    IndexedFilterEventNotFoundInABI(String, String),
    #[error("Indexed filter defined more than allowed for event {0} for contract {1} - indexed expected: {2} defined: {3}")]
    IndexedFilterDefinedMoreThanAllowed(String, String, usize, usize),
    #[error("Relationship contract {0} not found")]
    RelationshipContractNotFound(String),
    #[error("Relationship foreign key contract {0} not found")]
    RelationshipForeignKeyContractNotFound(String),
    #[error("Streams config is invalid: {0}")]
    StreamsConfigValidationError(String),
    #[error("Global ABI can only be a single string")]
    GlobalAbiCanOnlyBeASingleString(String),
}
fn validate_manifest(
    project_path: &Path,
    manifest: &Manifest,
) -> Result<(), ValidateManifestError> {
    let mut seen = HashSet::new();
    let duplicates_contract_names: Vec<String> = manifest
        .contracts
        .iter()
        .filter_map(|c| if seen.insert(&c.name) { None } else { Some(c.name.clone()) })
        .collect();
    if !duplicates_contract_names.is_empty() {
        return Err(ValidateManifestError::ContractNameMustBeUnique(
            duplicates_contract_names.join(", "),
        ));
    }
    for contract in &manifest.all_contracts() {
        if contract.name.to_lowercase().contains("filter") {
            return Err(ValidateManifestError::ContractNameCanNotIncludeFilter(
                contract.name.clone(),
            ));
        }
        let events = ABIItem::read_abi_items(project_path, contract)
            .map_err(|e| ValidateManifestError::InvalidABI(contract.name.clone(), e.to_string()))?;
        for detail in &contract.details {
            let has_network = manifest.networks.iter().any(|n| n.name == detail.network);
            if !has_network {
                return Err(ValidateManifestError::InvalidNetworkMappedToContract(
                    detail.network.clone(),
                    contract.name.clone(),
                ));
            }
            if let Some(filter_details) = &detail.filter {
                match filter_details {
                    ValueOrArray::Value(filter_details) => {
                        if !events.iter().any(|e| e.name == *filter_details.event_name) {
                            return Err(
                                ValidateManifestError::InvalidFilterEventNameDoesntExistInABI(
                                    filter_details.event_name.clone(),
                                    contract.name.clone(),
                                ),
                            );
                        }
                    }
                    ValueOrArray::Array(filters) => {
                        for filter_details in filters {
                            if !events.iter().any(|e| e.name == *filter_details.event_name) {
                                return Err(
                                    ValidateManifestError::InvalidFilterEventNameDoesntExistInABI(
                                        filter_details.event_name.clone(),
                                        contract.name.clone(),
                                    ),
                                );
                            }
                        }
                    }
                }
            }
            if let Some(indexed_filters) = &detail.indexed_filters {
                for indexed_filter in indexed_filters.iter() {
                    let event = events.iter().find(|e| e.name == indexed_filter.event_name);
                    if let Some(event) = event {
                        let indexed_allowed_length =
                            event.inputs.iter().filter(|i| i.indexed.unwrap_or(false)).count();
                        let indexed_filter_defined =
                            indexed_filter.indexed_1.as_ref().map_or(0, |_| 1)
                                + indexed_filter.indexed_2.as_ref().map_or(0, |_| 1)
                                + indexed_filter.indexed_3.as_ref().map_or(0, |_| 1);
                        if indexed_filter_defined > indexed_allowed_length {
                            return Err(
                                ValidateManifestError::IndexedFilterDefinedMoreThanAllowed(
                                    indexed_filter.event_name.clone(),
                                    contract.name.clone(),
                                    indexed_allowed_length,
                                    indexed_filter_defined,
                                ),
                            );
                        }
                    } else {
                        return Err(ValidateManifestError::IndexedFilterEventNotFoundInABI(
                            indexed_filter.event_name.clone(),
                            contract.name.clone(),
                        ));
                    }
                }
            }
        }
        if let Some(include_events) = &contract.include_events {
            for event in include_events {
                if !events.iter().any(|e| e.name == *event.name && e.type_ == "event") {
                    return Err(ValidateManifestError::EventIncludedNotFoundInABI(
                        event.name.clone(),
                        contract.name.clone(),
                    ));
                }
            }
        }
        if let Some(_dependency_events) = &contract.dependency_events {
            // TODO - validate the events all exist in the contract ABIs
        }
        if let Some(streams) = &contract.streams {
            if let Err(e) = streams.validate() {
                return Err(ValidateManifestError::StreamsConfigValidationError(e));
            }
        }
    }
    if let Some(postgres) = &manifest.storage.postgres {
        if let Some(relationships) = &postgres.relationships {
            for relationship in relationships {
                if !manifest.all_contracts().iter().any(|c| c.name == relationship.contract_name) {
                    return Err(ValidateManifestError::RelationshipContractNotFound(
                        relationship.contract_name.clone(),
                    ));
                }
                for foreign_key in &relationship.foreign_keys {
                    if !manifest.all_contracts().iter().any(|c| c.name == foreign_key.contract_name)
                    {
                        return Err(ValidateManifestError::RelationshipForeignKeyContractNotFound(
                            foreign_key.contract_name.clone(),
                        ));
                    }
                }
                // TODO - Add validation for the event names and event inputs match the ABIs
            }
        }
    }
    if let Some(contracts) = &manifest.global.contracts {
        for contract in contracts {
            match &contract.abi {
                StringOrArray::Single(_) => {}
                StringOrArray::Multiple(value) => {
                    return Err(ValidateManifestError::GlobalAbiCanOnlyBeASingleString(format!(
                        "Global ABI can only be a single string but found multiple: {value:?}"
                    )));
                }
            }
        }
    }
    Ok(())
}
#[derive(thiserror::Error, Debug)]
pub enum ReadManifestError {
    #[error("Could not open file: {0}")]
    CouldNotOpenFile(#[from] std::io::Error),
    #[error("Could not parse manifest: {0}")]
    CouldNotParseManifest(#[from] serde_yaml::Error),
    #[error("Could not substitute env variables: {0}")]
    CouldNotSubstituteEnvVariables(#[from] regex::Error),
    #[error("Could not validate manifest: {0}")]
    CouldNotValidateManifest(#[from] ValidateManifestError),
    #[error("No project path found using parent of manifest path")]
    NoProjectPathFoundUsingParentOfManifestPath,
}
pub fn read_manifest_raw(file_path: &PathBuf) -> Result<Manifest, ReadManifestError> {
    let mut file = File::open(file_path)?;
    let mut contents = String::new();
    file.read_to_string(&mut contents)?;
    let manifest: Manifest = serde_yaml::from_str(&contents)?;
    let project_path = file_path.parent();
    match project_path {
        None => Err(ReadManifestError::NoProjectPathFoundUsingParentOfManifestPath),
        Some(project_path) => {
            validate_manifest(project_path, &manifest)?;
            Ok(manifest)
        }
    }
}
#[derive(Debug, Serialize, Deserialize, Clone)]
struct ManifestNetworksOnly {
    pub networks: Vec<Network>,
}
fn extract_environment_path(contents: &str, file_path: &Path) -> Option<PathBuf> {
    let re = Regex::new(r"(?m)^environment_path:\s*(.+)$").unwrap();
    re.captures(contents).and_then(|cap| cap.get(1)).map(|m| {
        let path_str = m.as_str().trim().replace('\"', ""); // Remove any quotes
        let base_dir = file_path.parent().unwrap_or(Path::new(""));
        let full_path = base_dir.join(path_str);
        full_path.canonicalize().unwrap_or(full_path)
    })
}
pub fn read_manifest(file_path: &PathBuf) -> Result<Manifest, ReadManifestError> {
    let mut file = File::open(file_path)?;
    let mut contents = String::new();
    file.read_to_string(&mut contents)?;
    let environment_path = extract_environment_path(&contents, file_path);
    if let Some(ref path) = environment_path {
        load_env_from_full_path(path);
    }
    let contents_before_transform = contents.clone();
    contents = substitute_env_variables(&contents)?;
    let mut manifest_after_transform: Manifest = serde_yaml::from_str(&contents)?;
    // Assign networks to the Native Transfer if opted into without defining networks.
    // We treat None as "All available".
    manifest_after_transform.set_native_transfer_networks();
    // as we don't want to inject the RPC URL in rust projects in clear text we should change
    // the networks.rpc back to what it was before and the generated code will handle it
    if manifest_after_transform.project_type == ProjectType::Rust {
        let manifest_networks_only: ManifestNetworksOnly =
            serde_yaml::from_str(&contents_before_transform)?;
        for network in &mut manifest_after_transform.networks {
            network.rpc = manifest_networks_only
                .networks
                .iter()
                .find(|n| n.name == network.name)
                .map_or_else(
                    || replace_env_variable_to_raw_name(&network.rpc),
                    |n| replace_env_variable_to_raw_name(&n.rpc),
                );
        }
    }
    let project_path = file_path.parent();
    match project_path {
        None => Err(ReadManifestError::NoProjectPathFoundUsingParentOfManifestPath),
        Some(project_path) => {
            validate_manifest(project_path, &manifest_after_transform)?;
            Ok(manifest_after_transform)
        }
    }
}
#[derive(thiserror::Error, Debug)]
pub enum WriteManifestError {
    #[error("Could not open file: {0}")]
    CouldNotOpenFile(std::io::Error),
    #[error("Could not parse manifest to string: {0}")]
    CouldNotTurnManifestToString(serde_yaml::Error),
    #[error("Could not create file: {0}")]
    CouldNotCreateFile(std::io::Error),
    #[error("Could not write to file: {0}")]
    CouldNotWriteToFile(std::io::Error),
}
pub fn write_manifest(data: &Manifest, file_path: &PathBuf) -> Result<(), WriteManifestError> {
    let yaml_string =
        serde_yaml::to_string(data).map_err(WriteManifestError::CouldNotTurnManifestToString)?;
    let mut file = File::create(file_path).map_err(WriteManifestError::CouldNotCreateFile)?;
    file.write_all(yaml_string.as_bytes()).map_err(WriteManifestError::CouldNotWriteToFile)?;
    Ok(())
}
</file>

<file path="core/src/phantom/common.rs">
use std::{error::Error, fs::File, io::Read, path::Path};
use alloy::json_abi::JsonAbi;
use serde::Deserialize;
#[derive(Deserialize, Debug)]
pub struct CloneMeta {
    pub path: String,
    #[serde(rename = "targetContract")]
    pub target_contract: String,
    pub address: String,
    #[serde(rename = "constructorArguments")]
    pub constructor_arguments: String,
}
impl CloneMeta {
    fn get_out_contract_sol_from_path(&self) -> String {
        self.path.split('/').next_back().unwrap_or_default().to_string()
    }
}
pub fn read_contract_clone_metadata(contract_path: &Path) -> Result<CloneMeta, Box<dyn Error>> {
    let meta_file_path = contract_path.join(".clone.meta");
    let mut file = File::open(meta_file_path)?;
    let mut contents = String::new();
    file.read_to_string(&mut contents)?;
    let clone_meta: CloneMeta = serde_json::from_str(&contents)?;
    Ok(clone_meta)
}
#[derive(Deserialize, Debug)]
pub struct Bytecode {
    pub object: String,
}
#[derive(Deserialize, Debug)]
pub struct CompiledContract {
    pub abi: JsonAbi,
    pub bytecode: Bytecode,
}
pub fn read_compiled_contract(
    contract_path: &Path,
    clone_meta: &CloneMeta,
) -> Result<CompiledContract, Box<dyn Error>> {
    let compiled_file_path = contract_path.join("out").join(format!(
        "{}/{}.json",
        clone_meta.get_out_contract_sol_from_path(),
        clone_meta.target_contract
    ));
    let mut file = File::open(compiled_file_path)?;
    let mut contents = String::new();
    file.read_to_string(&mut contents)?;
    let compiled_contract: CompiledContract = serde_json::from_str(&contents)?;
    Ok(compiled_contract)
}
</file>

<file path="core/src/phantom/dyrpc.rs">
use std::error::Error;
use regex::Regex;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use crate::phantom::common::{CloneMeta, CompiledContract};
#[derive(Serialize, Debug)]
struct DeployDyrpcRequest<'a> {
    overlays: std::collections::HashMap<&'a str, DeployDyrpcDetails<'a>>,
}
#[derive(Serialize, Debug)]
struct DeployDyrpcDetails<'a> {
    #[serde(rename = "creationCode")]
    creation_code: &'a str,
    #[serde(rename = "constructorArgs")]
    constructor_args: &'a str,
}
#[derive(Deserialize)]
pub struct DeployDyrpcContractResponse {
    // don't care for now about these
    // pub message: String,
    //
    // #[serde(rename = "overlayHash")]
    // pub overlay_hash: String,
    //
    // pub addresses: Vec<String>,
    #[serde(rename = "overlayRpcUrl")]
    pub rpc_url: String,
}
#[derive(thiserror::Error, Debug)]
pub enum CreateDyrpcError {
    #[error("Failed to deploy dyRPC: {0}")]
    FailedToDeployContract(String, String),
    #[error("dyRPC response is not json: {0}")]
    ResponseNotJson(reqwest::Error),
    #[error("dyRPC api failed: {0}")]
    ApiFailed(reqwest::Error),
}
pub async fn create_dyrpc_api_key() -> Result<String, CreateDyrpcError> {
    let client = Client::new();
    let response = client
        .post("https://api.dyrpc.network/generate")
        .send()
        .await
        .map_err(CreateDyrpcError::ApiFailed)?;
    if response.status().is_success() {
        let api_key = response.text().await.map_err(CreateDyrpcError::ResponseNotJson)?;
        Ok(api_key)
    } else {
        Err(CreateDyrpcError::FailedToDeployContract(
            response.status().to_string(),
            response.text().await.unwrap_or_default(),
        ))
    }
}
pub async fn deploy_dyrpc_contract(
    api_key: &str,
    clone_meta: &CloneMeta,
    compiled_contract: &CompiledContract,
) -> Result<String, Box<dyn Error>> {
    let result = deploy_contract(
        &clone_meta.address,
        api_key,
        &compiled_contract.bytecode.object,
        &clone_meta.constructor_arguments,
    )
    .await?;
    let re = Regex::new(r"/eth/([a-fA-F0-9]{64})/").unwrap();
    let rpc_url = re
        .replace(&result.rpc_url, "/eth/{RINDEXER_PHANTOM_API_KEY}/")
        .to_string()
        .replace("{RINDEXER_PHANTOM_API_KEY}", "${RINDEXER_PHANTOM_API_KEY}");
    Ok(rpc_url)
}
async fn deploy_contract(
    address: &str,
    api_key: &str,
    new_bytecode: &str,
    constructor_args_bytecode: &str,
) -> Result<DeployDyrpcContractResponse, CreateDyrpcError> {
    let url = format!("https://node.dyrpc.network/eth/{api_key}/overlay/put");
    let mut overlays = std::collections::HashMap::new();
    overlays.insert(
        address,
        DeployDyrpcDetails {
            creation_code: new_bytecode,
            constructor_args: constructor_args_bytecode,
        },
    );
    let request_body = DeployDyrpcRequest { overlays };
    let client = Client::new();
    let response = client
        .put(&url)
        .header("Content-Type", "application/json")
        .json(&request_body)
        .send()
        .await
        .map_err(CreateDyrpcError::ApiFailed)?;
    if response.status().is_success() {
        let overlay_response: DeployDyrpcContractResponse =
            response.json().await.map_err(CreateDyrpcError::ResponseNotJson)?;
        Ok(overlay_response)
    } else {
        Err(CreateDyrpcError::FailedToDeployContract(
            response.status().to_string(),
            response.text().await.unwrap_or_default(),
        ))
    }
}
</file>

<file path="core/src/phantom/mod.rs">
pub mod common;
mod dyrpc;
pub mod shadow;
pub use dyrpc::{create_dyrpc_api_key, deploy_dyrpc_contract, CreateDyrpcError};
</file>

<file path="core/src/phantom/shadow.rs">
use std::{
    collections::BTreeMap,
    path::{Path, PathBuf},
    process::Command,
};
use foundry_compilers::{
    artifacts::{Contract, Contracts, Error, FileToContractsMap, SourceFile},
    CompilerOutput,
};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use serde_json::Value;
use crate::{manifest::phantom::PhantomShadow, phantom::common::CloneMeta};
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Default)]
pub struct ShadowSourceFile {
    id: u32,
}
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Default)]
pub struct ShadowCompilerOutput {
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub errors: Vec<String>,
    #[serde(default)]
    pub sources: BTreeMap<PathBuf, ShadowSourceFile>,
    #[serde(default)]
    pub contracts: Contracts,
}
type CompilerOut = CompilerOutput<Error, Contract>;
impl ShadowCompilerOutput {
    pub fn from_compile_output(output: CompilerOut) -> Self {
        let sources = output
            .sources
            .into_iter()
            .map(|(key, value)| {
                let new_key = PathBuf::from(
                    key.to_str()
                        .expect("path is valid utf8")
                        .split("lib/")
                        .last()
                        .unwrap_or_default()
                        .to_string(),
                );
                let new_value = ShadowSourceFile { id: value.id };
                (new_key, new_value)
            })
            .collect();
        let contracts = output
            .contracts
            .into_iter()
            .map(|(file, contracts)| {
                let new_file = PathBuf::from(
                    file.to_str()
                        .expect("path is valid utf8")
                        .split("lib/")
                        .last()
                        .unwrap_or_default()
                        .to_string(),
                );
                let new_contracts = contracts.into_iter().collect();
                (new_file, new_contracts)
            })
            .collect();
        ShadowCompilerOutput {
            errors: output.errors.into_iter().map(|e| e.message).collect(),
            sources,
            contracts,
        }
    }
}
#[derive(thiserror::Error, Debug)]
pub enum DeployShadowError {
    #[error("Could not run forge build")]
    CouldNotCompileContract,
    #[error("Failed to read format json from forge build")]
    CouldNotReadFormatJson,
    #[error("Invalid compiler output from format json")]
    InvalidCompilerOutputFromFormatJson,
    #[error("Failed to deploy contract: {0}")]
    FailedToDeployContract(String, String),
    #[error("dyRPC response is not json: {0}")]
    ResponseNotJson(reqwest::Error),
    #[error("dyRPC api failed: {0}")]
    ApiFailed(reqwest::Error),
}
pub async fn deploy_shadow_contract(
    api_key: &str,
    deploy_in: &Path,
    clone_meta: &CloneMeta,
    shadow_details: &PhantomShadow,
) -> Result<String, DeployShadowError> {
    let output = Command::new("forge")
        .arg("build")
        .arg("--format-json")
        .arg("--force")
        .current_dir(deploy_in)
        .output()
        .map_err(|_| DeployShadowError::CouldNotCompileContract)?;
    if output.status.success() {
        let stdout_str = std::str::from_utf8(&output.stdout)
            .map_err(|_| DeployShadowError::CouldNotReadFormatJson)?;
        let compiler_output: CompilerOut = forge_to_solc(stdout_str)
            .map_err(|_| DeployShadowError::InvalidCompilerOutputFromFormatJson)?;
        let shadow_compiler_output = ShadowCompilerOutput::from_compile_output(compiler_output);
        deploy_shadow(api_key, clone_meta, shadow_details, shadow_compiler_output).await
    } else {
        Err(DeployShadowError::CouldNotReadFormatJson)
    }
}
fn forge_to_solc(stdout_str: &str) -> Result<CompilerOut, serde_json::Error> {
    let val: Value = serde_json::from_str(stdout_str)?;
    let errors_arr = val["errors"].as_array().unwrap();
    let contract_objs_val = val["contracts"].as_object().unwrap();
    let sources_obj_val = val["sources"].as_object().unwrap();
    let mut contracts: FileToContractsMap<Contract> = BTreeMap::new();
    let mut sources: BTreeMap<PathBuf, SourceFile> = BTreeMap::new();
    let errors = errors_arr
        .iter()
        .map(|e| serde_json::from_value::<Error>(e.clone()).unwrap())
        .collect::<Vec<_>>();
    for (file, value) in contract_objs_val.into_iter() {
        let obj = value.as_object().unwrap();
        let modules = obj.into_iter().collect::<Vec<_>>();
        let mut contracts_map: BTreeMap<String, Contract> = BTreeMap::new();
        for (module_name, contract_objs_wrapper) in modules {
            let contract = &contract_objs_wrapper[0]["contract"];
            let parsed_contract: Contract = serde_json::from_value(contract.clone())?;
            contracts_map.insert(module_name.clone(), parsed_contract);
        }
        let path_file = PathBuf::from(&file);
        contracts.insert(path_file.clone(), contracts_map);
    }
    for (file, value) in sources_obj_val.into_iter() {
        let arr = value.as_array().unwrap();
        let source_file: SourceFile = serde_json::from_value(arr[0]["source_file"].clone())?;
        let path_file = PathBuf::from(&file);
        sources.insert(path_file.clone(), source_file);
    }
    Ok(CompilerOut { errors, sources, contracts, metadata: BTreeMap::new() })
}
#[derive(Serialize, Deserialize, Debug)]
struct ShadowBodyContract {
    address: String,
    #[serde(rename = "compilerOutput")]
    compiler_output: ShadowCompilerOutput,
}
#[derive(Serialize, Deserialize, Debug)]
struct DeployShadowBody {
    #[serde(rename = "shadowedContracts")]
    shadowed_contracts: Vec<ShadowBodyContract>,
}
#[derive(Serialize, Deserialize, Debug)]
#[serde(rename_all = "camelCase")]
pub struct DeployShadowResponse {
    pub fork_id: String,
    pub fork_version: u64,
    pub rpc_url: String,
}
async fn deploy_shadow(
    api_key: &str,
    clone_meta: &CloneMeta,
    shadow_details: &PhantomShadow,
    shadow_compiler_output: ShadowCompilerOutput,
) -> Result<String, DeployShadowError> {
    let client = Client::new();
    let response = client
        .post(format!("https://api.shadow.xyz/v1/{}/deploy", shadow_details.fork_id))
        .header("X-SHADOW-API-KEY", api_key)
        .json(&DeployShadowBody {
            shadowed_contracts: vec![ShadowBodyContract {
                address: clone_meta.address.clone(),
                compiler_output: shadow_compiler_output,
            }],
        })
        .send()
        .await
        .map_err(DeployShadowError::ApiFailed)?;
    if response.status().is_success() {
        let response: DeployShadowResponse =
            response.json().await.map_err(DeployShadowError::ResponseNotJson)?;
        Ok(response.rpc_url)
    } else {
        Err(DeployShadowError::FailedToDeployContract(
            response.status().to_string(),
            response.text().await.unwrap_or_default(),
        ))
    }
}
</file>

<file path="core/src/reth/exex.rs">
use crate::notifications::ChainStateNotification;
use futures::StreamExt;
use reth_exex::{ExExContext, ExExNotification};
use reth_node_api::FullNodeComponents;
use reth_tracing::tracing::info;
use tokio::sync::broadcast;
/// Minimal ExEx that only translates ExExNotifications to ChainStateNotifications
pub struct RindexerExEx<Node: FullNodeComponents> {
    /// The context of the ExEx
    ctx: ExExContext<Node>,
    /// Channel to send chain state notifications
    notification_tx: broadcast::Sender<ChainStateNotification>,
}
impl<Node: FullNodeComponents> RindexerExEx<Node> {
    /// Creates a new instance of the ExEx
    pub fn new(
        ctx: ExExContext<Node>,
        notification_tx: broadcast::Sender<ChainStateNotification>,
    ) -> Self {
        Self { ctx, notification_tx }
    }
    /// Starts listening for ExEx notifications and converts them to ChainStateNotifications
    pub async fn start(mut self) -> eyre::Result<()> {
        info!("Starting RindexerExEx notification translator");
        while let Some(notification) = self.ctx.notifications.next().await {
            let notification = notification?;
            match &notification {
                ExExNotification::ChainCommitted { new } => {
                    let range = new.range();
                    info!(
                        from = range.start(),
                        to = range.end(),
                        "Received chain committed notification"
                    );
                    let chain_notification = ChainStateNotification::Committed {
                        from_block: *range.start(),
                        to_block: *range.end(),
                        tip_hash: new.tip().hash(),
                    };
                    let _ = self.notification_tx.send(chain_notification);
                }
                ExExNotification::ChainReorged { old, new } => {
                    let old_range = old.range();
                    let new_range = new.range();
                    info!(
                        old_from = old_range.start(),
                        old_to = old_range.end(),
                        new_from = new_range.start(),
                        new_to = new_range.end(),
                        "Received chain reorg notification"
                    );
                    let chain_notification = ChainStateNotification::Reorged {
                        revert_from_block: *old_range.start(),
                        revert_to_block: *old_range.end(),
                        new_from_block: *new_range.start(),
                        new_to_block: *new_range.end(),
                        new_tip_hash: new.tip().hash(),
                    };
                    let _ = self.notification_tx.send(chain_notification);
                }
                ExExNotification::ChainReverted { old } => {
                    let range = old.range();
                    info!(
                        from = range.start(),
                        to = range.end(),
                        "Received chain revert notification"
                    );
                    let chain_notification = ChainStateNotification::Reverted {
                        from_block: *range.start(),
                        to_block: *range.end(),
                    };
                    let _ = self.notification_tx.send(chain_notification);
                }
            }
        }
        Ok(())
    }
}
</file>

<file path="core/src/reth/mod.rs">
#![cfg(feature = "reth")]
pub mod exex;
#[cfg(feature = "reth")]
pub mod node;
#[cfg(feature = "reth")]
pub mod utils;
#[cfg(feature = "reth")]
pub use reth::cli::Cli;
</file>

<file path="core/src/reth/node.rs">
use std::thread::Builder;
use futures::FutureExt;
use reth::cli::Cli;
use reth_node_ethereum::EthereumNode;
use tokio::sync::broadcast;
use crate::notifications::ChainStateNotification;
use crate::reth::exex::RindexerExEx;
use broadcast::Sender;
/// The stack size for the Reth node thread.
const STACK_SIZE: usize = 32 * 1024 * 1024; // 32 MB
/// The name of the execution extension.
const EXECUTION_EXTENSION_NAME: &str = "rindexer";
/// Starts a Reth node with the execution extension that forwards chain state notifications to the provided channel.
pub fn start_reth_node_with_exex(cli: Cli) -> eyre::Result<Sender<ChainStateNotification>> {
    // Create a broadcast channel for chain state notifications. Sender will go to ExEx, receiver
    // will be returned to the caller. Buffer size of 1000 to handle bursts.
    let (notification_tx, _notification_rx) = broadcast::channel::<ChainStateNotification>(1000);
    // Clone the sender to return it
    let notification_tx_clone = notification_tx.clone();
    // Spawn the node with a larger stack size, otherwise it will crash with a stack overflow
    let builder = Builder::new().stack_size(STACK_SIZE);
    let _ = builder.spawn(move || {
        let result = cli.run(|builder, _| async move {
            let handle = builder
                .node(EthereumNode::default())
                .install_exex(EXECUTION_EXTENSION_NAME, move |ctx| {
                    tokio::task::spawn_blocking(move || {
                        tokio::runtime::Handle::current().block_on(async move {
                            let exex = RindexerExEx::new(ctx, notification_tx);
                            eyre::Ok(exex.start())
                        })
                    })
                    .map(|result| result.map_err(Into::into).and_then(|result| result))
                })
                .launch()
                .await?;
            handle.wait_for_node_exit().await
        });
        if let Err(e) = result {
            eprintln!("Node thread error: {e:?}");
        }
    });
    Ok(notification_tx_clone)
}
</file>

<file path="core/src/reth/utils.rs">
use std::time::{Duration, Instant};
use crate::provider::RetryClientError;
/// Wait for IPC to be ready with retry logic
pub async fn wait_for_ipc_ready(ipc_path: &str, timeout: Duration) -> Result<(), RetryClientError> {
    let start = Instant::now();
    while start.elapsed() < timeout {
        #[cfg(unix)]
        if std::path::Path::new(ipc_path).exists() {
            // Socket file exists, assume it's ready
            return Ok(());
        }
        #[cfg(windows)]
        {
            // On Windows, we can't easily check if the named pipe exists
            // Just wait a bit and assume it's ready after some time
            if start.elapsed() > Duration::from_secs(2) {
                return Ok(());
            }
        }
        tokio::time::sleep(Duration::from_millis(100)).await;
    }
    Err(RetryClientError::ProviderCantBeCreated(
        ipc_path.to_string(),
        "IPC connection timeout".to_string(),
    ))
}
</file>

<file path="core/src/simple_file_formatters/csv.rs">
use std::{
    fs::File,
    path::{Path, PathBuf},
    sync::Arc,
};
use csv::Reader;
use csv::Writer;
use tokio::sync::Mutex;
pub struct AsyncCsvAppender {
    path: Arc<Path>,
    writer_lock: Arc<Mutex<()>>,
}
impl AsyncCsvAppender {
    pub fn new(file_path: &str) -> Self {
        AsyncCsvAppender {
            path: Arc::from(PathBuf::from(file_path)),
            writer_lock: Arc::new(Mutex::new(())),
        }
    }
    pub async fn append(&self, data: Vec<String>) -> Result<(), csv::Error> {
        let lock = Arc::clone(&self.writer_lock);
        let path = Arc::clone(&self.path);
        tokio::task::spawn_blocking(move || {
            let _guard = lock.lock();
            let file = File::options().create(true).append(true).open(path)?;
            let mut writer = Writer::from_writer(file);
            writer.write_record(data)?;
            Ok(())
        })
        .await
        .expect("Failed to run CSV write operation")
    }
    pub async fn append_bulk(&self, records: Vec<Vec<String>>) -> Result<(), csv::Error> {
        let lock = Arc::clone(&self.writer_lock);
        let path = Arc::clone(&self.path);
        tokio::task::spawn_blocking(move || {
            let _guard = lock.lock();
            let file = File::options().create(true).append(true).open(&path)?;
            let mut writer = Writer::from_writer(file);
            for record in records {
                writer.write_record(record)?;
            }
            Ok(())
        })
        .await
        .expect("Failed to run CSV bulk write operation")
    }
    pub async fn append_header(&self, header: Vec<String>) -> Result<(), csv::Error> {
        let lock = Arc::clone(&self.writer_lock);
        let path = Arc::clone(&self.path);
        tokio::task::spawn_blocking(move || {
            let _guard = lock.lock();
            // Create parent directories if they don't exist
            if let Some(parent) = path.parent() {
                std::fs::create_dir_all(parent).expect("Failed to create directory");
            }
            let file = File::options().create(true).append(true).open(&path)?;
            let mut writer = Writer::from_writer(file);
            writer.write_record(header)?;
            Ok(())
        })
        .await
        .expect("Failed to run CSV write operation")
    }
}
pub struct AsyncCsvReader {
    path: Arc<Path>,
}
impl AsyncCsvReader {
    pub fn new(file_path: &str) -> Self {
        AsyncCsvReader { path: Arc::from(PathBuf::from(file_path)) }
    }
    pub async fn read_all(&self) -> Result<Vec<Vec<String>>, csv::Error> {
        let path = Arc::clone(&self.path);
        tokio::task::spawn_blocking(move || {
            let file = File::open(&path)?;
            let mut reader = Reader::from_reader(file);
            let mut records = Vec::new();
            for result in reader.records() {
                let record = result?;
                records.push(record.iter().map(|s| s.to_string()).collect());
            }
            Ok(records)
        })
        .await
        .expect("Failed to run CSV read operation")
    }
}
</file>

<file path="core/src/simple_file_formatters/mod.rs">
pub mod csv;
</file>

<file path="core/src/streams/clients.rs">
use std::sync::Arc;
use aws_sdk_sns::{config::http::HttpResponse, error::SdkError, operation::publish::PublishError};
use futures::future::join_all;
use serde_json::Value;
use thiserror::Error;
use tokio::{
    task,
    task::{JoinError, JoinHandle},
};
use crate::{
    event::{filter_event_data_by_conditions, EventMessage},
    indexer::native_transfer::EVENT_NAME,
    manifest::stream::{
        CloudflareQueuesStreamConfig, CloudflareQueuesStreamQueueConfig, RabbitMQStreamConfig,
        RabbitMQStreamQueueConfig, RedisStreamConfig, RedisStreamStreamConfig,
        SNSStreamTopicConfig, StreamEvent, StreamsConfig, WebhookStreamConfig,
    },
    streams::{
        CloudflareQueues, CloudflareQueuesError, RabbitMQ, RabbitMQError, Redis, RedisError,
        Webhook, WebhookError, SNS,
    },
};
#[cfg(feature = "kafka")]
use crate::{
    manifest::stream::{KafkaStreamConfig, KafkaStreamQueueConfig},
    streams::kafka::{Kafka, KafkaError},
};
// we should limit the max chunk size we send over when streaming to 70KB - 100KB is most limits
// we can add this to yaml if people need it
const MAX_CHUNK_SIZE: usize = 75 * 1024; // 75 KB
type StreamPublishes = Vec<JoinHandle<Result<usize, StreamError>>>;
#[derive(Debug, Clone)]
struct SNSStream {
    config: Vec<SNSStreamTopicConfig>,
    client: Arc<SNS>,
}
#[allow(clippy::large_enum_variant)]
#[derive(Error, Debug)]
pub enum StreamError {
    #[error("SNS could not publish - {0}")]
    SnsCouldNotPublish(#[from] SdkError<PublishError, HttpResponse>),
    #[error("Webhook could not publish: {0}")]
    WebhookCouldNotPublish(#[from] WebhookError),
    #[error("RabbitMQ could not publish: {0}")]
    RabbitMQCouldNotPublish(#[from] RabbitMQError),
    #[cfg(feature = "kafka")]
    #[error("Kafka could not publish: {0}")]
    KafkaCouldNotPublish(#[from] KafkaError),
    #[error("Redis could not publish: {0}")]
    RedisCouldNotPublish(#[from] RedisError),
    #[error("Cloudflare Queues could not publish: {0}")]
    CloudflareQueuesCouldNotPublish(#[from] CloudflareQueuesError),
    #[error("Task failed: {0}")]
    JoinError(JoinError),
}
#[derive(Debug, Clone)]
struct WebhookStream {
    config: Vec<WebhookStreamConfig>,
    client: Arc<Webhook>,
}
#[derive(Debug)]
pub struct RabbitMQStream {
    config: RabbitMQStreamConfig,
    client: Arc<RabbitMQ>,
}
#[cfg(feature = "kafka")]
#[derive(Debug)]
pub struct KafkaStream {
    config: KafkaStreamConfig,
    client: Arc<Kafka>,
}
#[derive(Debug)]
pub struct RedisStream {
    config: RedisStreamConfig,
    client: Arc<Redis>,
}
#[derive(Debug)]
pub struct CloudflareQueuesStream {
    config: CloudflareQueuesStreamConfig,
    client: Arc<CloudflareQueues>,
}
#[derive(Debug)]
pub struct StreamsClients {
    sns: Option<SNSStream>,
    webhook: Option<WebhookStream>,
    rabbitmq: Option<RabbitMQStream>,
    #[cfg(feature = "kafka")]
    kafka: Option<KafkaStream>,
    redis: Option<RedisStream>,
    cloudflare_queues: Option<CloudflareQueuesStream>,
}
impl StreamsClients {
    pub async fn new(stream_config: StreamsConfig) -> Self {
        #[allow(clippy::manual_map)]
        let sns = if let Some(config) = &stream_config.sns {
            Some(SNSStream {
                config: config.topics.clone(),
                client: Arc::new(SNS::new(&config.aws_config).await),
            })
        } else {
            None
        };
        #[allow(clippy::manual_map)]
        let webhook = stream_config.webhooks.as_ref().map(|config| WebhookStream {
            config: config.clone(),
            client: Arc::new(Webhook::new()),
        });
        #[allow(clippy::manual_map)]
        let rabbitmq = if let Some(config) = stream_config.rabbitmq.as_ref() {
            Some(RabbitMQStream {
                config: config.clone(),
                client: Arc::new(RabbitMQ::new(&config.url).await),
            })
        } else {
            None
        };
        #[cfg(feature = "kafka")]
        #[allow(clippy::manual_map)]
        let kafka = if let Some(config) = stream_config.kafka.as_ref() {
            Some(KafkaStream {
                config: config.clone(),
                client: Arc::new(
                    Kafka::new(config)
                        .await
                        .unwrap_or_else(|e| panic!("Failed to create Kafka client: {e:?}")),
                ),
            })
        } else {
            None
        };
        #[allow(clippy::manual_map)]
        let redis = if let Some(config) = stream_config.redis.as_ref() {
            Some(RedisStream {
                config: config.clone(),
                client: Arc::new(
                    Redis::new(config)
                        .await
                        .unwrap_or_else(|e| panic!("Failed to create Redis client: {e:?}")),
                ),
            })
        } else {
            None
        };
        #[allow(clippy::manual_map)]
        let cloudflare_queues = if let Some(config) = stream_config.cloudflare_queues.as_ref() {
            Some(CloudflareQueuesStream {
                config: config.clone(),
                client: Arc::new(CloudflareQueues::new(
                    config.api_token.clone(),
                    config.account_id.clone(),
                )),
            })
        } else {
            None
        };
        Self {
            sns,
            webhook,
            rabbitmq,
            #[cfg(feature = "kafka")]
            kafka,
            redis,
            cloudflare_queues,
        }
    }
    fn has_any_streams(&self) -> bool {
        self.sns.is_some()
            || self.webhook.is_some()
            || self.rabbitmq.is_some()
            || {
                #[cfg(feature = "kafka")]
                {
                    self.kafka.is_some()
                }
                #[cfg(not(feature = "kafka"))]
                {
                    false
                }
            }
            || self.redis.is_some()
            || self.cloudflare_queues.is_some()
    }
    fn chunk_data(&self, data_array: &Vec<Value>) -> Vec<Vec<Value>> {
        let mut current_chunk = Vec::new();
        let mut current_size = 0;
        let mut chunks = Vec::new();
        for item in data_array {
            let item_str = serde_json::to_string(item).unwrap();
            let item_size = item_str.len();
            if current_size + item_size > MAX_CHUNK_SIZE {
                chunks.push(current_chunk);
                current_chunk = Vec::new();
                current_size = 0;
            }
            current_chunk.push(item.clone());
            current_size += item_size;
        }
        if !current_chunk.is_empty() {
            chunks.push(current_chunk);
        }
        chunks
    }
    /// Gets event name, which may be an optional alias, or the contract's event name.
    fn get_event_name(&self, events: &[StreamEvent], event_message: &EventMessage) -> String {
        let alias_name = events
            .iter()
            .find(|n| n.event_name == event_message.event_name)
            .and_then(|n| n.alias.clone());
        alias_name.unwrap_or_else(|| event_message.event_name.clone())
    }
    fn create_chunk_message_raw(
        &self,
        events: &[StreamEvent],
        event_message: &EventMessage,
        chunk: &[Value],
    ) -> String {
        let chunk_message = EventMessage {
            event_name: self.get_event_name(events, event_message),
            event_data: Value::Array(chunk.to_vec()),
            event_signature_hash: event_message.event_signature_hash,
            network: event_message.network.clone(),
        };
        serde_json::to_string(&chunk_message).unwrap()
    }
    fn create_chunk_message_json(
        &self,
        events: &[StreamEvent],
        event_message: &EventMessage,
        chunk: &[Value],
    ) -> Value {
        let chunk_message = EventMessage {
            event_name: self.get_event_name(events, event_message),
            event_data: Value::Array(chunk.to_vec()),
            event_signature_hash: event_message.event_signature_hash,
            network: event_message.network.clone(),
        };
        serde_json::to_value(&chunk_message).unwrap()
    }
    fn generate_publish_message_id(
        &self,
        id: &str,
        index: usize,
        prefix: &Option<String>,
    ) -> String {
        format!(
            "rindexer_stream__{}-{}-chunk-{}",
            prefix.as_ref().unwrap_or(&"".to_string()),
            id.to_lowercase(),
            index
        )
    }
    fn filter_chunk_event_data_by_conditions(
        &self,
        events: &[StreamEvent],
        event_message: &EventMessage,
        chunk: &[Value],
    ) -> Vec<Value> {
        let stream_event = events.iter().find(|e| e.event_name == event_message.event_name);
        // Allow no trace events to be defined, otherwise use the defined event config.
        if event_message.event_name == EVENT_NAME && stream_event.is_none() {
            return chunk.to_vec();
        }
        let stream_event = stream_event
            .expect("Failed to find stream event - should never happen please raise an issue");
        let filtered_chunk: Vec<Value> = chunk
            .iter()
            .filter(|event_data| {
                if let Some(conditions) = &stream_event.conditions {
                    filter_event_data_by_conditions(event_data, conditions)
                } else {
                    true
                }
            })
            .cloned()
            .collect();
        filtered_chunk
    }
    fn sns_stream_tasks(
        &self,
        config: &SNSStreamTopicConfig,
        client: Arc<SNS>,
        id: &str,
        event_message: &EventMessage,
        chunks: Arc<Vec<Vec<Value>>>,
    ) -> StreamPublishes {
        let tasks: Vec<_> = chunks
            .iter()
            .enumerate()
            .map(|(index, chunk)| {
                let filtered_chunk: Vec<Value> = self.filter_chunk_event_data_by_conditions(
                    &config.events,
                    event_message,
                    chunk,
                );
                let publish_message_id =
                    self.generate_publish_message_id(id, index, &config.prefix_id);
                let client = Arc::clone(&client);
                let topic_arn = config.topic_arn.clone();
                let publish_message =
                    self.create_chunk_message_raw(&config.events, event_message, &filtered_chunk);
                task::spawn(async move {
                    let _ =
                        client.publish(&publish_message_id, &topic_arn, &publish_message).await?;
                    Ok(filtered_chunk.len())
                })
            })
            .collect();
        tasks
    }
    fn webhook_stream_tasks(
        &self,
        config: &WebhookStreamConfig,
        client: Arc<Webhook>,
        id: &str,
        event_message: &EventMessage,
        chunks: Arc<Vec<Vec<Value>>>,
    ) -> StreamPublishes {
        let tasks: Vec<_> = chunks
            .iter()
            .enumerate()
            .map(|(index, chunk)| {
                let filtered_chunk: Vec<Value> = self.filter_chunk_event_data_by_conditions(
                    &config.events,
                    event_message,
                    chunk,
                );
                let publish_message_id = self.generate_publish_message_id(id, index, &None);
                let endpoint = config.endpoint.clone();
                let shared_secret = config.shared_secret.clone();
                let client = Arc::clone(&client);
                let publish_message =
                    self.create_chunk_message_json(&config.events, event_message, &filtered_chunk);
                task::spawn(async move {
                    client
                        .publish(&publish_message_id, &endpoint, &shared_secret, &publish_message)
                        .await?;
                    Ok(filtered_chunk.len())
                })
            })
            .collect();
        tasks
    }
    fn rabbitmq_stream_tasks(
        &self,
        config: &RabbitMQStreamQueueConfig,
        client: Arc<RabbitMQ>,
        id: &str,
        event_message: &EventMessage,
        chunks: Arc<Vec<Vec<Value>>>,
    ) -> StreamPublishes {
        let tasks: Vec<_> = chunks
            .iter()
            .enumerate()
            .map(|(index, chunk)| {
                let filtered_chunk: Vec<Value> = self.filter_chunk_event_data_by_conditions(
                    &config.events,
                    event_message,
                    chunk,
                );
                let publish_message_id = self.generate_publish_message_id(id, index, &None);
                let client = Arc::clone(&client);
                let exchange = config.exchange.clone();
                let exchange_type = config.exchange_type.clone();
                let routing_key = config.routing_key.clone();
                let publish_message =
                    self.create_chunk_message_json(&config.events, event_message, &filtered_chunk);
                task::spawn(async move {
                    client
                        .publish(
                            &publish_message_id,
                            &exchange,
                            &exchange_type,
                            &routing_key,
                            &publish_message,
                        )
                        .await?;
                    Ok(filtered_chunk.len())
                })
            })
            .collect();
        tasks
    }
    #[cfg(feature = "kafka")]
    fn kafka_stream_tasks(
        &self,
        config: &KafkaStreamQueueConfig,
        client: Arc<Kafka>,
        id: &str,
        event_message: &EventMessage,
        chunks: Arc<Vec<Vec<Value>>>,
    ) -> StreamPublishes {
        let tasks: Vec<_> = chunks
            .iter()
            .enumerate()
            .map(|(index, chunk)| {
                let filtered_chunk: Vec<Value> = self.filter_chunk_event_data_by_conditions(
                    &config.events,
                    event_message,
                    chunk,
                );
                let publish_message_id = self.generate_publish_message_id(id, index, &None);
                let client = Arc::clone(&client);
                let exchange = config.topic.clone();
                let routing_key = config.key.clone();
                let publish_message =
                    self.create_chunk_message_json(&config.events, event_message, &filtered_chunk);
                task::spawn(async move {
                    client
                        .publish(&publish_message_id, &exchange, &routing_key, &publish_message)
                        .await?;
                    Ok(filtered_chunk.len())
                })
            })
            .collect();
        tasks
    }
    fn redis_stream_tasks(
        &self,
        config: &RedisStreamStreamConfig,
        client: Arc<Redis>,
        id: &str,
        event_message: &EventMessage,
        chunks: Arc<Vec<Vec<Value>>>,
    ) -> StreamPublishes {
        let tasks: Vec<_> = chunks
            .iter()
            .enumerate()
            .map(|(index, chunk)| {
                let filtered_chunk: Vec<Value> = self.filter_chunk_event_data_by_conditions(
                    &config.events,
                    event_message,
                    chunk,
                );
                let publish_message_id = self.generate_publish_message_id(id, index, &None);
                let client = Arc::clone(&client);
                let stream_name = config.stream_name.clone();
                let publish_message =
                    self.create_chunk_message_json(&config.events, event_message, &filtered_chunk);
                task::spawn(async move {
                    client.publish(&publish_message_id, &stream_name, &publish_message).await?;
                    Ok(filtered_chunk.len())
                })
            })
            .collect();
        tasks
    }
    fn cloudflare_queues_stream_tasks(
        &self,
        config: &CloudflareQueuesStreamQueueConfig,
        client: Arc<CloudflareQueues>,
        id: &str,
        event_message: &EventMessage,
        chunks: Arc<Vec<Vec<Value>>>,
    ) -> StreamPublishes {
        let tasks: Vec<_> = chunks
            .iter()
            .enumerate()
            .map(|(index, chunk)| {
                let filtered_chunk: Vec<Value> = self.filter_chunk_event_data_by_conditions(
                    &config.events,
                    event_message,
                    chunk,
                );
                let publish_message_id = self.generate_publish_message_id(id, index, &None);
                let client = Arc::clone(&client);
                let queue_id = config.queue_id.clone();
                let publish_message =
                    self.create_chunk_message_json(&config.events, event_message, &filtered_chunk);
                task::spawn(async move {
                    client.publish(&publish_message_id, &queue_id, &publish_message).await?;
                    Ok(filtered_chunk.len())
                })
            })
            .collect();
        tasks
    }
    pub async fn stream(
        &self,
        id: String,
        event_message: &EventMessage,
        index_event_in_order: bool,
        is_trace_event: bool,
    ) -> Result<usize, StreamError> {
        if !self.has_any_streams() {
            return Ok(0);
        }
        // will always have something even if the event has no parameters due to the tx_information
        if let Value::Array(data_array) = &event_message.event_data {
            let chunks = Arc::new(self.chunk_data(data_array));
            let mut streams: Vec<StreamPublishes> = Vec::new();
            if let Some(sns) = &self.sns {
                for config in &sns.config {
                    let is_user_event =
                        config.events.iter().any(|e| e.event_name == event_message.event_name);
                    if (is_user_event || is_trace_event)
                        && config.networks.contains(&event_message.network)
                    {
                        streams.push(self.sns_stream_tasks(
                            config,
                            Arc::clone(&sns.client),
                            &id,
                            event_message,
                            Arc::clone(&chunks),
                        ));
                    }
                }
            };
            if let Some(webhook) = &self.webhook {
                for config in &webhook.config {
                    if config.events.iter().any(|e| e.event_name == event_message.event_name)
                        && config.networks.contains(&event_message.network)
                    {
                        streams.push(self.webhook_stream_tasks(
                            config,
                            Arc::clone(&webhook.client),
                            &id,
                            event_message,
                            Arc::clone(&chunks),
                        ));
                    }
                }
            }
            if let Some(rabbitmq) = &self.rabbitmq {
                for config in &rabbitmq.config.exchanges {
                    if config.events.iter().any(|e| e.event_name == event_message.event_name)
                        && config.networks.contains(&event_message.network)
                    {
                        streams.push(self.rabbitmq_stream_tasks(
                            config,
                            Arc::clone(&rabbitmq.client),
                            &id,
                            event_message,
                            Arc::clone(&chunks),
                        ));
                    }
                }
            }
            #[cfg(feature = "kafka")]
            if let Some(kafka) = &self.kafka {
                for config in &kafka.config.topics {
                    if config.events.iter().any(|e| e.event_name == event_message.event_name)
                        && config.networks.contains(&event_message.network)
                    {
                        streams.push(self.kafka_stream_tasks(
                            config,
                            Arc::clone(&kafka.client),
                            &id,
                            event_message,
                            Arc::clone(&chunks),
                        ));
                    }
                }
            }
            if let Some(redis) = &self.redis {
                for config in &redis.config.streams {
                    if config.events.iter().any(|e| e.event_name == event_message.event_name)
                        && config.networks.contains(&event_message.network)
                    {
                        streams.push(self.redis_stream_tasks(
                            config,
                            Arc::clone(&redis.client),
                            &id,
                            event_message,
                            Arc::clone(&chunks),
                        ));
                    }
                }
            }
            if let Some(cloudflare_queues) = &self.cloudflare_queues {
                for config in &cloudflare_queues.config.queues {
                    if config.events.iter().any(|e| e.event_name == event_message.event_name)
                        && config.networks.contains(&event_message.network)
                    {
                        streams.push(self.cloudflare_queues_stream_tasks(
                            config,
                            Arc::clone(&cloudflare_queues.client),
                            &id,
                            event_message,
                            Arc::clone(&chunks),
                        ));
                    }
                }
            }
            let mut streamed_total = 0;
            if index_event_in_order {
                for stream in streams {
                    for task in stream {
                        match task.await {
                            Ok(Ok(streamed)) => {
                                streamed_total += streamed;
                            }
                            Ok(Err(e)) => return Err(e),
                            Err(e) => return Err(StreamError::JoinError(e)),
                        }
                    }
                }
            } else {
                let tasks: Vec<_> = streams.into_iter().flatten().collect();
                let results = join_all(tasks).await;
                for result in results {
                    match result {
                        Ok(Ok(streamed)) => {
                            streamed_total += streamed;
                        }
                        Ok(Err(e)) => return Err(e),
                        Err(e) => return Err(StreamError::JoinError(e)),
                    }
                }
            }
            Ok(streamed_total)
        } else {
            unreachable!("Event data should be an array");
        }
    }
}
</file>

<file path="core/src/streams/cloudflare_queues.rs">
use reqwest::Client;
use serde_json::Value;
use thiserror::Error;
use crate::streams::STREAM_MESSAGE_ID_KEY;
#[derive(Error, Debug)]
#[allow(clippy::enum_variant_names)]
pub enum CloudflareQueuesError {
    #[error("Request error: {0}")]
    RequestError(#[from] reqwest::Error),
    #[error("Cloudflare API error: {status} - {message}")]
    ApiError { status: u16, message: String },
    #[error("Serialization error: {0}")]
    SerializationError(#[from] serde_json::Error),
}
#[derive(Debug, Clone)]
pub struct CloudflareQueues {
    client: Client,
    api_token: String,
    account_id: String,
}
impl CloudflareQueues {
    pub fn new(api_token: String, account_id: String) -> Self {
        Self { client: Client::new(), api_token, account_id }
    }
    pub async fn publish(
        &self,
        id: &str,
        queue_id: &str,
        message: &Value,
    ) -> Result<(), CloudflareQueuesError> {
        let url = format!(
            "https://api.cloudflare.com/client/v4/accounts/{}/queues/{}/messages",
            self.account_id, queue_id
        );
        let mut message_with_metadata = message.clone();
        if let Value::Object(ref mut map) = message_with_metadata {
            map.insert("message_id".to_string(), Value::String(id.to_string()));
        }
        let payload = serde_json::json!({
            "body": message_with_metadata
        });
        let response = self
            .client
            .post(&url)
            .header("Authorization", format!("Bearer {}", self.api_token))
            .header("Content-Type", "application/json")
            .header(STREAM_MESSAGE_ID_KEY, id)
            .json(&payload)
            .send()
            .await?;
        if response.status().is_success() {
            Ok(())
        } else {
            let status = response.status().as_u16();
            let error_text = response.text().await.unwrap_or_else(|_| "Unknown error".to_string());
            Err(CloudflareQueuesError::ApiError { status, message: error_text })
        }
    }
    #[allow(dead_code)]
    pub async fn publish_batch(
        &self,
        messages: Vec<(String, Value)>,
        queue_id: &str,
    ) -> Result<(), CloudflareQueuesError> {
        if messages.is_empty() {
            return Ok(());
        }
        // Cloudflare Queues supports up to 100 messages per batch
        const MAX_BATCH_SIZE: usize = 100;
        for chunk in messages.chunks(MAX_BATCH_SIZE) {
            let url = format!(
                "https://api.cloudflare.com/client/v4/accounts/{}/queues/{}/messages/batch",
                self.account_id, queue_id
            );
            let batch_messages: Vec<Value> = chunk
                .iter()
                .map(|(id, message)| {
                    let mut message_with_metadata = message.clone();
                    if let Value::Object(ref mut map) = message_with_metadata {
                        map.insert("message_id".to_string(), Value::String(id.to_string()));
                    }
                    serde_json::json!({
                        "body": message_with_metadata
                    })
                })
                .collect();
            let response = self
                .client
                .post(&url)
                .header("Authorization", format!("Bearer {}", self.api_token))
                .header("Content-Type", "application/json")
                .json(&batch_messages)
                .send()
                .await?;
            if !response.status().is_success() {
                let status = response.status().as_u16();
                let error_text =
                    response.text().await.unwrap_or_else(|_| "Unknown error".to_string());
                return Err(CloudflareQueuesError::ApiError { status, message: error_text });
            }
        }
        Ok(())
    }
}
</file>

<file path="core/src/streams/kafka.rs">
use std::fmt::{Debug, Formatter};
#[cfg(not(windows))]
use std::time::Duration;
#[cfg(not(windows))]
use rdkafka::{
    config::ClientConfig,
    message::{Header, OwnedHeaders},
    producer::{FutureProducer, FutureRecord},
    util::Timeout,
};
use serde_json::Value;
use thiserror::Error;
use crate::{manifest::stream::KafkaStreamConfig, streams::STREAM_MESSAGE_ID_KEY};
#[derive(Error, Debug)]
pub enum KafkaError {
    #[error("Kafka error: {0}")]
    RdkafkaError(String),
    #[error("Could not parse message: {0}")]
    CouldNotParseMessage(#[from] serde_json::Error),
}
#[derive(Clone)]
pub struct Kafka {
    #[cfg(not(windows))]
    producer: FutureProducer,
}
impl Debug for Kafka {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("Kafka { .. }").finish()
    }
}
impl Kafka {
    pub async fn new(config: &KafkaStreamConfig) -> Result<Self, KafkaError> {
        #[cfg(not(windows))]
        {
            let servers_list = config.brokers.join(",");
            let mut client_config = ClientConfig::new();
            client_config
                .set("bootstrap.servers", &servers_list)
                .set("security.protocol", &config.security_protocol)
                .set("acks", &config.acks);
            if let Some(ref sasl_mechanisms) = config.sasl_mechanisms {
                client_config.set("sasl.mechanisms", sasl_mechanisms);
            }
            if let Some(ref sasl_username) = config.sasl_username {
                client_config.set("sasl.username", sasl_username);
            }
            if let Some(ref sasl_password) = config.sasl_password {
                client_config.set("sasl.password", sasl_password);
            }
            let producer: FutureProducer =
                client_config.create().map_err(|e| KafkaError::RdkafkaError(e.to_string()))?;
            Ok(Self { producer })
        }
        #[cfg(windows)]
        {
            panic!("Kafka is not supported on Windows")
        }
    }
    pub async fn publish(
        &self,
        id: &str,
        topic: &str,
        key: &Option<String>,
        message: &Value,
    ) -> Result<(), KafkaError> {
        #[cfg(not(windows))]
        {
            let message_body = serde_json::to_vec(message)?;
            let record = if key.is_some() {
                FutureRecord::to(topic).key(key.as_ref().unwrap()).payload(&message_body).headers(
                    OwnedHeaders::new()
                        .insert(Header { key: STREAM_MESSAGE_ID_KEY, value: Some(id) }),
                )
            } else {
                FutureRecord::to(topic).payload(&message_body).headers(
                    OwnedHeaders::new()
                        .insert(Header { key: STREAM_MESSAGE_ID_KEY, value: Some(id) }),
                )
            };
            self.producer
                .send(record, Timeout::After(Duration::from_secs(0)))
                .await
                .map_err(|(e, _)| KafkaError::RdkafkaError(e.to_string()))?;
            Ok(())
        }
        #[cfg(windows)]
        {
            panic!("Kafka is not supported on Windows")
        }
    }
}
</file>

<file path="core/src/streams/mod.rs">
mod sns;
pub use sns::SNS;
mod webhook;
pub use webhook::{Webhook, WebhookError};
mod rabbitmq;
pub use rabbitmq::{RabbitMQ, RabbitMQError};
#[cfg(feature = "kafka")]
mod kafka;
mod clients;
mod redis;
pub use clients::StreamsClients;
pub use redis::{Redis, RedisError};
mod cloudflare_queues;
pub use cloudflare_queues::{CloudflareQueues, CloudflareQueuesError};
pub const STREAM_MESSAGE_ID_KEY: &str = "x-rindexer-id";
</file>

<file path="core/src/streams/rabbitmq.rs">
use deadpool::managed::PoolError;
use deadpool_lapin::{Manager, Pool};
use lapin::{options::*, types::FieldTable, BasicProperties, ConnectionProperties, ExchangeKind};
use serde_json::Value;
use crate::manifest::stream::ExchangeKindWrapper;
#[derive(thiserror::Error, Debug)]
pub enum RabbitMQError {
    #[error("Request error: {0}")]
    LapinError(#[from] lapin::Error),
    #[error("Could not parse message: {0}")]
    CouldNotParseMessage(#[from] serde_json::Error),
    #[error("Connection pool error")]
    PoolError(#[from] PoolError<lapin::Error>),
}
#[derive(Debug, Clone)]
pub struct RabbitMQ {
    pool: Pool,
}
impl RabbitMQ {
    pub async fn new(uri: &str) -> Self {
        let manager = Manager::new(uri, ConnectionProperties::default());
        let pool = Pool::builder(manager).max_size(16).build().expect("Failed to create pool");
        Self { pool }
    }
    pub async fn publish(
        &self,
        id: &str,
        exchange: &str,
        exchange_type: &ExchangeKindWrapper,
        routing_key: &Option<String>,
        message: &Value,
    ) -> Result<(), RabbitMQError> {
        let message_body = serde_json::to_vec(message)?;
        let conn = self.pool.get().await?;
        let channel = conn.create_channel().await?;
        channel
            .exchange_declare(
                exchange,
                exchange_type.0.clone(),
                ExchangeDeclareOptions::default(),
                FieldTable::default(),
            )
            .await?;
        channel
            .basic_publish(
                exchange,
                match exchange_type.0 {
                    ExchangeKind::Fanout => "", // Fanout exchange ignores the routing key
                    _ => routing_key.as_ref().expect("Routing key should be defined"),
                },
                BasicPublishOptions::default(),
                &message_body,
                BasicProperties::default()
                    .with_message_id(id.into())
                    .with_content_type("application/json".into()),
            )
            .await?;
        Ok(())
    }
}
</file>

<file path="core/src/streams/redis.rs">
use std::sync::Arc;
use bb8_redis::{
    bb8::{self, Pool, PooledConnection},
    redis::{cmd, AsyncCommands},
    RedisConnectionManager,
};
use serde_json::Value;
use thiserror::Error;
use crate::manifest::stream::RedisStreamConfig;
#[derive(Error, Debug)]
pub enum RedisError {
    #[error("Redis error: {0}")]
    RedisCmdError(#[from] redis::RedisError),
    #[error("Redis pool error: {0}")]
    PoolError(#[from] bb8::RunError<redis::RedisError>),
    #[error("Could not serialize message: {0}")]
    CouldNotSerialize(#[from] serde_json::Error),
}
#[derive(Debug, Clone)]
pub struct Redis {
    client: Arc<Pool<RedisConnectionManager>>,
}
async fn get_pooled_connection(
    pool: &'_ Arc<Pool<RedisConnectionManager>>,
) -> Result<PooledConnection<'_, RedisConnectionManager>, RedisError> {
    match pool.get().await {
        Ok(c) => Ok(c),
        Err(err) => Err(RedisError::PoolError(err)),
    }
}
impl Redis {
    pub async fn new(config: &RedisStreamConfig) -> Result<Self, RedisError> {
        let connection_manager = RedisConnectionManager::new(config.connection_uri.as_str())?;
        let redis_pool = Arc::new(
            Pool::builder().max_size(config.max_pool_size).build(connection_manager).await?,
        );
        let mut connection = get_pooled_connection(&redis_pool).await?;
        let _ = cmd("PING").query_async::<String>(&mut *connection).await?;
        Ok(Self { client: redis_pool.clone() })
    }
    pub async fn publish(
        &self,
        message_id: &str,
        stream_name: &str,
        message: &Value,
    ) -> Result<(), RedisError> {
        // redis stream message ids need to be a timestamp with guaranteed unique identification
        // so instead, we attach the message_id to the message value.
        let mut message_with_id = message.clone();
        if let Value::Object(ref mut map) = message_with_id {
            map.insert("message_id".to_string(), Value::String(message_id.to_string()));
        }
        let json_value = serde_json::to_string(&message_with_id)?;
        let mut con = get_pooled_connection(&self.client).await?;
        let _: () = con.xadd(stream_name, "*", &[("payload", &json_value)]).await?;
        Ok(())
    }
}
</file>

<file path="core/src/streams/sns.rs">
use aws_config::{meta::region::RegionProviderChain, BehaviorVersion, Region};
use aws_sdk_sns::{
    config::{http::HttpResponse, Credentials},
    error::SdkError,
    operation::publish::{PublishError, PublishOutput},
    Client,
};
use tracing::{error, info};
use crate::types::aws_config::AwsConfig;
#[derive(Debug, Clone)]
#[allow(clippy::upper_case_acronyms)]
pub struct SNS {
    client: Client,
}
impl SNS {
    pub async fn new(config: &AwsConfig) -> Self {
        let region_provider = RegionProviderChain::first_try(Region::new(config.region.clone()));
        let credentials_provider = Credentials::new(
            &config.access_key,
            &config.secret_key,
            config.session_token.clone(),
            None,
            "manual",
        );
        let mut sdk_config = aws_config::defaults(BehaviorVersion::latest())
            .region(region_provider)
            .credentials_provider(credentials_provider)
            .load()
            .await;
        // Conditionally set endpoint if it exists
        if let Some(endpoint_url) = &config.endpoint_url {
            if !endpoint_url.trim().is_empty() {
                sdk_config = sdk_config.to_builder().endpoint_url(endpoint_url).build();
            }
        }
        let client = Client::new(&sdk_config);
        // Test the connection by listing SNS topics
        match client.list_topics().send().await {
            Ok(_) => {
                info!("Successfully connected to SNS.");
            }
            Err(error) => {
                error!("Error connecting to SNS: {}", error);
                panic!("Error connecting to SNS: {error}");
            }
        }
        Self { client }
    }
    pub async fn publish(
        &self,
        id: &str,
        topic_arn: &str,
        message: &str,
    ) -> Result<PublishOutput, SdkError<PublishError, HttpResponse>> {
        if topic_arn.contains(".fifo") {
            let result = self
                .client
                .publish()
                .message(message)
                .topic_arn(topic_arn)
                // fifo needs to have group id and deduplication id
                .message_group_id("default")
                .message_deduplication_id(id)
                .send()
                .await?;
            Ok(result)
        } else {
            let result = self.client.publish().topic_arn(topic_arn).message(message).send().await?;
            Ok(result)
        }
    }
}
</file>

<file path="core/src/streams/webhook.rs">
use reqwest::Client;
use serde_json::Value;
use crate::streams::STREAM_MESSAGE_ID_KEY;
#[derive(thiserror::Error, Debug)]
pub enum WebhookError {
    #[error("Request error: {0}")]
    RequestError(#[from] reqwest::Error),
    #[error("Webhook error: {0}")]
    WebhookError(String),
}
#[derive(Debug, Clone)]
pub struct Webhook {
    client: Client,
}
impl Webhook {
    pub fn new() -> Self {
        Self { client: Client::new() }
    }
    pub async fn publish(
        &self,
        id: &str,
        endpoint: &str,
        shared_secret: &str,
        message: &Value,
    ) -> Result<(), WebhookError> {
        let response = self
            .client
            .post(endpoint)
            .header("Content-Type", "application/json")
            .header("x-rindexer-shared-secret", shared_secret)
            .header(STREAM_MESSAGE_ID_KEY, id)
            .json(message)
            .send()
            .await?;
        if response.status().is_success() {
            Ok(())
        } else {
            Err(WebhookError::WebhookError(format!(
                "Failed to send webhook: {}",
                response.status()
            )))
        }
    }
}
</file>

<file path="core/src/types/aws_config.rs">
use serde::{Deserialize, Serialize};
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct AwsConfig {
    pub region: String,
    pub access_key: String,
    pub secret_key: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub session_token: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub endpoint_url: Option<String>,
}
</file>

<file path="core/src/types/code.rs">
use std::fmt::Display;
#[derive(Debug)]
pub struct Code(String);
impl From<String> for Code {
    fn from(value: String) -> Self {
        Code(value)
    }
}
impl Display for Code {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}
impl Code {
    pub fn as_str(&self) -> &str {
        &self.0
    }
    pub fn as_string(&self) -> String {
        self.0.to_string()
    }
    pub fn push_str(&mut self, value: &Code) {
        self.0.push_str(&value.0);
    }
    pub fn new(value: String) -> Self {
        Code(value)
    }
    pub fn blank() -> Self {
        Code(String::new())
    }
}
</file>

<file path="core/src/types/core.rs">
use alloy::dyn_abi::DynSolValue;
use alloy::json_abi::Param;
/// Decoded log param.
#[derive(Debug, PartialEq, Clone)]
pub struct LogParam {
    /// Decoded log name.
    pub name: String,
    /// Decoded log value.
    pub value: DynSolValue,
    /// If the parameter is a compound type (a struct or tuple), a list of the
    /// parameter's components, in order. Empty otherwise
    pub components: Vec<Param>,
}
impl LogParam {
    pub fn new(name: String, value: DynSolValue) -> Self {
        Self { name, value, components: vec![] }
    }
    pub fn get_param_value(&self, name: &str) -> Option<DynSolValue> {
        if self.components.is_empty() {
            return None;
        }
        let mut current_component = self.components.clone();
        let mut current_value = self.value.clone();
        for part in name.split(".") {
            match current_component.iter().enumerate().find(|(_, param)| param.name == part) {
                Some((idx, value)) => {
                    current_component = value.components.clone();
                    current_value = current_value
                        .as_fixed_seq()
                        .expect("Must be a complex type")
                        .get(idx)
                        .expect("Complex type value must be present")
                        .clone();
                }
                None => return None,
            }
        }
        Some(current_value)
    }
}
/// Decoded log.
#[derive(Debug, PartialEq, Clone)]
pub struct ParsedLog {
    /// Log params.
    pub params: Vec<LogParam>,
}
impl ParsedLog {
    /// Extracts param by name. Supports deep paths like `foo.bar.baz`.
    pub fn get_param_value(&self, name: &str) -> Option<DynSolValue> {
        match name.split_once('.') {
            Some((root, rest)) if !rest.is_empty() => self
                .params
                .iter()
                .find(|param| param.name == *root)
                .and_then(|param| param.get_param_value(rest)),
            _ => {
                self.params.iter().find(|param| param.name == name).map(|param| param.value.clone())
            }
        }
    }
}
</file>

<file path="core/src/types/mod.rs">
pub mod aws_config;
pub mod code;
pub mod core;
pub mod single_or_array;
</file>

<file path="core/src/types/single_or_array.rs">
use serde::{Deserialize, Serialize};
#[derive(Debug, Serialize, Deserialize, Clone)]
#[serde(untagged)]
pub enum StringOrArray {
    Single(String),
    Multiple(Vec<String>),
}
impl From<String> for StringOrArray {
    fn from(s: String) -> Self {
        Self::Single(s)
    }
}
</file>

<file path="core/src/abi.rs">
use std::{collections::HashSet, fs, path::Path};
use alloy::{
    primitives::{keccak256, B256},
    rpc::types::ValueOrArray,
};
use serde::{Deserialize, Serialize};
use crate::database::clickhouse::generate::solidity_type_to_clickhouse_type;
use crate::database::sql_type_wrapper::{
    solidity_type_to_ethereum_sql_type_wrapper, EthereumSqlTypeWrapper,
};
use crate::{
    database::postgres::generate::solidity_type_to_db_type,
    helpers::camel_to_snake,
    manifest::contract::{Contract, ParseAbiError},
};
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ABIInput {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub indexed: Option<bool>,
    pub name: String,
    #[serde(rename = "type")]
    pub type_: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub components: Option<Vec<ABIInput>>,
}
#[derive(thiserror::Error, Debug)]
pub enum ParamTypeError {
    #[error("tuple type specified but no components found")]
    MissingComponents,
}
#[derive(PartialEq)]
pub enum GenerateAbiPropertiesType {
    PostgresWithDataTypes,
    PostgresColumnsNamesOnly,
    CsvHeaderNames,
    Object,
    ClickhouseWithDataTypes,
}
#[derive(Debug, Clone)]
pub struct AbiNamePropertiesPath {
    pub abi_type: String,
    pub abi_name: String,
}
impl AbiNamePropertiesPath {
    pub fn new(abi_name: &str, abi_type: &str) -> Self {
        Self { abi_type: abi_type.to_string(), abi_name: abi_name.to_string() }
    }
}
#[derive(Debug)]
pub struct AbiProperty {
    pub value: String,
    pub abi_type: String,
    pub abi_name: String,
    /// The path to the property, none if the property is at the root level.
    pub path: Option<Vec<AbiNamePropertiesPath>>,
    pub ethereum_sql_type_wrapper: Option<EthereumSqlTypeWrapper>,
}
impl AbiProperty {
    pub fn new(
        value: String,
        name: &str,
        abi_type: &str,
        path: Option<Vec<AbiNamePropertiesPath>>,
    ) -> Self {
        let has_array_in_path =
            path.as_ref().is_some_and(|p| p.iter().any(|pp| pp.abi_type.ends_with("[]")));
        let adjusted_abi_type =
            if has_array_in_path { format!("{abi_type}[]") } else { abi_type.to_string() };
        Self {
            value,
            ethereum_sql_type_wrapper: solidity_type_to_ethereum_sql_type_wrapper(
                &adjusted_abi_type,
            ),
            abi_type: abi_type.to_string(),
            abi_name: name.to_string(),
            path,
        }
    }
}
impl ABIInput {
    pub fn format_param_type(&self) -> Result<String, ParamTypeError> {
        match self.type_.as_str() {
            "tuple" => {
                let components =
                    self.components.as_ref().ok_or(ParamTypeError::MissingComponents)?;
                let formatted_components = components
                    .iter()
                    .map(|component| component.format_param_type())
                    .collect::<Result<Vec<_>, ParamTypeError>>()?
                    .join(",");
                Ok(format!("({formatted_components})"))
            }
            _ => Ok(self.type_.to_string()),
        }
    }
    pub fn generate_abi_name_properties(
        inputs: &[ABIInput],
        properties_type: &GenerateAbiPropertiesType,
        path: Option<Vec<AbiNamePropertiesPath>>,
    ) -> Vec<AbiProperty> {
        inputs
            .iter()
            .flat_map(|input| {
                if let Some(components) = &input.components {
                    let new_path = path.clone().map_or_else(
                        || vec![AbiNamePropertiesPath::new(&input.name, &input.type_)],
                        |mut p| {
                            p.push(AbiNamePropertiesPath::new(&input.name, &input.type_));
                            p
                        },
                    );
                    ABIInput::generate_abi_name_properties(
                        components,
                        properties_type,
                        Some(new_path),
                    )
                } else {
                    let prefix = path.as_ref().map(|p| {
                        p.iter()
                            .map(|pp| camel_to_snake(&pp.abi_name))
                            .collect::<Vec<_>>()
                            .join("_")
                    });
                    match properties_type {
                        GenerateAbiPropertiesType::PostgresWithDataTypes => {
                            let value = format!(
                                "\"{}{}\" {}",
                                prefix.map_or_else(|| "".to_string(), |p| format!("{p}_")),
                                camel_to_snake(&input.name),
                                solidity_type_to_db_type(&input.type_)
                            );
                            vec![AbiProperty::new(value, &input.name, &input.type_, path.clone())]
                        }
                        GenerateAbiPropertiesType::PostgresColumnsNamesOnly
                        | GenerateAbiPropertiesType::CsvHeaderNames => {
                            let value = format!(
                                "{}{}",
                                prefix.map_or_else(|| "".to_string(), |p| format!("{p}_")),
                                camel_to_snake(&input.name),
                            );
                            vec![AbiProperty::new(value, &input.name, &input.type_, path.clone())]
                        }
                        GenerateAbiPropertiesType::Object => {
                            let value = format!(
                                "{}{}",
                                prefix.map_or_else(|| "".to_string(), |p| format!("{p}.")),
                                camel_to_snake(&input.name),
                            );
                            vec![AbiProperty::new(value, &input.name, &input.type_, path.clone())]
                        }
                        GenerateAbiPropertiesType::ClickhouseWithDataTypes => {
                            let value = format!(
                                "\"{}{}\" {}",
                                prefix.map_or_else(|| "".to_string(), |p| format!("{}_", p)),
                                camel_to_snake(&input.name),
                                solidity_type_to_clickhouse_type(&input.type_)
                            );
                            vec![AbiProperty::new(value, &input.name, &input.type_, path.clone())]
                        }
                    }
                }
            })
            .collect()
    }
}
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ABIItem {
    #[serde(default)]
    pub inputs: Vec<ABIInput>,
    #[serde(default)]
    pub name: String,
    #[serde(rename = "type", default)]
    pub type_: String,
}
#[derive(thiserror::Error, Debug)]
pub enum ReadAbiError {
    #[error("Could not find ABI path: {0}")]
    AbiPathDoesNotExist(String),
    #[error("Could not read ABI string: {0}")]
    CouldNotReadAbiString(#[from] std::io::Error),
    #[error("Could not read ABI JSON: {0}")]
    CouldNotReadAbiJson(#[from] serde_json::Error),
    #[error("{0}")]
    ParseAbiError(#[from] ParseAbiError),
}
impl ABIItem {
    pub fn format_event_signature(&self) -> Result<String, ParamTypeError> {
        let name = &self.name;
        let params = self
            .inputs
            .iter()
            .map(Self::format_param_type)
            .collect::<Result<Vec<_>, _>>()?
            .join(",");
        Ok(format!("{name}({params})"))
    }
    fn format_param_type(input: &ABIInput) -> Result<String, ParamTypeError> {
        let base_type = input.type_.split('[').next().unwrap_or(&input.type_);
        let array_suffix = input.type_.strip_prefix(base_type).unwrap_or("");
        let type_str = match base_type {
            "tuple" => {
                let inner = input
                    .components
                    .as_ref()
                    .ok_or(ParamTypeError::MissingComponents)?
                    .iter()
                    .map(Self::format_param_type)
                    .collect::<Result<Vec<_>, _>>()?
                    .join(",");
                format!("({inner})")
            }
            _ => base_type.to_string(),
        };
        Ok(format!("{type_str}{array_suffix}"))
    }
    pub fn extract_event_names_and_signatures_from_abi(
        abi_json: Vec<ABIItem>,
    ) -> Result<Vec<EventInfo>, ParamTypeError> {
        let mut events = Vec::new();
        for item in abi_json.into_iter() {
            if item.type_ == "event" {
                events.push(EventInfo::new(item)?);
            }
        }
        Ok(events)
    }
    pub fn read_abi_items(
        project_path: &Path,
        contract: &Contract,
    ) -> Result<Vec<ABIItem>, ReadAbiError> {
        let abi_str = contract.parse_abi(project_path)?;
        let abi_items: Vec<ABIItem> = serde_json::from_str(&abi_str)?;
        let filtered_abi_items = match &contract.include_events {
            Some(events) => abi_items
                .into_iter()
                .filter(|item| {
                    item.type_ != "event"
                        || events
                            .iter()
                            .map(|a| a.name.clone())
                            .collect::<Vec<_>>()
                            .contains(&item.name)
                })
                .collect(),
            None => abi_items,
        };
        Ok(filtered_abi_items)
    }
    pub fn get_abi_items(
        project_path: &Path,
        contract: &Contract,
        is_filter: bool,
    ) -> Result<Vec<ABIItem>, ReadAbiError> {
        let mut abi_items = ABIItem::read_abi_items(project_path, contract)?;
        if is_filter {
            let filter_event_names: HashSet<String> = contract
                .details
                .iter()
                .filter_map(|detail| detail.filter.clone())
                .flat_map(|events| match events {
                    ValueOrArray::Value(event) => vec![event.event_name],
                    ValueOrArray::Array(event_array) => {
                        event_array.into_iter().map(|e| e.event_name).collect()
                    }
                })
                .collect();
            abi_items = abi_items
                .iter()
                .filter(|item| item.type_ == "event" && filter_event_names.contains(&item.name))
                .cloned()
                .collect();
        }
        Ok(abi_items)
    }
}
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EventInfo {
    pub name: String,
    pub inputs: Vec<ABIInput>,
    signature: String,
    struct_result: String,
    struct_data: String,
}
#[derive(thiserror::Error, Debug)]
pub enum CreateCsvFileForEvent {
    #[error("Could not create the dir {0}")]
    CreateDirFailed(std::io::Error),
}
impl EventInfo {
    pub fn new(item: ABIItem) -> Result<Self, ParamTypeError> {
        let struct_result = format!("{}Result", item.name);
        let struct_data = format!("{}Data", item.name);
        let signature = item.format_event_signature()?;
        Ok(EventInfo {
            name: item.name,
            inputs: item.inputs,
            signature,
            struct_result,
            struct_data,
        })
    }
    pub fn topic_id(&self) -> B256 {
        let event_signature = self.signature.clone();
        keccak256(event_signature)
    }
    pub fn topic_id_as_hex_string(&self) -> String {
        format!("{:x}", self.topic_id())
    }
    pub fn struct_result(&self) -> &str {
        &self.struct_result
    }
    pub fn struct_data(&self) -> &str {
        &self.struct_data
    }
    pub fn csv_headers_for_event(&self) -> Vec<String> {
        let mut headers: Vec<String> = ABIInput::generate_abi_name_properties(
            &self.inputs,
            &GenerateAbiPropertiesType::CsvHeaderNames,
            None,
        )
        .into_iter()
        .map(|m| m.value)
        .collect();
        headers.insert(0, r#"contract_address"#.to_string());
        headers.push(r#"tx_hash"#.to_string());
        headers.push(r#"block_number"#.to_string());
        headers.push(r#"block_hash"#.to_string());
        headers.push(r#"network"#.to_string());
        headers.push(r#"tx_index"#.to_string());
        headers.push(r#"log_index"#.to_string());
        headers
    }
    pub fn create_csv_file_for_event(
        &self,
        project_path: &Path,
        contract_name: &str,
        csv_path: &str,
    ) -> Result<String, CreateCsvFileForEvent> {
        let csv_file_name = format!("{}-{}.csv", contract_name, self.name).to_lowercase();
        let csv_folder = project_path.join(csv_path).join(contract_name);
        // Create directory if it does not exist.
        if let Err(e) = fs::create_dir_all(&csv_folder) {
            return Err(CreateCsvFileForEvent::CreateDirFailed(e));
        }
        // Create last-synced-blocks if it does not exist.
        if let Err(e) = fs::create_dir_all(csv_folder.join("last-synced-blocks")) {
            return Err(CreateCsvFileForEvent::CreateDirFailed(e));
        }
        Ok(csv_folder.join(csv_file_name).display().to_string())
    }
}
pub struct GetAbiItemWithDbMap {
    pub abi_item: ABIInput,
    pub db_column_name: String,
}
#[derive(thiserror::Error, Debug)]
pub enum GetAbiItemWithDbMapError {
    #[error("Parameter not found: {0}")]
    ParameterNotFound(String),
}
pub fn get_abi_item_with_db_map(
    abi_items: &[ABIItem],
    event_name: &str,
    parameter_mapping: &[&str],
) -> Result<GetAbiItemWithDbMap, GetAbiItemWithDbMapError> {
    let event_item = abi_items.iter().find(|item| item.name == event_name && item.type_ == "event");
    match event_item {
        Some(item) => {
            let mut current_inputs = &item.inputs;
            let mut db_column_name = String::new();
            for param in parameter_mapping {
                match current_inputs.iter().find(|input| input.name == *param) {
                    Some(input) => {
                        if !db_column_name.is_empty() {
                            db_column_name.push('_');
                        }
                        db_column_name.push_str(&camel_to_snake(&input.name));
                        if param
                            == parameter_mapping
                                .last()
                                .expect("Parameter mapping should have at least one element")
                        {
                            return Ok(GetAbiItemWithDbMap {
                                abi_item: input.clone(),
                                db_column_name,
                            });
                        } else {
                            current_inputs = match input.type_.as_str() {
                                "tuple" => {
                                    if let Some(ref components) = input.components {
                                        components
                                    } else {
                                        return Err(GetAbiItemWithDbMapError::ParameterNotFound(format!(
                                            "Parameter {param} is not a nested structure in event {event_name} of contract"
                                        )));
                                    }
                                },
                                _ => return Err(GetAbiItemWithDbMapError::ParameterNotFound(format!(
                                    "Parameter {param} is not a nested structure in event {event_name} of contract"
                                ))),
                            };
                        }
                    }
                    None => {
                        return Err(GetAbiItemWithDbMapError::ParameterNotFound(format!(
                            "Parameter {param} not found in event {event_name} of contract"
                        )));
                    }
                }
            }
            Err(GetAbiItemWithDbMapError::ParameterNotFound(format!(
                "Parameter {} not found in event {} of contract",
                parameter_mapping.join("."),
                event_name
            )))
        }
        None => Err(GetAbiItemWithDbMapError::ParameterNotFound(format!(
            "Event {event_name} not found in contract ABI"
        ))),
    }
}
</file>

<file path="core/src/events.rs">
/// Events emitted during the indexing process for external consumption.
#[derive(Debug, Clone)]
pub enum RindexerEvent {
    /// The indexing process has completed indexing historical events (happens every restart of the indexer)
    HistoricalIndexingCompleted,
}
/// A handle to subscribe to indexer events
#[derive(Clone)]
pub struct RindexerEventStream {
    tx: tokio::sync::broadcast::Sender<RindexerEvent>,
}
impl Default for RindexerEventStream {
    fn default() -> Self {
        Self::new()
    }
}
impl RindexerEventStream {
    pub fn new() -> Self {
        let (tx, _) = tokio::sync::broadcast::channel(100);
        Self { tx }
    }
    /// Subscribe to indexer events
    pub fn subscribe(&self) -> tokio::sync::broadcast::Receiver<RindexerEvent> {
        self.tx.subscribe()
    }
}
/// A handle to subscribe to indexer events
#[derive(Clone)]
pub struct RindexerEventEmitter {
    tx: tokio::sync::broadcast::Sender<RindexerEvent>,
}
impl RindexerEventEmitter {
    pub fn from_stream(stream: RindexerEventStream) -> Self {
        Self { tx: stream.tx.clone() }
    }
    pub fn emit(&self, event: RindexerEvent) {
        let _ = self.tx.send(event);
    }
}
</file>

<file path="core/src/health.rs">
use std::{net::SocketAddr, sync::Arc};
use axum::{extract::State, http::StatusCode, response::Json, routing::get, Router};
use serde::{Deserialize, Serialize};
use tokio::net::TcpListener;
use tracing::{error, info};
use crate::{
    database::postgres::client::PostgresClient, indexer::task_tracker::active_indexing_count,
    manifest::core::Manifest, system_state::is_running,
};
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthStatus {
    pub status: HealthStatusType,
    pub timestamp: String,
    pub services: HealthServices,
    pub indexing: IndexingStatus,
}
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthServices {
    pub database: HealthStatusType,
    pub indexing: HealthStatusType,
    pub sync: HealthStatusType,
}
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IndexingStatus {
    pub active_tasks: usize,
    pub is_running: bool,
}
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum HealthStatusType {
    Healthy,
    Unhealthy,
    Unknown,
    NotConfigured,
    Disabled,
    NoData,
    Stopped,
}
#[derive(Clone)]
pub struct HealthServerState {
    pub manifest: Arc<Manifest>,
    pub postgres_client: Option<Arc<PostgresClient>>,
}
pub struct HealthServer {
    port: u16,
    state: HealthServerState,
}
impl HealthServer {
    pub fn new(
        port: u16,
        manifest: Arc<Manifest>,
        postgres_client: Option<Arc<PostgresClient>>,
    ) -> Self {
        Self { port, state: HealthServerState { manifest, postgres_client } }
    }
    pub async fn start(self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let app = Router::new().route("/health", get(health_handler)).with_state(self.state);
        let addr = SocketAddr::from(([0, 0, 0, 0], self.port));
        let listener = TcpListener::bind(addr).await?;
        info!("🩺 Health server started on http://0.0.0.0:{}/health", self.port);
        axum::serve(listener, app).await?;
        Ok(())
    }
}
async fn health_handler(
    State(state): State<HealthServerState>,
) -> Result<(StatusCode, Json<HealthStatus>), StatusCode> {
    let database_health = check_database_health(&state).await;
    let indexing_health = check_indexing_health();
    let sync_health = check_sync_health(&state).await;
    let overall_status = determine_overall_status(&database_health, &indexing_health, &sync_health);
    let health_status =
        build_health_status(overall_status, database_health, indexing_health, sync_health);
    let status_code = if matches!(health_status.status, HealthStatusType::Healthy) {
        StatusCode::OK
    } else {
        StatusCode::SERVICE_UNAVAILABLE
    };
    Ok((status_code, Json(health_status)))
}
fn build_health_status(
    overall_status: HealthStatusType,
    database_health: HealthStatusType,
    indexing_health: HealthStatusType,
    sync_health: HealthStatusType,
) -> HealthStatus {
    HealthStatus {
        status: overall_status,
        timestamp: chrono::Utc::now().to_rfc3339(),
        services: HealthServices {
            database: database_health,
            indexing: indexing_health,
            sync: sync_health,
        },
        indexing: IndexingStatus {
            active_tasks: active_indexing_count(),
            is_running: is_running(),
        },
    }
}
async fn check_database_health(state: &HealthServerState) -> HealthStatusType {
    if !state.manifest.storage.postgres_enabled() {
        return HealthStatusType::Disabled;
    }
    match &state.postgres_client {
        Some(client) => match client.query_one("SELECT 1", &[]).await {
            Ok(_) => HealthStatusType::Healthy,
            Err(e) => {
                error!("Database health check failed: {}", e);
                HealthStatusType::Unhealthy
            }
        },
        None => HealthStatusType::NotConfigured,
    }
}
fn check_indexing_health() -> HealthStatusType {
    if is_running() {
        HealthStatusType::Healthy
    } else {
        HealthStatusType::Stopped
    }
}
async fn check_sync_health(state: &HealthServerState) -> HealthStatusType {
    if state.manifest.storage.postgres_enabled() {
        check_postgres_sync_health(state).await
    } else if state.manifest.storage.csv_enabled() {
        check_csv_sync_health(state)
    } else {
        HealthStatusType::Disabled
    }
}
async fn check_postgres_sync_health(state: &HealthServerState) -> HealthStatusType {
    match &state.postgres_client {
        Some(client) => {
            match client.query_one_or_none(
                r#"SELECT 1 FROM information_schema.tables WHERE table_schema NOT IN ('information_schema', 'pg_catalog', 'rindexer_internal') AND table_name NOT LIKE 'latest_block' AND table_name NOT LIKE '%_last_known_%' AND table_name NOT LIKE '%_last_run_%' LIMIT 1"#,
                &[]
            ).await {
                Ok(Some(_)) => HealthStatusType::Healthy,
                Ok(None) => HealthStatusType::NoData,
                Err(e) => {
                    error!("Sync health check failed: {}", e);
                    HealthStatusType::Unhealthy
                }
            }
        }
        None => HealthStatusType::NotConfigured,
    }
}
fn check_csv_sync_health(state: &HealthServerState) -> HealthStatusType {
    match &state.manifest.storage.csv {
        Some(csv_details) => {
            let csv_path = std::path::Path::new(&csv_details.path);
            if !csv_path.exists() {
                return HealthStatusType::NoData;
            }
            match std::fs::read_dir(csv_path) {
                Ok(entries) => {
                    let csv_files: Vec<_> = entries
                        .filter_map(|entry| entry.ok())
                        .filter(|entry| entry.path().extension().is_some_and(|ext| ext == "csv"))
                        .collect();
                    if csv_files.is_empty() {
                        HealthStatusType::NoData
                    } else {
                        HealthStatusType::Healthy
                    }
                }
                Err(_) => HealthStatusType::Unhealthy,
            }
        }
        None => HealthStatusType::NotConfigured,
    }
}
fn determine_overall_status(
    database: &HealthStatusType,
    indexing: &HealthStatusType,
    sync: &HealthStatusType,
) -> HealthStatusType {
    if matches!(database, HealthStatusType::Unhealthy | HealthStatusType::NotConfigured)
        || matches!(indexing, HealthStatusType::Stopped)
        || matches!(sync, HealthStatusType::Unhealthy | HealthStatusType::NotConfigured)
    {
        HealthStatusType::Unhealthy
    } else if matches!(sync, HealthStatusType::NoData) {
        // Sync NoData is acceptable when no event tables exist yet
        HealthStatusType::Healthy
    } else {
        HealthStatusType::Healthy
    }
}
pub async fn start_health_server(
    port: u16,
    manifest: Arc<Manifest>,
    postgres_client: Option<Arc<PostgresClient>>,
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    let health_server = HealthServer::new(port, manifest, postgres_client);
    health_server.start().await
}
</file>

<file path="core/src/layer_extensions.rs">
use crate::{rindexer_error, rindexer_info};
use alloy::{
    rpc::json_rpc::{RequestPacket, ResponsePacket},
    transports::TransportError,
};
use std::{
    future::Future,
    pin::Pin,
    task::{Context, Poll},
    time::Instant,
};
use tower::{Layer, Service};
#[derive(Clone)]
pub struct RpcLoggingLayer {
    chain_id: u64,
    rpc_url: String,
}
impl RpcLoggingLayer {
    pub fn new(chain_id: u64, rpc_url: String) -> Self {
        Self { chain_id, rpc_url }
    }
}
impl<S> Layer<S> for RpcLoggingLayer {
    type Service = RpcLoggingService<S>;
    fn layer(&self, inner: S) -> Self::Service {
        RpcLoggingService { inner, chain_id: self.chain_id, rpc_url: self.rpc_url.clone() }
    }
}
#[derive(Debug, Clone)]
pub struct RpcLoggingService<S> {
    inner: S,
    chain_id: u64,
    rpc_url: String,
}
impl<S> Service<RequestPacket> for RpcLoggingService<S>
where
    S: Service<RequestPacket, Response = ResponsePacket, Error = TransportError>,
    S::Future: Send + 'static,
{
    type Response = S::Response;
    type Error = S::Error;
    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;
    fn poll_ready(&mut self, cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
        self.inner.poll_ready(cx)
    }
    fn call(&mut self, req: RequestPacket) -> Self::Future {
        let start_time = Instant::now();
        let chain_id = self.chain_id;
        let rpc_url = self.rpc_url.clone();
        let method_name = match &req {
            RequestPacket::Single(r) => r.method().to_string(),
            RequestPacket::Batch(reqs) => {
                if reqs.is_empty() {
                    "empty_batch".to_string()
                } else if reqs.len() == 1 {
                    reqs[0].method().to_string()
                } else {
                    format!("batch_{}_requests", reqs.len())
                }
            }
        };
        let fut = self.inner.call(req);
        Box::pin(async move {
            match fut.await {
                Ok(response) => {
                    let duration = start_time.elapsed();
                    if duration.as_secs() >= 10 {
                        rindexer_info!(
                            "SLOW RPC call - chain_id: {}, method: {}, duration: {:?}, url: {}",
                            chain_id,
                            method_name,
                            duration,
                            rpc_url
                        );
                    }
                    Ok(response)
                }
                Err(err) => {
                    let duration = start_time.elapsed();
                    let error_str = err.to_string();
                    let is_known_error = is_known_retryable_error(&error_str);
                    if !is_known_error {
                        if error_str.contains("timeout") || error_str.contains("timed out") {
                            rindexer_error!("RPC TIMEOUT (free public nodes do this a lot consider a using a paid node) - chain_id: {}, method: {}, duration: {:?}, url: {}, error: {}",
                                           chain_id, method_name, duration, rpc_url, err);
                        } else if error_str.contains("429") || error_str.contains("rate limit") {
                            // TODO: Sampling this would be nice since this is actually an expected
                            //       part of the flow for many high-throughput applications.
                            rindexer_info!("RPC RATE LIMITED (free public nodes do this a lot consider a using a paid node) - chain_id: {}, method: {}, duration: {:?}, url: {}, error: {}",
                                          chain_id, method_name, duration, rpc_url, err);
                        } else if error_str.contains("connection") || error_str.contains("network")
                        {
                            rindexer_error!("RPC CONNECTION ERROR (free public nodes do this a lot consider a using a paid node) - chain_id: {}, method: {}, duration: {:?}, url: {}, error: {}",
                                           chain_id, method_name, duration, rpc_url, err);
                        } else {
                            rindexer_error!("RPC ERROR (free public nodes do this a lot consider a using a paid node) - chain_id: {}, method: {}, duration: {:?}, url: {}, error: {}",
                                           chain_id, method_name, duration, rpc_url, err);
                        }
                    }
                    Err(err)
                }
            }
        })
    }
}
fn is_known_retryable_error(error_message: &str) -> bool {
    // mirror handled logic which is in the `retry_with_block_range`
    error_message.contains("this block range should work")
        || error_message.contains("try with this block range")
        || error_message.contains("block range is too wide")
        || error_message.contains("limited to a")
        || error_message.contains("block range too large")
        || error_message.contains("response is too big")
        || error_message.contains("error decoding response body")
}
</file>

<file path="core/src/lib.rs">
// public
pub mod generator;
pub mod indexer;
pub mod layer_extensions;
pub mod manifest;
pub mod reth;
mod system_state;
pub use system_state::{initiate_shutdown, is_running};
mod health;
pub use health::{start_health_server, HealthServer, HealthServerState, HealthStatus};
mod database;
pub use database::{
    clickhouse::{client::ClickhouseClient, setup::setup_clickhouse},
    generate::drop_tables_for_indexer_sql,
    postgres::{
        client::{PostgresClient, ToSql},
        setup::setup_postgres,
    },
};
mod simple_file_formatters;
pub use simple_file_formatters::csv::AsyncCsvAppender;
mod helpers;
pub use helpers::{
    format_all_files_for_project, generate_random_id, load_env_from_project_path,
    public_read_env_value, write_file, WriteFileError,
};
mod api;
pub use api::{generate_graphql_queries, GraphqlOverrideSettings};
mod logger;
pub use logger::setup_info_logger;
mod abi;
pub use abi::ABIItem;
mod chat;
pub mod event;
pub mod notifications;
pub use notifications::ChainStateNotification;
pub mod blockclock;
pub mod phantom;
pub mod provider;
mod start;
mod streams;
mod types;
mod events;
pub use events::{RindexerEvent, RindexerEventStream};
// export 3rd party dependencies
pub use async_trait::async_trait;
pub use colored::Colorize as RindexerColorize;
pub use database::sql_type_wrapper::EthereumSqlTypeWrapper;
pub use futures::FutureExt;
pub use lazy_static::lazy_static;
pub use reqwest::header::HeaderMap;
pub use start::{
    start_rindexer, start_rindexer_no_code, IndexerNoCodeDetails, IndexingDetails, StartDetails,
    StartNoCodeDetails,
};
pub use tokio::main as rindexer_main;
pub use tokio_postgres::types::Type as PgType;
pub use tracing::{error as rindexer_error, info as rindexer_info};
pub use types::single_or_array::StringOrArray;
</file>

<file path="core/src/logger.rs">
use std::{
    io::Write,
    sync::atomic::{AtomicBool, Ordering},
};
use once_cell::sync::Lazy;
use tracing::level_filters::LevelFilter;
use tracing_subscriber::{
    fmt::{
        format::{Format, Writer},
        MakeWriter,
    },
    EnvFilter,
};
static SHUTDOWN_IN_PROGRESS: Lazy<AtomicBool> = Lazy::new(|| AtomicBool::new(false));
struct ShutdownAwareWriter {
    buffer: std::io::BufWriter<std::io::Stdout>,
}
impl ShutdownAwareWriter {
    fn new() -> Self {
        Self { buffer: std::io::BufWriter::new(std::io::stdout()) }
    }
}
impl Write for ShutdownAwareWriter {
    fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {
        if SHUTDOWN_IN_PROGRESS.load(Ordering::Relaxed) {
            // During shutdown, write directly to stdout
            let stdout = std::io::stdout();
            let mut handle = stdout.lock();
            handle.write(buf)
        } else {
            self.buffer.write(buf)
        }
    }
    fn flush(&mut self) -> std::io::Result<()> {
        if SHUTDOWN_IN_PROGRESS.load(Ordering::Relaxed) {
            let stdout = std::io::stdout();
            let mut handle = stdout.lock();
            handle.flush()
        } else {
            self.buffer.flush()
        }
    }
}
struct ShutdownAwareWriterMaker;
impl<'a> MakeWriter<'a> for ShutdownAwareWriterMaker {
    type Writer = ShutdownAwareWriter;
    fn make_writer(&'a self) -> Self::Writer {
        ShutdownAwareWriter::new()
    }
}
struct CustomTimer;
impl tracing_subscriber::fmt::time::FormatTime for CustomTimer {
    fn format_time(&self, writer: &mut Writer<'_>) -> std::fmt::Result {
        // Use a simpler time format during shutdown
        if SHUTDOWN_IN_PROGRESS.load(Ordering::Relaxed) {
            let now = chrono::Local::now();
            write!(writer, "{}", now.format("%H:%M:%S"))
        } else {
            let now = chrono::Local::now();
            write!(writer, "{} - {}", now.format("%d %B"), now.format("%H:%M:%S%.6f"))
        }
    }
}
const LOG_LEVEL_ENV: &str = "RINDEXER_LOG";
pub fn setup_logger(default_log_level: LevelFilter) {
    let filter = EnvFilter::try_from_env(LOG_LEVEL_ENV).unwrap_or(
        EnvFilter::builder().with_default_directive(default_log_level.into()).parse_lossy(""),
    );
    let format = Format::default().with_timer(CustomTimer).with_level(true).with_target(false);
    let subscriber = tracing_subscriber::fmt()
        .with_writer(ShutdownAwareWriterMaker)
        .with_env_filter(filter)
        .event_format(format)
        .finish();
    if tracing::subscriber::set_global_default(subscriber).is_err() {
        // Use println! here since logging might not be set up yet
        println!("Logger has already been set up, continuing...");
    }
}
pub fn setup_info_logger() {
    setup_logger(LevelFilter::INFO);
}
// Call this when starting shutdown
pub fn mark_shutdown_started() {
    SHUTDOWN_IN_PROGRESS.store(true, Ordering::Relaxed);
}
// Optional guard for temporary logger suppression
#[allow(dead_code)]
pub struct LoggerGuard;
impl Drop for LoggerGuard {
    fn drop(&mut self) {
        SHUTDOWN_IN_PROGRESS.store(false, Ordering::Relaxed);
    }
}
</file>

<file path="core/src/notifications.rs">
use alloy::primitives::{BlockNumber, B256};
/// Represents different types of chain state changes
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ChainStateNotification {
    /// New blocks have been committed to the canonical chain
    Committed {
        /// Starting block number of the committed range
        from_block: BlockNumber,
        /// Ending block number of the committed range
        to_block: BlockNumber,
        /// Hash of the new chain tip
        tip_hash: B256,
    },
    /// Chain reorganization occurred
    Reorged {
        /// Starting block number to revert
        revert_from_block: BlockNumber,
        /// Ending block number to revert
        revert_to_block: BlockNumber,
        /// Starting block number of new chain segment
        new_from_block: BlockNumber,
        /// Ending block number of new chain segment
        new_to_block: BlockNumber,
        /// Hash of the new chain tip after reorg
        new_tip_hash: B256,
    },
    /// Blocks have been reverted (chain rollback)
    Reverted {
        /// Starting block number of reverted range
        from_block: BlockNumber,
        /// Ending block number of reverted range
        to_block: BlockNumber,
    },
}
#[cfg(test)]
mod tests {
    use super::*;
    use tokio::sync::mpsc;
    #[test]
    fn test_notification_creation() {
        // Test Committed variant
        let committed = ChainStateNotification::Committed {
            from_block: 100,
            to_block: 200,
            tip_hash: B256::ZERO,
        };
        match committed {
            ChainStateNotification::Committed { from_block, to_block, tip_hash } => {
                assert_eq!(from_block, 100);
                assert_eq!(to_block, 200);
                assert_eq!(tip_hash, B256::ZERO);
            }
            _ => panic!("Expected Committed variant"),
        }
        // Test Reorged variant
        let reorged = ChainStateNotification::Reorged {
            revert_from_block: 150,
            revert_to_block: 200,
            new_from_block: 150,
            new_to_block: 210,
            new_tip_hash: B256::from([1u8; 32]),
        };
        match reorged {
            ChainStateNotification::Reorged {
                revert_from_block,
                revert_to_block,
                new_from_block,
                new_to_block,
                new_tip_hash,
            } => {
                assert_eq!(revert_from_block, 150);
                assert_eq!(revert_to_block, 200);
                assert_eq!(new_from_block, 150);
                assert_eq!(new_to_block, 210);
                assert_eq!(new_tip_hash, B256::from([1u8; 32]));
            }
            _ => panic!("Expected Reorged variant"),
        }
        // Test Reverted variant
        let reverted = ChainStateNotification::Reverted { from_block: 100, to_block: 150 };
        match reverted {
            ChainStateNotification::Reverted { from_block, to_block } => {
                assert_eq!(from_block, 100);
                assert_eq!(to_block, 150);
            }
            _ => panic!("Expected Reverted variant"),
        }
    }
    #[tokio::test]
    async fn test_notification_channel() {
        let (tx, mut rx) = mpsc::unbounded_channel();
        // Send a notification
        let notification =
            ChainStateNotification::Committed { from_block: 1, to_block: 10, tip_hash: B256::ZERO };
        tx.send(notification.clone()).unwrap();
        // Receive and verify
        let received = rx.recv().await.unwrap();
        assert_eq!(received, notification);
    }
}
</file>

<file path="core/src/provider.rs">
use crate::notifications::ChainStateNotification;
use alloy::network::{AnyNetwork, AnyRpcBlock, AnyTransactionReceipt};
use alloy::rpc::types::{Filter, ValueOrArray};
use alloy::{
    eips::{BlockId, BlockNumberOrTag},
    primitives::{Address, Bytes, TxHash, U256, U64},
    providers::{
        ext::TraceApi,
        fillers::{BlobGasFiller, ChainIdFiller, FillProvider, GasFiller, JoinFill, NonceFiller},
        Identity, IpcConnect, Provider, ProviderBuilder, RootProvider,
    },
    rpc::{
        client::RpcClient,
        types::{
            trace::parity::{
                Action, CallAction, CallType, LocalizedTransactionTrace, TransactionTrace,
            },
            Log,
        },
    },
    transports::{
        http::{
            reqwest::{header::HeaderMap, Client, Error as ReqwestError},
            Http,
        },
        layers::RetryBackoffLayer,
        RpcError, TransportErrorKind,
    },
};
use alloy_chains::{Chain, NamedChain};
use futures::future::try_join_all;
use serde::{Deserialize, Serialize};
use serde_json::json;
use std::collections::HashSet;
use std::future::IntoFuture;
use std::{
    sync::Arc,
    time::{Duration, Instant},
};
use thiserror::Error;
use tokio::sync::{broadcast::Sender, Mutex, Semaphore};
use tokio::task::JoinError;
use tracing::{debug, debug_span, error, Instrument};
use url::Url;
use crate::helpers::chunk_hashset;
use crate::layer_extensions::RpcLoggingLayer;
use crate::manifest::network::{AddressFiltering, BlockPollFrequency};
use crate::{event::RindexerEventFilter, manifest::core::Manifest};
/// An alias type for a complex alloy Provider
pub type RindexerProvider = FillProvider<
    JoinFill<
        Identity,
        JoinFill<GasFiller, JoinFill<BlobGasFiller, JoinFill<NonceFiller, ChainIdFiller>>>,
    >,
    RootProvider<AnyNetwork>,
    AnyNetwork,
>;
// RPC providers have maximum supported addresses that can be provided in a filter
// We play safe and limit to 1000 by default, but that can be overridden in the configuration
const DEFAULT_RPC_SUPPORTED_ACCOUNT_FILTERS: usize = 1000;
/// Maximum RPC batching size available for the provider.
pub const RPC_CHUNK_SIZE: usize = 1000;
/// Recommended chunk sizes for batch RPC requests.
/// See: https://www.alchemy.com/docs/best-practices-when-using-alchemy#2-avoid-high-batch-cardinality
pub const RECOMMENDED_RPC_CHUNK_SIZE: usize = 50;
#[derive(Debug)]
pub struct JsonRpcCachedProvider {
    provider: Arc<RindexerProvider>,
    client: RpcClient,
    cache: Mutex<Option<(Instant, Arc<AnyRpcBlock>)>>,
    is_zk_chain: bool,
    pub chain: Chain,
    block_poll_frequency: Option<BlockPollFrequency>,
    address_filtering: Option<AddressFiltering>,
    pub max_block_range: Option<U64>,
    pub chain_state_notification: Option<Sender<ChainStateNotification>>,
}
#[derive(Error, Debug)]
pub enum ProviderError {
    #[error("Failed to make rpc request: {0}")]
    RequestFailed(#[from] RpcError<TransportErrorKind>),
    #[error("Failed to make batched rpc request: {0}")]
    BatchRequestFailed(#[from] JoinError),
    #[error("Failed to serialize rpc request data: {0}")]
    SerializationError(#[from] serde_json::Error),
    #[error("Unknown error: {0}")]
    CustomError(String),
}
/// TODO: This is a temporary type until we migrate to alloy
#[derive(Debug, Clone, Default, PartialEq, Eq, Serialize, Deserialize)]
pub struct WrappedLog {
    #[serde(flatten)]
    pub inner: Log,
    #[serde(rename = "blockTimestamp")]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub block_timestamp: Option<U256>,
}
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TraceCall {
    pub from: Address,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub gas: Option<String>,
    #[serde(rename = "gasUsed")]
    pub gas_used: U256,
    pub to: Option<Address>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub error: Option<String>,
    pub input: Bytes,
    #[serde(default)]
    pub value: U256,
    #[serde(rename = "type")]
    pub typ: String,
    #[serde(default)]
    pub calls: Vec<TraceCall>,
}
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TraceCallFrame {
    /// Zksync chains do not return `tx_hash` in their call trace response.
    #[serde(rename = "txHash")]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tx_hash: Option<TxHash>,
    pub result: TraceCall,
}
/// A faster than network-call method for determining if a chain is Zk-Rollup.
fn is_known_zk_evm_compatible_chain(chain: Chain) -> Option<bool> {
    if let Some(name) = chain.named() {
        match name {
            // Known zkEVM-compatible chains
            NamedChain::Lens
            | NamedChain::ZkSync
            | NamedChain::Sophon
            | NamedChain::Abstract
            | NamedChain::Scroll
            | NamedChain::Linea => Some(true),
            // Known non-zkEVM chains
            NamedChain::Mainnet
            | NamedChain::Sepolia
            | NamedChain::Arbitrum
            | NamedChain::Soneium
            | NamedChain::Avalanche
            | NamedChain::Polygon
            | NamedChain::Hyperliquid
            | NamedChain::Blast
            | NamedChain::World
            | NamedChain::Unichain
            | NamedChain::Base
            | NamedChain::Optimism
            | NamedChain::ApeChain
            | NamedChain::BinanceSmartChain
            | NamedChain::Fantom
            | NamedChain::Cronos
            | NamedChain::Gnosis
            | NamedChain::BaseSepolia
            | NamedChain::Sonic
            | NamedChain::Metis
            | NamedChain::Celo
            | NamedChain::Plasma => Some(false),
            // Fallback for unknown chains
            _ => None,
        }
    } else {
        None
    }
}
impl JsonRpcCachedProvider {
    /// Return a duration for block poll caching based on user configuration.
    fn block_poll_frequency(&self) -> Duration {
        let Some(block_poll_frequency) = self.block_poll_frequency else {
            return Duration::from_millis(50);
        };
        match block_poll_frequency {
            BlockPollFrequency::Rapid => Duration::from_millis(50),
            BlockPollFrequency::PollRateMs { millis } => Duration::from_millis(millis),
            BlockPollFrequency::Division { divisor } => self
                .chain
                .average_blocktime_hint()
                .and_then(|t| t.checked_div(divisor))
                .unwrap_or(Duration::from_millis(50)),
            BlockPollFrequency::RpcOptimized => self
                .chain
                .average_blocktime_hint()
                .and_then(|t| t.checked_div(3))
                .map(|t| t.max(Duration::from_millis(500)))
                .unwrap_or(Duration::from_millis(1000)),
        }
    }
    #[tracing::instrument(skip_all)]
    pub async fn get_latest_block(&self) -> Result<Option<Arc<AnyRpcBlock>>, ProviderError> {
        let mut cache_guard = self.cache.lock().await;
        let cache_time = self.block_poll_frequency();
        // Fetches the latest block only if it is likely that a new block has been produced for
        // this specific network. Consider this to be equal to half the block-time.
        //
        // If we want to reduce RPC calls further at the cost of we could consider indexing delay we
        // could set this to block-time directly.
        if let Some((timestamp, block)) = &*cache_guard {
            if timestamp.elapsed() < cache_time {
                return Ok(Some(Arc::clone(block)));
            }
        }
        let latest_block = self
            .provider
            .get_block(BlockId::Number(BlockNumberOrTag::Latest))
            .into_future()
            .instrument(debug_span!("fetching latest block", name = ?self.chain.named()))
            .await?;
        if let Some(block) = latest_block {
            let arc_block = Arc::new(block);
            *cache_guard = Some((Instant::now(), Arc::clone(&arc_block)));
            return Ok(Some(arc_block));
        } else {
            *cache_guard = None;
        }
        Ok(None)
    }
    #[tracing::instrument(skip_all)]
    pub async fn get_block_number(&self) -> Result<U64, ProviderError> {
        let number = self.provider.get_block_number().await?;
        Ok(U64::from(number))
    }
    /// Prefer using `trace_block` where possible as it returns more information.
    ///
    /// The current ethers version does not allow batching, we should upgrade to alloy.
    ///
    /// # Example of `alloy` supported fetch
    ///
    /// ```rs
    ///  let options = if self.is_zk_chain {
    ///       GethDebugTracingOptions::call_tracer(CallConfig::default())
    ///   } else {
    ///       GethDebugTracingOptions::call_tracer(CallConfig::default().only_top_call())
    ///   };
    ///
    ///   let valid_traces = self.provider.debug_trace_block_by_number(
    ///       BlockNumberOrTag::Number(block_number.as_limbs()[0]),
    ///       options,
    ///   ).await?;
    /// ```
    #[tracing::instrument(skip_all)]
    pub async fn debug_trace_block_by_number(
        &self,
        block_number: U64,
    ) -> Result<Vec<LocalizedTransactionTrace>, ProviderError> {
        // TODO: Consider the need to use `arbtrace_block` for early arbitrum blocks?
        let block = json!(serde_json::to_string_pretty(&block_number)?.replace("\"", ""));
        let options = if self.is_zk_chain {
            json!({ "tracer": "callTracer" })
        } else {
            json!({ "tracer": "callTracer", "tracerConfig": { "onlyTopCall": true } })
        };
        let valid_traces: Vec<TraceCallFrame> =
            self.provider.raw_request("debug_traceBlockByNumber".into(), [block, options]).await?;
        let mut flattened_calls = Vec::new();
        for trace in valid_traces {
            flattened_calls.push(TraceCallFrame {
                tx_hash: trace.tx_hash,
                result: TraceCall { calls: vec![], ..trace.result },
            });
            let mut stack = vec![];
            stack.extend(trace.result.calls.into_iter());
            while let Some(call) = stack.pop() {
                flattened_calls.push(TraceCallFrame {
                    tx_hash: None,
                    result: TraceCall { calls: vec![], ..call },
                });
                stack.extend(call.calls.into_iter());
            }
        }
        let traces = flattened_calls
            .into_iter()
            .filter_map(|frame| {
                // It's not clear in what situation this is None, but it does happen so it's
                // better to avoid deserialization errors for now and remove them from the list.
                //
                // We know they cannot be a valid native token transfer.
                if let Some(to) = frame.result.to {
                    Some(LocalizedTransactionTrace {
                        trace: TransactionTrace {
                            action: Action::Call(CallAction {
                                from: frame.result.from,
                                to,
                                value: frame.result.value,
                                gas: frame
                                    .result
                                    .gas
                                    .and_then(|a| {
                                        U64::from_str_radix(a.trim_start_matches("0x"), 16).ok()
                                    })
                                    .unwrap_or_default()
                                    .as_limbs()[0],
                                input: frame.result.input,
                                call_type: CallType::Call,
                            }),
                            result: None,
                            trace_address: vec![],
                            subtraces: 0,
                            error: frame.result.error,
                        },
                        transaction_hash: frame.tx_hash,
                        transaction_position: None, // not provided by debug_trace
                        block_number: Some(block_number.as_limbs()[0]),
                        block_hash: None, // not provided by debug_trace
                    })
                } else {
                    None
                }
            })
            .collect();
        Ok(traces)
    }
    /// Request `trace_block` information. This currently does not support batched multi-calls.
    #[tracing::instrument(skip_all)]
    pub async fn trace_block(
        &self,
        block_number: U64,
    ) -> Result<Vec<LocalizedTransactionTrace>, ProviderError> {
        let traces = self
            .provider
            .trace_block(BlockId::Number(BlockNumberOrTag::Number(block_number.as_limbs()[0])))
            .await?;
        Ok(traces)
    }
    /// Fetches blocks in concurrent rpc batches.
    #[tracing::instrument(skip_all, fields(len = block_numbers.len()))]
    pub async fn get_block_by_number_batch(
        &self,
        block_numbers: &[U64],
        include_txs: bool,
    ) -> Result<Vec<AnyRpcBlock>, ProviderError> {
        let chain_id = self.chain.id();
        if block_numbers.is_empty() {
            return Ok(Vec::new());
        }
        let mut block_numbers = block_numbers.to_vec();
        block_numbers.dedup();
        // Max concurrency within an oversized batch request
        let semaphore = Arc::new(Semaphore::new(2));
        let futures = block_numbers
            .chunks(RECOMMENDED_RPC_CHUNK_SIZE)
            .map(|chunk| {
                let client = self.client.clone();
                let owned_chunk = chunk.to_vec();
                let semaphore = semaphore.clone();
                tokio::spawn(async move {
                    let _permit = semaphore.acquire_owned().await.expect("Semaphore closed");
                    let mut batch = client.new_batch();
                    let mut request_futures = Vec::with_capacity(owned_chunk.len());
                    for block_num in owned_chunk {
                        let params = (BlockNumberOrTag::Number(block_num.to()), include_txs);
                        let call = batch.add_call("eth_getBlockByNumber", &params)?;
                        request_futures.push(call)
                    }
                    if let Err(e) = batch.send().await {
                        error!(
                            "Failed to send {} batch 'eth_getBlockByNumber' request for {}: {:?}",
                            request_futures.len(),
                            chain_id,
                            e
                        );
                        return Err(e);
                    }
                    try_join_all(request_futures).await
                })
            })
            .collect::<Vec<_>>();
        let chunk_results: Vec<Result<Vec<AnyRpcBlock>, _>> = try_join_all(futures).await?;
        let results = chunk_results
            .into_iter()
            .collect::<Result<Vec<_>, _>>()?
            .into_iter()
            .flatten()
            .collect();
        Ok(results)
    }
    /// Fetch tx receipts in a batch rpc call
    #[tracing::instrument(skip_all)]
    pub async fn get_tx_receipts_batch(
        &self,
        hashes: &[TxHash],
    ) -> Result<Vec<AnyTransactionReceipt>, ProviderError> {
        if hashes.is_empty() {
            return Ok(Vec::new());
        }
        let futures = hashes
            .chunks(RPC_CHUNK_SIZE)
            .map(|chunk| {
                let client = self.client.clone();
                let owned_chunk = chunk.to_vec();
                tokio::spawn(async move {
                    let mut batch = client.new_batch();
                    let mut request_futures = Vec::with_capacity(owned_chunk.len());
                    for hash in owned_chunk {
                        let call = batch.add_call(
                            "eth_getTransactionReceipt",
                            &(
                                hash,
                                /* one element tuple from dangling comma */
                            ),
                        )?;
                        request_futures.push(call)
                    }
                    if let Err(e) = batch.send().await {
                        error!("Failed to send batch tx receipt request: {:?}", e);
                        return Err(e);
                    }
                    try_join_all(request_futures).await
                })
            })
            .collect::<Vec<_>>();
        let chunk_results: Vec<Result<Vec<AnyTransactionReceipt>, _>> =
            try_join_all(futures).await?;
        let results = chunk_results
            .into_iter()
            .collect::<Result<Vec<_>, _>>()?
            .into_iter()
            .flatten()
            .collect();
        Ok(results)
    }
    #[tracing::instrument(skip_all)]
    pub async fn get_logs(
        &self,
        event_filter: &RindexerEventFilter,
    ) -> Result<Vec<Log>, ProviderError> {
        let addresses = event_filter.contract_addresses().await;
        let base_filter = Filter::new()
            .event_signature(event_filter.event_signature())
            .topic1(event_filter.topic1())
            .topic2(event_filter.topic2())
            .topic3(event_filter.topic3())
            .from_block(event_filter.from_block())
            .to_block(event_filter.to_block());
        let logs = match addresses {
            // no addresses, which means nothing to get
            // different rpc providers implement an empty array differently,
            // therefore, we assume an empty addresses array means no events to fetch
            Some(addresses) if addresses.is_empty() => Ok(vec![]),
            Some(addresses) => match self.address_filtering {
                Some(AddressFiltering::InMemory) => {
                    self.get_logs_for_address_in_memory(&base_filter, addresses).await
                }
                Some(AddressFiltering::MaxAddressPerGetLogsRequest(
                    max_address_per_get_logs_request,
                )) => {
                    self.get_logs_for_address_in_batches(
                        &base_filter,
                        addresses,
                        max_address_per_get_logs_request,
                    )
                    .await
                }
                None => {
                    self.get_logs_for_address_in_batches(
                        &base_filter,
                        addresses,
                        DEFAULT_RPC_SUPPORTED_ACCOUNT_FILTERS,
                    )
                    .await
                }
            },
            None => Ok(self.provider.get_logs(&base_filter).await?),
        };
        logs
    }
    /// Get logs by chunking addresses and fetching asynchronously in batches
    #[tracing::instrument(skip_all)]
    async fn get_logs_for_address_in_batches(
        &self,
        filter: &Filter,
        addresses: HashSet<Address>,
        chunk_size: usize,
    ) -> Result<Vec<Log>, ProviderError> {
        let address_chunks = chunk_hashset(addresses, chunk_size);
        let logs_futures = address_chunks.into_iter().map(|chunk| async move {
            let filter =
                filter.clone().address(ValueOrArray::Array(chunk.into_iter().collect::<Vec<_>>()));
            self.provider.get_logs(&filter).await
        });
        let chunked_logs = try_join_all(logs_futures).await?;
        Ok(chunked_logs.concat())
    }
    /// Gets all logs for a given filter and then filters by addresses in memory
    #[tracing::instrument(skip_all)]
    async fn get_logs_for_address_in_memory(
        &self,
        filter: &Filter,
        addresses: HashSet<Address>,
    ) -> Result<Vec<Log>, ProviderError> {
        let logs = self.provider.get_logs(filter).await?;
        let filtered_logs =
            logs.into_iter().filter(|log| addresses.contains(&log.address())).collect::<Vec<_>>();
        Ok(filtered_logs)
    }
    pub fn get_inner_provider(&self) -> Arc<RindexerProvider> {
        Arc::clone(&self.provider)
    }
    pub fn get_chain_state_notification(&self) -> Option<Sender<ChainStateNotification>> {
        self.chain_state_notification.clone()
    }
    #[cfg(test)]
    pub fn mock(chain_id: u64) -> Arc<Self> {
        let chain = Chain::from(chain_id);
        let client = RpcClient::new_http(
            Url::parse("http://localhost:8545").expect("mock URL must be valid"),
        );
        let provider =
            ProviderBuilder::new().network::<AnyNetwork>().connect_client(client.clone());
        let is_zk_chain = is_known_zk_evm_compatible_chain(chain).unwrap_or(false);
        Arc::new(Self {
            provider: Arc::new(provider),
            client,
            cache: Mutex::new(None),
            is_zk_chain,
            chain,
            block_poll_frequency: None,
            address_filtering: None,
            max_block_range: None,
            chain_state_notification: None,
        })
    }
}
#[derive(Error, Debug)]
pub enum RetryClientError {
    #[error("Provider can't be created for {0}: {1}")]
    ProviderCantBeCreated(String, String),
    #[error("Invalid client chain id for {0}. Expected {1}, received {2}")]
    InvalidClientChainId(String, u64, u64),
    #[error("Could not build client: {0}")]
    CouldNotBuildClient(#[from] ReqwestError),
    #[error("Could not connect to client for chain_id: {0}")]
    CouldNotConnectClient(#[from] RpcError<TransportErrorKind>),
    #[error("Could not start reth node for network {0}: {1}")]
    RethNodeStartError(String, String),
}
#[allow(clippy::too_many_arguments)]
pub async fn create_client(
    rpc_url: &str,
    chain_id: u64,
    compute_units_per_second: Option<u64>,
    max_block_range: Option<U64>,
    block_poll_frequency: Option<BlockPollFrequency>,
    custom_headers: HeaderMap,
    address_filtering: Option<AddressFiltering>,
    chain_state_notification: Option<Sender<ChainStateNotification>>,
) -> Result<Arc<JsonRpcCachedProvider>, RetryClientError> {
    let chain = Chain::from(chain_id);
    let (client, provider) = if rpc_url.ends_with(".ipc") {
        let ipc = IpcConnect::new(rpc_url.to_string());
        let retry_layer =
            RetryBackoffLayer::new(5000, 1000, compute_units_per_second.unwrap_or(660));
        let logging_layer = RpcLoggingLayer::new(chain_id, rpc_url.to_string());
        let rpc_client =
            RpcClient::builder().layer(retry_layer).layer(logging_layer).ipc(ipc.clone()).await?;
        let provider =
            ProviderBuilder::new().network::<AnyNetwork>().connect_ipc(ipc).await.map_err(|e| {
                RetryClientError::ProviderCantBeCreated(
                    rpc_url.to_string(),
                    format!("IPC connection failed: {e}"),
                )
            })?;
        (rpc_client, provider)
    } else {
        let rpc_url = Url::parse(rpc_url).map_err(|e| {
            RetryClientError::ProviderCantBeCreated(rpc_url.to_string(), e.to_string())
        })?;
        let client_with_auth = Client::builder()
            .default_headers(custom_headers)
            .timeout(Duration::from_secs(90))
            .build()?;
        let logging_layer = RpcLoggingLayer::new(chain_id, rpc_url.to_string());
        let http = Http::with_client(client_with_auth, rpc_url);
        let retry_layer =
            RetryBackoffLayer::new(5000, 1000, compute_units_per_second.unwrap_or(660));
        let rpc_client =
            RpcClient::builder().layer(retry_layer).layer(logging_layer).transport(http, false);
        let provider =
            ProviderBuilder::new().network::<AnyNetwork>().connect_client(rpc_client.clone());
        (rpc_client, provider)
    };
    let real_rpc_chain_id = provider.get_chain_id().await.map_err(|e| {
        RetryClientError::CouldNotConnectClient(RpcError::LocalUsageError(Box::new(e)))
    })?;
    if real_rpc_chain_id != chain_id {
        return Err(RetryClientError::InvalidClientChainId(
            rpc_url.to_string(),
            chain_id,
            real_rpc_chain_id,
        ));
    }
    let is_zk_chain = match is_known_zk_evm_compatible_chain(chain) {
        Some(zk) => zk,
        None => {
            let response: Result<String, _> =
                provider.raw_request("zks_L1ChainId".into(), [&()]).await;
            let is_zk_chain = response.is_ok();
            if is_zk_chain {
                debug!("Chain {} is zk chain. Trace indexing adjusted if enabled.", chain_id);
            }
            is_zk_chain
        }
    };
    Ok(Arc::new(JsonRpcCachedProvider {
        provider: Arc::new(provider),
        cache: Mutex::new(None),
        max_block_range,
        client,
        chain,
        is_zk_chain,
        block_poll_frequency,
        address_filtering,
        chain_state_notification,
    }))
}
pub async fn get_chain_id(rpc_url: &str) -> Result<U256, RpcError<TransportErrorKind>> {
    let url = Url::parse(rpc_url).map_err(|e| RpcError::LocalUsageError(Box::new(e)))?;
    let provider = ProviderBuilder::new().connect_http(url);
    let call = provider.get_chain_id().await?;
    Ok(U256::from(call))
}
#[derive(Debug)]
pub struct CreateNetworkProvider {
    pub network_name: String,
    pub disable_logs_bloom_checks: bool,
    pub client: Arc<JsonRpcCachedProvider>,
}
impl CreateNetworkProvider {
    pub async fn create(
        manifest: &Manifest,
    ) -> Result<Vec<CreateNetworkProvider>, RetryClientError> {
        let provider_futures = manifest.networks.iter().map(|network| async move {
            #[cfg(not(feature = "reth"))]
            let provider_url = network.rpc.clone();
            #[cfg(not(feature = "reth"))]
            let reth_tx: Option<Sender<ChainStateNotification>> = None;
            #[cfg(feature = "reth")]
            // if reth is enabled for this network, we need to start the reth node.
            // once reth is started, we can use the reth ipc path to create a provider.
            let reth_tx = network.try_start_reth_node().await.map_err(|e| {
                RetryClientError::RethNodeStartError(network.name.clone(), e.to_string())
            })?;
            // if reth is enabled and started successfully, we can use the reth ipc path to create a provider.
            // else, we will use the rpc url provided in the manifest.
            #[cfg(feature = "reth")]
            let provider_url = if reth_tx.is_some() {
                network.get_reth_ipc_path().unwrap()
            } else {
                network.rpc.clone()
            };
            // create the provider
            let provider = create_client(
                &provider_url,
                network.chain_id,
                network.compute_units_per_second,
                network.max_block_range,
                network.block_poll_frequency,
                manifest.get_custom_headers(),
                network.get_logs_settings.clone().map(|settings| settings.address_filtering),
                reth_tx.clone(),
            )
            .await?;
            Ok::<_, RetryClientError>(CreateNetworkProvider {
                network_name: network.name.clone(),
                disable_logs_bloom_checks: network.disable_logs_bloom_checks.unwrap_or_default(),
                client: provider,
            })
        });
        try_join_all(provider_futures).await
    }
    /// Get the chain state notification for this network
    pub fn chain_state_notification(&self) -> Option<Sender<ChainStateNotification>> {
        self.client.chain_state_notification.clone()
    }
}
/// Get a provider for a specific network
pub fn get_network_provider<'a>(
    network: &str,
    providers: &'a [CreateNetworkProvider],
) -> Option<&'a CreateNetworkProvider> {
    providers.iter().find(|item| item.network_name == network)
}
</file>

<file path="core/src/start.rs">
use std::path::PathBuf;
use std::sync::Arc;
use tokio::signal;
use tracing::{error, info};
use crate::database::clickhouse::setup::SetupClickhouseError;
use crate::events::RindexerEventEmitter;
use crate::indexer::start::{start_historical_indexing, start_live_indexing};
use crate::{
    api::{start_graphql_server, GraphqlOverrideSettings, StartGraphqlServerError},
    database::postgres::{
        client::PostgresConnectionError,
        indexes::{ApplyPostgresIndexesError, PostgresIndexResult},
        relationship::{ApplyAllRelationships, Relationship},
        setup::{setup_postgres, SetupPostgresError},
    },
    event::callback_registry::{EventCallbackRegistry, TraceCallbackRegistry},
    health::start_health_server,
    indexer::{
        no_code::{setup_no_code, SetupNoCodeError},
        start::StartIndexingError,
        ContractEventDependencies, ContractEventDependenciesMapFromRelationshipsError,
    },
    initiate_shutdown,
    logger::mark_shutdown_started,
    manifest::{
        core::ProjectType,
        storage::RelationshipsAndIndexersError,
        yaml::{read_manifest, ReadManifestError},
    },
    setup_clickhouse, setup_info_logger, RindexerEventStream,
};
pub struct IndexingDetails {
    pub registry: EventCallbackRegistry,
    pub trace_registry: TraceCallbackRegistry,
    pub event_stream: Option<RindexerEventStream>,
}
pub struct StartDetails<'a> {
    pub manifest_path: &'a PathBuf,
    pub indexing_details: Option<IndexingDetails>,
    pub graphql_details: GraphqlOverrideSettings,
}
#[derive(thiserror::Error, Debug)]
pub enum StartRindexerError {
    #[error("Could not work out project path from the parent of the manifest")]
    NoProjectPathFoundUsingParentOfManifestPath,
    #[error("Could not read manifest: {0}")]
    CouldNotReadManifest(#[from] ReadManifestError),
    #[error("Could not start graphql error {0}")]
    CouldNotStartGraphqlServer(#[from] StartGraphqlServerError),
    #[error("Failed to listen to graphql socket")]
    FailedToListenToGraphqlSocket,
    #[error("Could not setup postgres: {0}")]
    SetupPostgresError(#[from] SetupPostgresError),
    #[error("Could not setup clickhouse: {0}")]
    SetupClickhouseError(#[from] SetupClickhouseError),
    #[error("Could not start indexing: {0}")]
    CouldNotStartIndexing(#[from] StartIndexingError),
    #[error("{0}")]
    PostgresConnectionError(#[from] PostgresConnectionError),
    #[error("{0}")]
    ApplyRelationshipError(#[from] ApplyAllRelationships),
    #[error("Could not apply indexes: {0}")]
    ApplyPostgresIndexesError(#[from] ApplyPostgresIndexesError),
    #[error("{0}")]
    ContractEventDependenciesMapFromRelationshipsError(
        #[from] ContractEventDependenciesMapFromRelationshipsError,
    ),
    #[error("{0}")]
    RelationshipsAndIndexersError(#[from] RelationshipsAndIndexersError),
    #[error("Shutdown handler failed with error: {0}")]
    ShutdownHandlerFailed(String),
    #[error("Port conflict: {0}")]
    PortConflict(String),
    #[error("Could not start Reth node: {0}")]
    CouldNotStartRethNode(#[from] eyre::Error),
    #[error("Reth CLI error: {0}")]
    RethCliError(#[from] Box<dyn std::error::Error>),
}
async fn handle_shutdown(signal: &str) {
    // Mark shutdown state only once, at the very beginning of the shutdown process
    mark_shutdown_started();
    info!("Received {} signal gracefully shutting down...", signal);
    initiate_shutdown().await;
    // These info! calls work because they're before/after the shutdown process
    info!("Graceful shutdown completed for {}", signal);
    std::process::exit(0);
}
pub async fn start_rindexer(details: StartDetails<'_>) -> Result<(), StartRindexerError> {
    info!(
        "🚀 start_rindexer called with indexing_details.is_some() = {}",
        details.indexing_details.is_some()
    );
    let project_path = details.manifest_path.parent();
    match project_path {
        Some(project_path) => {
            #[cfg(unix)]
            let shutdown_handle = {
                let mut sigterm = signal::unix::signal(signal::unix::SignalKind::terminate())
                    .map_err(|e| StartRindexerError::ShutdownHandlerFailed(e.to_string()))?;
                let mut sigint = signal::unix::signal(signal::unix::SignalKind::interrupt())
                    .map_err(|e| StartRindexerError::ShutdownHandlerFailed(e.to_string()))?;
                let mut sigquit = signal::unix::signal(signal::unix::SignalKind::quit())
                    .map_err(|e| StartRindexerError::ShutdownHandlerFailed(e.to_string()))?;
                tokio::spawn(async move {
                    tokio::select! {
                        _ = sigterm.recv() => handle_shutdown("SIGTERM").await,
                        _ = sigint.recv() => handle_shutdown("SIGINT (Ctrl+C)").await,
                        _ = sigquit.recv() => handle_shutdown("SIGQUIT").await,
                    }
                })
            };
            // On Windows, we just use Ctrl+C to trigger shutdown
            #[cfg(windows)]
            let shutdown_handle = tokio::spawn(async move {
                if let Err(e) = signal::ctrl_c().await {
                    error!("Failed to register Ctrl+C handler: {}", e);
                    panic!("Ctrl+C handler failed: {}", e);
                }
                handle_shutdown("Ctrl+C").await
            });
            let manifest = Arc::new(read_manifest(details.manifest_path)?);
            if manifest.project_type != ProjectType::NoCode {
                setup_info_logger();
                info!("Starting rindexer rust project");
            }
            // Spawn a separate task for the GraphQL server if specified
            let graphql_server_handle =
                if details.graphql_details.enabled && manifest.storage.postgres_enabled() {
                    let manifest_clone = Arc::clone(&manifest);
                    let indexer = manifest_clone.to_indexer();
                    let mut graphql_settings = manifest.graphql.clone().unwrap_or_default();
                    if let Some(override_port) = &details.graphql_details.override_port {
                        graphql_settings.set_port(*override_port);
                    }
                    Some(tokio::spawn(async move {
                        if let Err(e) = start_graphql_server(&indexer, &graphql_settings).await {
                            error!("Failed to start GraphQL server: {:?}", e);
                        }
                    }))
                } else {
                    None
                };
            // Check for port conflicts between GraphQL and health servers
            let graphql_port = if details.graphql_details.enabled {
                let mut graphql_settings = manifest.graphql.clone().unwrap_or_default();
                if let Some(override_port) = &details.graphql_details.override_port {
                    graphql_settings.set_port(*override_port);
                }
                Some(graphql_settings.port)
            } else {
                None
            };
            let health_port = manifest.global.health_port;
            if let Some(graphql_port) = graphql_port {
                if graphql_port == health_port {
                    return Err(StartRindexerError::PortConflict(format!(
                        "GraphQL and health servers cannot use the same port: {}",
                        graphql_port
                    )));
                }
            }
            // Health server follows the indexer lifecycle - only runs when indexer is running
            let health_server_handle = if details.indexing_details.is_some() {
                let manifest_clone = Arc::clone(&manifest);
                Some(tokio::spawn(async move {
                    info!("🩺 Starting health server on port {}", health_port);
                    let postgres_client = if manifest_clone.storage.postgres_enabled() {
                        match crate::indexer::start::initialize_database(&manifest_clone).await {
                            Ok(Some(client)) => Some(client),
                            Ok(None) => {
                                error!("PostgreSQL is enabled but no database client was created for health server");
                                None
                            }
                            Err(e) => {
                                error!("Failed to initialize database for health server: {:?}", e);
                                None
                            }
                        }
                    } else {
                        None
                    };
                    if let Err(e) =
                        start_health_server(health_port, manifest_clone, postgres_client).await
                    {
                        error!("Failed to start health server: {:?}", e);
                    }
                }))
            } else {
                None
            };
            if graphql_server_handle.is_none() && details.graphql_details.enabled {
                error!("GraphQL can not run without postgres storage enabled, you have tried to run GraphQL which will now be skipped.");
            }
            if let Some(mut indexing_details) = details.indexing_details {
                let postgres_enabled = &manifest.storage.postgres_enabled();
                let clickhouse_enabled = &manifest.storage.clickhouse_enabled();
                // setup postgres is already called in no-code startup
                if manifest.project_type != ProjectType::NoCode && *postgres_enabled {
                    setup_postgres(project_path, &manifest).await?;
                }
                // setup clickhouse is already called in no-code startup
                if manifest.project_type != ProjectType::NoCode && *clickhouse_enabled {
                    setup_clickhouse(project_path, &manifest).await?;
                }
                let (relationships, postgres_indexes) = manifest
                    .storage
                    .create_relationships_and_indexes(
                        project_path,
                        &manifest.name,
                        &manifest.all_contracts(),
                    )
                    .await?;
                let mut dependencies: Vec<ContractEventDependencies> =
                    ContractEventDependencies::parse(&manifest);
                let processed_network_contracts = start_historical_indexing(
                    &manifest,
                    project_path,
                    &dependencies,
                    indexing_details.registry.complete(),
                    indexing_details.trace_registry.complete(),
                    indexing_details.event_stream.map(RindexerEventEmitter::from_stream),
                )
                .await?;
                // TODO if graphql isn't up yet, and we apply this on graphql wont refresh we need to handle this
                PostgresIndexResult::apply_indexes(postgres_indexes).await?;
                if !relationships.is_empty() {
                    // TODO if graphql isn't up yet, and we apply this on graphql wont refresh we
                    // need to handle this
                    info!("Applying constraints relationships back to the database as historic resync is complete");
                    Relationship::apply_all(&relationships).await?;
                }
                if manifest.has_any_contracts_live_indexing() {
                    if dependencies.is_empty() {
                        dependencies =
                            ContractEventDependencies::map_from_relationships(&relationships)?;
                    } else {
                        info!("Manual dependency_events found, skipping auto-applying the dependency_events with the relationships");
                    }
                    start_live_indexing(
                        &manifest,
                        project_path,
                        &dependencies,
                        indexing_details
                            .registry
                            .reapply_after_historic(processed_network_contracts),
                        indexing_details.trace_registry.complete(),
                    )
                    .await
                    .map_err(StartRindexerError::CouldNotStartIndexing)?;
                }
                // Do not need now with the main shutdown keeping around in-case
                // if details.graphql_details.enabled {
                //     signal::ctrl_c()
                //         .await
                //         .map_err(|_| StartRindexerError::FailedToListenToGraphqlSocket)?;
                // }
            }
            if graphql_server_handle.is_none() && !manifest.has_any_contracts_live_indexing() {
                return Ok(());
            }
            match (graphql_server_handle, health_server_handle, shutdown_handle) {
                (Some(graphql_handle), Some(health_handle), shutdown_handle) => {
                    info!("Waiting on GraphQL server, health server, and shutdown signal...");
                    tokio::select! {
                        result = graphql_handle => {
                            if let Err(e) = result {
                                error!("GraphQL server task failed: {:?}", e);
                            }
                        }
                        result = health_handle => {
                            if let Err(e) = result {
                                error!("Health server task failed: {:?}", e);
                            }
                        }
                        result = shutdown_handle => {
                            result.map_err(|e| {
                                error!("Shutdown handler failed: {:?}", e);
                                StartRindexerError::ShutdownHandlerFailed(e.to_string())
                            })?;
                        }
                    }
                }
                (Some(graphql_handle), None, shutdown_handle) => {
                    info!("Waiting on GraphQL server and shutdown signal...");
                    tokio::select! {
                        result = graphql_handle => {
                            if let Err(e) = result {
                                error!("GraphQL server task failed: {:?}", e);
                            }
                        }
                        result = shutdown_handle => {
                            result.map_err(|e| {
                                error!("Shutdown handler failed: {:?}", e);
                                StartRindexerError::ShutdownHandlerFailed(e.to_string())
                            })?;
                        }
                    }
                }
                (None, Some(health_handle), shutdown_handle) => {
                    info!("Waiting on health server and shutdown signal...");
                    tokio::select! {
                        result = health_handle => {
                            if let Err(e) = result {
                                error!("Health server task failed: {:?}", e);
                            }
                        }
                        result = shutdown_handle => {
                            result.map_err(|e| {
                                error!("Shutdown handler failed: {:?}", e);
                                StartRindexerError::ShutdownHandlerFailed(e.to_string())
                            })?;
                        }
                    }
                }
                (None, None, shutdown_handle) => {
                    info!("Waiting for shutdown signal...");
                    shutdown_handle.await.map_err(|e| {
                        error!("Shutdown handler failed: {:?}", e);
                        StartRindexerError::ShutdownHandlerFailed(e.to_string())
                    })?;
                }
            }
            Ok(())
        }
        None => Err(StartRindexerError::NoProjectPathFoundUsingParentOfManifestPath),
    }
}
pub struct IndexerNoCodeDetails {
    pub enabled: bool,
}
pub struct StartNoCodeDetails<'a> {
    pub manifest_path: &'a PathBuf,
    pub indexing_details: IndexerNoCodeDetails,
    pub graphql_details: GraphqlOverrideSettings,
}
#[derive(thiserror::Error, Debug)]
pub enum StartRindexerNoCode {
    #[error("{0}")]
    StartRindexerError(#[from] StartRindexerError),
    #[error("{0}")]
    SetupNoCodeError(#[from] SetupNoCodeError),
}
pub async fn start_rindexer_no_code(
    details: StartNoCodeDetails<'_>,
) -> Result<(), StartRindexerNoCode> {
    let start_details = setup_no_code(details).await?;
    start_rindexer(start_details).await.map_err(StartRindexerNoCode::StartRindexerError)
}
</file>

<file path="core/src/system_state.rs">
use std::{
    sync::atomic::{AtomicBool, Ordering},
    time::Duration,
};
use once_cell::sync::Lazy;
use tracing::info;
use crate::indexer::task_tracker::active_indexing_count;
static IS_RUNNING: Lazy<AtomicBool> = Lazy::new(|| AtomicBool::new(true));
pub async fn initiate_shutdown() {
    IS_RUNNING.store(false, Ordering::SeqCst);
    let mut active = active_indexing_count();
    info!("Starting shutdown with {} active tasks", active);
    loop {
        if active == 0 {
            info!("All active indexing tasks finished shutting down system...");
            break;
        }
        info!("{} active indexing tasks pending.. shutting them down gracefully", active);
        tokio::time::sleep(Duration::from_millis(100)).await;
        active = active_indexing_count();
    }
    info!("Shutdown complete");
}
pub fn is_running() -> bool {
    IS_RUNNING.load(Ordering::SeqCst)
}
</file>

<file path="core/build.rs">
use std::{
    env, fs,
    path::{Path, PathBuf},
    process::Command,
};
fn main() {
    build_graphql_if_needed();
    build_blockclock_if_needed();
}
/// Copies the BlockClock binary files to the resources directory.
fn build_blockclock_if_needed() {
    let blockclock_dir_og = PathBuf::from("resources/blockclock");
    // Verify BlockClock source directory exists before proceeding
    if !blockclock_dir_og.exists() {
        println!("cargo:warning=BlockClock directory not found, skipping BlockClock binary build");
        return;
    }
    let out_dir = env::var("OUT_DIR").expect("OUT_DIR not set");
    let target_dir = PathBuf::from(&out_dir)
        .join("../../../..") // Navigate up to the top-level target directory
        .join("resources");
    if target_dir.exists() {
        fs::remove_dir_all(&target_dir).expect("Failed to remove old resources directory");
    }
    fs::create_dir_all(&target_dir).expect("Failed to create resources directory");
    let blockclock_dir = target_dir.join("blockclock");
    if blockclock_dir.exists() {
        fs::remove_dir_all(&blockclock_dir).expect("Failed to remove old blockclock directory");
    }
    fs::create_dir_all(&blockclock_dir).expect("Failed to recreate blockclock directory");
    for entry in fs::read_dir(blockclock_dir_og).expect("Failed to read blockclock directory") {
        let entry = entry.expect("Failed to read directory entry");
        let path = entry.path();
        if path.is_file() {
            fs::copy(
                &path,
                blockclock_dir.join(path.file_name().expect("Failed to get file name")),
            )
            .expect("Failed to copy file");
        }
    }
    println!("cargo:rerun-if-changed=resources");
    println!("cargo:rustc-env=BLOCKCLOCK_RESOURCES={}", out_dir);
}
/// Builds the GraphQL binary if needed.
fn build_graphql_if_needed() {
    let manifest_dir =
        PathBuf::from(env::var("CARGO_MANIFEST_DIR").expect("CARGO_MANIFEST_DIR not set"));
    let graphql_dir = manifest_dir.join("../graphql");
    // Verify GraphQL directory exists before proceeding
    if !graphql_dir.exists() {
        println!("cargo:warning=GraphQL directory not found, skipping GraphQL binary build");
        return;
    }
    // Check for Node.js and npm before attempting to use them
    check_node_availability();
    let out_dir = PathBuf::from(env::var("OUT_DIR").expect("OUT_DIR not set"));
    let resources_dir = out_dir.join("resources");
    // Ensure the resources directory exists
    fs::create_dir_all(&resources_dir)
        .unwrap_or_else(|e| panic!("Failed to create resources directory: {e}"));
    let target_info = get_target_info();
    let final_exe_path = resources_dir.join(&target_info.exe_name);
    // Check if we need to build
    if should_rebuild(&final_exe_path, &graphql_dir) {
        // Remove old binary if it exists
        if final_exe_path.exists() {
            if let Err(e) = fs::remove_file(&final_exe_path) {
                println!(
                    "cargo:warning=Failed to remove existing binary: {e}. Continuing with build."
                );
            }
        }
        println!(
            "cargo:warning=Building GraphQL binary for {}-{}...",
            target_info.os, target_info.arch
        );
        build_graphql_binary(&graphql_dir, &final_exe_path, &target_info, &out_dir);
    } else {
        println!("cargo:warning=GraphQL binary is up to date, skipping build");
        // Still create the embedded binary file for include_bytes!
        create_embedded_binary(&final_exe_path, &out_dir);
    }
    // Register build dependencies
    register_build_dependencies(&manifest_dir);
}
fn create_embedded_binary(exe_path: &Path, out_dir: &Path) {
    // Copy the binary to a predictable location for include_bytes!
    let embed_path = out_dir.join("graphql_binary");
    if let Err(e) = fs::copy(exe_path, &embed_path) {
        panic!("Failed to copy binary for embedding: {e}");
    }
    // Set environment variables for runtime
    println!("cargo:rustc-env=RINDEXER_GRAPHQL_EXE={}", exe_path.display());
    println!("cargo:rustc-env=RINDEXER_GRAPHQL_EMBED={}", embed_path.display());
}
fn should_rebuild(exe_path: &Path, graphql_dir: &Path) -> bool {
    // If binary doesn't exist, rebuild
    if !exe_path.exists() {
        println!("cargo:warning=Binary doesn't exist, rebuilding");
        return true;
    }
    // Check if package-lock.json is newer than the binary (dependencies changed)
    let package_lock = graphql_dir.join("package-lock.json");
    if package_lock.exists() {
        if let (Ok(exe_time), Ok(lock_time)) = (
            exe_path.metadata().and_then(|m| m.modified()),
            package_lock.metadata().and_then(|m| m.modified()),
        ) {
            if lock_time > exe_time {
                println!("cargo:warning=package-lock.json is newer than binary, rebuilding");
                return true;
            }
        }
    }
    // Check if any main source files are newer than the binary
    let source_files = ["index.js", "package.json"];
    if let Ok(exe_time) = exe_path.metadata().and_then(|m| m.modified()) {
        for source_file in &source_files {
            let source_path = graphql_dir.join(source_file);
            if source_path.exists() {
                if let Ok(source_time) = source_path.metadata().and_then(|m| m.modified()) {
                    if source_time > exe_time {
                        println!(
                            "cargo:warning=Source file {source_file} is newer than binary, rebuilding"
                        );
                        return true;
                    }
                }
            }
        }
    }
    false
}
struct TargetInfo {
    os: String,
    arch: String,
    exe_name: String,
    pkg_target: String,
}
fn get_target_info() -> TargetInfo {
    let os = env::var("CARGO_CFG_TARGET_OS").expect("CARGO_CFG_TARGET_OS not set");
    let arch = env::var("CARGO_CFG_TARGET_ARCH").expect("CARGO_CFG_TARGET_ARCH not set");
    let node_arch = match arch.as_str() {
        "x86_64" => "x64",
        "aarch64" => "arm64",
        _ => panic!("Unsupported architecture: {arch}. Supported: x86_64, aarch64"),
    };
    let pkg_os = if os == "windows" { "win".to_string() } else { os.clone() };
    // Using Node.js v22 as it's the current LTS version.
    let pkg_target = format!("node22-{pkg_os}-{node_arch}");
    let exe_suffix = if os == "windows" { ".exe" } else { "" };
    let exe_name = format!("rindexer-graphql-{os}-{node_arch}{exe_suffix}");
    TargetInfo { os: os.clone(), arch: node_arch.to_string(), exe_name, pkg_target }
}
fn check_node_availability() {
    let node_check = Command::new("node").arg("--version").output();
    match node_check {
        Ok(output) if output.status.success() => {
            let version = String::from_utf8_lossy(&output.stdout);
            println!("cargo:warning=Found Node.js version: {}", version.trim());
        }
        Ok(_) => {
            panic!("Node.js is installed but not working properly. Please reinstall Node.js.");
        }
        Err(_) => {
            panic!(
                "Node.js is not installed or not in your PATH. \
                Please install Node.js (LTS version is recommended) to build the GraphQL server.\
                \nVisit: https://nodejs.org/"
            );
        }
    }
    // Check npm
    let npm_command =
        if env::var("CARGO_CFG_TARGET_OS").unwrap() == "windows" { "npm.cmd" } else { "npm" };
    if Command::new(npm_command).arg("--version").output().is_err() {
        panic!("npm is not available. Please ensure npm is installed with Node.js.");
    }
}
fn build_graphql_binary(
    graphql_dir: &Path,
    final_exe_path: &Path,
    target_info: &TargetInfo,
    out_dir: &Path,
) {
    let npm_command = if target_info.os == "windows" { "npm.cmd" } else { "npm" };
    // 1. Install npm dependencies
    run_command(
        npm_command,
        &["install"],
        graphql_dir,
        "npm install failed. Please ensure package.json is valid.",
    );
    // 2. Build the binary using pkg, passing the final output path
    run_command(
        npm_command,
        &[
            "run",
            "build",
            "--",
            "--targets",
            &target_info.pkg_target,
            "--output",
            final_exe_path.to_str().expect("Invalid final_exe_path"),
        ],
        graphql_dir,
        "npm run build failed. Check the build script in package.json.",
    );
    if !final_exe_path.exists() {
        panic!("Build did not produce the expected binary: {}", final_exe_path.display());
    }
    println!("cargo:warning=Successfully built GraphQL binary: {}", final_exe_path.display());
    // Create the embedded version
    create_embedded_binary(final_exe_path, out_dir);
}
fn register_build_dependencies(manifest_dir: &Path) {
    // Watch for changes in the graphql directory, which contains the node project.
    let graphql_dir = manifest_dir.join("../graphql");
    println!("cargo:rerun-if-changed={}", graphql_dir.display());
    // Also explicitly watch the package-lock.json to ensure dependency changes trigger a rebuild.
    let lock_file = graphql_dir.join("package-lock.json");
    if lock_file.exists() {
        println!("cargo:rerun-if-changed={}", lock_file.display());
    }
    // Watch for changes to this build script itself
    println!("cargo:rerun-if-changed=build.rs");
}
/// Executes a command with enhanced error reporting and validation.
fn run_command(command: &str, args: &[&str], cwd: &Path, error_msg: &str) {
    let output = Command::new(command).args(args).current_dir(cwd).output().unwrap_or_else(|e| {
        panic!(
            "Failed to execute command '{}': {}\nCWD: {}\nError: {}",
            command,
            args.join(" "),
            cwd.display(),
            e
        )
    });
    if !output.status.success() {
        let stdout = String::from_utf8_lossy(&output.stdout);
        let stderr = String::from_utf8_lossy(&output.stderr);
        panic!(
            "{}\
            ╭─ Command Failed ─────────────────────────────────────\n\
            │ Command: {} {}\n\
            │ Working Directory: {}\n\
            │ Exit Status: {}\n\
            ├─ Stdout ────────────────────────────────────────────\n\
            │ {}\n\
            ├─ Stderr ────────────────────────────────────────────\n\
            │ {}\n\
            ╰──────────────────────────────────────────────────────",
            error_msg,
            command,
            args.join(" "),
            cwd.display(),
            output.status,
            if stdout.is_empty() { "(empty)" } else { stdout.trim() },
            if stderr.is_empty() { "(empty)" } else { stderr.trim() }
        );
    }
}
</file>

<file path="core/Cargo.toml">
[package]
name = "rindexer"
version = "0.29.0"
edition = "2021"
description = "A no-code or framework to build blazing fast EVM indexers - built in rust."
license = "MIT"
repository = "https://github.com/joshstevens19/rindexer"
readme = "README.md"
resolver = "2"
build = "build.rs"

include = ["src/**", "resources/**", "Cargo.toml", "build.rs"]

[dev-dependencies]
tempfile = "3.23"
mockito = "1.7.0"

[dependencies]
# TODO: Trim down alloy features needed later, for now opt iun to all.
alloy = { version = "1.1.3", features = ["full", "json-rpc"] }
async-trait = "0.1.89"
aws-config = "1.8.8"
aws-sdk-sns = "1.88.0"
bb8 = "0.9.0"
bb8-postgres = "0.9.0"
bb8-redis = "0.24.0"
bytes = "1.10.1"
chrono = { version = "0.4", features = ["serde"] }
colored = "3.0.0"
csv = "1.4.0"
clickhouse = { version = "0.13.3", features = ["rustls-tls"] }
deadpool = { version = "0.12", features = ["rt_tokio_1"] }
deadpool-lapin = "0.13"
dotenv = "0.15.0"
futures = "0.3.31"
hex = "0.4.3"
lapin = "3.7.1"
lazy_static = "1.5.0"
lru = "0.16.2"
native-tls = "0.2"
once_cell = "1.21.3"
postgres-native-tls = "0.5"
rand = "0.9.2"
# Redis version must match bb8-redis internal version. Can be removed in favor of just
# bb8-redis once https://github.com/djc/bb8/pull/183 is merged.
redis = { version = "0.32.7", features = ["streams"] }
regex = "1.12.2"
reqwest = { version = "0.12.24", features = ["json", "gzip"] }
rust_decimal = { version = "1.39.0", features = ["db-tokio-postgres"] }
serde = "1.0"
serde_json = "1.0"
serde_yaml = "0.9.34"
serenity = { version = "0.12", features = ["client", "framework"] }
teloxide = "0.17"
tempfile = "3.23.0"
thiserror = "2.0.17"
thread_local = "1.1"
tokio = { version = "1", features = ["full"] }
tokio-postgres = { version = "0.7", features = [
    "with-uuid-1",
    "with-chrono-0_4",
    "with-serde_json-1",
] }
tokio-stream = "0.1.17"
tracing = "0.1"
tracing-subscriber = { version = "0.3.20", features = [
    "env-filter",
    "fmt",
    "time",
] }
mini-moka = "0.10.3"
url = "2.5.7"
uuid = "1"
eyre = { version = "0.6.12" }
cfg-if = "1.0.4"
zstd = "0.13.3"
bincode = "2.0.1"
anyhow = "1.0.100"
winnow = "0.7.13"
tower = "0.5.2"
port-killer = "0.1.0"
axum = "0.8"

# build
jemallocator = { version = "0.6.1", package = "tikv-jemallocator", optional = true }
jemalloc-ctl = { version = "0.6.1", package = "tikv-jemalloc-ctl", optional = true }
foundry-compilers = "0.19.5"
alloy-chains = "0.2.15"

# reth
reth = { git = "https://github.com/paradigmxyz/reth", tag = "v1.8.2", optional = true }
reth-exex = { git = "https://github.com/paradigmxyz/reth", tag = "v1.8.2", features = ["serde"], optional = true }
reth-node-api = { git = "https://github.com/paradigmxyz/reth", tag = "v1.8.2", optional = true }
reth-node-ethereum = { git = "https://github.com/paradigmxyz/reth", tag = "v1.8.2", optional = true }
reth-tracing = { git = "https://github.com/paradigmxyz/reth", tag = "v1.8.2", optional = true }

[target.'cfg(not(windows))'.dependencies]
rdkafka = { version = "0.38.0", features = ["tokio"], optional = true }

[target.'cfg(windows)'.dependencies]
rdkafka = { version = "0.38.0", features = ["tokio", "cmake-build"], optional = true }

[profile.release]
lto = "fat"
codegen-units = 1
incremental = false

[features]
jemalloc = ["dep:jemallocator", "dep:jemalloc-ctl"]
debug-json = []
kafka = ["dep:rdkafka"]
reth = [
    "dep:reth",
    "dep:reth-exex",
    "dep:reth-node-api",
    "dep:reth-node-ethereum",
    "dep:reth-tracing",
]
</file>

<file path="core/README.md">
# 🦀 rindexer 🦀

Note rindexer is brand new and actively under development, things will change and bugs will exist - if you find any bugs or have any
feature requests please open an issue on [github](https://github.com/joshstevens19/rindexer/issues).

rindexer is an opensource powerful, high-speed indexing toolset developed in Rust, designed for compatibility with any EVM chain.
This tool allows you to index chain events using a simple YAML file, requiring no additional coding.
For more advanced needs, the rindexer provides foundations and advanced capabilities to build whatever you want.
It's highly extendable, enabling you to construct indexing pipelines with ease and focus exclusively on the logic.
rindexer out the box also gives you a GraphQL API to query the data you have indexed instantly.

You can get to the full rindexer [documentation](https://rindexer.xyz/).

rust crate is [here](https://crates.io/crates/rindexer) but you can also install via github.

all rust documentation can be found [here](https://rindexer.xyz/docs/start-building/rust-project-deep-dive)[Cargo.toml](Cargo.toml)
</file>

<file path="documentation/docs/pages/docs/accessing-data/direct-sql.mdx">
# Direct SQL

## DBeaver - recommended

If you are wanting to access the data directly from the database you can use a tool like DBeaver to connect to the database.
you can download it [here](https://dbeaver.io/download/) and is supported on all platforms.

If wanting to connect to your docker postgres instance database you can create a new connection in DBeaver and use the following settings:

- host: localhost
- port: 5440
- database: postgres
- username: postgres
- password: rindexer

press test connection and you should be able to connect to the database. You can then go to `postgres` -> `schemas` and you see
the indexer schemas and tables. From there you can run SQL queries to get the data you need inside DBeaver.

## PSQL

If you are wanting to use the command line you can use psql to connect to the database.

These instructions run you through how to install psql - https://www.timescale.com/blog/how-to-install-psql-on-mac-ubuntu-debian-windows/

### Connect

```bash
psql 'postgresql://username:password@localhost:5432/your_database'
```

### Listing all tables across all schemas

rindexer uses schemas to break up the tables and by default psql only shows public
so you need to run the following command to see all tables across all schemas.

```bash
\dt *.*
```

You can also run the following command to see all tables in a specific schema

```bash
\dn schema_name.*
```

### Query data

You can now just run SQL queries to get the data you need.

```sql
SELECT * FROM my_project_rocket_pool_eth.transfer;
```

### Exit

To exit the `psql` terminal run

```bash
exit
```
</file>

<file path="documentation/docs/pages/docs/accessing-data/graphql.mdx">
# GraphQL

GraphQL is a query language for your API, and a server-side runtime for executing queries using a type system you define for your data.

you can learn all about graphql [here](https://graphql.org)

## Hot Tip

As GraphQL is a type system this means building queries can be a bit tricky, if you are not familiar with GraphQL.
The beauty of this is using the http://localhost:3001/playground supplied for you allows you to build up all your queries
but also understand every single filter and ordering you can do.

## Querying the data

The GraphQL will expose a playground for you which you can get to on http://localhost:3001/playground
this uses apollo server sandbox which is a great tool for testing and building up your queries - https://studio.apollographql.com/sandbox/explorer.

Note in these examples we will put the raw parameters in the graphql query but you can pass parameters in using the `$` syntax allowing
code to define the parameters.

:::code-group

```graphql [hardcoded parameter]
query AllTransfers {
  allTransfers(first: 20) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

```graphql [parameter passed in]
query AllTransfers($first: Int!) {
  allTransfers(first: $$first) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

:::


### Query naming conventions

lets say we had 2 events `Approval` and `Transfer` from the ERC20 standard, the ABI would look like the below:

```json
{
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "spender",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Approval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "from",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Transfer",
    "type": "event"
  }
```

with rindexer graphql you could generate the following queries to get the transfer data you need:

:::code-group

```graphql [list of transfers]
query AllTransfers {
  allTransfers {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

```graphql [single transfer]
query Transfer($nodeId: ID!) {
  transfer(nodeId: $nodeId) {
    nodeId
    rindexerId
    contractAddress
    from
    to
    value
    txHash
    blockNumber
    blockHash
    network
  }
}
```

:::

The format of the query names are:
- list items = `all{event_name}s` = `All` + `Transfer` + `s` = `AllTransfers`
- single item = `{event_name}` (lowercase) = `transfer`

For single item queries you can use the `nodeId` to query single items which is always returned as a field
in the list results alongside the singular item query.

#### Conflicting event naming

:::warning
Important to read if you have 2 events with matching names across contracts.
:::

If you have 2 events which have exactly the same name as another contract this is a conflict of naming for graphql so rindexer will render
it as `{contract_name}{event_name}` in pascal case, for example `Transfer` would turn into `{contract_name}Transfer`

So its is super clear lets say i had a yaml like this:

```yaml
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: 0xae78736cd615f374d3085123a210448e74fc6393
    start_block: '18600000'
    end_block: '18718056'
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
  - Transfer
- name: RocketPoolETHFork
  details:
  - network: ethereum
    address: 0xba78736cb615f374d3035123a210448e74fc6392
    start_block: '18600000'
    end_block: '18718056'
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
  - Transfer
```

My query names for `allTransfers` would be:

- `AllRocketPoolETHTransfers`
- `AllRocketPoolETHForkTransfers`

### Ordering

:::info
All filtering options and ordering can both be used together.
:::

You can order the results by any field you wish, you can also order by multiple fields the first item in the array will
be the applied ordering first then the next will be applied after and so on.

:::warning
It is advised to have indexes on any fields you which to filter on in your database to make the queries faster.
You can define your own indexes in the [storage](/docs/start-building/yaml-config/storage#indexes) section of the YAML configuration file.
:::

This example will get the first 20 transfers ordered by the block number ascending.

```graphql
query AllTransfers {
  allTransfers(first: 20, orderBy: [BLOCK_NUMBER_ASC]) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

### Filtering

:::info
All filtering options and ordering can both be used together.
:::

You can do condition filters as well as advanced filters on all the events indexed.

:::warning
It is advised to have indexes on any fields you which to filter on in your database to make the queries faster.
You can define your own indexes in the [storage](/docs/start-building/yaml-config/storage#indexes) section of the YAML configuration file.
:::

#### Condition

You can filter in every event property you want using the `condition` input fields.

The example below im filtering on all transfer based on the block number, which has to be a string as its a BigFloat.

```graphql
query AllTransfers {
  allTransfers(first: 20, condition: {
    blockNumber: "18600181"
  }) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

You can mix the filtering in every direction with any field so you can filter `blockNumber` with `from` and `to` with `value`
or even `network` with `contractAddress` and `txHash`, anything you wish.

```graphql
query AllTransfers {
  allTransfers(first: 20, condition: {
    blockNumber: "18600181",
    value: "2000000000000000000"
    from: "0x0338ce5020c447f7e668dc2ef778025ce398266b"
  }) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

#### Filter

:::info
Advanced filtering is enabled by default but these filters easily be abused and cause performance issues, if you wish to disable
it you can set `disable_advanced_filters` to true in the [graphql](/docs/start-building/yaml-config/graphql#disable_advanced_filters) section of the YAML configuration file.
:::

For more advanced filtering you can use the `filter` input field. For example if we wanted to get all transfer events
over 1 rEth (wei would be 1000000000000000000) and after block number 18600181 we can use the following query.

```graphql
query AllTransfers {
  allTransfers(first: 20, condition: {
    value: "1000000000000000000",
  }, filter: {
    blockNumber: {
      greaterThan: "18600181"
    }
  }) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

### Result limits

You can define how many you which to return using the `first` and `last` properties, you can not return more
then 1000 in a single query but you can use offset to get the item you wish to get. We advise to always
set a limit on the amount of items you wish to return.

- first will return the first inserted x items
- last will return the last inserted x items
- offset will return the first/last x items after the offset

:::code-group

```graphql [first]
query AllTransfers {
  allTransfers(first: 20) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

```graphql [last]
query AllTransfers {
  allTransfers(last: 20) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

```graphql [offset]
query AllTransfers {
  allTransfers(first: 20, offset: 20) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

:::

### Page info

The page info will give you the following information:

- endCursor: The cursor to continue from
- hasNextPage: If there is a next page
- hasPreviousPage: If there is a previous page
- startCursor: The cursor to start from

#### Cursor based pagination

Cursor-based pagination is a common approach to pagination that avoids some of the pitfalls of "classic" page-based pagination.
The idea is to encode the current state of the query into a "cursor" that can be passed back to the server to get the next page of results.

You can page through the data using `before` and `after` cursors, you can get the cursors from the `pageInfo` object.

- `before` will get the items before the cursor - this is how you go back in the data so say page 2 to page 1
- `after` will get the items after the cursor - this is how you go forward in the data so say page 1 to page 2

:::code-group

```graphql [next results]
query AllTransfers {
  allTransfers(
      first: 1,
      orderBy: [BLOCK_NUMBER_ASC],
      after: "WyJibG9ja19udW1iZXJfYXNjIixbMTg2MDAxODEsMV1d"
    ) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

```graphql [preview results]
query AllTransfers {
  allTransfers(
      first: 1,
      orderBy: [BLOCK_NUMBER_ASC],
      before: "WyJibG9ja19udW1iZXJfYXNjIixbMTg2MDAxODEsMV1d"
    ) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

:::

### Relationships

When you define [relationships](/docs/start-building/yaml-config/storage#relationships) between events rindexer will
automatically create relationships between the events in the database and expose them on the `GraphQL` interface, this
means you can query the relationships within a single query avoiding having to have multiple queries to get the data you need.

Lets walk through an example imagine we were playing around with the `lens` data and we want to get the profile metadata back
when we get quotes created. we can create a relationship between the `QuoteCreated` `quoteParams.profileId` and the `ProfileMetadataSet`
`profileId` events, note you should read about [relationships config](/docs/start-building/yaml-config/storage#relationships) first.

Your `rindexer.yaml` would look like:

```yaml
name: LensIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: polygon
    chain_id: 137
    rpc: https://polygon.gateway.tenderly.co
storage:
  postgres:
    enabled: true
    relationships: // [!code focus]
      - contract_name: LensHub // [!code focus]
        event_name: QuoteCreated // [!code focus]
        event_input_name: "quoteParams.profileId" // [!code focus]
        linked_to: // [!code focus]
          - contract_name: LensHub // [!code focus]
            event_name: ProfileMetadataSet // [!code focus]
            event_input_name: profileId // [!code focus]
contracts:
  - name: LensHub // [!code focus]
    details:
      - network: polygon
        address: 0xDb46d1Dc155634FbC732f92E853b10B288AD5a1d
        start_block: 59034400
        end_block: 59034400
    abi: ./abis/lens-hub-events-abi.json
    include_events: // [!code focus]
      - QuoteCreated // [!code focus]
      - ProfileMetadataSet // [!code focus]
```

So in this example the `allQuoteCreateds` and `quoteCreated` queries will allow you to get the `ProfileMetadataSet` event
in the same query. This is a basic example but you can see how you can query the relationships within the same query.

```
query AllQuoteCreateds {
  allQuoteCreateds {
    nodes {
      nodeId
      quoteParamsContentUri
      quoteParamsPointedProfileId
      quoteParamsPointedPubId
      by: profileMetadataSetByQuoteParamsProfileId {
        profileId
        metadata
        transactionExecutor
        timestamp
        txHash
        blockNumber
        blockHash
        network
      }
      timestamp
      txHash
    }
  }
}
```

:::info
GraphQL supports aliases to make your queries read even nicer, you can read more about them [here](https://graphql.org/learn/queries/#aliases).
People may not like the event input names and can easily alias them to something more readable.
:::
</file>

<file path="documentation/docs/pages/docs/advanced/using-reth-exex.md">
# Using Reth Execution Extensions (ExEx)

Reth Execution Extensions (ExEx) is a powerful framework introduced by Reth for building high-performance off-chain infrastructure as post-execution hooks. rindexer leverages ExEx to provide superior indexing performance and native reorg handling.

## What is ExEx?

ExEx provides a reorg-aware stream called `ExExNotification` which includes:
- Blocks with full transaction data
- Receipts with logs and state changes
- Native reorg notifications
- Trie updates for state verification

This allows rindexer to:
- Process blocks at native speed without RPC overhead
- Handle reorganizations automatically
- Maintain consistency during chain splits
- Access pending transactions and state

## Architecture

When running in Reth mode, rindexer operates as an execution extension within the Reth node:

```
┌─────────────┐     ┌──────────────┐     ┌─────────────┐
│  Reth Node  │────▶│ rindexer ExEx│────▶│ PostgreSQL  │
│             │     │              │     │             │
│             │◀────│   Indexing   │     │   Storage   │
└─────────────┘     └──────────────┘     └─────────────┘
     ExEx              Process              Write
  Notifications         Events              Data
```

## Chain State Notifications

rindexer processes three types of chain state notifications:

### 1. Committed
Emitted when new blocks are added to the canonical chain:
```rust
Committed {
    from_block: 19000000,
    to_block: 19000100,
    tip_hash: 0x123...
}
```

### 2. Reorged
Emitted during reorganizations:
```rust
Reorged {
    // Blocks to revert
    revert_from_block: 19000098,
    revert_to_block: 19000100,
    // New canonical blocks
    new_from_block: 19000098,
    new_to_block: 19000101,
    new_tip_hash: 0x456...
}
```

### 3. Reverted
Emitted when blocks are reverted (chain rollback):
```rust
Reverted {
    from_block: 19000099,
    to_block: 19000100
}
```

## Configuration

### Basic Configuration

```yaml [rindexer.yaml]
name: HighPerformanceIndexer
networks:
- name: ethereum
  chain_id: 1
  rpc: https://eth.llamarpc.com  # Fallback RPC
  reth:
    enabled: true
    logging: true  # Enable Reth logs
    cli_args:
      - "--datadir /data/reth"
      - "--authrpc.jwtsecret /secrets/jwt.hex"
      - "--authrpc.port 8551"
      - "--chain mainnet"
```

### Advanced Configuration

```yaml [rindexer.yaml]
networks:
- name: ethereum
  chain_id: 1
  reth:
    enabled: true
    logging: false  # Disable for production
    cli_args:
      # Core settings
      - "--datadir /nvme/reth"  # Fast NVMe storage
      - "--authrpc.jwtsecret /secrets/jwt.hex"
      - "--authrpc.addr 127.0.0.1"
      - "--authrpc.port 8551"
      
      # Archive node (required)
      - "--full false"
      
      # Performance tuning
      - "--db.log-level error"
      - "--max-outbound-peers 100"
      - "--max-inbound-peers 50"
      
      # Metrics
      - "--metrics 127.0.0.1:9001"
      
      # HTTP RPC (optional)
      - "--http"
      - "--http.addr 0.0.0.0"
      - "--http.port 8545"
      - "--http.api eth,net,web3,debug,trace"
```

## Performance Considerations

### Hardware Requirements

For optimal ExEx performance:
- **CPU**: 8+ cores recommended
- **RAM**: 32GB minimum, 64GB recommended
- **Storage**: NVMe SSD with 2TB+ for mainnet archive
- **Network**: Stable connection for peer synchronization


## Best Practices

1. **Use Archive Node**: Run Reth in archive mode for ExEx.
2. **Monitor Resources**: Set up alerts for disk, CPU, and memory

## Migration from Standard Mode

To migrate an existing project to ExEx:

1. **Sync Reth Node**: Ensure fully synced archive node
2. **Update Config**: Add `reth` section to networks
4. **Reindex**: Consider full reindex for consistency

## Further Resources

- [Reth ExEx Documentation](https://reth.rs/developers/exex.html)
- [Running Reth on Ethereum](https://reth.rs/run/ethereum)
- [rindexer Reth Mode Guide](/docs/start-building/create-new-project/reth-mode)
</file>

<file path="documentation/docs/pages/docs/deploying/aws.mdx">
# AWS

## Prerequisites

Ensure that you have the following installed and configured:

- **[AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)**: Configured with necessary permissions.
- **[kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/)**: Installed and configured.
- **[Helm](https://helm.sh/docs/intro/install/)**: Installed.
- **[eksctl](https://eksctl.io/installation/)**: Installed.

## 1. Create an EKS Cluster

This command creates a new EKS cluster with a managed node group. Adjust the `--region`, `--node-type`, and node count options as needed.

```bash
eksctl create cluster --name my-cluster --region us-west-2 --nodegroup-name standard-workers --node-type t3.medium --nodes 1 --nodes-min 1 --nodes-max 2 --managed
```

Output:

```bash
2024-08-20 18:21:15 [ℹ]  eksctl version 0.189.0-dev+c9afc4260.2024-08-19T12:43:03Z
2024-08-20 18:21:15 [ℹ]  using region us-west-2
2024-08-20 18:21:16 [ℹ]  setting availability zones to [us-west-2c us-west-2d us-west-2b]
2024-08-20 18:21:16 [ℹ]  subnets for us-west-2c - public:192.168.0.0/19 private:192.168.96.0/19
2024-08-20 18:21:16 [ℹ]  subnets for us-west-2d - public:192.168.32.0/19 private:192.168.128.0/19
2024-08-20 18:21:16 [ℹ]  subnets for us-west-2b - public:192.168.64.0/19 private:192.168.160.0/19
2024-08-20 18:21:16 [ℹ]  nodegroup "standard-workers" will use "" [AmazonLinux2/1.30]
2024-08-20 18:21:16 [ℹ]  using Kubernetes version 1.30
2024-08-20 18:21:16 [ℹ]  creating EKS cluster "my-cluster" in "us-west-2" region with managed nodes
2024-08-20 18:21:16 [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
2024-08-20 18:21:16 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=my-cluster'
2024-08-20 18:21:16 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "my-cluster" in "us-west-2"
2024-08-20 18:21:16 [ℹ]  CloudWatch logging will not be enabled for cluster "my-cluster" in "us-west-2"
2024-08-20 18:21:16 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-west-2 --cluster=my-cluster'
2024-08-20 18:21:16 [ℹ]  default addons coredns, vpc-cni, kube-proxy were not specified, will install them as EKS addons
2024-08-20 18:21:16 [ℹ]
2 sequential tasks: { create cluster control plane "my-cluster",
    2 sequential sub-tasks: {
        2 sequential sub-tasks: {
            1 task: { create addons },
            wait for control plane to become ready,
        },
        create managed nodegroup "standard-workers",
    }
}
2024-08-20 18:21:16 [ℹ]  building cluster stack "eksctl-my-cluster-cluster"
2024-08-20 18:21:18 [ℹ]  deploying stack "eksctl-my-cluster-cluster"
2024-08-20 18:21:48 [ℹ]  waiting for CloudFormation stack "eksctl-my-cluster-cluster"
...
2024-08-20 18:30:29 [ℹ]  creating addon
2024-08-20 18:30:29 [ℹ]  successfully created addon
2024-08-20 18:30:30 [!]  recommended policies were found for "vpc-cni" addon, but since OIDC is disabled on the cluster, eksctl cannot configure the requested permissions; the recommended way to provide IAM permissions for "vpc-cni" addon is via pod identity associations; after addon creation is completed, add all recommended policies to the config file, under `addon.PodIdentityAssociations`, and run `eksctl update addon`
2024-08-20 18:30:30 [ℹ]  creating addon
2024-08-20 18:30:31 [ℹ]  successfully created addon
2024-08-20 18:30:32 [ℹ]  creating addon
2024-08-20 18:30:32 [ℹ]  successfully created addon
2024-08-20 18:32:35 [ℹ]  building managed nodegroup stack "eksctl-my-cluster-nodegroup-standard-workers"
2024-08-20 18:32:37 [ℹ]  deploying stack "eksctl-my-cluster-nodegroup-standard-workers"
2024-08-20 18:32:37 [ℹ]  waiting for CloudFormation stack "eksctl-my-cluster-nodegroup-standard-workers"
...
2024-08-20 18:37:39 [✔]  saved kubeconfig as "/Users/rindexer/.kube/config"
2024-08-20 18:37:39 [ℹ]  no tasks
2024-08-20 18:37:39 [✔]  all EKS cluster resources for "my-cluster" have been created
2024-08-20 18:37:39 [✔]  created 0 nodegroup(s) in cluster "my-cluster"
2024-08-20 18:37:40 [ℹ]  nodegroup "standard-workers" has 1 node(s)
2024-08-20 18:37:40 [ℹ]  node "ip-192-168-22-89.us-west-2.compute.internal" is ready
2024-08-20 18:37:40 [ℹ]  waiting for at least 1 node(s) to become ready in "standard-workers"
2024-08-20 18:37:40 [ℹ]  nodegroup "standard-workers" has 1 node(s)
2024-08-20 18:37:40 [ℹ]  node "ip-192-168-22-89.us-west-2.compute.internal" is ready
2024-08-20 18:37:40 [✔]  created 1 managed nodegroup(s) in cluster "my-cluster"
2024-08-20 18:37:41 [ℹ]  kubectl command should work with "/Users/rindexer/.kube/config", try 'kubectl get nodes'
2024-08-20 18:37:41 [✔]  EKS cluster "my-cluster" in "us-west-2" region is ready
```

```bash
eksctl get cluster --name my-cluster --region us-west-2
```

Output:

```bash
NAME		VERSION	STATUS	CREATED			VPC			SUBNETS														SECURITYGROUPS		PROVIDER
my-cluster	1.30	ACTIVE	2024-08-20T16:21:42Z	vpc-090d3761130933be4	subnet-00f479ddeb9bc51f7,subnet-0123eaaf4d9fb037a,subnet-09256a39c7e39ad7c,subnet-0df075e1795076648,subnet-0ed78cc4efed47b11,subnet-0f64d1e62abe83d4d	sg-0939a7fb80a664be9	EKS
```

`eksctl` automatically configures your `kubeconfig` file. To check your nodes:

```bash
kubectl get nodes
```

Output:

```bash
NAME                                          STATUS   ROLES    AGE     VERSION
ip-192-168-22-89.us-west-2.compute.internal   Ready    <none>   6m33s   v1.30.2-eks-1552ad0
```

## 2. Deploy the Helm Chart

### 2.1. Download the rindexer repository

```bash
git clone https://github.com/joshstevens19/rindexer.git
```

### 2.2. Configure the `values.yaml` File

Customize the `values.yaml` for your deployment:

```yaml
replicaCount: 2

image:
  repository: ghcr.io/joshstevens19/rindexer
  tag: "latest"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 3001

ingress:
  enabled: false

postgresql:
  enabled: false
```

:::info
If you are using AWS RDS for your PostgreSQL database with `sslmode=require`, you will need to include the RDS certificates in your connection configuration. You can find the necessary certificates in the [AWS RDS SSL documentation](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html).
:::

### 2.3. Install the Helm Chart

```bash
helm install rindexer ./helm/rindexer -f helm/rindexer/values.yaml
```

Output:

```bash
NAME: rindexer
LAST DEPLOYED: Tue Aug 20 18:43:58 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=rindexer,app.kubernetes.io/instance=rindexer" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
```

### 2.4. Verify the Deployment

```bash
kubectl get pods
```

Output:

```bash
NAME                                READY   STATUS    RESTARTS     AGE
rindexer-rindexer-94dd58475-p8g5d   1/1     Running   0            17s
```

## 3. Monitor and Manage the Deployment

### 3.1. Health Monitoring

Rindexer includes a built-in health monitoring server that provides comprehensive system status information. The health server runs on port `8080` by default and provides real-time insights into:

- **Database connectivity** - PostgreSQL connection status
- **Indexing status** - Whether the indexer is running and how many tasks are active
- **Sync status** - Data synchronization health between different storage backends
- **Overall system health** - Aggregated status across all components

#### 3.1.1. Health Server Lifecycle

The health server's lifecycle depends on which services you start:

- **`rindexer start indexer` (with end_block set)**: Short-lived - dies when historical indexing completes
- **`rindexer start indexer` (no end_block set)**: Long-lived - stays alive for continuous live indexing
- **`rindexer start graphql`**: No health server - health monitoring not available
- **`rindexer start all`**: Long-lived - follows the GraphQL server lifecycle

#### 3.1.2. Accessing Health Endpoints

The health server is automatically started when you run rindexer with indexing enabled. It provides the following endpoint:

- `GET /health` - Complete health status with detailed service information

Example health response:

```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00Z",
  "services": {
    "database": "healthy",
    "indexing": "healthy",
    "sync": "healthy"
  },
  "indexing": {
    "active_tasks": 2,
    "is_running": true
  }
}
```

#### 3.1.3. Health Status Types

The health endpoint returns different status types:

- `healthy` - Service is functioning normally
- `unhealthy` - Service has encountered an error
- `unknown` - Status cannot be determined
- `not_configured` - Service is not set up
- `disabled` - Service is intentionally disabled
- `no_data` - Service is working but no data is available
- `stopped` - Service is not running

#### 3.1.4. Service Health Checks

**Database Health Check:**
- **`healthy`**: PostgreSQL is enabled and a simple `SELECT 1` query succeeds
- **`unhealthy`**: PostgreSQL is enabled but the connection fails or query errors occur
- **`not_configured`**: PostgreSQL is enabled but no database client is available
- **`disabled`**: PostgreSQL is not enabled in the configuration

**Indexing Health Check:**
- **`healthy`**: The indexer is currently running (system state flag is set)
- **`stopped`**: The indexer is not running (system state flag is not set)

**Sync Health Check:**
- **PostgreSQL storage**: Checks for event tables (excluding system tables)
- **CSV storage**: Checks if CSV directory exists and contains CSV files
- **`healthy`**: Data synchronization is working properly
- **`no_data`**: No data tables/files exist yet (acceptable for new deployments)
- **`unhealthy`**: Sync process has errors
- **`disabled`**: Sync is not configured

#### 3.1.5. Monitoring in Production

For production deployments, you can:

1. **Set up monitoring alerts** based on HTTP status codes:
- `200 OK` - System is healthy
- `503 Service Unavailable` - System has issues

2. **Configure load balancer health checks** to point to `/health`

3. **Use monitoring tools** like Prometheus, Grafana, or DataDog to track health metrics

4. **Set up automated alerts** when the health status changes to `unhealthy`

#### 3.1.6. Custom Health Port

You can configure the health server port in your `rindexer.yaml` file:

```yaml
global:
  health_port: 8081
```

### 3.2. View Logs

```bash
kubectl logs -l app.kubernetes.io/name=rindexer
```

Output:

```bash
20 August - 16:44:17.710908  INFO RocketPoolETH::Transfer - network ethereum - 100.00% progress
20 August - 16:44:17.779423  INFO RocketPoolETH::Transfer - No events found between blocks 18999946 - 19000000
20 August - 16:44:17.779458  INFO RocketPoolETH::Transfer - COMPLETED - Finished indexing historic events
20 August - 16:44:18.825983  INFO RocketPoolETH::Approval - INDEXED - 4884 events - blocks: 18900000 - 19000000 - network: ethereum
20 August - 16:44:18.827845  INFO RocketPoolETH::Approval - network ethereum - 100.00% progress
20 August - 16:44:18.906260  INFO RocketPoolETH::Approval - No events found between blocks 18999896 - 19000000
20 August - 16:44:18.906299  INFO RocketPoolETH::Approval - COMPLETED - Finished indexing historic events
20 August - 16:44:18.906347  INFO Historical indexing complete - time taken: 2.599786906s
20 August - 16:44:18.906407  INFO Applying indexes if any back to the database as historic resync is complete
20 August - 16:44:18.906414  INFO rindexer resync is complete
```

### 3.3. Upgrade the Helm Chart

```bash
helm upgrade rindexer ./rindexer -f values.yaml
```

## 4. Clean Up

### 4.1. Uninstall the Helm Chart

```bash
helm uninstall rindexer
```

Output:

```bash
release "rindexer" uninstalled
```

### 4.2. Delete the EKS cluster

```bash
eksctl delete cluster --name my-cluster --region us-west-2
```

Ouput:

```bash
2024-08-20 18:49:04 [ℹ]  deleting EKS cluster "my-cluster"
2024-08-20 18:49:05 [ℹ]  will drain 0 unmanaged nodegroup(s) in cluster "my-cluster"
2024-08-20 18:49:05 [ℹ]  starting parallel draining, max in-flight of 1
2024-08-20 18:49:05 [✖]  failed to acquire semaphore while waiting for all routines to finish: context canceled
2024-08-20 18:49:07 [ℹ]  deleted 0 Fargate profile(s)
2024-08-20 18:49:09 [✔]  kubeconfig has been updated
2024-08-20 18:49:09 [ℹ]  cleaning up AWS load balancers created by Kubernetes objects of Kind Service or Ingress
2024-08-20 18:49:12 [ℹ]
2 sequential tasks: { delete nodegroup "standard-workers", delete cluster control plane "my-cluster" [async]
}
2024-08-20 18:49:12 [ℹ]  will delete stack "eksctl-my-cluster-nodegroup-standard-workers"
2024-08-20 18:49:12 [ℹ]  waiting for stack "eksctl-my-cluster-nodegroup-standard-workers" to get deleted
2024-08-20 18:49:13 [ℹ]  waiting for CloudFormation stack "eksctl-my-cluster-nodegroup-standard-workers"
....
2024-08-20 18:58:09 [ℹ]  will delete stack "eksctl-my-cluster-cluster"
2024-08-20 18:58:10 [✔]  all cluster resources were deleted
```

This guide provides the necessary steps to deploy the `rindexer` Helm chart on AWS EKS using `eksctl`.
</file>

<file path="documentation/docs/pages/docs/deploying/gcp.mdx">
# GCP

## Prerequisites

Ensure that you have the following installed and configured:

- **[Google Cloud SDK](https://cloud.google.com/sdk/docs/install)**: Installed and configured with necessary permissions.
- **[kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/)**: Installed and configured.
- **[Helm](https://helm.sh/docs/intro/install/)**: Installed.

## 1. Create a GKE Cluster

This command creates a new GKE cluster. Adjust the --zone, --machine-type, and node count options as needed.

```bash
gcloud container clusters create my-cluster --zone us-west1-a --machine-type n1-standard-1 --num-nodes=1 --enable-autoscaling --min-nodes=1 --max-nodes=3
```

Output:

```bash
Creating cluster my-cluster in us-west1-a... Cluster is being created.
Created [https://container.googleapis.com/v1/projects/my-project/zones/us-west1-a/clusters/my-cluster].
To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-west1-a/my-cluster?project=my-project
kubeconfig entry generated for my-cluster.
NAME        LOCATION    MASTER_VERSION    MASTER_IP     MACHINE_TYPE   NODE_VERSION      NUM_NODES  STATUS
my-cluster  us-west1-a  v1.30.2-gke.100  35.233.164.24 n1-standard-1  v1.30.2-gke.100  1          RUNNING
```

`gcloud` automatically configures your `kubeconfig` file. To check your nodes:

```bash
kubectl get nodes
```

Output:

```bash
NAME                                          STATUS   ROLES    AGE     VERSION
gke-my-cluster-default-pool-1a2b3c4d-e123     Ready    <none>   6m33s   v1.30.2-gke.100
```

## 2. Deploy the Helm Chart

### 2.1. Download the rindexer repository

```bash
git clone https://github.com/joshstevens19/rindexer.git
```

### 2.2. Configure the `values.yaml` File

Customize the `values.yaml` for your deployment:

```yaml
replicaCount: 2

image:
  repository: ghcr.io/joshstevens19/rindexer
  tag: "latest"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 3001

ingress:
  enabled: false

postgresql:
  enabled: false
```

### 2.3. Install the Helm Chart

```bash
helm install rindexer ./helm/rindexer -f helm/rindexer/values.yaml
```

Output:

```bash
NAME: rindexer
LAST DEPLOYED: Tue Aug 21 18:23:34 2024
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=rindexer,app.kubernetes.io/instance=rindexer" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
```

### 2.4. Verify the Deployment

```bash
kubectl get pods
```

Output:

```bash
NAME                                READY   STATUS    RESTARTS     AGE
rindexer-rindexer-35bb35619-t9r2l   1/1     Running   1 (7s ago)   17s
```

## 3. Monitor and Manage the Deployment

### 3.1. View Logs

```bash
kubectl logs -l app.kubernetes.io/name=rindexer
```

Output:

```bash
21 August - 17:32:17.710908  INFO RocketPoolETH::Transfer - network ethereum - 100.00% progress
21 August - 17:32:17.779423  INFO RocketPoolETH::Transfer - No events found between blocks 18999946 - 19000000
21 August - 17:32:17.779458  INFO RocketPoolETH::Transfer - COMPLETED - Finished indexing historic events
21 August - 17:32:18.825983  INFO RocketPoolETH::Approval - INDEXED - 4884 events - blocks: 18900000 - 19000000 - network: ethereum
21 August - 17:32:18.827845  INFO RocketPoolETH::Approval - network ethereum - 100.00% progress
21 August - 17:32:18.906260  INFO RocketPoolETH::Approval - No events found between blocks 18999896 - 19000000
21 August - 17:32:18.906299  INFO RocketPoolETH::Approval - COMPLETED - Finished indexing historic events
21 August - 17:32:18.906347  INFO Historical indexing complete - time taken: 2.599786906s
21 August - 17:32:18.906407  INFO Applying indexes if any back to the database as historic resync is complete
21 August - 17:32:18.906414  INFO rindexer resync is complete
```

### 3.2. Upgrade the Helm Chart

```bash
helm upgrade rindexer ./rindexer -f values.yaml
```

## 4. Clean Up

### 4.1. Uninstall the Helm Chart

```bash
helm uninstall rindexer
```

Output:

```bash
release "rindexer" uninstalled
```

### 4.2. Delete the EKS cluster

```bash
gcloud container clusters delete my-cluster --zone us-west1-a
```

Ouput:

```bash
The following clusters will be deleted.
 - [my-cluster] in [us-west1-a]

Do you want to continue (Y/n)?  Y
Deleting cluster my-cluster...done.
Deleted [https://container.googleapis.com/v1/projects/my-project/zones/us-west1-a/clusters/my-cluster].
```

This guide provides the necessary steps to deploy the rindexer Helm chart on Google Kubernetes Engine (GKE) using gcloud and kubectl.

## Health Monitoring

Rindexer includes a built-in health monitoring server that provides comprehensive system status information. The health server runs on port `8080` by default and provides real-time insights into:

- **Database connectivity** - PostgreSQL connection status
- **Indexing status** - Whether the indexer is running and how many tasks are active
- **Sync status** - Data synchronization health between different storage backends
- **Overall system health** - Aggregated status across all components

### Health Server Lifecycle

The health server's lifecycle depends on which services you start:

- **`rindexer start indexer` (with end_block set)**: Short-lived - dies when historical indexing completes
- **`rindexer start indexer` (no end_block set)**: Long-lived - stays alive for continuous live indexing
- **`rindexer start graphql`**: No health server - health monitoring not available
- **`rindexer start all`**: Long-lived - follows the GraphQL server lifecycle

### Accessing Health Endpoints

The health server is automatically started when you run rindexer with indexing enabled. It provides the following endpoint:

- `GET /health` - Complete health status with detailed service information

Example health response:

```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00Z",
  "services": {
    "database": "healthy",
    "indexing": "healthy",
    "sync": "healthy"
  },
  "indexing": {
    "active_tasks": 2,
    "is_running": true
  }
}
```

### Health Status Types

The health endpoint returns different status types:

- `healthy` - Service is functioning normally
- `unhealthy` - Service has encountered an error
- `unknown` - Status cannot be determined
- `not_configured` - Service is not set up
- `disabled` - Service is intentionally disabled
- `no_data` - Service is working but no data is available
- `stopped` - Service is not running

### Monitoring in Production

For production deployments on GCP, you can:

1. **Set up monitoring alerts** based on HTTP status codes:
- `200 OK` - System is healthy
- `503 Service Unavailable` - System has issues

2. **Use Google Cloud Monitoring** to track health metrics

3. **Set up automated alerts** when the health status changes to `unhealthy`

4. **Configure load balancer health checks** to point to `/health`

5. **Access health endpoints** through your GCP load balancer:
```
https://your-load-balancer-ip:8080/health
   ```

### Custom Health Port

You can configure the health server port in your `rindexer.yaml` file:

```yaml
global:
  health_port: 8081
```
</file>

<file path="documentation/docs/pages/docs/deploying/railway.mdx">
# Railway

## One-click Deploy Example

[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/Rqrlcf?referralCode=eD4laT)

## Deploy an example project

https://github.com/joshstevens19/rindexer/tree/master/providers/railway

1. Clone the relevant directory

  ```bash
  # this will clone the railway directory
  mkdir rindexer-railway && cd rindexer-railway
  git clone \
    --depth=1 \
    --no-checkout \
    --filter=tree:0 \
    https://github.com/joshstevens19/rindexer .
  git sparse-checkout set --no-cone providers/railway .
  git checkout && cp -r providers/railway/* . && rm -rf providers
  ```

2. Initialize a new Railway project

Install [Railway CLI](https://docs.railway.com/guides/cli) if not already installed.

  ```bash
  railway login
  ```


  ```bash
  railway init --name rindexer-example
  ```

3. Create a service and link it to the project

  ```bash
  railway up --detach
  railway link
  ? Select a project
  > rindexer-example
  ? Select an environment
  > production
  ? Select a service
  > rindexer-example
  ```

4. Create a Postgres database

  ```bash
  railway add --database postgres
  ```

5. Configure environment variables

  ```bash
  railway open
  ```

  - Open the service "Variables" tab:

- Select "Add Variable Reference" and add a reference for `DATABASE_URL` and append ?sslmode=disable to the end of the value. The result should look like `${{Postgres.DATABASE_URL}}?sslmode=disable`.

- Select "Add Variable Reference" and add a reference for `POSTGRES_PASSWORD`.

- Select "New Variable" with name `PORT` and value `3001` (This is the default value for the rindexer service, update this variable accordingly if the value is changed in the rindexer Dockerfile).

  - Hit "Deploy" or press Shift+Enter.

6. Create a domain to access GraphQL Playground

  ```bash
  railway domain
  ```

7. Redeploy the service

  ```bash
  railway up
  ```

## Health Monitoring

Rindexer includes a built-in health monitoring server that provides comprehensive system status information. The health server runs on port `8080` by default and provides real-time insights into:

- **Database connectivity** - PostgreSQL connection status
- **Indexing status** - Whether the indexer is running and how many tasks are active
- **Sync status** - Data synchronization health between different storage backends
- **Overall system health** - Aggregated status across all components

### Health Server Lifecycle

The health server's lifecycle depends on which services you start:

- **`rindexer start indexer` (with end_block set)**: Short-lived - dies when historical indexing completes
- **`rindexer start indexer` (no end_block set)**: Long-lived - stays alive for continuous live indexing
- **`rindexer start graphql`**: No health server - health monitoring not available
- **`rindexer start all`**: Long-lived - follows the GraphQL server lifecycle

### Accessing Health Endpoints

The health server is automatically started when you run rindexer with indexing enabled. It provides the following endpoint:

- `GET /health` - Complete health status with detailed service information

Example health response:

```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00Z",
  "services": {
    "database": "healthy",
    "indexing": "healthy",
    "sync": "healthy"
  },
  "indexing": {
    "active_tasks": 2,
    "is_running": true
  }
}
```

### Health Status Types

The health endpoint returns different status types:

- `healthy` - Service is functioning normally
- `unhealthy` - Service has encountered an error
- `unknown` - Status cannot be determined
- `not_configured` - Service is not set up
- `disabled` - Service is intentionally disabled
- `no_data` - Service is working but no data is available
- `stopped` - Service is not running

### Monitoring in Production

For production deployments on Railway, you can:

1. **Set up monitoring alerts** based on HTTP status codes:
- `200 OK` - System is healthy
- `503 Service Unavailable` - System has issues

2. **Use Railway's built-in monitoring** to track health metrics

3. **Set up automated alerts** when the health status changes to `unhealthy`

4. **Access health endpoints** through your Railway domain:
```
https://your-app.railway.app/health
   ```

### Custom Health Port

You can configure the health server port in your `rindexer.yaml` file:

```yaml
global:
  health_port: 8081
```
</file>

<file path="documentation/docs/pages/docs/introduction/installation.mdx">
# Installation

rindexer installation process is simple and can be done with a few steps.

:::info
rindexer uses docker to spin up postgres databases for you when it runs locally so its recommended to install [docker](https://www.docker.com/products/docker-desktop/)
if you don't have it installed already.
:::

## rindexer CLI

rindexer operates as a CLI toolset to make it easy to create new rindexer projects or run existing ones.

### Installing

:::warning
If you’re on Windows, you will need to install and use Git BASH or WSL, as your terminal,
since rindexer installation does not support Powershell or Cmd.
:::

:::code-group

```bash [latest]
curl -L https://rindexer.xyz/install.sh | bash
```

```bash [exact version]
curl -L https://rindexer.xyz/install.sh | bash -s -- --version <version_number>
```

:::

Once installed you can run the following command to check the installation was successful:

```bash
rindexer --help
```

```bash
Blazing fast EVM indexing tool built in rust

Usage: rindexer [COMMAND]

Commands:
  new           Creates a new rindexer no-code project or rust project
  start         Start various services like indexers, GraphQL APIs or both together
  add           Add elements such as contracts to the rindexer.yaml file
  codegen       Generates rust code based on rindexer.yaml or graphql queries
  delete        Delete data from the postgres database or csv files
  phantom       Use phantom events to add your own events to contracts
  help          Print this message or the help of the given subcommand(s)

Options:
  -h, --help     Print help
  -V, --version  Print version
```

you can also get help on any of the commands for example to get help on the new command you can run:

```bash
rindexer new --help
```

To upgrade to the latest version of rindexer you can run the following command:

```bash
rindexerup
```

To uninstall rindexer you can run the following command:

```bash
rindexerdown
```

## Docker pre-built image

There's is pre-built docker image which can be used to run `rindexer` inside your dockerized infra:

- Docker image: [`ghcr.io/joshstevens19/rindexer`](https://github.com/users/joshstevens19/packages/container/package/rindexer)

### Create new project
To create a new `no-code` project in your current directory, you can run the following:

```bash
docker run -it -v $PWD:/app/project_path ghcr.io/joshstevens19/rindexer new -p /app/project_path no-code
```

### Use with existing project
To use it with an existing project and a running postgres instance you can simply invoke:

```
export PROJECT_PATH=/path/to/your/project
export DATABASE_URL="postgresql://user:pass@postgres/db"

docker-compose up -d
```

This will start all local indexing and if you have enabled the graphql endpoint, it will become exposed under:

http://localhost:3001


**If you are using csv you do not need to install docker, it is only recommended with postgres or if you're deploying rindexer in cloud environments.**

## Rust - optional

If you are only doing no-code projects you do not need rust installed but if you are doing rust projects you will need to install rust.
You can install rust by following the instructions [here](https://www.rust-lang.org/tools/install).

That is it lets now walk through how you can start using rindexer.
</file>

<file path="documentation/docs/pages/docs/introduction/other-indexing-tools.mdx">
# Other indexing tools

rindexer is not a tool to take market share from other indexing tools, it is a tool to provide more options for developers
to index data on EVM chains. rindexer allows you to index data with no learning curve and no-code if you pick that option. 
The current indexing tools are mainly JavaScript based alongside require code to be written to use them, 
rindexer is a Rust based indexing tool and has no-code features built into it. 
Diversity is very important in the industry and rindexer is here to provide more options for developers to index data on EVM chains.
rindexer is not a company or a business it is an open-source project and is here to help the industry move forward.

## TheGraph - Pay per query

rindexer is not trying to replace `TheGraph` or even sees itself as a competitor to `TheGraph`.
`TheGraph` vision is inspirational and the future of data should be decentralised provable indexing 
and we are not trying to replace that or take away from all the amazing things `TheGraph` has done for our industry. 
`TheGraph` have now sunset the hosting service being true to their ethos of decentralisation which you have to
respect.

rindexer was created to make indexing easier and faster. In the future I see a world where 
rindexer and `TheGraph` can work together to provide the best indexing experience for developers, you should be able
to resync decentralised verified data from `TheGraph` from rindexer using no-code. 

If you want decentralised provable indexing `TheGraph` is the tool you should be using not rindexer.

## Shadow - Paid

:::note
rindexer has first-party support for phantom events powered by Shadow, you can read more about it [here](/docs/start-building/phantom).
:::

`Shadow` allows you to add custom events and view functions to smart contracts and is a very powerful indexing service.
The `Shadow` team are doing great work and I have a lot of respect for them and the work they are doing.

## dyRPC - Paid

:::note
rindexer has first-party support for phantom events powered by dyRPC, you can read more about it [here](/docs/start-building/phantom).
:::

`dyRPC` is a tool built on top of overlay which can be ran on any erigon node and allows you to also modify the contract's source code
adding gasless custom events and view functions. The `dyRPC` team are awesome.

## Cryo - Free

`Cryo` is a great way to extract data from all EVM chains and is awesome for data analysis and research. It also is powered
by a CLI tool allowing you to get this data using the CLI tool only. Really great tool with an incredible team behind it.

## TrueBlocks - Free

TrueBlocks.io is a blockchain data indexing and querying tool designed to provide highly detailed and decentralized
access to Ethereum blockchain data. It aims to enable users to efficiently extract and interact with blockchain data for various applications such as analytics, auditing, and reporting.
Awesome team and amazing tool.

## Ponder - Free

`Ponder` is a great indexing tool which is very feature rich and has a lot of great features. I really respect
the work that has gone into `Ponder` and I think it is a great tool for indexing data on EVM chains. rindexer 
took some inspiration from the `Ponder` codebase and only have love for the `Ponder` team.
It is built in JavaScript and you can build your own custom indexers with it.

## Goldsky - Paid

`Goldsky` is a great indexing tool which has no-code elements, they also offer bespoke services to build custom indexing solutions.
Great team and great product.

## Subsquid - Free features and Paid features

`Subsquid` is a great indexing tool which supports EVM based indexing as well as non-EVM based indexing. It is built
around the subsquid network which is a decentralised query engine. They have a great team and a great product.
Accessing all the historical data is free with subsquid but if you want the SQD cloud hosting then it costs.

## GhostGraph - Free features and Paid features

`GhostGraph` is a first-of-a-kind indexing solution that lets you write your index transformations in solidity. `GhostGraph` makes building fast indexers (subgraphs) for smart contracts easy. It is currently in beta.

## Envio - Free features and Paid features
`Envio` (now referred to as HyperIndex) is a multi-chain indexer focused on performance and flexibility. It connects to an RPC node or HyperSync, an optimized Rust-built data node that massively improves indexing speeds.
</file>

<file path="documentation/docs/pages/docs/introduction/what-is-rindexer.mdx">
# What is rindexer ?

:::info
Note rindexer is brand new and actively under development, things will change and bugs will exist - if you find any bugs or have any
feature requests please open an issue on [github](https://github.com/joshstevens19/rindexer/issues).
:::

rindexer is an opensource powerful, high-speed indexing toolset developed in Rust, designed for compatibility with any EVM chain.
This tool allows you to index chain events using a simple YAML file, requiring no additional coding.
For more advanced needs, the rindexer provides foundations and advanced capabilities to build whatever you want.
It's highly extendable, enabling you to construct indexing pipelines with ease and focus exclusively on the logic.
rindexer out the box also gives you a GraphQL API to query the data you have indexed instantly.

# What can I use rindexer for?

- Hackathons: spin up a quick indexer to index events for your dApp with an API without any code needed
- Data reporting
- Building advanced indexers
- Building a custom indexer for your project
- Fast prototyping and MVP developments
- Quick proof-of-concept projects
- Enterprise standard indexing solutions for projects
- Much more...

# What networks do you support?

rindexer supports any EVM chain out of the box. If you have a custom chain, you can easily add support for it by
adding the chain's RPC URL to the YAML configuration file and defining the chain ID. No code changes are required.
</file>

<file path="documentation/docs/pages/docs/introduction/why-rindexer.mdx">
# Why rindexer?

## Why do we need rindexer in general?

Indexing data on EVM chains is crucial for developers creating dApps or just general data reporting.
Building the necessary indexing infrastructure, however, presents significant challenges.
It is complex, time-consuming, and can divert focus from the task at hand and in a lot of cases even stop the task from moving forward.
As applications become more complex with more advanced features, the need for robust, easily extendable indexing solutions grows.
Some great indexing tools exist already mainly all in JavaScript so adding a rust based indexer tool creates more indexing options
which is important for the industry.

## The Problems

Traditional indexing solutions come with steep learning curves and require substantial initial development to meet dApps
specific needs. This requirement can delay overall application development and add complexity, especially when integrating
with different chain environments. Additionally, many tools do not offer the flexibility needed to quickly adapt to
changing project requirements.

Effective indexing tools should be easy to use, requiring no-code for basic data reporting or basic indexing needs, while also
being highly customizable for more advanced requirements. We are building more and more advanced applications that require
more advanced indexing tools, and rindexer is designed to meet these needs as well.

The use of Rust in EVM chain tools like Foundry and Reth has demonstrated significant performance improvements on existing toolsets, 
which rindexer also leverages.

## Developer Experience

rindexer significantly enhances the developer experience by simplifying the setup and management of indexing tasks.
Its straightforward YAML-based configuration allows anyone to begin indexing events without writing any code,
enabling them to concentrate more on their application's logic or profiling the data itself.
For those seeking to build more advanced indexing rindexer provides a framework abstracting the complexity of how you get the chains data,
allowing developers to focus on the projects logic and not the low-level chain indexing specifics.

## Performance

Rust was chosen for developing rindexer due to its unmatched performance and efficiency. Its capacity for handling intensive
computation and its memory safety—without needing a garbage collector allows rindexer to manage high-throughput data with minimal
latency. This makes rindexer a very fast indexing solution, essential for applications that require real-time
data analysis and for developers who value speed and efficiency.

"Speed is everything in software."
</file>

<file path="documentation/docs/pages/docs/references/cli.mdx">
# rindexer CLI

rindexer is a CLI first tool allowing you to do everything you need to do with rindexer.

```bash
Usage: rindexer <COMMAND>

Commands:
  new           Creates a new rindexer no-code project or rust project
  start         Start various services like indexers, GraphQL APIs or both together
  add           Add elements such as contracts to the rindexer.yaml file
  codegen       Generates rust code based on rindexer.yaml or graphql queries
  delete        Delete data from the postgres database or csv files
  phantom       Use phantom events to add your own events to contracts
  help          Print this message or the help of the given subcommand(s)

Options:
  -h, --help     Print help
  -V, --version  Print version
```

## new

Creates a new rindexer no-code project or rust project. This will walk you through setting up your project by asking
you a series of questions in the terminal.

```bash
Usage: rindexer new [OPTIONS] <COMMAND>

Commands:
  no-code  Creates a new no-code project
  rust     Creates a new rust project
  help     Print this message or the help of the given subcommand(s)

Options:
  -p, --path <PATH>
          optional - The path to create the project in, default will be where the command is run

  -h, --help
          Print help (see a summary with '-h')
```

### Subcommand Options

Both `no-code` and `rust` subcommands support:

```bash
Options:
  -r, --reth
          optional - Enable Reth support for high-performance indexing

  [-- <RETH_ARGS>...]
          Additional arguments to pass to reth when --reth is enabled
          These should be provided after -- e.g. -- --datadir /path --http true

Examples:
  # Standard project
  rindexer new no-code

  # Reth-enabled project
  rindexer new no-code --reth

  # Reth project with custom arguments
  rindexer new rust --reth -- --datadir /custom/path --http true
```

## start

Start various services like indexers, GraphQL APIs or both together. This will start the services based on the rindexer.yaml file. A health monitoring server is automatically started alongside these services.

```bash
`rindexer start indexer` or `rindexer start graphql` or `rindexer start all`

Usage: rindexer start [OPTIONS] <COMMAND>

Commands:
  indexer  Starts the indexing service based on the rindexer.yaml file
  graphql  Starts the GraphQL server based on the rindexer.yaml file
  all      Starts the indexers and the GraphQL together based on the rindexer.yaml file
  help     Print this message or the help of the given subcommand(s)

Options:
  -p, --path <PATH>
          optional - The path to run the command in, default will be where the command is run

  -h, --help
          Print help (see a summary with '-h')

:::info
The health monitoring server runs on port 8080 by default. You can configure it in your `rindexer.yaml` file using the `health_port` setting. See the [Running](/docs/start-building/running#health-monitoring) documentation for more details.
:::
```

## add

These commands allow you to through the CLI add elements to your YAML file.

```bash
Usage: rindexer_cli add [OPTIONS] <COMMAND>

Commands:
  contract  Add a contract from a network to the rindexer.yaml file. It will download the ABI and add it to the abis folder and map it in the yaml file.
  help      Print this message or the help of the given subcommand(s)

Options:
  -p, --path <PATH>
          optional - The path to run the command in, default will be where the command is run

  -h, --help
          Print help (see a summary with '-h')
```

## codegen

Generates rust code based on rindexer.yaml or graphql queries. This will generate the code based on the command you run.

```bash
Example: `rindexer codegen typings` or `rindexer codegen handlers` or `rindexer codegen graphql --endpoint=graphql_api` or `rindexer codegen rust-all`

Usage: rindexer codegen [OPTIONS] <COMMAND>

Commands:
  typings  Generates the rindexer rust typings based on the rindexer.yaml file
  indexer  Generates the rindexer rust indexers handlers based on the rindexer.yaml file
  graphql  Generates the GraphQL queries from a GraphQL schema
  all      Generates both typings and indexers handlers based on the rindexer.yaml file
  help     Print this message or the help of the given subcommand(s)

Options:
  -p, --path <PATH>
          optional - The path to run the command in, default will be where the command is run

  -h, --help
          Print help (see a summary with '-h')
```

## delete

This can be used to delete data from the postgres database or csv files.
It will ask you questions in the terminal to determine what you want to delete.

```bash
Usage: rindexer delete
```

## phantom

```bash
Example: `rindexer phantom init` or `rindexer phantom clone --contract-name <CONTRACT_NAME> --network <NETWORK>` or `rindexer phantom compile --contract-name <CONTRACT_NAME> --network <NETWORK>` or `rindexer phantom deploy --contract-name <CONTRACT_NAME> --network <NETWORK>`

Usage: rindexer phantom [OPTIONS] <COMMAND>

Commands:
  init     Sets up phantom events on rindexer
  clone    Clone the contract with the network you wish to add phantom events to
  compile  Compiles the phantom contract
  deploy   Deploy the modified phantom contract
  help     Print this message or the help of the given subcommand(s)

Options:
  -p, --path <PATH>
          optional - The path to create the project in, default will be where the command is run

  -h, --help
          Print help (see a summary with '-h')
```
</file>

<file path="documentation/docs/pages/docs/references/rpc-node-providers.mdx">
# RPC node providers

RPC providers speed has a direct link to how fast you can index data, providers who try to return data as fast as possible are the best providers to have.
With RPC providers the fastest providers are ones which return you the to and from block ranges even if you supply an out of range block request.
This means you can extract that data from the error message and use it to get the the biggest depth of logs out of a single request.
The slower providers give you a max block range and this means you have to crawl through the blocks to get the logs even if no data is in the blocks, 
this is a lot slower.

## Tenderly

[Tenderly](https://tenderly.co/) has some very fast nodes and in internal testing with free nodes they blew everyone out the water
for returning the most event logs in the fastest time.

check out their free nodes here - https://docs.tenderly.co/supported-networks-and-languages

:::info
Keep in mind that the public RPC are rate limited.

For production use, it’s recommended use a dedicated Node RPC.
:::

## Alchemy

[Alchemy](https://www.alchemy.com/) is another great provider with a free tier that is generous.

## Other top providers

Infura and thirdweb are also good providers. 

## All other networks

You can use [chainlist](https://chainlist.org/) to find all the providers which support your network.

## Local nodes

rindexer should work with any local nodes you run including [Anvil](https://book.getfoundry.sh/anvil/) and [Hardhat](https://hardhat.org/)

## RPC Proxy and Caching

[eRPC](https://github.com/erpc/erpc) is a fault-tolerant EVM RPC proxy and re-org aware permanent caching solution. It is built with read-heavy use-cases in mind such as data indexing and high-load frontend usage.

### Setup

1. Create your [`erpc.yaml`](https://docs.erpc.cloud/config/example) configuration file:

```yaml filename="erpc.yaml"
logLevel: debug
projects:
  - id: main
    upstreams:
      # You don't need to define architecture (e.g. evm) or chain id (e.g. 137)
      # as they will be detected automatically by eRPC.
      - endpoint: https://eth-mainnet.blastapi.io/xxxx
      - endpoint: https://polygon-mainnet.blastapi.io/xxxx
      - endpoint: evm+alchemy://xxxx-my-alchemy-api-key-xxxx
```

See [a complete config example](https://docs.erpc.cloud/config/example) for inspiration.

2. Use the Docker image:

```bash
docker run -v $(pwd)/erpc.yaml:/root/erpc.yaml -p 4000:4000 -p 4001:4001 ghcr.io/erpc/erpc:latest
```

or add the below configs to the rindexer's [docker-compose.yaml](https://github.com/joshstevens19/rindexer/blob/master/docker-compose.yml) as a service and run `docker-compose up -d`:

```yaml [rindexer.yaml]
  services:
    ...

    erpc: // [!code focus]
      image: ghcr.io/erpc/erpc:latest // [!code focus]
      platform: linux/amd64 // [!code focus]
      volumes: // [!code focus]
        - ${PROJECT_PATH}/erpc.yaml:/root/erpc.yaml // [!code focus]
      ports: // [!code focus]
        - 4000:4000 // [!code focus]
        - 4001:4001 // [!code focus]
      restart: always // [!code focus]
```

3. Set erpc url in [rindexer network config](https://rindexer.xyz/docs/start-building/yaml-config/networks#rpc):

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: http://erpc:4000/main/evm/1 // [!code focus]
```

and you are set to go. the rpc requests now will be redirected toward erpc and it will handle caching, failover, auto-batching, rate-limiting, auto-discovery of node providers, etc. behind the scenes.
</file>

<file path="documentation/docs/pages/docs/start-building/chatbots/discord.mdx">
# Discord

Discord is one of the most popular chat platforms, and is great to build bots and notifications when things happen on chain.

:::info
Due to rate limits out of rindexer control ChatBots will only send messages with max block range of 10 blocks.
Most people who will use rindexer Chatbots will only want to start sending messages of the live data anyway.
The ChatBots are really only meant to be ran sending live data not historic data.
:::

## Setup a bot on discord

1. go to https://discordapp.com/developers/applications/
2. If you already have a bot created, click it in the list. If you don’t have any discord bots, click the “New Application” button.
3. Give Your Bot a Name (you can then after add a description and icon for it)
4. Your next step is to go over the menu on the left side of the screen and click “Bot”. It’s the icon that looks like a little puzzle piece.
5. Click the “Add Bot” button and press "Yes, do it!"
6. You see a section called “Token” you need to generate your bot token and save it somewhere safe for later.
7. In order to add your bot to your Discord Server, you’ll need to navigate back to the “OAuth2” tab.
8. Once there, scroll down to the “Oauth2 URL Generator” section. In the “Scopes” section, you’ll want to select the “bot” checkbox.
9. You’ll notice that a URL appeared as soon as you clicked “bot” — this will be your URL for adding your bot to a server.
10. Scroll down some more to the “Bot Permissions” section. This is where you choose what permissions to give your bot, and what it can and can’t do.
11. You want to do tick "Send messages" as rindexer does not read any messages from the server.
12. After you’ve selected your permissions, scroll up a little bit and look at the URL that was generated and copy and go to that url in your browser.
13. Here you’ll want to select the server you’re adding your bot to and press “Continue”
14. It then confirm permissions make sure you have ticked "Send Messages" and press "Authorize"
14. You are now done you will need the bot token to setup the discord bot with rindexer


## Configure rindexer

`discord` property accepts an array allowing you to split up the channels any way you wish.

## Example

:::code-group

```yaml [contract events]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN} // [!code focus]
        channel_id: 123456789012345678 // [!code focus]
        networks: // [!code focus]
          - ethereum // [!code focus]
        messages:
          - event_name: Transfer // [!code focus]
            # filter_expression is optional // [!code focus]
            filter_expression: "from = '0x0338ce5020c447f7e668dc2ef778025ce3982662' || from = '0x0338ce5020c447f7e668dc2ef778025ce398266u' && value >= 10 && value <= 2000000000000000000" // [!code focus]
            template_inline: "*New RETH Transfer Event* // [!code focus]

                              from: {{from}} // [!code focus]

                              to: {{to}} // [!code focus]

                              amount: {{format_value(value, 18)}} // [!code focus]

                              RETH contract: {{transaction_information.address}} // [!code focus]

                              [etherscan](https://etherscan.io/tx/{{transaction_information.transaction_hash}}) // [!code focus]
                              " // [!code focus]
```

```yaml [native transfers]
name: ETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN} // [!code focus]
        channel_id: 123456789012345678 // [!code focus]
        networks: // [!code focus]
          - ethereum // [!code focus]
        messages:
          - event_name: NativeTransfer // [!code focus]
            template_inline: "*New ETH Transfer Event* // [!code focus]

                              from: {{from}} // [!code focus]

                              to: {{to}} // [!code focus]

                              amount: {{format_value(value, 18)}} // [!code focus]

                              Token address: {{transaction_information.address}} // [!code focus]

                              [etherscan](https://etherscan.io/tx/{{transaction_information.transaction_hash}}) // [!code focus]
                              " // [!code focus]
```

:::

## bot_token

This is your discord bot token which you generate using @BotFather.

:::info
We advise you to put this in a environment variables.
:::

```yaml
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN} // [!code focus]
```

## channel_id

You have add your bot to channel to use it, so this is the channel ID you wish the bot to send messages to.

```yaml
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN}
        channel_id: -4223616270 // [!code focus]
```

## networks

This is an array of networks you want to send messages to this discord channel.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN}
        channel_id: 123456789012345678
        networks: // [!code focus]
          - ethereum // [!code focus]
```

## messages

This is an array of messages you want to send to this discord channel. It is an array as you can define many different
messages to send to this channel with different conditions.

### event_name

This is the name of the event you want to send a message for, must match the ABI event name.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN}
        channel_id: 123456789012345678
        networks:
          - ethereum
        messages:
          - event_name: Transfer // [!code focus]
```

### filter_expression

This accepts a filter expression to filter the events before sending a message to this discord channel

:::info
This is optional, if you do not provide any filter expression all the events will be sent to this discord channel.
:::

Filter expressions allow for condition checking of the event data and support logical operators to combine multiple conditions.

#### Supported types and operations:
| Type | Description | Operators | Notes |
|------|-------------|-------------------|-------|
| Numeric (uint/int variants) | Integer values (e.g., `42`, `-100`) or decimal values (e.g., `3.14`, `-0.5`) | `>`, `<`, `=`, `>=`, `<=` | Numeric comparisons | Numbers must have digits before and after a decimal point if one is present (e.g., `.5` or `5.` are not valid standalone numbers) |
| Address | Ethereum addresses (e.g., `0x1234567890abcdef1234567890abcdef12345678`) | `=`, `!=` | Comparisons (e.g., `from == '0xABC...'`) are typically case-insensitive regarding the hex characters of the address value itself |
| String | Text values. Can be single-quoted (e.g., `'hello'`) or, on the right-hand side of a comparison, unquoted (e.g., `active`) | `=`, `!=` | Quoted strings support `\'` to escape a single quote and `\\` to escape a backslash. All string comparison operations (e.g., `name == 'Alice'`, `description contains 'error'`) are performed case-insensitively during evaluation. |
| Boolean | True or false values | `=`, `!=` | Represented as `true` or `false`. These keywords are parsed case-insensitively (e.g., `TRUE`, `False` are also valid in expressions). |
| Hex String Literal | A string literal starting with `0x` or `0X` followed by hexadecimal characters (0-9, a-f, A-F). | `=`, `!=` | Treated as a string for comparison purposes (e.g., `input_data starts_with '0xa9059cbb'`). Comparison is case-sensitive for the hex characters after `0x`. |
| Array | Ordered list of items | `==`, `!=`, `[index]` | See "Array Type Operations" below

#### Logical Operators
- `&&` - All conditions must be true
- `||` - At least one condition must be true
- `()` - Parentheses for grouping
- `&&` has higher precedence than `||` (i.e., `&&` operations are evaluated before `||` operations if not grouped by parentheses)

#### Array Type Operations
For array types, you can use the following operations:
- `array_param == '["raw_json_array_string"]'` string comparison of the array's entire JSON string representation against the provided string
- `array_param != '["raw_json_array_string"]'` the negation of the above
- `array_param[0]` indexed access. The index must be a non-negative integer.

#### Whitespace
Flexible whitespace is generally allowed around operators, parentheses, and keywords for readability. However, whitespace within quoted string literals is significant and preserved.

#### Examples
- `value > 1000` - Numeric comparison, checks if `value` is greater than 1000.
- `from = '0x1234567890abcdef1234567890abcdef12345678'` - Address comparison, checks if `from` matches the specified address.
- `name != 'Alice'` - String comparison, checks if `name` is not equal to 'Alice'.
- `active = true` - Boolean comparison, checks if `active` is true.
- `value >= 1000 && value <= 2000` - Numeric range check, checks if `value` is between 1000 and 2000 inclusive.
- `from = '0x1234567890abcdef1234567890abcdef12345678' || from = '0xabcdefabcdefabcdefabcdefabcdefabcdef'` - Address comparison with logical OR, checks if `from` matches either of the two addresses.
- `value > 1000 && (from = '0x1234567890abcdef1234567890abcdef12345678' || from = '0xabcdefabcdefabcdefabcdefabcdefabcdef')` - Combined numeric and address checks with logical AND and OR, checks if `value` is greater than 1000 and `from` matches either of the two addresses.

### conditions

This accepts an array of conditions you want to apply to the event data before sending a message to this discord channel.

:::warning
Conditions are supported for backwards compatibility. It is recommended to use `filter_expression` instead of `conditions`.
:::

:::info
This is optional, if you do not provide any conditions all the events will be sent to this discord channel.
:::

You may want to filter on the message based on the event data, if the event data has not got an index on the on the
solidity event you can not filter it over the logs. The `conditions` filter is here to help you with this,
based on your ABI you can filter on the event data.

rindexer has enabled a special syntax which allows you to define on your ABI fields what you want to filter on.

1. `>` - higher then (for numbers only)
2. `<` - lower then (for numbers only)
3. `=` - equals
4. `>=` - higher then or equals (for numbers only)
5. `<=` - lower then or equals (for numbers only)
6. `||` - or
7. `&&` - and

So lets look at an example lets say i only want to get transfer events which are higher then `2000000000000000000` RETH wei

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN}
        channel_id: 123456789012345678
        networks:
          - ethereum
        messages:
          - event_name: Transfer // [!code focus]
            conditions: // [!code focus]
              - "value": ">=2000000000000000000" // [!code focus]
```

We use the ABI input name `value` to filter on the value field, you can find these names in the ABI file.

```json
{
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value", // [!code focus]
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
}
```

You can use the `||` or `&&` to combine conditions.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN}
        channel_id: 123456789012345678
        networks:
          - ethereum
        messages:
          - event_name: Transfer
            conditions: // [!code focus]
              - "value": ">=2000000000000000000 && value <=4000000000000000000" // [!code focus]
```

You can use the `=` to filter on other aspects like the `from` or `to` address.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN}
        channel_id: 123456789012345678
        networks:
          - ethereum
        messages:
          - event_name: Transfer
              conditions: // [!code focus]
                - "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662 || 0x0338ce5020c447f7e668dc2ef778025ce398266u" // [!code focus]
                - "value": ">=2000000000000000000 || value <=4000000000000000000" // [!code focus]
```

:::info
Note we advise you to filer any `indexed` fields in the contract details in the `rindexer.yaml` file.
As these can be filtered out on the request level and not filtered out in rindexer itself.
You can read more about it [here](/docs/start-building/yaml-config/contracts#indexed_1-indexed_2-indexed_3).
:::

If you have a tuple and you want to get that value you just use the object notation.

For example lets say we want to only get the events for `profileId` from the `quoteParams` tuple which equals `1`:

```json
{
     "anonymous": false,
     "inputs": [
       {
         "components": [
           {
             "internalType": "uint256",
             "name": "profileId", // [!code focus]
             "type": "uint256"
           },
           ...
         ],
         "indexed": false,
         "internalType": "struct Types.QuoteParams",
         "name": "quoteParams", // [!code focus]
         "type": "tuple"
       },
       ...
     ],
     "name": "QuoteCreated", // [!code focus]
     "type": "event"
}
```

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN}
        channel_id: 123456789012345678
        networks:
          - ethereum
        messages:
          - event_name: Transfer
            conditions: // [!code focus]
              - "quoteParams.profileId": "=1" // [!code focus]
```

### template_inline

You can then write your own template inline, this is the template you want to send to the channel.
You have to use the ABI input names in object notation for example if i wanted to put value in the template
i just have to write `{{value}}` in the template and it will be replaced with the value of the event itself.

The template supports:
- bold text = \*bold text\*
- italic text = \_italic text\_
- inline url = \[inline URL\](YOUR_URL)
- inline fixed-width code = \`inline fixed-width code\`
- pre-formatted fixed-width code block = \`\`\`pre-formatted fixed-width code block\`\`\`
- pre-formatted fixed-width known code block = \`\`\`rust pre-formatted fixed-width known code block\`\`\`
- breaks = just line break in the template

#### transaction_information

You also can use the `transaction_information` object to get common information about the transaction, this is the
transaction information for the event.

```rs
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct TxInformation {
    pub network: String,
    // This will convert to a hex string in the template
    pub address: Address,
    // This will convert to a hex string in the template
    pub block_hash: BlockHash,
    // This will convert to a string decimal in the template
    pub block_number: U64,
    // This will convert to a hex string in the template
    pub transaction_hash: TxHash,
    // This will convert to a string decimal in the template
    pub log_index: U256,
    // This will convert to a string decimal in the template
    pub transaction_index: U64,
}
```

:::info
To avoid confusion `address` in `transaction_information` is the address of the contract the event was emitted from.
:::

#### format_value

You can use the `format_value` function to format the value of the event to a decimal value with the specified decimals.

Lets put it all together:

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    discord: // [!code focus]
      - bot_token: ${DISCORD_BOT_TOKEN}
        channel_id: 123456789012345678
        networks:
          - ethereum
        messages:
          - event_name: Transfer // [!code focus]
            template_inline: "*New RETH Transfer Event* // [!code focus]

                              from: {{from}} // [!code focus]

                              to: {{to}} // [!code focus]

                              amount: {{format_value(value, 18)}} // [!code focus]

                              RETH contract: {{transaction_information.address}} // [!code focus]

                              [etherscan](https://etherscan.io/tx/{{transaction_information.transaction_hash}}) // [!code focus]
                              " // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/chatbots/index.mdx">
# Chatbots

:::info
rindexer Chatbots can be used without any other storage providers. It can also be used with storage providers.
:::

rindexer has first-class support for Chatbots, this means you can use your favourite chat platform to
send messages to when events happen on chain.

:::info
Due to rate limits out of rindexer control ChatBots will only send messages with max block range of 10 blocks.
Most people who will use rindexer Chatbots will only want to start sending messages of the live data anyway.
The ChatBots are really only meant to be ran sending live data not historic data.
:::

:::info
Rust projects do not get exposed to the stream clients yet but it can easily be exposed in the future.
:::

Supported Chatbots providers:

- [Telegram](/docs/start-building/chatbots/telegram) - Send messages to your Telegram chats
- [Discord](/docs/start-building/chatbots/discord) - Send messages to your Discord chats
- [Slack](/docs/start-building/chatbots/slack) - Send messages to your Slack channels

Want any other chat provider to be supported? [Create an issue](https://github.com/joshstevens19/rindexer/issues/new) and we will look into it.
</file>

<file path="documentation/docs/pages/docs/start-building/chatbots/slack.mdx">
# Slack

Slack is one of the most popular chat platforms, and is great to build bots and notifications when things happen on chain.

:::info
Due to rate limits out of rindexer control ChatBots will only send messages with max block range of 10 blocks.
Most people who will use rindexer Chatbots will only want to start sending messages of the live data anyway.
The ChatBots are really only meant to be ran sending live data not historic data.
:::

## Setup a bot on slack

1. Go to api.slack.com, log into your workspace and click on Create an app
2. Click on From scratch and then give it a name and select your workspace. We will call ours RethTransferEvents.
3. Click on the Bots box under the Add features and functionality header
4. Click on Review scopes to add
5. Scroll down to the Bot token scopes header and add `chat:write`. These are the permissions the bot needs to write messages
6. Finally, scroll all the way up and click on Install to workspace, and Allow on the following screen.
This should now show a screen with the Bot User OAuth Token visible. Take note of this token,
since it’s the one we will be using to send messages.
7. Now add the bot to the channel you want to send a message to, channels are the `#` followed by the channel name.
You need to include the `#` in the channel name.

## Configure rindexer

`slack` property accepts an array allowing you to split up the channels any way you wish.

## Example

:::code-group

```yaml [contract events]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN} // [!code focus]
        channel: "#RethTransferEvents" // [!code focus]
        networks: // [!code focus]
          - ethereum // [!code focus]
        messages:
          - event_name: Transfer // [!code focus]
            # filter_expression is optional // [!code focus]
            filter_expression: "from = '0x0338ce5020c447f7e668dc2ef778025ce3982662' || from = '0x0338ce5020c447f7e668dc2ef778025ce398266u' && value >= 10 && value <= 2000000000000000000" // [!code focus]
            template_inline: "*New RETH Transfer Event* // [!code focus]

                              from: {{from}} // [!code focus]

                              to: {{to}} // [!code focus]

                              amount: {{format_value(value, 18)}} // [!code focus]

                              RETH contract: {{transaction_information.address}} // [!code focus]

                              <https://etherscan.io/tx/{{transaction_information.transaction_hash}} | etherscan> // [!code focus]
                              " // [!code focus]
```

```yaml [native transfers]
name: ETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN} // [!code focus]
        channel: "#EthTransferEvents" // [!code focus]
        networks: // [!code focus]
          - ethereum // [!code focus]
        messages:
          - event_name: NativeTransfer // [!code focus]
            template_inline: "*New ETH Transfer Event* // [!code focus]

                              from: {{from}} // [!code focus]

                              to: {{to}} // [!code focus]

                              amount: {{format_value(value, 18)}} // [!code focus]

                              Token address: {{transaction_information.address}} // [!code focus]

                              [etherscan](https://etherscan.io/tx/{{transaction_information.transaction_hash}}) // [!code focus]
                              " // [!code focus]
```

:::

## bot_token

This is your slack bot token which you generate using @BotFather.

:::info
We advise you to put this in a environment variables.
:::

```yaml
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN} // [!code focus]
```

## channel

This is the channel you want to send messages to.

:::info
The `#` must be included in the channel name.
:::

```yaml
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN}
        channel: "#RethTransferEvents" // [!code focus]
```

## networks

This is an array of networks you want to send messages to this slack channel.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN}
        channel: "#RethTransferEvents"
        networks: // [!code focus]
          - ethereum // [!code focus]
```

## messages

This is an array of messages you want to send to this slack channel. It is an array as you can define many different
messages to send to this channel with different conditions.

### event_name

This is the name of the event you want to send a message for, must match the ABI event name.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN}
        channel: "#RethTransferEvents"
        networks:
          - ethereum
        messages:
          - event_name: Transfer // [!code focus]
```

### filter_expression

This accepts a filter expression to filter the events before sending a message to this discord channel

:::info
This is optional, if you do not provide any filter expression all the events will be sent to this discord channel.
:::

Filter expressions allow for condition checking of the event data and support logical operators to combine multiple conditions.

#### Supported types and operations:
| Type | Description | Operators | Notes |
|------|-------------|-------------------|-------|
| Numeric (uint/int variants) | Integer values (e.g., `42`, `-100`) or decimal values (e.g., `3.14`, `-0.5`) | `>`, `<`, `=`, `>=`, `<=` | Numeric comparisons | Numbers must have digits before and after a decimal point if one is present (e.g., `.5` or `5.` are not valid standalone numbers) |
| Address | Ethereum addresses (e.g., `0x1234567890abcdef1234567890abcdef12345678`) | `=`, `!=` | Comparisons (e.g., `from == '0xABC...'`) are typically case-insensitive regarding the hex characters of the address value itself |
| String | Text values. Can be single-quoted (e.g., `'hello'`) or, on the right-hand side of a comparison, unquoted (e.g., `active`) | `=`, `!=` | Quoted strings support `\'` to escape a single quote and `\\` to escape a backslash. All string comparison operations (e.g., `name == 'Alice'`, `description contains 'error'`) are performed case-insensitively during evaluation. |
| Boolean | True or false values | `=`, `!=` | Represented as `true` or `false`. These keywords are parsed case-insensitively (e.g., `TRUE`, `False` are also valid in expressions). |
| Hex String Literal | A string literal starting with `0x` or `0X` followed by hexadecimal characters (0-9, a-f, A-F). | `=`, `!=` | Treated as a string for comparison purposes (e.g., `input_data starts_with '0xa9059cbb'`). Comparison is case-sensitive for the hex characters after `0x`. |
| Array | Ordered list of items | `==`, `!=`, `[index]` | See "Array Type Operations" below

#### Logical Operators
- `&&` - All conditions must be true
- `||` - At least one condition must be true
- `()` - Parentheses for grouping
- `&&` has higher precedence than `||` (i.e., `&&` operations are evaluated before `||` operations if not grouped by parentheses)

#### Array Type Operations
For array types, you can use the following operations:
- `array_param == '["raw_json_array_string"]'` string comparison of the array's entire JSON string representation against the provided string
- `array_param != '["raw_json_array_string"]'` the negation of the above
- `array_param[0]` indexed access. The index must be a non-negative integer.

#### Whitespace
Flexible whitespace is generally allowed around operators, parentheses, and keywords for readability. However, whitespace within quoted string literals is significant and preserved.

#### Examples
- `value > 1000` - Numeric comparison, checks if `value` is greater than 1000.
- `from = '0x1234567890abcdef1234567890abcdef12345678'` - Address comparison, checks if `from` matches the specified address.
- `name != 'Alice'` - String comparison, checks if `name` is not equal to 'Alice'.
- `active = true` - Boolean comparison, checks if `active` is true.
- `value >= 1000 && value <= 2000` - Numeric range check, checks if `value` is between 1000 and 2000 inclusive.
- `from = '0x1234567890abcdef1234567890abcdef12345678' || from = '0xabcdefabcdefabcdefabcdefabcdefabcdef'` - Address comparison with logical OR, checks if `from` matches either of the two addresses.
- `value > 1000 && (from = '0x1234567890abcdef1234567890abcdef12345678' || from = '0xabcdefabcdefabcdefabcdefabcdefabcdef')` - Combined numeric and address checks with logical AND and OR, checks if `value` is greater than 1000 and `from` matches either of the two addresses.

### conditions

This accepts an array of conditions you want to apply to the event data before sending a message to this slack channel.

:::warning
Conditions are supported for backwards compatibility. It is recommended to use `filter_expression` instead of `conditions`.
:::

:::info
This is optional, if you do not provide any conditions all the events will be sent to this slack channel.
:::

You may want to filter on the message based on the event data, if the event data has not got an index on the on the
solidity event you can not filter it over the logs. The `conditions` filter is here to help you with this,
based on your ABI you can filter on the event data.

rindexer has enabled a special syntax which allows you to define on your ABI fields what you want to filter on.

1. `>` - higher then (for numbers only)
2. `<` - lower then (for numbers only)
3. `=` - equals
4. `>=` - higher then or equals (for numbers only)
5. `<=` - lower then or equals (for numbers only)
6. `||` - or
7. `&&` - and

So lets look at an example lets say i only want to get transfer events which are higher then `2000000000000000000` RETH wei

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN}
        channel: "#RethTransferEvents"
        networks:
          - ethereum
        messages:
          - event_name: Transfer // [!code focus]
            conditions: // [!code focus]
              - "value": ">=2000000000000000000" // [!code focus]
```

We use the ABI input name `value` to filter on the value field, you can find these names in the ABI file.

```json
{
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value", // [!code focus]
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
}
```

You can use the `||` or `&&` to combine conditions.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN}
        channel: "#RethTransferEvents"
        networks:
          - ethereum
        messages:
          - event_name: Transfer
            conditions: // [!code focus]
              - "value": ">=2000000000000000000 && value <=4000000000000000000" // [!code focus]
```

You can use the `=` to filter on other aspects like the `from` or `to` address.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN}
        channel: "#RethTransferEvents"
        networks:
          - ethereum
        messages:
          - event_name: Transfer
              conditions: // [!code focus]
                - "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662 || 0x0338ce5020c447f7e668dc2ef778025ce398266u" // [!code focus]
                - "value": ">=2000000000000000000 || value <=4000000000000000000" // [!code focus]
```

:::info
Note we advise you to filer any `indexed` fields in the contract details in the `rindexer.yaml` file.
As these can be filtered out on the request level and not filtered out in rindexer itself.
You can read more about it [here](/docs/start-building/yaml-config/contracts#indexed_1-indexed_2-indexed_3).
:::

If you have a tuple and you want to get that value you just use the object notation.

For example lets say we want to only get the events for `profileId` from the `quoteParams` tuple which equals `1`:

```json
{
     "anonymous": false,
     "inputs": [
       {
         "components": [
           {
             "internalType": "uint256",
             "name": "profileId", // [!code focus]
             "type": "uint256"
           },
           ...
         ],
         "indexed": false,
         "internalType": "struct Types.QuoteParams",
         "name": "quoteParams", // [!code focus]
         "type": "tuple"
       },
       ...
     ],
     "name": "QuoteCreated", // [!code focus]
     "type": "event"
}
```

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN}
        channel: "#RethTransferEvents"
        networks:
          - ethereum
        messages:
          - event_name: Transfer
            conditions: // [!code focus]
              - "quoteParams.profileId": "=1" // [!code focus]
```

### template_inline

You can then write your own template inline, this is the template you want to send to the channel.
You have to use the ABI input names in object notation for example if i wanted to put value in the template
i just have to write `{{value}}` in the template and it will be replaced with the value of the event itself.

The template supports:
- bold text = \*bold text\*
- italic text = \_italic text\_
- strikethrough text = \~strikethrough text\~
- block qoute = \> block quote
- inline url = \<URL \| etherscan\>
- inline fixed-width code = \`inline fixed-width code\`
- pre-formatted fixed-width code block = \`\`\`pre-formatted fixed-width code block\`\`\`
- pre-formatted fixed-width known code block = \`\`\`rust pre-formatted fixed-width known code block\`\`\`
- breaks = just line break in the template

#### transaction_information

You also can use the `transaction_information` object to get common information about the transaction, this is the
transaction information for the event.

```rs
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct TxInformation {
    pub network: String,
    // This will convert to a hex string in the template
    pub address: Address,
    // This will convert to a hex string in the template
    pub block_hash: BlockHash,
    // This will convert to a string decimal in the template
    pub block_number: U64,
    // This will convert to a hex string in the template
    pub transaction_hash: TxHash,
    // This will convert to a string decimal in the template
    pub log_index: U256,
    // This will convert to a string decimal in the template
    pub transaction_index: U64,
}
```

:::info
To avoid confusion `address` in `transaction_information` is the address of the contract the event was emitted from.
:::

##### format_value

You can use the `format_value` function to format the value of the event to a decimal value with the specified decimals.

Lets put it all together:

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    slack: // [!code focus]
      - bot_token: ${SLACK_BOT_TOKEN}
        channel: "#RethTransferEvents"
        networks:
          - ethereum
        messages:
          - event_name: Transfer // [!code focus]
            template_inline: "*New RETH Transfer Event* // [!code focus]

                              from: {{from}} // [!code focus]

                              to: {{to}} // [!code focus]

                              amount: {{format_value(value, 18)}} // [!code focus]

                              RETH contract: {{transaction_information.address}} // [!code focus]

                              <https://etherscan.io/tx/{{transaction_information.transaction_hash}} | etherscan> // [!code focus]
                              " // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/chatbots/telegram.mdx">
# Telegram

Telegram is one of the most popular chat platforms, and is great to build bots and notifications when things happen on chain.

:::info
Due to rate limits out of rindexer control ChatBots will only send messages with max block range of 10 blocks.
Most people who will use rindexer Chatbots will only want to start sending messages of the live data anyway.
The ChatBots are really only meant to be ran sending live data not historic data.
:::

## Setup a bot on telegram

You have to use telegram itself to setup a bot:

1. Search BotFather on Telegram.
2. Type /start to get started.
3. Type /newbot to get a bot.
4. Enter your Bot name and unique Username, which should end with the bot.
5. Then, you will get your Bot token. (keep this safe you need it shortly)

## Configure rindexer

`telegram` property accepts an array allowing you to split up the chats any way you wish.

## Example

:::code-group

```yaml [contract events]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN} // [!code focus]
        chat_id: -4223616270 // [!code focus]
        networks: // [!code focus]
          - ethereum // [!code focus]
        messages:
          - event_name: Transfer // [!code focus]
            # filter_expression is optional // [!code focus]
            filter_expression: "from = '0x0338ce5020c447f7e668dc2ef778025ce3982662' || from = '0x0338ce5020c447f7e668dc2ef778025ce398266u' && value >= 10 && value <= 2000000000000000000" // [!code focus]
            template_inline: "*New RETH Transfer Event* // [!code focus]

                              from: {{from}} // [!code focus]

                              to: {{to}} // [!code focus]

                              amount: {{format_value(value, 18)}} // [!code focus]

                              RETH contract: {{transaction_information.address}} // [!code focus]

                              [etherscan](https://etherscan.io/tx/{{transaction_information.transaction_hash}}) // [!code focus]
                              " // [!code focus]
```

```yaml [native transfers]
name: ETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN} // [!code focus]
        chat_id: -4223616270 // [!code focus]
        networks: // [!code focus]
          - ethereum // [!code focus]
        messages:
          - event_name: NativeTransfer // [!code focus]
            template_inline: "*New ETH Transfer Event* // [!code focus]

                              from: {{from}} // [!code focus]

                              to: {{to}} // [!code focus]

                              amount: {{format_value(value, 18)}} // [!code focus]

                              Token address: {{transaction_information.address}} // [!code focus]

                              [etherscan](https://etherscan.io/tx/{{transaction_information.transaction_hash}}) // [!code focus]
                              " // [!code focus]
```

:::

## bot_token

This is your telegram bot token which you generate using @BotFather.

:::info
We advise you to put this in a environment variables.
:::

```yaml
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN} // [!code focus]
```

## chat_id

You have add your bot to chats to use it, so this is the chat ID you wish the bot to send messages to.

```yaml
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN}
        chat_id: -4223616270 // [!code focus]
```

## networks

This is an array of networks you want to send messages to this telegram chat.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN}
        chat_id: -4223616270
        networks: // [!code focus]
          - ethereum // [!code focus]
```

## messages

This is an array of messages you want to send to this telegram chat. It is an array as you can define many different
messages to send to this chat with different conditions.

### event_name

This is the name of the event you want to send a message for, must match the ABI event name.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN}
        chat_id: -4223616270
        networks:
          - ethereum
        messages:
          - event_name: Transfer // [!code focus]
```

### filter_expression

This accepts a filter expression to filter the events before sending a message to this discord channel

:::info
This is optional, if you do not provide any filter expression all the events will be sent to this discord channel.
:::

Filter expressions allow for condition checking of the event data and support logical operators to combine multiple conditions.

#### Supported types and operations:
| Type | Description | Operators | Notes |
|------|-------------|-------------------|-------|
| Numeric (uint/int variants) | Integer values (e.g., `42`, `-100`) or decimal values (e.g., `3.14`, `-0.5`) | `>`, `<`, `=`, `>=`, `<=` | Numeric comparisons | Numbers must have digits before and after a decimal point if one is present (e.g., `.5` or `5.` are not valid standalone numbers) |
| Address | Ethereum addresses (e.g., `0x1234567890abcdef1234567890abcdef12345678`) | `=`, `!=` | Comparisons (e.g., `from == '0xABC...'`) are typically case-insensitive regarding the hex characters of the address value itself |
| String | Text values. Can be single-quoted (e.g., `'hello'`) or, on the right-hand side of a comparison, unquoted (e.g., `active`) | `=`, `!=` | Quoted strings support `\'` to escape a single quote and `\\` to escape a backslash. All string comparison operations (e.g., `name == 'Alice'`, `description contains 'error'`) are performed case-insensitively during evaluation. |
| Boolean | True or false values | `=`, `!=` | Represented as `true` or `false`. These keywords are parsed case-insensitively (e.g., `TRUE`, `False` are also valid in expressions). |
| Hex String Literal | A string literal starting with `0x` or `0X` followed by hexadecimal characters (0-9, a-f, A-F). | `=`, `!=` | Treated as a string for comparison purposes (e.g., `input_data starts_with '0xa9059cbb'`). Comparison is case-sensitive for the hex characters after `0x`. |
| Array | Ordered list of items | `==`, `!=`, `[index]` | See "Array Type Operations" below

#### Logical Operators
- `&&` - All conditions must be true
- `||` - At least one condition must be true
- `()` - Parentheses for grouping
- `&&` has higher precedence than `||` (i.e., `&&` operations are evaluated before `||` operations if not grouped by parentheses)

#### Array Type Operations
For array types, you can use the following operations:
- `array_param == '["raw_json_array_string"]'` string comparison of the array's entire JSON string representation against the provided string
- `array_param != '["raw_json_array_string"]'` the negation of the above
- `array_param[0]` indexed access. The index must be a non-negative integer.

#### Whitespace
Flexible whitespace is generally allowed around operators, parentheses, and keywords for readability. However, whitespace within quoted string literals is significant and preserved.

#### Examples
- `value > 1000` - Numeric comparison, checks if `value` is greater than 1000.
- `from = '0x1234567890abcdef1234567890abcdef12345678'` - Address comparison, checks if `from` matches the specified address.
- `name != 'Alice'` - String comparison, checks if `name` is not equal to 'Alice'.
- `active = true` - Boolean comparison, checks if `active` is true.
- `value >= 1000 && value <= 2000` - Numeric range check, checks if `value` is between 1000 and 2000 inclusive.
- `from = '0x1234567890abcdef1234567890abcdef12345678' || from = '0xabcdefabcdefabcdefabcdefabcdefabcdef'` - Address comparison with logical OR, checks if `from` matches either of the two addresses.
- `value > 1000 && (from = '0x1234567890abcdef1234567890abcdef12345678' || from = '0xabcdefabcdefabcdefabcdefabcdefabcdef')` - Combined numeric and address checks with logical AND and OR, checks if `value` is greater than 1000 and `from` matches either of the two addresses.

### conditions

This accepts an array of conditions you want to apply to the event data before sending a message to this telegram chat.

:::warning
Conditions are supported for backwards compatibility. It is recommended to use `filter_expression` instead of `conditions`.
:::

:::info
This is optional, if you do not provide any conditions all the events will be sent to this telegram chat.
:::

You may want to filter on the message based on the event data, if the event data has not got an index on the on the
solidity event you can not filter it over the logs. The `conditions` filter is here to help you with this,
based on your ABI you can filter on the event data.

rindexer has enabled a special syntax which allows you to define on your ABI fields what you want to filter on.

1. `>` - higher then (for numbers only)
2. `<` - lower then (for numbers only)
3. `=` - equals
4. `>=` - higher then or equals (for numbers only)
5. `<=` - lower then or equals (for numbers only)
6. `||` - or
7. `&&` - and

So lets look at an example lets say i only want to get transfer events which are higher then `2000000000000000000` RETH wei

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN}
        chat_id: -4223616270
        networks:
          - ethereum
        messages:
          - event_name: Transfer // [!code focus]
            conditions: // [!code focus]
              - "value": ">=2000000000000000000" // [!code focus]
```

We use the ABI input name `value` to filter on the value field, you can find these names in the ABI file.

```json
{
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value", // [!code focus]
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
}
```

You can use the `||` or `&&` to combine conditions.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN}
        chat_id: -4223616270
        networks:
          - ethereum
        messages:
          - event_name: Transfer
            conditions: // [!code focus]
              - "value": ">=2000000000000000000 && value <=4000000000000000000" // [!code focus]
```

You can use the `=` to filter on other aspects like the `from` or `to` address.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN}
        chat_id: -4223616270
        networks:
          - ethereum
        messages:
          - event_name: Transfer
              conditions: // [!code focus]
                - "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662 || 0x0338ce5020c447f7e668dc2ef778025ce398266u" // [!code focus]
                - "value": ">=2000000000000000000 || value <=4000000000000000000" // [!code focus]
```

:::info
Note we advise you to filer any `indexed` fields in the contract details in the `rindexer.yaml` file.
As these can be filtered out on the request level and not filtered out in rindexer itself.
You can read more about it [here](/docs/start-building/yaml-config/contracts#indexed_1-indexed_2-indexed_3).
:::

If you have a tuple and you want to get that value you just use the object notation.

For example lets say we want to only get the events for `profileId` from the `quoteParams` tuple which equals `1`:

```json
{
     "anonymous": false,
     "inputs": [
       {
         "components": [
           {
             "internalType": "uint256",
             "name": "profileId", // [!code focus]
             "type": "uint256"
           },
           ...
         ],
         "indexed": false,
         "internalType": "struct Types.QuoteParams",
         "name": "quoteParams", // [!code focus]
         "type": "tuple"
       },
       ...
     ],
     "name": "QuoteCreated", // [!code focus]
     "type": "event"
}
```

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN}
        chat_id: -4223616270
        networks:
          - ethereum
        messages:
          - event_name: Transfer
            conditions: // [!code focus]
              - "quoteParams.profileId": "=1" // [!code focus]
```

### template_inline

You can then write your own template inline, this is the template you want to send to the chat.
You have to use the ABI input names in object notation for example if i wanted to put value in the template
i just have to write `{{value}}` in the template and it will be replaced with the value of the event itself.

The template supports:
- bold text = \*bold text\*
- italic text = \_italic text\_
- inline url = \[inline URL\](YOUR_URL)
- inline fixed-width code = \`inline fixed-width code\`
- pre-formatted fixed-width code block = \`\`\`pre-formatted fixed-width code block\`\`\`
- pre-formatted fixed-width known code block = \`\`\`rust pre-formatted fixed-width known code block\`\`\`
- breaks = just line break in the template

#### transaction_information

You also can use the `transaction_information` object to get common information about the transaction, this is the
transaction information for the event.

```rs
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct TxInformation {
    pub network: String,
    // This will convert to a hex string in the template
    pub address: Address,
    // This will convert to a hex string in the template
    pub block_hash: BlockHash,
    // This will convert to a string decimal in the template
    pub block_number: U64,
    // This will convert to a hex string in the template
    pub transaction_hash: TxHash,
    // This will convert to a string decimal in the template
    pub log_index: U256,
    // This will convert to a string decimal in the template
    pub transaction_index: U64,
}
```

:::info
To avoid confusion `address` in `transaction_information` is the address of the contract the event was emitted from.
:::

#### format_value

You can use the `format_value` function to format the value of the event to a decimal value with the specified decimals.

Lets put it all together:

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  chat: // [!code focus]
    telegram: // [!code focus]
      - bot_token: ${TELEGRAM_BOT_TOKEN}
        chat_id: -4223616270
        networks:
          - ethereum
        messages:
          - event_name: Transfer // [!code focus]
            template_inline: "*New RETH Transfer Event* // [!code focus]

                              from: {{from}} // [!code focus]

                              to: {{to}} // [!code focus]

                              amount: {{format_value(value, 18)}} // [!code focus]

                              RETH contract: {{transaction_information.address}} // [!code focus]

                              [etherscan](https://etherscan.io/tx/{{transaction_information.transaction_hash}}) // [!code focus]
                              " // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/create-new-project/index.mdx">
# Create New Project

rindexer provides two modes for creating new projects, each optimized for different use cases and infrastructure setups.

## Choose Your Mode

### Standard Mode (Most common setup)

The default way to create and run rindexer projects. This mode connects to blockchain nodes via RPC endpoints and is suitable for every use case.

[Create a Standard Project →](/docs/start-building/create-new-project/standard)

### Reth Mode (Advanced)

An advanced mode that integrates directly with Reth nodes using Execution Extensions (ExEx).

**Requirements:**
- Running Reth archive node
- Advanced knowledge of Ethereum infrastructure

[Create a Reth Project →](/docs/start-building/create-new-project/reth-mode)
</file>

<file path="documentation/docs/pages/docs/start-building/create-new-project/reth-mode.mdx">
# Create New Project - Reth Mode

:::warning
Reth mode requires running a Reth archive node and is intended for advanced users. If you're just getting started, you should be using [Standard Mode](/docs/start-building/create-new-project/standard) instead.
:::

:::info
Make sure you have the CLI installed before starting a new project.
You can find the installation instructions [here](/docs/introduction/installation).
:::

## Prerequisites

Before creating a Reth mode project, ensure you have:

1. **Reth Archive Node**: A fully synced Reth archive node running locally or on your infrastructure
2. **Hardware Requirements**: Sufficient disk space and memory for running both Reth and rindexer

:::tip
For detailed instructions on setting up a Reth archive node, see the official [Running Reth on Ethereum](https://reth.rs/run/ethereum) guide.
:::

## 1. Create a New Reth Project

The `--reth` flag enables Reth mode when creating a new project. You can also pass additional Reth configuration arguments after `--`.

:::code-group

```bash [no-code]
rindexer new no-code --reth
```

```bash [rust]
rindexer new rust --reth
```

```bash [with custom args]
rindexer new no-code --reth -- --datadir /custom/path --http true
```

:::

:::info
Starting rust projects with Reth mode will add the `reth` feature to your project, which automatically installs the dependencies for Reth. The user does not need to install Reth separately.
:::

### Example New with Reth

```bash
rindexer new no-code --reth

Initializing new rindexer project with Reth support...

Project Name: RocketPoolETHIndexer
Project Description (skip by pressing Enter): High-performance rETH indexer using Reth
Repository (skip by pressing Enter): https://github.com/joshstevens19/rindexer
What Storages To Enable? (graphql can only be supported if postgres is enabled) [postgres, csv, both, none]: postgres
Postgres Docker Support Out The Box? [yes, no]: yes

Reth Configuration:
Data Directory (default: ~/.reth): /data/reth
Chain (default: mainnet) [mainnet, sepolia, holesky]: mainnet
Enable HTTP RPC? [yes, no]: yes
Auth RPC Port (default: 8551): 8551

rindexer no-code project created with Reth support with a rETH transfer events YAML template.
```

## 2. Reth Configuration in YAML

When you create a project with `--reth`, the generated `rindexer.yaml` includes Reth configuration:

```yaml
name: RocketPoolETHIndexer
description: High-performance rETH indexer using Reth
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co  # Fallback RPC
    reth:
      enabled: true
      logging: true  # Show Reth logs in stdout
      cli_args:
        - "--datadir /data/reth"
        - "--http"
        - "--full false"  # Archive mode
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
    - network: ethereum
      address: "0xae78736cd615f374d3085123a210448e74fc6393"
      start_block: 18600000
      end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
    - Transfer
    - Approval
```

### Key Reth Configuration Options

- **enabled**: Enable/disable Reth integration
- **logging**: Show Reth logs in stdout (useful for debugging)
- **cli_args**: Array of Reth CLI arguments in "flag value" format

:::warning
The JWT secret for authenticated RPC is not included in the YAML for security reasons. You'll need to add it manually or via environment variables:
```yaml
cli_args:
  - "--authrpc.jwtsecret /path/to/jwt.hex"
```
:::

### Common Reth CLI Arguments

| Argument | Description | Example |
|----------|-------------|---------|
| `--datadir` | Reth data directory | `--datadir /data/reth` |
| `--authrpc.jwtsecret` | Path to JWT secret | `--authrpc.jwtsecret /secrets/jwt.hex` |
| `--authrpc.addr` | Auth RPC address | `--authrpc.addr 127.0.0.1` |
| `--authrpc.port` | Auth RPC port | `--authrpc.port 8551` |
| `--full` | Run as full node (false for archive) | `--full false` |
| `--chain` | Network to sync | `--chain mainnet` |
| `--http` | Enable HTTP RPC | `--http` |
| `--metrics` | Enable metrics | `--metrics 127.0.0.1:9001` |

## 3. Environment Variables

All Reth configuration can be overridden using environment variables:

```bash
# .env file
RETH_DATA_DIR=/data/reth
RETH_JWT_SECRET=/secrets/jwt.hex
RETH_AUTH_PORT=8551
```

```yaml
# rindexer.yaml using environment variables
networks:
  - name: ethereum
    chain_id: 1
    rpc: ${FALLBACK_RPC_URL}
    reth:
      enabled: true
      cli_args:
        - "--datadir ${RETH_DATA_DIR}"
        - "--authrpc.jwtsecret ${RETH_JWT_SECRET}"
        - "--authrpc.port ${RETH_AUTH_PORT}"
```

## 4. Start the Project

Starting a Reth mode project will:
1. Start your Reth node, and connect to it
2. Set up ExEx (Execution Extensions) for reorg-aware streaming
3. Begin indexing with minimal latency

:::info
Ensure your Reth node is fully synced before starting the indexer.
:::

:::code-group

```bash [all services]
rindexer start all
```

```bash [indexer only]
rindexer start indexer
```

```bash [graphql only]
rindexer start graphql
```

:::

## 5. Performance Benefits

Reth mode provides several advantages over standard RPC-based indexing:

### Native Reorg Handling
- ExEx notifications include reorg information
- Automatic rollback and reprocessing (coming soon)
- No missed events during reorgs

### Minimal Latency
- Direct connection to Reth node
- No network overhead
- Access to pending transactions

### Better Performance
- Efficient state access
- Reduced RPC calls

## 6. Monitoring and Debugging

### Enable Reth Logging

Set `logging: true` in your Reth configuration to see detailed logs:

```yaml
reth:
  enabled: true
  logging: true  # Enable Reth logs
```

## 7. Troubleshooting

### Common Issues

**Cannot connect to Reth node**
- Ensure Reth is running and fully synced
- Verify JWT secret is correct
- Try running reth node separately to see if it works. There might be a problem with the arguments.

**Performance issues**
- Monitor Reth resource usage
- Ensure sufficient disk I/O

### Getting Help

For Reth-specific issues:
- [Reth Documentation](https://reth.rs)
- [Reth GitHub](https://github.com/paradigmxyz/reth)
- [rindexer GitHub Issues](https://github.com/joshstevens19/rindexer/issues)

## Next Steps

- Learn about [Reth Execution Extensions](/docs/advanced/using-reth-exex)
- Configure [advanced indexing options](/docs/start-building/yaml-config)
</file>

<file path="documentation/docs/pages/docs/start-building/create-new-project/standard.mdx">
# Create New Project - Standard Mode

:::info
Make sure you have the CLI installed before starting a new project.
You can find the installation instructions [here](/docs/introduction/installation).
:::

We advise anyone using rindexer to install docker which makes running locally with postgres storage a lot easier.
If you not got docker you can install it [here](https://docs.docker.com/get-docker/).

## 1. Create a new project

This will walk you through setting up your project by asking you a series of questions in the terminal.

:::code-group

```bash [no-code]
rindexer new no-code
```

```bash [rust]
rindexer new rust
```

:::

### Example New

```bash
Initializing new rindexer project...

Project Name: RocketPoolETHIndexer
Project Description (skip by pressing Enter): My first rindexer project
Repository (skip by pressing Enter): https://github.com/joshstevens19/rindexer
What Storages To Enable? (graphql can only be supported if postgres is enabled) [postgres, csv, both, none]: postgres
Postgres Docker Support Out The Box? [yes, no]: yes

rindexer no-code project created with a rETH transfer events YAML template.
```

If any of the steps are unclear, you can find more information in the [New Project Appendix](#new-project-appendix).

Once completed a new boilerplate project will be created in the current directory. You can navigate to the project directory and start building your project.
The boilerplate project is configured to index rETH transfer and approval events from ethereum mainnet between a specific block range.

## 2. Add Environment Variables

If you are not using `postgres` you can move straight to [starting your project](#3-config-your-rindexeryaml-file).

If you selected `yes` to the `Postgres Docker Support Out The Box?` question, a `.env` file has be generated for you
with the required environment variables. You can move straight to [starting your project](#3-config-your-rindexeryaml-file).

--------------------------------

Open up the generated `.env` file and fill in the required environment variables.

### DATABASE_URL

:::warning
Can skip if:
- on question "What Storages To Enable?" you selected csv or none
- on question "Postgres Docker Support Out The Box?" you selected yes
:::

For ease of running locally we suggest you enable docker support on the rindexer project, if you did not 
enable docker support with postgres storage you will need to provide a postgres database information in the `.env` file 
which has been generated for you.

`sslmode=require` is supported as well just include it in the connection string.

### POSTGRES_PASSWORD

:::warning
Can skip if:
- on question "What Storages To Enable?" you selected csv or none
- on question "Postgres Docker Support Out The Box?" you selected no
:::

This is injected into the `.env` for your if you selected `yes` to the `Postgres Docker Support Out The Box?` question.
This is used for the docker to create a postgres database for you locally. You do not need this if you have your own postgres database or
on deployed environments. It is purely for local development.

```bash
POSTGRES_PASSWORD=password
```

### Other Environment Variables

Every part of the `rindexer.yaml` file can be overridden by an environment variable.
The syntax for this in the `rindexer.yaml` is `${ENV_VARIABLE_NAME}` example `${POLYGON_RPC_URL}`. This can be used in ANY field in the YAML file.
Read more about the environment variables in the [yaml configuration documentation](/docs/start-building/yaml-config#environment-variables).

## 3. Config your `rindexer.yaml` file

Generating a rindexer project will generate a `rindexer.yaml` file for you. This is where you will configure your project.
You can read all about the rindexer.yaml settings in the [yaml configuration documentation](/docs/start-building/yaml-config).
You can also use the [rindexer add](/docs/start-building/add) command to add contracts to your project and pull in ABIs for you.

It will generate you an boilerplate project which is configured to index rETH transfer and approval events from ethereum mainnet between a specific block range.

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
    - network: ethereum
      address: "0xae78736cd615f374d3085123a210448e74fc6393"
      start_block: 18600000
      end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
    - Transfer
    - Approval
```

## 4. Start the project

:::info
rindexer starts your postgres docker compose file up for you automatically if the DATABASE_URL can not connect to the database and docker-compose.yml is present in the parent directory.
You will need to make sure you have docker running on your machine before starting the project. If you have not got docker you can install it [here](https://docs.docker.com/get-docker/).
You can also run docker manually by using `docker compose up -d`.
:::

:::info
graphql can only run if you have postgres storage enabled
:::

:::code-group

```bash [graphql and indexer]
rindexer start all
```

```bash [indexer only]
rindexer start indexer
```

```bash [graphql only]
rindexer start graphql
```

:::

:::warning
The boilerplate template uses a free node which may get rate limited. We recommend using a paid node for production.
:::

## 5. Query the GraphQL API

:::info
It is worth noting that the graphql API is here for convenience as it works out the box with the postgres storage,
if your building an advance indexer you may want to build your own API on top of the data you have indexed.
If that's the case it is fine just to use rindexer as the indexing tool and build your own API on top of it.
:::

:::info
Each page request will have a max query limit of 1000 per page to avoid memory and database issues.
:::

GraphQL will be available at http://localhost:3001/graphql and the playground at http://localhost:3001/playground.
You can read more about rindexer GraphQL API in the [API documentation](/docs/accessing-data/graphql).

:::code-group

```graphql [request]
query AllTransfers($orderBy: [TransfersOrderBy!] = [BLOCK_NUMBER_DESC], $first: Int = 5) {
  allTransfers(orderBy: $orderBy, first: $first) {
    nodes {
      blockHash
      blockNumber
      contractAddress
      from
      network
      nodeId
      to
      txHash
      value
    }
    pageInfo {
      endCursor
      hasNextPage
      hasPreviousPage
      startCursor
    }
  }
}
```

```json [response]
{
  "data": {
    "allTransfers": {
      "nodes": [
        {
          "blockHash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
          "blockNumber": "18718011",
          "contractAddress": "0xae78736cd615f374d3085123a210448e74fc6393                        ",
          "from": "0xfac5ddb4e3eb6941a458544bfe2588ee566bd4ff",
          "network": "ethereum",
          "nodeId": "WyJ0cmFuc2ZlcnMiLDU4Nzld",
          "to": "0x2201d2400d30bfd8172104b4ad046d019ca4e7bd",
          "txHash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
          "value": "263518859"
        },
        {
          "blockHash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
          "blockNumber": "18718011",
          "contractAddress": "0xae78736cd615f374d3085123a210448e74fc6393                        ",
          "from": "0xe4f719c11fc5ab883e32068df99962985645e860",
          "network": "ethereum",
          "nodeId": "WyJ0cmFuc2ZlcnMiLDU4ODBd",
          "to": "0xc5c2dd38d29960e7bb015e77be44aefbb08f192b",
          "txHash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
          "value": "19152486233394367"
        },
        {
          "blockHash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
          "blockNumber": "18718011",
          "contractAddress": "0xae78736cd615f374d3085123a210448e74fc6393                        ",
          "from": "0xc5c2dd38d29960e7bb015e77be44aefbb08f192b",
          "network": "ethereum",
          "nodeId": "WyJ0cmFuc2ZlcnMiLDU4ODFd",
          "to": "0x882a41fd4c5d09d01900db378903c5c00cc31d64",
          "txHash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
          "value": "19159007520480803"
        },
        {
          "blockHash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
          "blockNumber": "18718011",
          "contractAddress": "0xae78736cd615f374d3085123a210448e74fc6393                        ",
          "from": "0x882a41fd4c5d09d01900db378903c5c00cc31d64",
          "network": "ethereum",
          "nodeId": "WyJ0cmFuc2ZlcnMiLDU4ODJd",
          "to": "0x2201d2400d30bfd8172104b4ad046d019ca4e7bd",
          "txHash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
          "value": "19159007520480803"
        },
        {
          "blockHash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
          "blockNumber": "18718011",
          "contractAddress": "0xae78736cd615f374d3085123a210448e74fc6393                        ",
          "from": "0x882a41fd4c5d09d01900db378903c5c00cc31d64",
          "network": "ethereum",
          "nodeId": "WyJ0cmFuc2ZlcnMiLDU4ODNd",
          "to": "0xc5c2dd38d29960e7bb015e77be44aefbb08f192b",
          "txHash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
          "value": "0"
        }
      ],
      "pageInfo": {
        "endCursor": "WyJibG9ja19udW1iZXJfZGVzYyIsWzE4NzE4MDExLDU4ODNdXQ==",
        "hasNextPage": true,
        "hasPreviousPage": false,
        "startCursor": "WyJibG9ja19udW1iZXJfZGVzYyIsWzE4NzE4MDExLDU4NzldXQ=="
      }
    }
  }
}
```

:::

### Generate graphql queries

You can generate .graphql prebuilt queries to get up and running in seconds.
These will be generated in a `queries` folder.

```bash
rindexer codegen graphql
```

#### TypeScript

[graphql-codegen](https://the-guild.dev/graphql/codegen) is the best tool on the market to generate TypeScript typings for your GraphQL queries, mutations, and subscriptions.

learn about the `codegen.ts` config [here](https://the-guild.dev/graphql/codegen/docs/config-reference/codegen-config)

the graphql API url is the `schema` in the config, you can set this to your graphql endpoint like so:

```ts
import { CodegenConfig } from '@graphql-codegen/cli'

const config: CodegenConfig = {
  // this is YOUR_GRAPHQL_API_URL // [!code focus]
  schema: 'http://localhost:3001/graphql', // [!code focus]
  ...
}

export default config
```

then how you hook up the config with your tool of choice, below are some links to documentation:

- React Apollo - https://the-guild.dev/graphql/codegen/plugins/typescript/typescript-react-apollo#with-react-hooks
- React Query - https://the-guild.dev/graphql/codegen/plugins/typescript/typescript-react-query
- Node app - https://the-guild.dev/graphql/codegen/plugins/typescript/typescript-urql

#### .NET, Dart, Java, Flow

codegen for other languages can be found [here](https://the-guild.dev/graphql/codegen)

## New Project Appendix

If you are not sure what to select this section will explain each step in more detail.

### What Storages To Enable?

- Postgres - This will use a postgres database to store the data.
- Csv - This will store the data in a csv file on the machine.
- Both - This will store the data in both a postgres database and a csv file.
- None - This will not store the data anywhere.

### Postgres Docker

- Yes - This will use docker to spin up a postgres database for you, great for local development.
- No - This will not use docker and you will need to provide a postgres database information in the `.env` file.
</file>

<file path="documentation/docs/pages/docs/start-building/project-types/index.mdx">
# Project Types

rindexer has two types of projects you can create:

- [No-code Project](/docs/start-building/project-types/no-code-project) - No-code project is where you can start indexing chain events without writing any code.

This is what a lot of people will use to get started with rindexer.
- [Rust Project](/docs/start-building/project-types/rust-project) - Rust project is where you can create custom indexing systems using Rust.

This is for more advanced users who want to build more complex indexing systems.
</file>

<file path="documentation/docs/pages/docs/start-building/project-types/no-code-project.mdx">
# No-code Project

The No-code Project type in rindexer is designed for users who wish to quickly set up and deploy indexing solutions without delving into
the complexities of coding. This project type leverages a YAML-based configuration that guides you through specifying what data to index and
how to index it. It's an ideal solution for those who need to implement standard indexing tasks, or for developers who prefer to focus more
on application logic than on the intricacies of the indexing process.

**Features:**
- **Easy Configuration**: Set up your indexing with a simple YAML file. Define what chain events to listen
for and how they should be processed with just a few lines of configuration.
- **Quick Deployment**: With the no-code setup, your project can be up and running in minutes. This rapid deployment
allows you to see results quickly and make adjustments as needed without digging into code.
- **Focus on Use Cases**: This project type is perfect for use cases that don't require specialised processing of the data.
It allows you to focus on the application's functionality rather than on the backend logistics of data handling.

**Ideal For:**
- Data reporting.
- Projects with straightforward indexing needs.
- Fast prototyping and MVP developments.
- Hackathons and quick proof-of-concept projects.

The no-code project functionality will keep growing and evolving to provide more features and capabilities to users who prefer a
configuration-driven approach to indexing. Any suggestions for no-code indexing ideas please create a github issue and we will look into it.
</file>

<file path="documentation/docs/pages/docs/start-building/project-types/rust-project.mdx">
# Rust Project

Requires you to have rust installed on your machine. You can install rust by following the instructions [here](https://www.rust-lang.org/tools/install).

The Rust Project type is for developers looking to build sophisticated and highly customised indexing systems.
This approach utilises the full power of Rust, offering you the tools to write safe, efficient, and highly concurrent code.
By choosing the Rust Project, you can extend the basic capabilities of rindexer with custom logic, complex data transformations,
and optimisations tailored to your specific needs.

**Features:**
- **High Performance**: Leverage Rust’s renowned performance and efficiency to handle large-scale data processing and high-throughput applications.
- **Customizable Indexing Logic**: Implement custom indexing logic and data transformations that go beyond the default configurations offered in the no-code projects.
- **Advanced Data Handling**: Integrate advanced data handling capabilities, such as contract code lookups, API lookups, complex queries, and data aggregation, to suit your specific application needs.

**Ideal For:**
- Advanced projects requiring custom indexing solutions.
- Custom indexing solutions that go beyond the capabilities of the no-code projects.
- Advanced projects who want to data aggregate and transform data in complex ways.

Each project type addresses different user needs and expertise levels, providing flexibility in how you choose to implement
and scale your indexing solutions with rindexer. Whether you prefer a straightforward, configuration-driven approach or a
custom, code-intensive implementation, rindexer supports your development journey.
</file>

<file path="documentation/docs/pages/docs/start-building/rust-project-deep-dive/ethers-alloy-migration.mdx">
# Ethers to Alloy Migration Guide

Rindexer released a breaking change for all Rust projects, which internally migrates from ethers to alloy.

This means that exposed types and primitives are now from Alloy rather than Ethers-rs; some methods have also been
deprecated and this will require breaking changes to be implemented within any rindexer rust project.

The simple steps to take are as follows:

1. Add `alloy` to your `Cargo.toml`
2. Run `rindexer codegen typings` to get the latest codegen state
3. Change all parameter names and types
4. Change advanced methods

You can see more details on each section below.

## 1. Add `alloy` to your `Cargo.toml`

First you should add `alloy` to your `Cargo.toml` and delete `ethers`.  You can now remove any references to the `ethers` crate and replace
this will the newer `alloy` crate. It should be pegged to the same version used by `rindexer` internally to ensure type compatibility.

Replace in `Cargo.toml`
```diff
- ethers = "2.0.14"
+ alloy = { version = "1.1.3", features = ["full"] }
```

Then any references to ethers types in your own code can be replaced with the alloy types. You can read more
in the [migration docs](https://alloy.rs/migrating-from-ethers/reference/).

```diff
- use ethers::prelude::U256;
+ use alloy::primitives::U256;
```

## 2. Run `rindexer codegen typings` to get the latest codegen state

Once you have added the `alloy` crate and changed any of your project-specific code uses you can now run the rindexer
codegen with `rindexer codegen typings`. This will regenerate the bindings with alloy code and types.

:::warn
If for some reason there are errors in your codegen file, please try deleting it before re-running. Otherwise raise an
issue on github: https://github.com/joshstevens19/rindexer/issues.
:::

This should lead to working error-free code in the codegen directory. At this point you should expect to errors in most
of the actual custom indexing files if you have specified them. This is mainly due to the reasons below (parameter name
changes & type changes).

## 3. Change all parameter names and types

You must now go through and deal with the errors in your specific handler functions. These will almost exclusively be
related to casing changes i.e. `token_id` -> `tokenId`. Or it will be a type's name change, i.e `H256` -> `B256`.

This should be relatively self explanatory, you can read more details below but it's just a simple manual process.

**Solidity contract parameters**

The casing of solidity contract parameters is now transparent, meaning the prior `snake_case` conversion will no longer
be retained and instead you will be required to access an event parameter by it's source name.

This will typically be `camelCase` or occasionally `UPPERCASE`, the rust compiler will help with this and the conversion
should be relatively simple, albeit it a manual process.

```diff
- EthereumSqlTypeWrapper::AddressBytes(t.event_data.on_behalf_of),
+ EthereumSqlTypeWrapper::AddressBytes(t.event_data.onBehalfOf),
```

**Solidity types**

The core `EthereumSqlTypeWrapper` have been ported to use the new alloy primitives internally, so if you are manually
passing any `ethers` types such as the `Address` derived manually (i.e. not as an automatically exposed type from rindexer)
then you will need ensure this is migrated to the new types.

Most simple types can be found in the alloy migration guide: https://alloy.rs/migrating-from-ethers/reference/

The most common types which experiences the rename are the `Hash` types, renamed to `Byte` types:
- `H256` -> `B256`
- `H128` -> `B128`
- `H256` -> `B256`
- `H512` -> `B512`

The above includes all Vec, and Byte representation variants e.g. `VecB512`, etc.

The following types have been retained but should be consisted deprecated and may be removed:
- `H160` -> Use `Address` types instead (Including all Vec, and Byte representation variants)

:::warn
A simple find and replace should handle most of these cases
:::

## 4. Change advanced methods

:::warn
Most projects will by default use the underlying type, this is for when manual type manipulation is needed.
:::

Replace any methods that are erroring out that previously were not. The most common of this
will be the existence of `as_u[BITS]`.

These methods are no longer exposed directly on all solidity primitive `uint` types. For example the `as_u32()` method
which existed on a `U256` is no longer directly accessible.

Instead, now, if you wish to downcast, you can use the `TryInto` trait on most `uint` types to achieve the same outcome and
handle errors gracefully in the event of an overflow (where before a panic would occur implicitly).

```diff
- EthereumSqlTypeWrapper::U32(t.tx_information.log_index.as_u32()),
+ EthereumSqlTypeWrapper::U32(t.tx_information.log_index.try_into().expect("log_index should fit in u32")),
```

You may also opt to match the underlying type

```diff
- EthereumSqlTypeWrapper::U32(t.tx_information.log_index.as_u32()),
+ EthereumSqlTypeWrapper::U256(t.tx_information.log_index),
```

## 5. Await `EventCallbackRegistry` `register` calls

The `register` function is now `async`, requiring `.await` to be called for the `Future` to complete execution. This is a breaking change that requires modification of existing custom handler code:

```diff
async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                ...
                return Ok(());
            },
            no_extensions(),
        )
        .await,
    )
-   .register(manifest_path, registry);
+   .register(manifest_path, registry).await; // [!code focus]
}
```
</file>

<file path="documentation/docs/pages/docs/start-building/rust-project-deep-dive/index.mdx">
# Rust Project Deep Dive

As explained in the [rust](/docs/start-building/project-types/rust-project) project type,
the Rust project is a project that is meant to be changed and extended. The template gives you a starting point,
and you may choose to run the indexer differently, use your own custom logic, do http requests, do on chain lookups
or anything else you can think of. rindexer is also a framework that can be used to build your own custom indexer
and not just a no-code indexer.
</file>

<file path="documentation/docs/pages/docs/start-building/rust-project-deep-dive/indexers.mdx">
# Indexers

When creating a new rust project with rindexer it will create you a indexers folder, this is where you will write
your custom logic for the indexer. This is where you will do all your indexing logic, you can do anything you want
in here, you can do http requests, on chain lookups, custom logic, custom DBs, anything you can think of. rindexer gives you
the foundations and also baked in extendability. Rust enforces a strong type system, all logs will be streamed to you
just focus on the logic you want.

By default if you turn storage postgres on in the YAML configuration file it will also create you postgres tables,
also write SQL for you to use and expose you a postgres client. This is a great starting point for you to build on.

The tables creation can be skipped by using the [disable_create_tables](docs/start-building/yaml-config/storage#disable_create_tables)
in the YAML configuration file.

If you also enable the CSV storage it will also generate code in the handler to write to that CSV files.

You can regenerate the indexers folder by running the following command:

:::warning
This will overwrite any custom logic you have written in the indexers folder so be careful.
:::

```bash
rindexer codegen indexer
```

To help understand the interfaces and ways rindexer handlers can be extended we will look at an example.

Take this YAML file - All transfer events for reth on Ethereum between block 18600000 and 18718056 will be indexed.

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
    - Transfer
```

This would generate you a `rocket_pool_eth.rs` file in the indexers folder, this file will have a handler function,
note that the name of the file is the contract name in snake case alongside if you are doing filters `_filter` appended to it.
If you are using multiple events to index on a contract the file it will generate will have all the handlers in the single file.

## `Handlers`

As you see here with this example out the box it will generate you all your indexer handlers for in this case the Transfer event.
If you have postgres storage enabled it will have the bulk insert code written for you.
The boilerplate code is runnable out the box.

```rs
use super::super::super::typings::rust::events::rocket_pool_eth::{
    no_extensions, ApprovalEvent, RocketPoolETHEventType, TransferEvent,
};
use rindexer::{
    event::callback_registry::EventCallbackRegistry, rindexer_error, rindexer_info,
    EthereumSqlTypeWrapper, PgType, RindexerColorize,
};

async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                if results.is_empty() {
                   return Ok(());
                }

                let mut bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
                for result in results.iter() {
                    let data = vec![
                        EthereumSqlTypeWrapper::Address(result.tx_information.address),
                        EthereumSqlTypeWrapper::Address(result.event_data.from),
                        EthereumSqlTypeWrapper::Address(result.event_data.to),
                        EthereumSqlTypeWrapper::U256(result.event_data.value),
                        EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                        EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                        EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                        EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    ];
                    bulk_data.push(data);
                }

                if bulk_data.is_empty() {
                   return Ok(());
                }

                let result = context
                    .database
                    .insert_bulk(
                        "rust_rocket_pool_eth.transfer",
                        &[
                            "contract_address".to_string(),
                            "from".to_string(),
                            "to".to_string(),
                            "value".to_string(),
                            "tx_hash".to_string(),
                            "block_number".to_string(),
                            "block_hash".to_string(),
                            "network".to_string(),
                        ],
                        &bulk_data,
                    )
                    .await;

                if let Err(e) = result {
                    rindexer_error!(
                        "RocketPoolETHEventType::Transfer inserting bulk data: {:?}",
                        e
                    );

                    return Err(e.to_string());
                }

                rindexer_info!(
                    "RocketPoolETH::Transfer - {} - {} events",
                    "INDEXED".green(),
                    results.len(),
                );

                 Ok(())
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}

pub async fn rocket_pool_eth_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    transfer_handler(registry).await;
}
```

## Event::Handler

rindexer hides all the complex rust types and abstracts everything for you so you can easily just build the logic within the handler itself.

As you see with the below you just write the logic and `results` holds all the decoded event data and `context`
holds the database client and any extensions you pass to it.

The naming convention for the handler is `{AbiEventName}Event::handler` so in this case `TransferEvent::handler` this is so you can.

```rs
async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler( // [!code focus]
            |results, context| async move { // [!code focus]
                // logic here // [!code focus]
                return Ok(());
            }, // [!code focus]
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

## Why an async move?

rindexer has abstracted all the complex types for you so you can just focus on the logic you want to write, that said
rust demands knowing all the memory location of every element and when to drop references, this is why you need to use
`async move` in the handler.

## Results

This holds the event logs decoded information and the transaction information for the events.

```rs
async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            // results = Vec<TransferResult> // [!code focus]
            |results, context| async move { // [!code focus]
                // logic here
                return Ok(());
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

```rs
#[derive(Debug, Clone)]
pub struct TransferResult {
    pub event_data: TransferData,
    pub tx_information: TxInformation,
}
```

The `event_data` will be pointing to the ABI type generated using `alloy` `sol!` macro.

```rs
sol!(
    #[sol(rpc, all_derives)]
    TransferFilter,
    r#"
    [{
        "anonymous": false,
        "inputs": [
          {
            "indexed": true,
            "name": "from",
            "type": "address"
          },
          {
            "indexed": true,
            "name": "to",
            "type": "address"
          },
          {
            "indexed": false,
            "name": "value",
            "type": "uint256"
          }
        ],
        "name": "Transfer",
        "type": "event"
      }]
    "#
}
```

Which will expand to a struct like:
```rs
#[allow(
    non_camel_case_types,
    non_snake_case,
    clippy::pub_underscore_fields,
    clippy::style
)]
pub struct Transfer {
    #[allow(missing_docs)]
    pub from: ::alloy::sol_types::private::Address,
    #[allow(missing_docs)]
    pub to: ::alloy::sol_types::private::Address,
    #[allow(missing_docs)]
    pub value: ::alloy::sol_types::private::primitives::aliases::U256,
}
```

The `tx_information` will be the transaction related information for the event

```rs
#[derive(Debug, Clone)]
pub struct TxInformation {
    pub chain_id: u64,
    pub network: String,
    pub address: Address,
    pub block_hash: BlockHash,
    pub block_number: U64,
    pub block_timestamp: Option<U256>,
    pub transaction_hash: TxHash,
    pub log_index: U256,
    pub transaction_index: U64,
}
```

As you see the `network` is always passed in the `tx_information` struct, this is so you can index multiple networks
within the same handler if you wish.

## Context

The `context` is a struct that is passed to the handler which has thread safe services exposed for ease of use
within the handler.

```rs
async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            // context = Arc<EventContext<NoExtensions>> // [!code focus]
            |results, context| async move { // [!code focus]
                // logic here
                return Ok(());
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

```rs
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<PostgresClient>,
    pub csv: Arc<AsyncCsvAppender>,
    pub extensions: Arc<TExtensions>,
}
```

Note here that if you have postgres storage off in the YAML configuration file the `database` will not be present in
this struct and you will not be able to use it. The same goes for the `csv` if you have csv storage off in the YAML.

## Event Callback Result

The callback has to return a `Result<(), String>` so it can be handled by rindexer, rindexer by default will keep
retrying the event if it fails with exponential backoff.

### Success

```rs
async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                // logic here
                return Ok(()); // [!code focus]
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

### Error

Error takes in a string which then is logged in the rindexer console to help debugging traces.

```rs
async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                // logic here
                return Err("this is an error".to_string()); // [!code focus]
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```


## Extensions

You can also pass in your own custom thread safe extensions to the context if you wish, this is a way to pass in custom logic.
For example say you wanted to use a different database or call something from outside the indexer using an http request then
this is the place to pass it in from.

Example below uses the [reqwest](https://docs.rs/reqwest/latest/reqwest/) rust library to make a http request.

```rs
use reqwest::blocking::Client; // [!code focus]
use std::error::Error;

struct HttpClient { // [!code focus]
    client: Client, // [!code focus]
} // [!code focus]

impl HttpClient { // [!code focus]
    fn new() -> Self { // [!code focus]
        HttpClient { // [!code focus]
            client: Client::new(), // [!code focus]
        } // [!code focus]
    } // [!code focus]

    fn get(&self, url: &str) -> Result<String, Box<dyn Error>> { // [!code focus]
        let response = self.client.get(url).send()?.text()?; // [!code focus]
        Ok(response) // [!code focus]
    }
}

async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                let response = context.extensions.client.get("https://example.com"); // [!code focus]
                match response { // [!code focus]
                    Ok(response) => {
                        println!("{}", response), // [!code focus]
                        return Ok(()); // [!code focus]
                    }
                    Err(e) => {
                        println!("Error: {:?}", e) // [!code focus]
                        return Err(e.to_string()); // [!code focus]
                    }, // [!code focus]
                } // [!code focus]
            },
            HttpClient::new(), // [!code focus]
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

## Network providers

You get exposed to all the network thread safe json rpc providers you have defined in the network YAML configuration file, this
allows you to do on chain lookups at indexing time.

This is exposed in the `typings` folder. The naming for the provider function is the network name defined in your
YAML configuration file in snake case with `get_` prefixed to it and `_provider` appended to it.

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum // [!code focus]
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
    - Transfer
```

for example with network `ethereum` the provider function would be `get_ethereum_provider`.

```rs
use crate::rindexer_lib::typings::networks::get_ethereum_provider;// [!code focus]

async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                let provider = get_ethereum_provider(); // [!code focus]
                let chain_id = provider.get_chainid().await; // [!code focus]
                match chain_id { // [!code focus]
                    Ok(result) => {
                        println!("Chain id: {:?}", result) // [!code focus]
                        return Ok(()); // [!code focus]
                    }
                    Err(e) => {
                        println!("Error getting chain id: {:?}", e) // [!code focus]
                        return Err(e.to_string()); // [!code focus]
                    }
                } // [!code focus]
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

:::info
You can also pass in the provider to the extensions if you wish to use it in the handler using the context struct.
:::


## External Contract calls

You can also make contract calls within the handler, this is useful if you want to get the current state of a contract.
You get exposed to the contract for the event you are indexing on but you can also use the [global](docs/start-building/yaml-config/global) YAML
to define other contracts you want to use.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
    - Transfer
    - Approval
global: // [!code focus]
  contracts: // [!code focus]
    - name: USDT // [!code focus]
      details: // [!code focus]
        - address: 0xdac17f958d2ee523a2206206994597c13d831ec7 // [!code focus]
          network: ethereum // [!code focus]
      abi: ./abis/erc20.abi.json // [!code focus]
```

### Global contract calls

It as easy as importing the contract and calling the function you want. The naming convention for the contract is the contract name defined in your
YAML configuration file in snake case with `_contract` appended to it.

```yaml
...
global: // [!code focus]
  contracts:
    - name: USDT // [!code focus]
      details:
        - address: 0xdac17f958d2ee523a2206206994597c13d831ec7
          network: ethereum
      abi: ./abis/erc20.abi.json
```

```rs
use crate::rindexer_lib::typings::global_contracts::usdt_contract; // [!code focus]

async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                let usdt = usdt_contract(); // [!code focus]
                let name = usdt.name().await; // [!code focus]
                match name { // [!code focus]
                    Ok(result) => {
                        println!("USDT name: {:?}", name) // [!code focus]
                        return Ok(()); // [!code focus]
                    }
                    Err(e) => {
                        println!("Error getting USDT name: {:?}", e)  // [!code focus]
                        return Err(e.to_string()); // [!code focus]
                    }
                } // [!code focus]
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

#### Multiple addresses

If you have defined [multiple addresses](/docs/start-building/yaml-config/contracts#address) for a contract in the YAML configuration file
you have to pass in the address into the contract function.


```rs
use crate::rindexer_lib::typings::global_contracts::usdt_contract; // [!code focus]

async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                let address: Address = "0xdac17f958d2ee523a2206206994597c13d831ec7" // [!code focus]
                                        .parse() // [!code focus]
                                        .expect("Invalid address"); // [!code focus]
                let usdt = usdt_contract(address); // [!code focus]
                let name = usdt.name().await; // [!code focus]
                match name { // [!code focus]
                    Ok(result) => {
                        println!("USDT name: {:?}", name) // [!code focus]
                        return Ok(()); // [!code focus]
                    }
                    Err(e) => {
                        println!("Error getting USDT name: {:?}", e)  // [!code focus]
                        return Err(e.to_string()); // [!code focus]
                    }
                } // [!code focus]
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

### Contract calls

Each event to index is defined in a contract within the YAML configuration file, you can also make calls to this contract
within the handler. The naming convention for the contract is the contract name defined in your YAML configuration file
in snake case with `_contract` appended to it.

```yaml
contracts:
- name: RocketPoolETH // [!code focus]
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
    - Transfer // [!code focus]
    - Approval
```

```rs
use crate::rindexer_lib::typings::rust::events::rocket_pool_eth::rocket_pool_eth_contract; // [!code focus]

async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                // have to pass in network name here // [!code focus]
                let rocket_pool_eth = rocket_pool_eth_contract("ethereum"); // [!code focus]
                let name = rocket_pool_eth.name().await; // [!code focus]
                match name { // [!code focus]
                    Ok(result) => {
                        println!("rETH name: {:?}", name) // [!code focus]
                        return Ok(()); // [!code focus]
                    }
                    Err(e) => {
                        println!("Error getting rETH name: {:?}", e) // [!code focus]
                        return Err(e.to_string()); // [!code focus]
                    }
                } // [!code focus]
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

#### Multiple addresses

If you have defined [multiple addresses](/docs/start-building/yaml-config/contracts#address) or you have a [filter](/docs/start-building/yaml-config/contracts#filter)
for a contract in the YAML configuration file you will have to pass in the address into the contract function.

```rs
use crate::rindexer_lib::typings::rust::events::rocket_pool_eth::rocket_pool_eth_contract; // [!code focus]

async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                let address: Address = "0xdac17f958d2ee523a2206206994597c13d831ec7" // [!code focus]
                                                    .parse() // [!code focus]
                                                    .expect("Invalid address"); // [!code focus]
                // have to pass in network name here // [!code focus]
                let rocket_pool_eth = rocket_pool_eth_contract("ethereum", address); // [!code focus]
                let name = rocket_pool_eth.name().await; // [!code focus]
                match name { // [!code focus]
                    Ok(result) => {
                        println!("rETH name: {:?}", name) // [!code focus]
                        return Ok(()); // [!code focus]
                    }
                    Err(e) => {
                        println!("Error getting rETH name: {:?}", e) // [!code focus]
                        return Err(e.to_string()); // [!code focus]
                    }
                } // [!code focus]
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

## Postgres

By default if you set the postgres storage on in the YAML configuration file it will generate you a postgres connected client. This
uses the [tokio-postgres](https://docs.rs/tokio-postgres/latest/tokio_postgres/) library. This is a great starting point for you to build on. It uses connection pools by default.

```rs
async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                // database client here // [!code focus]
                context.database  // [!code focus]

                return Ok(());
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}
```

### Disable Postgres Create Tables

The tables creation can be skipped by using the [disable_create_tables](/docs/start-building/yaml-config/storage#disable_create_tables)
in the YAML configuration file. This will generate you a blank handler with no logic inside.

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
    disable_create_tables: true // [!code focus]
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
  - Transfer
```

You can query data from the database and write data to the database, here are the postgres methods exposed.

- `context.database.new` - This is for creating a new client.
- `context.database.batch_execute` - This is for executing multiple queries at once.
- `context.database.execute` - This is for executing a single query.
- `context.database.prepare` - This is for preparing a query to be executed multiple times.
- `context.database.transaction` - This is for starting a transaction.
- `context.database.query` - This is for querying data from the database.
- `context.database.query_one` - This is for querying a single row from the database.
- `context.database.query_one_or_none` - This is for querying a single row from the database or returning None if no rows are found.
- `context.database.insert_bulk` - This is for inserting multiple rows into the database efficiently.
- `context.database.copy_in` - This is for inserting multiple rows into the database using the COPY command.

### EthereumSqlTypeWrapper

Ethereum types are not 1 to 1 with postgres types, so rindexer has a wrapper to help you with this. This is a enum
called EthereumSqlTypeWrapper which has all the types you need to pass into the postgres write functions.

```rs
#[derive(Debug, Clone)]
pub enum EthereumSqlTypeWrapper {
    // Boolean
    Bool(bool),
    VecBool(Vec<bool>),

    // 8-bit integers
    U8(u8),
    I8(i8),
    VecU8(Vec<u8>),
    VecI8(Vec<i8>),

    // 16-bit integers
    U16(u16),
    I16(i16),
    VecU16(Vec<u16>),
    VecI16(Vec<i16>),

    // 32-bit integers
    U32(u32),
    I32(i32),
    VecU32(Vec<u32>),
    VecI32(Vec<i32>),

    // 64-bit integers
    U64(U64),
    U64Nullable(U64),
    U64BigInt(U64),
    I64(i64),
    VecU64(Vec<U64>),
    VecI64(Vec<i64>),

    // 128-bit integers
    U128(u128),
    I128(i128),
    VecU128(Vec<u128>),
    VecI128(Vec<i128>),

    // 256-bit integers
    U256(U256),
    U256Numeric(U256),
    U256NumericNullable(Option<U256>),
    U256Nullable(U256),
    U256Bytes(U256),
    U256BytesNullable(U256),
    I256(I256),
    I256Numeric(I256),
    I256Nullable(I256),
    I256Bytes(I256),
    I256BytesNullable(I256),
    VecU256(Vec<U256>),
    VecU256Bytes(Vec<U256>),
    VecU256Numeric(Vec<U256>),
    VecI256(Vec<I256>),
    VecI256Bytes(Vec<I256>),

    // 512-bit integers
    U512(U512),
    VecU512(Vec<U512>),

    // Hashes
    B128(B128),
    H160(B160), // DEPRECATED - Use Address instead
    B256(B256),
    B256Bytes(B256),
    B512(B512),
    VecB128(Vec<B128>),
    VecB256(Vec<B256>),
    VecB256Bytes(Vec<B256>),
    VecB512(Vec<B512>),

    // Deprecated Hash. Move to use Address
    VecH160(Vec<B160>),

    // Address
    Address(Address),
    AddressNullable(Address),
    AddressBytes(Address),
    AddressBytesNullable(Address),
    VecAddress(Vec<Address>),
    VecAddressBytes(Vec<Address>),

    // Strings and Bytes
    String(String),
    StringVarchar(String),
    StringChar(String),
    StringNullable(String),
    StringVarcharNullable(String),
    StringCharNullable(String),
    VecString(Vec<String>),
    VecStringVarchar(Vec<String>),
    VecStringChar(Vec<String>),
    Bytes(Bytes),
    BytesNullable(Bytes),
    VecBytes(Vec<Bytes>),

    // UUID
    Uuid(Uuid),

    // DateTime
    DateTime(DateTime<Utc>),
    DateTimeNullable(Option<DateTime<Utc>>),

    // JSON
    JSONB(Value),
}

// to use it you just pass the value in the enum
// example
EthereumSqlTypeWrapper::Address(result.tx_information.address)
```

## CSV

csv storage is disabled by default in the YAML configuration file, if you turn it on it will generate you a csv client

methods:

- `context.csv.append_header` - This is for appending a header to the csv file.
- `context.csv.append` - This is for appending a row to the csv file.

```rs
async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                // csv client here // [!code focus]
                context.csv  // [!code focus]

                return Ok(());
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await;
}

```

### Disable CSV Create Headers

If you turn on csv storage then by default rindexer will create headers for you automatically inline with the ABI event data.
The CSV header creation can be skipped by using the [disable_create_headers](/docs/start-building/yaml-config/storage#disable_create_headers)
in the YAML configuration file.

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  csv:
    enabled: true
    disable_create_headers: true // [!code focus]
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
  - Transfer
```

## register

rindexer needs to know which handlers are required to be indexed so you need to register them with the `EventCallbackRegistry`.
This passing of `&mut EventCallbackRegistry` is taken care of you by the rindexer framework, you just need to call the `register` function.

```rs
async fn transfer_handler(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    RocketPoolETHEventType::Transfer(
        TransferEvent::handler(
            |results, context| async move {
                ...
                return Ok(());
            },
            no_extensions(),
        )
        .await,
    )
    .register(manifest_path, registry).await; // [!code focus]
}
```


The `main.rs` calls the `register_all_handlers` function which lives in `all_handlers.rs` this registers all the handlers, this code is
all generated for you and you do not need to worry about it.

```rs
use std::path::PathBuf;
use super::rust::rocket_pool_eth::rocket_pool_eth_handlers;
use rindexer::event::callback_registry::EventCallbackRegistry;

pub async fn register_all_handlers(manifest_path: &PathBuf) -> EventCallbackRegistry {
    let mut registry = EventCallbackRegistry::new();
    rocket_pool_eth_handlers(manifest_path, &mut registry).await;
    registry
}
```

## main.rs

The rust project will generate you a main.rs which can be ran out the box. This is just boilerplate code to get you
started, you can customise this as you wish and should be if your building a custom indexer.

```rs
use std::env;

use self::rindexer_lib::indexers::all_handlers::register_all_handlers;
use rindexer::{
    event::callback_registry::TraceCallbackRegistry, start_rindexer, GraphQLServerDetails, GraphQLServerSettings, IndexingDetails, StartDetails,
};

mod rindexer_lib;

#[tokio::main]
async fn main() {
    let args: Vec<String> = env::args().collect();

    let mut enable_graphql = false;
    let mut enable_indexer = false;

    let mut port: Option<u16> = None;

    for arg in args.iter() {
        match arg.as_str() {
            "--graphql" => enable_graphql = true,
            "--indexer" => enable_indexer = true,
            _ if arg.starts_with("--port=") || arg.starts_with("--p") => {
                if let Some(value) = arg.split('=').nth(1) {
                    let overridden_port = value.parse::<u16>();
                    match overridden_port {
                        Ok(overridden_port) => port = Some(overridden_port),
                        Err(_) => {
                            println!("Invalid port number");
                            return;
                        }
                    }
                }
            }
            _ => {}
        }
    }

    let path = env::current_dir();
    match path {
        Ok(path) => {
            let manifest_path = path.join("rindexer.yaml");
            let result = start_rindexer(StartDetails {
                manifest_path: &manifest_path,
                indexing_details: if enable_indexer {
                    Some(IndexingDetails {
                        registry: register_all_handlers(&manifest_path).await,
                        trace_registry: TraceCallbackRegistry { events: vec![] },
                        event_stream: None,
                    })
                } else {
                    None
                },
                graphql_details: GraphqlOverrideSettings {
                    enabled: enable_graphql,
                    override_port: port,
                },
            })
            .await;

            match result {
                Ok(_) => {}
                Err(e) => {
                    println!("Error starting rindexer: {:?}", e);
                }
            }
        }
        Err(e) => {
            println!("Error getting current directory: {:?}", e);
        }
    }
}

```

### Running

If you want to run this with docker support for the postgres first run:

```bash
docker compose up -d
```

Then to run the boilerplate code generated for you, you can run the following command:

:::info
You are creating a rust rindexer project you should be wanting to change all of this logic to suit your needs.
Just like react create app exposes you to the boilerplate code to get you started, this is the same.
:::

:::code-group

```bash [everything]
cargo run
```

```bash [indexer only]
cargo run -- --indexer
```

```bash [graphql only]
cargo run -- --graphql
```

:::

## Managing changes when generating typings

When you start changing your YAML configuration file and regenerating your typings the indexer functions may break or need editing
to match the new typings. You can regenerate the indexers folder but this will overwrite any changes you did.
Luckily the rust compiler is very good at telling you what you need to change.

## Subscribing to indexer events

More advanced use cases may require reacting to some internal rindexer events, which can be done through the `RindexerEventStream` attached to `IndexingDetails`.

```rs
use self::rindexer_lib::indexers::all_handlers::register_all_handlers;
use rindexer::{
    event::callback_registry::TraceCallbackRegistry, start_rindexer, GraphqlOverrideSettings,
    IndexingDetails, RindexerEvent, RindexerEventStream, StartDetails,
};
use std::env;
use tokio::sync::broadcast;
use tokio::task;

mod rindexer_lib;

fn handle_rindexer_events(mut receiver: broadcast::Receiver<RindexerEvent>) {
    task::spawn(async move {
        loop {
            match receiver.recv().await {
                Ok(event) => {
                    println!("Received {:?}", event);
                }
                Err(e) => {
                    println!("Error {:?}", e);
                }
            }
        }
    });
}

#[tokio::main]
async fn main() {
    let indexer_event_stream = RindexerEventStream::new();

    handle_rindexer_events(indexer_event_stream.subscribe());

    let path = env::current_dir();
    match path {
        Ok(path) => {
            let manifest_path = path.join("rindexer.yaml");
            let result = start_rindexer(StartDetails {
                manifest_path: &manifest_path,
                indexing_details: Some(IndexingDetails {
                    registry: register_all_handlers(&manifest_path).await,
                    trace_registry: TraceCallbackRegistry { events: vec![] },
                    event_stream: Some(indexer_event_stream),
                }),
                graphql_details: GraphqlOverrideSettings {
                    enabled: false,
                    override_port: None,
                },
            })
            .await;

            match result {
                Ok(_) => {}
                Err(e) => {
                    println!("Error starting rindexer: {:?}", e);
                }
            }
        }
        Err(e) => {
            println!("Error getting current directory: {:?}", e);
        }
    }
}
```

For now only a single event is emitted:
- `HistoricalIndexingCompleted` - emitted when historical indexing was completed (happens every restart of the indexer once it catches up with the latest block)
</file>

<file path="documentation/docs/pages/docs/start-building/rust-project-deep-dive/typings.mdx">
# Typings

When creating a new rust project with rindexer it will create you a typings folder, this has pretty advanced
typings for all your contracts, events and network information. This is generated from the ABIs you provide in the YAML configuration file.
This folder is not meant to be manually edited and should always be generated using codegen.

You can regenerate the typings folder by running the following command:

:::info
rindexer tries to be as smart as possible when it comes to updating the typings based on the `rindexer.yaml`,
it will resolves as much as it can without needing a regenerate but like any codegen tool if you
change certain aspects it does need to be regenerated.

if you change any of these properties in the `rindexer.yaml` file it will need to be regenerated:

- [indexer name](/docs/start-building/yaml-config/top-level-fields#name)
- anything in the [network](/docs/start-building/yaml-config/networks) section including adding and removing networks
- enabling or disabling a new [storage provider](/docs/start-building/yaml-config/storage)
- changing the [contract name](/docs/start-building/yaml-config/contracts#name)
- changing from [address](/docs/start-building/yaml-config/contracts#address)
contract indexing to [filter](/docs/start-building/yaml-config/contracts#filter) indexing or vice versa
- changing the contract [ABI](/docs/start-building/yaml-config/contracts#abi)
- anything in the [global](/docs/start-building/yaml-config/global) section

Also if you do regenerate your indexer files may need to be updated to match the new typings, you can manually migrate them or
generate them again using [indexer codegen command](/docs/start-building/codegen#indexers)
:::

```bash
rindexer codegen typings
```
</file>

<file path="documentation/docs/pages/docs/start-building/streams/cloudflare-queues.mdx">
# Cloudflare Queues

:::info
rindexer streams can be used without any other storage providers. It can also be used with storage providers.
:::

rindexer allows you to stream blockchain events to [Cloudflare Queues](https://developers.cloudflare.com/queues/), enabling realtime processing of blockchain data in your Cloudflare Workers. This integration provides guaranteed message delivery, global distribution, and seamless integration with your Cloudflare-based backend infrastructure.

This goes under the [contracts](/docs/start-building/yaml-config/contracts) or [native_transfers](/docs/start-building/yaml-config/native-transfers) section of the YAML configuration file.

## Configuration with rindexer

Cloudflare Queues configuration requires your Cloudflare API token, account ID, and queue definitions.

## Example

:::code-group

```yaml [contract events]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN} // [!code focus]
      account_id: ${CLOUDFLARE_ACCOUNT_ID} // [!code focus]
      queues: // [!code focus]
        - queue_id: blockchain-transfers // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              alias: RocketPoolTransfer
```

```yaml [native transfers]
name: ETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN} // [!code focus]
      account_id: ${CLOUDFLARE_ACCOUNT_ID} // [!code focus]
      queues: // [!code focus]
        - queue_id: native-transfers // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
          events: // [!code focus]
            - event_name: NativeTransfer // [!code focus]
```

:::

## Message Format

The message sent to your Cloudflare Queue is already decoded and parsed into a JSON object with a `message_id` field added for tracking.

- `message_id` - Unique identifier for this message from rindexer
- `event_name` - The name of the event
- `event_signature_hash` - The event signature hash (keccak256 hash of the event signature)
- `body` > `event_data` - Array of decoded event data with transaction information
- `network` - The network the event was emitted on

For example a transfer event would look like:

```json
{
    "message_id": "rindexer_stream__blockchain-transfers-transfer-chunk-0",
    "event_name": "Transfer",
    "event_signature_hash": "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
    "body": {
        "event_data": [
            {
                "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
                "to": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
                "value": "1000000000000000000",
                "tx_information": {
                    "address": "0xae78736cd615f374d3085123a210448e74fc6393",
                    "block_hash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
                    "block_number": "18718011",
                    "log_index": "0",
                    "network": "ethereum",
                    "transaction_hash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
                    "transaction_index": "0"
                }
            }
        ]
    },
    "network": "ethereum"
}
```

## Cloudflare Worker Consumer

Here's an example of a Cloudflare Worker that consumes messages from your queues:

```javascript
export default {
  async queue(batch, env, ctx) {
    for (const message of batch.messages) {
      try {
        for (const eventInfo of message.body.event_data) {
            console.log(eventInfo);
            // Result:
            // {
            //   "from": "0xf081470f5c6fbccf48cc4e5b82dd926409dcdd67",
            //   "to": "0x58bd88f0c826bdc2d8adaf66abb66bb99d961a3d",
            //   "transaction_information": {
            //     "address": "0xae78736cd615f374d3085123a210448e74fc6393",
            //     "block_hash": "0x58d7fd9aab0a4023f812dd0919d70f63ffa9a92ee26d83d34b04fbb000b74e9b",
            //     "block_timestamp": "0x6567a397",
            //     "log_index": "0x1b0",
            //     "network": "ethereum",
            //     "transaction_hash": "0x7cef018bc6090ac7d5a73d6ffe1975c6d52353cad315de9b7c771db6648c7a44",
            //     "block_number": 18679752,
            //     "chain_id": 1,
            //     "transaction_index": 148
            //   },
            //   "value": "45336822342319436"
            // }
        }

        message.ack();
      } catch (error) {
        console.error('Error processing message:', error);
        message.retry();
      }
    }
  }
};
```

## YAML Config

### api_token

Your Cloudflare API token with permissions to access Queues API.

:::info
We strongly recommend using environment variables for your API token.
:::

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN} // [!code focus]
```

### account_id

Your Cloudflare Account ID where your queues are created.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN}
      account_id: ${CLOUDFLARE_ACCOUNT_ID} // [!code focus]
```

### queues

This is an array allowing you to configure multiple queues with different settings.

#### queue_id

The ID of the Cloudflare Queue to send messages to. This queue must already exist in your Cloudflare account. You can find the queue ID in your Cloudflare dashboard or via the Queues API.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN}
      account_id: ${CLOUDFLARE_ACCOUNT_ID}
      queues: // [!code focus]
        - queue_id: blockchain-transfers // [!code focus]
```

### networks

This is an array of networks you want to stream to this queue.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN}
      account_id: ${CLOUDFLARE_ACCOUNT_ID}
      queues: // [!code focus]
        - queue_id: blockchain-transfers
          networks: // [!code focus]
            - ethereum // [!code focus]
```

### events

This is an array of events you want to stream to this queue.

#### event_name

This is the name of the event you want to stream to this queue, must match the ABI event name.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN}
      account_id: ${CLOUDFLARE_ACCOUNT_ID}
      queues: // [!code focus]
        - queue_id: blockchain-transfers
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
```

##### alias

This is an optional `alias` you wish to assign to the event you want published to this Queue.

It is paired with the event name and allows consumers to have unique discriminator keys in the event of
naming conflicts. E.g Transfer (ERC20) and Transfer (ERC721).

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN}
      account_id: ${CLOUDFLARE_ACCOUNT_ID}
      queues: // [!code focus]
        - queue_id: blockchain-transfers
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              alias: RocketPoolTransfer // [!code focus]
```

### conditions

This accepts an array of conditions you want to apply to the event data before sending to the queue.

:::info
This is optional, if you do not provide any conditions all data will be streamed.
:::

You may want to filter on the stream based on the event data, if the event data has not got an index on the on the
solidity event you can not filter it over the logs. The `conditions` filter is here to help you with this,
based on your ABI you can filter on the event data.

rindexer has enabled a special syntax which allows you to define on your ABI fields what you want to filter on.

1. `>` - higher then (for numbers only)
2. `<` - lower then (for numbers only)
3. `=` - equals
4. `>=` - higher then or equals (for numbers only)
5. `<=` - lower then or equals (for numbers only)
6. `||` - or
7. `&&` - and

So lets look at an example lets say i only want to get transfer events which are higher then `2000000000000000000` RETH wei

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN}
      account_id: ${CLOUDFLARE_ACCOUNT_ID}
      queues: // [!code focus]
        - queue_id: blockchain-transfers
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              conditions: // [!code focus]
                - "value": ">=2000000000000000000" // [!code focus]
```

We use the ABI input name `value` to filter on the value field, you can find these names in the ABI file.

```json
{
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value", // [!code focus]
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
}
```

You can use the `||` or `&&` to combine conditions.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN}
      account_id: ${CLOUDFLARE_ACCOUNT_ID}
      queues: // [!code focus]
        - queue_id: blockchain-transfers
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "value": ">=2000000000000000000 && value <=4000000000000000000" // [!code focus]
```

You can use the `=` to filter on other aspects like the `from` or `to` address.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN}
      account_id: ${CLOUDFLARE_ACCOUNT_ID}
      queues: // [!code focus]
        - queue_id: blockchain-transfers
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662 || 0x0338ce5020c447f7e668dc2ef778025ce398266u" // [!code focus]
                - "value": ">=2000000000000000000 || value <=4000000000000000000" // [!code focus]
```

:::info
Note we advise you to filer any `indexed` fields in the contract details in the `rindexer.yaml` file.
As these can be filtered out on the request level and not filtered out in rindexer itself.
You can read more about it [here](/docs/start-building/yaml-config/contracts#indexed_1-indexed_2-indexed_3).
:::

If you have a tuple and you want to get that value you just use the object notation.

For example lets say we want to only get the events for `profileId` from the `quoteParams` tuple which equals `1`:

```json
{
     "anonymous": false,
     "inputs": [
       {
         "components": [
           {
             "internalType": "uint256",
             "name": "profileId", // [!code focus]
             "type": "uint256"
           },
           ...
         ],
         "indexed": false,
         "internalType": "struct Types.QuoteParams",
         "name": "quoteParams", // [!code focus]
         "type": "tuple"
       },
       ...
     ],
     "name": "QuoteCreated", // [!code focus]
     "type": "event"
}
```

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    cloudflare_queues: // [!code focus]
      api_token: ${CLOUDFLARE_API_TOKEN}
      account_id: ${CLOUDFLARE_ACCOUNT_ID}
      queues: // [!code focus]
        - queue_id: blockchain-transfers
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "quoteParams.profileId": "=1" // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/streams/index.mdx">
# Streams

:::info
rindexer streams can be used without any other storage providers. It can also be used with storage providers.
:::

rindexer supports streaming data from rindexer to anywhere you want. This allows you to
build your own data indexing solutions in any language you wish, alongside stream the data to any location
you want with tons of use cases. Streams also support advanced filtering and conditions which allows you to filter the data
before it is streamed. This can all be done using no-code and set in the YAML configuration file.

:::info
Rust projects do not get exposed to the stream clients yet but it can easily be exposed in the future.
:::

Note you can use all the streams together they are independent of each other, so if you wanted to us `kafka`,
`webhooks`, `rabbitmq`, `sns`, `redis`, and `cloudflare_queues` together you can do that.

Supported stream providers:

- [Webhooks](/docs/start-building/streams/webhooks) - Fire webhooks to your own APIs
- [Kafka](/docs/start-building/streams/kafka) - Find out more about [Apache Kafka](https://kafka.apache.org/)
- [RabbitMQ](/docs/start-building/streams/rabbitmq) - Find out more about [RabbitMQ](https://www.rabbitmq.com/)
- [SNS/SQS](/docs/start-building/streams/sns) - Find out more about [Simple Notification Service](https://aws.amazon.com/sns/) and [Simple Queue Service](https://aws.amazon.com/sqs/)
- [Redis Streams](/docs/start-building/streams/redis) - Find out more about [Redis Streams](https://redis.io/docs/latest/develop/data-types/streams/)
- [Cloudflare Queues](/docs/start-building/streams/cloudflare-queues) - Find out more about [Cloudflare Queues](https://developers.cloudflare.com/queues/)
</file>

<file path="documentation/docs/pages/docs/start-building/streams/kafka.mdx">
# Kafka

:::warn
Kafka streams do not work with windows from the CLI installation, it will panic if you try to use it with windows.
If you are on windows and want to use kafka streams you should use the docker image.
:::

:::info
**Feature Gate:** If you're including the rindexer crate in your Rust project, Kafka support is gated behind the `kafka` feature flag. Add it to your `Cargo.toml`:

```toml
[dependencies]
rindexer = { version = "*", features = ["kafka"] }
```

**Docker Images:** Kafka support is enabled by default in all official rindexer Docker images - no additional configuration needed.
:::

:::info
rindexer streams can be used without any other storage providers. It can also be used with storage providers.
:::

rindexer allows you to configure [Kafka](https://kafka.apache.org/) to stream any data to. This goes under
the [contracts](/docs/start-building/yaml-config/contracts) or [native_transfers](/docs/start-building/yaml-config/native-transfers)
section of the YAML configuration file.

Find out more about [Kafka](https://kafka.apache.org/).

rindexer kafka integration supports SSL queues and none SSL queues.

## Configuration with rindexer

`kafka` property accepts an array of `topics` allowing you to split up the streams any way you wish.

## Example

Kafka has to be configured to use SASL_SSL or PLAINTEXT. You can read more about it [here](https://kafka.apache.org/documentation/#security_sasl).

:::code-group

```yaml [none-ssl]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers: // [!code focus]
        - ${KAFKA_BROKER_URL_1} // [!code focus]
        - ${KAFKA_BROKER_URL_2} // [!code focus]
      acks: all // [!code focus]
      security_protocol: PLAINTEXT // [!code focus]
      topics: // [!code focus]
        - topic: test-topic // [!code focus]
          # key is optional // [!code focus]
          key: my-routing-key // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              alias: RocketPoolTransfer
```

```yaml [ssl]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers: // [!code focus]
        - ${KAFKA_BROKER_URL_1} // [!code focus]
        - ${KAFKA_BROKER_URL_2} // [!code focus]
      acks: all // [!code focus]
      security_protocol: SASL_SSL // [!code focus]
      sasl_mechanisms: PLAIN // [!code focus]
      sasl_username: $<CLUSTER_API_KEY> // [!code focus]
      sasl_password: $<CLUSTER_API_SECRET> // [!code focus]
      topics:
        - topic: test-topic
          # key is optional // [!code focus]
          key: my-routing-key
          networks:
            - ethereum
          events:
            - event_name: Transfer
```

```yaml [native transfers (ssl)]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers: // [!code focus]
        - ${KAFKA_BROKER_URL_1} // [!code focus]
        - ${KAFKA_BROKER_URL_2} // [!code focus]
      acks: all // [!code focus]
      security_protocol: SASL_SSL // [!code focus]
      sasl_mechanisms: PLAIN // [!code focus]
      sasl_username: $<CLUSTER_API_KEY> // [!code focus]
      sasl_password: $<CLUSTER_API_SECRET> // [!code focus]
      topics:
        - topic: test-topic
          # key is optional // [!code focus]
          key: my-routing-key
          networks:
            - ethereum
          events:
            - event_name: NativeTransfer  // [!code focus]
```

:::

## Response

:::info
Note SNS/SQS may wrap the message body into their own object so the below is just what we send to the stream.
:::

The response sent to you is already decoded and parsed into a JSON object.

- `event_name` - The name of the event
- `event_signature_hash` - The event signature hash example the keccak256 hash of "Transfer(address,address,uint256)", this is topics[0] in the logs
- `event_data` - The event data which has all the event fields decoded and the transaction information which is under `transaction_information`
- `network` - The network the event was emitted on

For example a transfer event would look like:

```json
{
    "event_name": "Transfer",
    "event_signature_hash": "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
    "event_data": {
        "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
        "to": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
        "value": "1000000000000000000",
        "transaction_information": {
            "address": "0xae78736cd615f374d3085123a210448e74fc6393",
            "block_hash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
            "block_number": "18718011",
            "log_index": "0",
            "network": "ethereum",
            "transaction_hash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
            "transaction_index": "0"
        }
    },
    "network": "ethereum"
}
```

## brokers

You define the kafka brokers you wish to connect to, you can pass in multiple brokers if you wish. A single broker
will of course work as well.

:::info
We advise brokers should be set in your environment variables.
:::

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers: // [!code focus]
        - ${KAFKA_BROKER_URL_1} // [!code focus]
        - ${KAFKA_BROKER_URL_2} // [!code focus]
```

## acks

- `acks=0` - When acks=0 producers consider messages as "written successfully" the moment the message was sent without waiting for the broker to accept it at all.
- `acks=1` - When acks=1 , producers consider messages as "written successfully" when the message was acknowledged by only the leader.
- `acks=all` - When acks=all, producers consider messages as "written successfully" when the message is accepted by all in-sync replicas (ISR).

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      # all or 0 or 1
      acks: all // [!code focus]
      security_protocol: SASL_SSL // [!code focus]
```


## security_protocol

This is either `PLAINTEXT` or `SASL_SSL`. You can read more about it [here](https://kafka.apache.org/documentation/#security_sasl).

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL // [!code focus]
```

## sasl_mechanisms

:::info
This is optional, if you are using SASL_SSL you will need to provide this.
:::

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN // [!code focus]
```

## sasl_username

:::info
This is optional, if you are using SASL_SSL you will need to provide this.
<br/>
We advise you to put this in your environment variables.
:::

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY> // [!code focus]
```

## sasl_password

:::info
This is optional, if you are using SASL_SSL you will need to provide this.
<br/>
We advise you to put this in your environment variables.
:::

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY>
      sasl_password: $<CLUSTER_API_SECRET> // [!code focus]
```

## topics

This is an array of topics you want to stream to this kafka.

### topic

This is the topic name.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY>
      sasl_password: $<CLUSTER_API_SECRET>
      topics:
        - topic: test-topic // [!code focus]
```

### key

:::info
This is optional
:::

You can route your messages to a specific partition in the topic, this is useful if you have multiple consumers
on the same topic.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY>
      sasl_password: $<CLUSTER_API_SECRET>
      topics:
        - topic: test-topic
          key: my-routing-key // [!code focus]
          networks:
            - ethereum
          events:
            - event_name: Transfer
```

## networks

This is an array of networks you want to stream to this kafka.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY>
      sasl_password: $<CLUSTER_API_SECRET>
      topics:
        - topic: test-topic
          key: my-routing-key
          networks: // [!code focus]
            - ethereum // [!code focus]
          events:
            - event_name: Transfer
```

## events

This is an array of events you want to stream to this kafka.

### event_name

This is the name of the event you want to stream to this kafka, must match the ABI event name.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY>
      sasl_password: $<CLUSTER_API_SECRET>
      topics:
        - topic: test-topic
          key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
```

#### alias

This is an optional `alias` you wish to assign to the event you want to stream to this Kafka topic.

It is paired with the event name and allows consumers to have unique discriminator keys in the event of
naming conflicts. E.g Transfer (ERC20) and Transfer (ERC721).

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY>
      sasl_password: $<CLUSTER_API_SECRET>
      topics:
        - topic: test-topic
          key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              alias: RocketPoolTransfer // [!code focus]
```

### conditions

This accepts an array of conditions you want to apply to the event data before streaming to this kafka.

:::info
This is optional, if you do not provide any conditions all data will be streamed.
:::

You may want to filter on the stream based on the event data, if the event data has not got an index on the on the
solidity event you can not filter it over the logs. The `conditions` filter is here to help you with this,
based on your ABI you can filter on the event data.

rindexer has enabled a special syntax which allows you to define on your ABI fields what you want to filter on.

1. `>` - higher then (for numbers only)
2. `<` - lower then (for numbers only)
3. `=` - equals
4. `>=` - higher then or equals (for numbers only)
5. `<=` - lower then or equals (for numbers only)
6. `||` - or
7. `&&` - and

So lets look at an example lets say i only want to get transfer events which are higher then `2000000000000000000` RETH wei

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY>
      sasl_password: $<CLUSTER_API_SECRET>
      topics:
        - topic: test-topic
          key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              conditions: // [!code focus]
                - "value": ">=2000000000000000000" // [!code focus]
```

We use the ABI input name `value` to filter on the value field, you can find these names in the ABI file.

```json
{
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value", // [!code focus]
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
}
```

You can use the `||` or `&&` to combine conditions.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY>
      sasl_password: $<CLUSTER_API_SECRET>
      topics:
        - topic: test-topic
          key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "value": ">=2000000000000000000 && value <=4000000000000000000" // [!code focus]
```

You can use the `=` to filter on other aspects like the `from` or `to` address.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY>
      sasl_password: $<CLUSTER_API_SECRET>
      topics:
        - topic: test-topic
          key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662 || 0x0338ce5020c447f7e668dc2ef778025ce398266u" // [!code focus]
                - "value": ">=2000000000000000000 || value <=4000000000000000000" // [!code focus]
```

:::info
Note we advise you to filer any `indexed` fields in the contract details in the `rindexer.yaml` file.
As these can be filtered out on the request level and not filtered out in rindexer itself.
You can read more about it [here](/docs/start-building/yaml-config/contracts#indexed_1-indexed_2-indexed_3).
:::

If you have a tuple and you want to get that value you just use the object notation.

For example lets say we want to only get the events for `profileId` from the `quoteParams` tuple which equals `1`:

```json
{
     "anonymous": false,
     "inputs": [
       {
         "components": [
           {
             "internalType": "uint256",
             "name": "profileId", // [!code focus]
             "type": "uint256"
           },
           ...
         ],
         "indexed": false,
         "internalType": "struct Types.QuoteParams",
         "name": "quoteParams", // [!code focus]
         "type": "tuple"
       },
       ...
     ],
     "name": "QuoteCreated", // [!code focus]
     "type": "event"
}
```

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    kafka: // [!code focus]
      brokers:
        - ${KAFKA_BROKER_URL_1}
        - ${KAFKA_BROKER_URL_2}
      acks: all
      security_protocol: SASL_SSL
      sasl_mechanisms: PLAIN
      sasl_username: $<CLUSTER_API_KEY>
      sasl_password: $<CLUSTER_API_SECRET>
      topics:
        - topic: test-topic
          key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "quoteParams.profileId": "=1" // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/streams/rabbitmq.mdx">
# RabbitMQ

:::info
rindexer streams can be used without any other storage providers. It can also be used with storage providers.
:::

rindexer allows you to configure [RabbitMQ](https://www.rabbitmq.com/) to stream any data to. This goes under
the [contracts](/docs/start-building/yaml-config/contracts) or [native_transfers](/docs/start-building/yaml-config/native-transfers)
section of the YAML configuration file.

Find out more about [RabbitMQ](https://www.rabbitmq.com/).

rindexer rabbitmq integration supports `direct`, `topic` and `fanout` exchanges.
You can read more about what they do differently [here](https://medium.com/trendyol-tech/rabbitmq-exchange-types-d7e1f51ec825).

## Configuration with rindexer

`rabbitmq` property accepts an array of `exchanges` allowing you to split up the streams any way you wish.

## Example

:::code-group

```yaml [direct]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL} // [!code focus]
      exchanges: // [!code focus]
        - exchange: transfer // [!code focus]
          #  expected one of `direct`, `topic` or `fanout` // [!code focus]
          exchange_type: direct // [!code focus]
          routing_key: my-routing-key // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              alias: RocketPoolTransfer
```

```yaml [topic]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL} // [!code focus]
      exchanges: // [!code focus]
        - exchange: transfer // [!code focus]
          #  expected one of `direct`, `topic` or `fanout` // [!code focus]
          exchange_type: topic // [!code focus]
          routing_key: my-routing-key // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
```

```yaml [fanout]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL} // [!code focus]
      exchanges: // [!code focus]
        - exchange: transfer // [!code focus]
          #  expected one of `direct`, `topic` or `fanout` // [!code focus]
          exchange_type: fanout // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
```

```yaml [native transfers (fanout)]
name: ETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL} // [!code focus]
      exchanges: // [!code focus]
        - exchange: transfer // [!code focus]
          #  expected one of `direct`, `topic` or `fanout` // [!code focus]
          exchange_type: fanout // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
          events: // [!code focus]
            - event_name: NativeTransfer // [!code focus]
```

:::

## Response

:::info
Note SNS/SQS may wrap the message body into their own object so the below is just what we send to the stream.
:::

The response sent to you is already decoded and parsed into a JSON object.

- `event_name` - The name of the event
- `event_signature_hash` - The event signature hash example the keccak256 hash of "Transfer(address,address,uint256)", this is topics[0] in the logs
- `event_data` - The event data which has all the event fields decoded and the transaction information which is under `transaction_information`
- `network` - The network the event was emitted on

For example a transfer event would look like:

```json
{
    "event_name": "Transfer",
    "event_signature_hash": "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
    "event_data": {
        "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
        "to": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
        "value": "1000000000000000000",
        "transaction_information": {
            "address": "0xae78736cd615f374d3085123a210448e74fc6393",
            "block_hash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
            "block_number": "18718011",
            "log_index": "0",
            "network": "ethereum",
            "transaction_hash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
            "transaction_index": "0"
        }
    },
    "network": "ethereum"
}
```

## url

This is the rabbitmq connection url we advise to put this in a environment variable.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL} // [!code focus]
```


## exchanges

This is an array of exchanges you want to stream to this rabbitmq.

### exchange

This is the exchange name.

```yaml [standard]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables
      url: ${RABBITMQ_URL}
      exchanges: // [!code focus]
          - exchange: transfer // [!code focus]
```

### exchange_type

This is the exchange type, you can read more about them [here](https://medium.com/trendyol-tech/rabbitmq-exchange-types-d7e1f51ec825).
rindexer supports `direct`, `topic` and `fanout` exchanges.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables
      url: ${RABBITMQ_URL}
      exchanges: // [!code focus]
        - exchange: transfer
          #  expected one of `direct`, `topic` or `fanout`
          exchange_type: direct // [!code focus]
```

### routing_key

This is the routing key for the exchange. You do not need to provide this if you are using a `fanout` exchange. This
is mandatory for `direct` and `topic` exchanges.

:::info
This is optional for `fanout` exchanges, required for `direct` and `topic` exchanges.
:::

```yaml [fifo]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables
      url: ${RABBITMQ_URL}
      exchanges: // [!code focus]
        - exchange: transfer
          #  expected one of `direct`, `topic` or `fanout`
          exchange_type: direct
          routing_key: my-routing-key // [!code focus]
```

## networks

This is an array of networks you want to stream to this rabbitmq.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL}
      exchanges: // [!code focus]
        - exchange: transfer
          #  expected one of `direct`, `topic` or `fanout`
          exchange_type: direct
          routing_key: my-routing-key
          networks: // [!code focus]
            - ethereum // [!code focus]
```

## events

This is an array of events you want to stream to this rabbitmq.

### event_name

This is the name of the event you want to stream to this rabbitmq, must match the ABI event name.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL}
      exchanges: // [!code focus]
        - exchange: transfer
          #  expected one of `direct`, `topic` or `fanout`
          exchange_type: direct
          routing_key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
```

#### alias

This is an optional `alias` you wish to assign to the event you want to stream to this RabbitMQ exchange or queue.

It is paired with the event name and allows consumers to have unique discriminator keys in the event of
naming conflicts. E.g Transfer (ERC20) and Transfer (ERC721).

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL}
      exchanges: // [!code focus]
        - exchange: transfer
          #  expected one of `direct`, `topic` or `fanout`
          exchange_type: direct
          routing_key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              alias: RocketPoolTransfer // [!code focus]
```

### conditions

This accepts an array of conditions you want to apply to the event data before streaming to this rabbitmq.

:::info
This is optional, if you do not provide any conditions all data will be streamed.
:::

You may want to filter on the stream based on the event data, if the event data has not got an index on the on the
solidity event you can not filter it over the logs. The `conditions` filter is here to help you with this,
based on your ABI you can filter on the event data.

rindexer has enabled a special syntax which allows you to define on your ABI fields what you want to filter on.

1. `>` - higher then (for numbers only)
2. `<` - lower then (for numbers only)
3. `=` - equals
4. `>=` - higher then or equals (for numbers only)
5. `<=` - lower then or equals (for numbers only)
6. `||` - or
7. `&&` - and

So lets look at an example lets say i only want to get transfer events which are higher then `2000000000000000000` RETH wei

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL}
      exchanges: // [!code focus]
        - exchange: transfer
          #  expected one of `direct`, `topic` or `fanout`
          exchange_type: direct
          routing_key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              conditions: // [!code focus]
                - "value": ">=2000000000000000000" // [!code focus]
```

We use the ABI input name `value` to filter on the value field, you can find these names in the ABI file.

```json
{
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value", // [!code focus]
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
}
```

You can use the `||` or `&&` to combine conditions.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL}
      exchanges: // [!code focus]
        - exchange: transfer
          #  expected one of `direct`, `topic` or `fanout`
          exchange_type: direct
          routing_key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "value": ">=2000000000000000000 && value <=4000000000000000000" // [!code focus]
```

You can use the `=` to filter on other aspects like the `from` or `to` address.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL}
      exchanges: // [!code focus]
        - exchange: transfer
          #  expected one of `direct`, `topic` or `fanout`
          exchange_type: direct
          routing_key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662 || 0x0338ce5020c447f7e668dc2ef778025ce398266u" // [!code focus]
                - "value": ">=2000000000000000000 || value <=4000000000000000000" // [!code focus]
```

:::info
Note we advise you to filer any `indexed` fields in the contract details in the `rindexer.yaml` file.
As these can be filtered out on the request level and not filtered out in rindexer itself.
You can read more about it [here](/docs/start-building/yaml-config/contracts#indexed_1-indexed_2-indexed_3).
:::

If you have a tuple and you want to get that value you just use the object notation.

For example lets say we want to only get the events for `profileId` from the `quoteParams` tuple which equals `1`:

```json
{
     "anonymous": false,
     "inputs": [
       {
         "components": [
           {
             "internalType": "uint256",
             "name": "profileId", // [!code focus]
             "type": "uint256"
           },
           ...
         ],
         "indexed": false,
         "internalType": "struct Types.QuoteParams",
         "name": "quoteParams", // [!code focus]
         "type": "tuple"
       },
       ...
     ],
     "name": "QuoteCreated", // [!code focus]
     "type": "event"
}
```

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    rabbitmq: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      url: ${RABBITMQ_URL}
      exchanges: // [!code focus]
        - exchange: transfer
          #  expected one of `direct`, `topic` or `fanout`
          exchange_type: direct
          routing_key: my-routing-key
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "quoteParams.profileId": "=1" // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/streams/redis.mdx">
# Redis Streams

:::info
rindexer streams can be used without any other storage providers. It can also be used with storage providers.
:::

rindexer allows you to configure [Redis Streams](https://redis.io/docs/latest/develop/data-types/streams/) to stream any data to. This goes under
the [contracts](docs/start-building/yaml-config/contracts) or [native_transfers](/docs/start-building/yaml-config/native-transfers)
section of the YAML configuration file.

Found out more about [Redis Streams](https://redis.io/docs/latest/develop/data-types/streams/).

## Configuration with rindexer
`redis` `streams` property accepts an array allowing you to split up the streams any way you wish.

## Example

:::code-group

```yaml [contract events]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    redis: // [!code focus]
      connection_uri: ${REDIS_CONNECTION_URI} // [!code focus]
      streams: // [!code focus]
        - stream_name: "ethereum_rocketpool_transfer_stream" // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              alias: RocketPoolTransfer
```

```yaml [native transfers]
name: ETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
  streams: // [!code focus]
    redis: // [!code focus]
      connection_uri: ${REDIS_CONNECTION_URI} // [!code focus]
      streams: // [!code focus]
        - stream_name: "ethereum_transfer_stream" // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
          events: // [!code focus]
            - event_name: NativeTransfer // [!code focus]
              alias: Transfer
```

:::

## Response

:::info
Redis streams may wrap the message body into their own object so the below is just what we send to the stream.
:::

The response sent to you is already decoded and parsed into a JSON stringify object.

- `event_name` - The name of the event
- `event_signature_hash` - The event signature hash example the keccak256 hash of "Transfer(address,address,uint256)", this is topics[0] in the logs
- `event_data` - The event data which has all the event fields decoded and the transaction information which is under `transaction_information`
- `network` - The network the event was emitted on

For example a transfer event would look like:

```json
{
    "event_name": "Transfer",
    "event_signature_hash": "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
    "event_data": {
        "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
        "to": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
        "value": "1000000000000000000",
        "transaction_information": {
            "address": "0xae78736cd615f374d3085123a210448e74fc6393",
            "block_hash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
            "block_number": "18718011",
            "log_index": "0",
            "network": "ethereum",
            "transaction_hash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
            "transaction_index": "0"
        }
    },
    "network": "ethereum"
}
```

## connection_uri

This is the Redis connection url we advise to put this in a environment variable.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    redis: // [!code focus]
      # we advise to put this in a environment variables // [!code focus]
      connection_uri: ${REDIS_CONNECTION_URI} // [!code focus]
```

## streams

This is where you configure each of the Redis Streams you want to push to.

### stream_name

The name of the stream that you are streaming events to.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    redis: // [!code focus]
      connection_uri: ${REDIS_CONNECTION_URI}
      streams: // [!code focus]
        - stream_name: "ethereum_rocketpool_transfer_stream" // [!code focus]
```

### events

This is an array of events you want to stream to this Redis Stream..

#### event_name

This is the name of the event you want to stream to this Redis Stream, must match the ABI event name.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    redis: // [!code focus]
      connection_uri: ${REDIS_CONNECTION_URI}
      streams: // [!code focus]
        - stream_name: "ethereum_rocketpool_transfer_stream"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
```

#### alias

This is an optional `alias` you wish to assign to the event you want to stream to this Redis Stream.

It is paired with the event name and allows consumers to have unique discriminator keys in the event of
naming conflicts. E.g Transfer (ERC20) and Transfer (ERC721).

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    redis: // [!code focus]
      connection_uri: ${REDIS_CONNECTION_URI}
      streams: // [!code focus]
        - stream_name: "ethereum_rocketpool_transfer_stream"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              alias: RocketPoolTransfer // [!code focus]
```

#### conditions

This accepts an array of conditions you want to apply to the event data before streaming to this Redis Stream topic.

:::info
This is optional, if you do not provide any conditions all data will be streamed.
:::

You may want to filter on the stream based on the event data, if the event data has not got an index on the on the
solidity event you can not filter it over the logs. The `conditions` filter is here to help you with this,
based on your ABI you can filter on the event data.

rindexer has enabled a special syntax which allows you to define on your ABI fields what you want to filter on.

1. `>` - higher then (for numbers only)
2. `<` - lower then (for numbers only)
3. `=` - equals
4. `>=` - higher then or equals (for numbers only)
5. `<=` - lower then or equals (for numbers only)
6. `||` - or
7. `&&` - and

So lets look at an example lets say i only want to get transfer events which are higher then `2000000000000000000` RETH wei

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    redis: // [!code focus]
      connection_uri: ${REDIS_CONNECTION_URI}
      streams: // [!code focus]
        - stream_name: "ethereum_rocketpool_transfer_stream"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "value": ">=2000000000000000000" // [!code focus]
```

We use the ABI input name `value` to filter on the value field, you can find these names in the ABI file.

```json
{
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value", // [!code focus]
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
}
```

You can use the `||` or `&&` to combine conditions.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    redis: // [!code focus]
      connection_uri: ${REDIS_CONNECTION_URI}
      streams: // [!code focus]
        - stream_name: "ethereum_rocketpool_transfer_stream"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "value": ">=2000000000000000000 && value <=4000000000000000000" // [!code focus]
```

You can use the `=` to filter on other aspects like the `from` or `to` address.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    redis: // [!code focus]
      connection_uri: ${REDIS_CONNECTION_URI}
      streams: // [!code focus]
        - stream_name: "ethereum_rocketpool_transfer_stream"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662 || 0x0338ce5020c447f7e668dc2ef778025ce398266u" // [!code focus]
                - "value": ">=2000000000000000000 || value <=4000000000000000000" // [!code focus]
```

:::info
Note we advise you to filer any `indexed` fields in the contract details in the `rindexer.yaml` file.
As these can be filtered out on the request level and not filtered out in rindexer itself.
You can read more about it [here](/docs/start-building/yaml-config/contracts#indexed_1-indexed_2-indexed_3).
:::

If you have a tuple and you want to get that value you just use the object notation.

For example lets say we want to only get the events for `profileId` from the `quoteParams` tuple which equals `1`:

```json
{
     "anonymous": false,
     "inputs": [
       {
         "components": [
           {
             "internalType": "uint256",
             "name": "profileId", // [!code focus]
             "type": "uint256"
           },
           ...
         ],
         "indexed": false,
         "internalType": "struct Types.QuoteParams",
         "name": "quoteParams", // [!code focus]
         "type": "tuple"
       },
       ...
     ],
     "name": "QuoteCreated", // [!code focus]
     "type": "event"
}
```

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    redis: // [!code focus]
      connection_uri: ${REDIS_CONNECTION_URI}
      streams: // [!code focus]
        - stream_name: "ethereum_rocketpool_transfer_stream"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "quoteParams.profileId": "=1" // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/streams/sns.mdx">
# SNS / SQS

:::info
rindexer streams can be used without any other storage providers. It can also be used with storage providers.
:::

rindexer allows you to configure AWS SNS and AWS SQS to stream any data to. This goes under
the [contracts](/docs/start-building/yaml-config/contracts) or [native_transfers](/docs/start-building/yaml-config/native-transfers)
section of the YAML configuration file.

Find out more about [Simple Notification Service](https://aws.amazon.com/sns/) and [Simple Queue Service](https://aws.amazon.com/sqs/)

## Configuration with rindexer

`sns` `topics` property accepts an array allowing you to split up the streams any way you wish.

## Example

:::code-group

```yaml [contract events]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config: // [!code focus]
        region: us-east-1 // [!code focus]
        access_key: ${AWS_ACCESS_KEY_ID} // [!code focus]
        secret_key: ${AWS_SECRET_ACCESS_KEY} // [!code focus]
        # session_token is optional // [!code focus]
        session_token: ${AWS_SESSION_TOKEN} // [!code focus]
        # endpoint_url is optional // [!code focus]
        endpoint_url: ${ENDPOINT_URL} // ![code focus]
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:test" // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              alias: RocketPoolTransfer
```


```yaml [native transfers]
name: ETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config: // [!code focus]
        region: us-east-1 // [!code focus]
        access_key: ${AWS_ACCESS_KEY_ID} // [!code focus]
        secret_key: ${AWS_SECRET_ACCESS_KEY} // [!code focus]
        # session_token is optional // [!code focus]
        session_token: ${AWS_SESSION_TOKEN} // [!code focus]
        # endpoint_url is optional // [!code focus]
        endpoint_url: ${ENDPOINT_URL} // ![code focus]
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:ethereum-transfers" // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
```

:::

## Response

:::info
Note SNS/SQS may wrap the message body into their own object so the below is just what we send to the stream.
:::

The response sent to you is already decoded and parsed into a JSON stringify object.

- `event_name` - The name of the event
- `event_signature_hash` - The event signature hash example the keccak256 hash of "Transfer(address,address,uint256)", this is topics[0] in the logs
- `event_data` - The event data which has all the event fields decoded and the transaction information which is under `transaction_information`
- `network` - The network the event was emitted on

For example a transfer event would look like:

```json
{
    "event_name": "Transfer",
    "event_signature_hash": "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
    "event_data": {
        "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
        "to": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
        "value": "1000000000000000000",
        "transaction_information": {
            "address": "0xae78736cd615f374d3085123a210448e74fc6393",
            "block_hash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
            "block_number": "18718011",
            "log_index": "0",
            "network": "ethereum",
            "transaction_hash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
            "transaction_index": "0"
        }
    },
    "network": "ethereum"
}
```

## aws_config

This is the AWS configuration for the SNS client.

### region

The AWS region to connect to.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config: // [!code focus]
        region: us-east-1 // [!code focus]
```

### access_key

:::info
We advise you to put this in a environment variables.
:::

The AWS access key to connect to.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config: // [!code focus]
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID} // [!code focus]
```

### secret_key

:::info
We advise you to put this in a environment variables.
:::

The AWS secret key to connect to.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config: // [!code focus]
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY} // [!code focus]
```

### session_token

:::info
This is optional
:::

:::info
We advise you to put this in a environment variables.
:::


The AWS session token to connect to.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config: // [!code focus]
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        session_token: ${AWS_SESSION_TOKEN} // [!code focus]
```

### endpoint_url

:::info
This is optional
:::

:::info
We advise you to put this in a environment variables.
:::


The AWS endpoint to connect to.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config: // [!code focus]
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        session_token: ${AWS_SESSION_TOKEN}
        endpoint_url: ${ENDPOINT_URL} // ![code focus]
```

## topics

This is an array of topics you want to stream to this sns.

### topic_arn

This is your SNS topic arn. It supports first-in-first-out and standard topics.
You can read about the different here [here](https://aws.amazon.com/sns/features/).

:::code-group

```yaml [standard]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        # session_token is optional
        session_token: ${AWS_SESSION_TOKEN}
        # endpoint_url is optional
        endpoint_url: ${ENDPOINT_URL}
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:test" // [!code focus]
```

```yaml [fifo]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        # session_token is optional
        session_token: ${AWS_SESSION_TOKEN}
        # endpoint_url is optional
        endpoint_url: ${ENDPOINT_URL}
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:test.fifo" // [!code focus]
```

:::

#### networks

This is an array of networks you want to stream to this webhook.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        # session_token is optional
        session_token: ${AWS_SESSION_TOKEN}
        # endpoint_url is optional
        endpoint_url: ${ENDPOINT_URL}
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:test"
          networks: // [!code focus]
            - ethereum // [!code focus]
```

### events

This is an array of events you want to stream to this SNS topic.

#### event_name

This is the name of the event you want to stream to this SNS topic, must match the ABI event name.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        # session_token is optional
        session_token: ${AWS_SESSION_TOKEN}
        # endpoint_url is optional
        endpoint_url: ${ENDPOINT_URL}
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:test"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
```

#### alias

This is an optional `alias` you wish to assign to the event you want to stream to this SNS topic.

It is paired with the event name and allows consumers to have unique discriminator keys in the event of
naming conflicts. E.g Transfer (ERC20) and Transfer (ERC721).

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        # session_token is optional
        session_token: ${AWS_SESSION_TOKEN}
        # endpoint_url is optional
        endpoint_url: ${ENDPOINT_URL}
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:test"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer // [!code focus]
              alias: RocketPoolTransfer // [!code focus]
```

#### conditions

This accepts an array of conditions you want to apply to the event data before streaming to this SNS topic.

:::info
This is optional, if you do not provide any conditions all data will be streamed.
:::

You may want to filter on the stream based on the event data, if the event data has not got an index on the on the
solidity event you can not filter it over the logs. The `conditions` filter is here to help you with this,
based on your ABI you can filter on the event data.

rindexer has enabled a special syntax which allows you to define on your ABI fields what you want to filter on.

1. `>` - higher then (for numbers only)
2. `<` - lower then (for numbers only)
3. `=` - equals
4. `>=` - higher then or equals (for numbers only)
5. `<=` - lower then or equals (for numbers only)
6. `||` - or
7. `&&` - and

So lets look at an example lets say i only want to get transfer events which are higher then `2000000000000000000` RETH wei

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        # session_token is optional
        session_token: ${AWS_SESSION_TOKEN}
        # endpoint_url is optional
        endpoint_url: ${ENDPOINT_URL}
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:test"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "value": ">=2000000000000000000" // [!code focus]
```

We use the ABI input name `value` to filter on the value field, you can find these names in the ABI file.

```json
{
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value", // [!code focus]
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
}
```

You can use the `||` or `&&` to combine conditions.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        # session_token is optional
        session_token: ${AWS_SESSION_TOKEN}
        # endpoint_url is optional
        endpoint_url: ${ENDPOINT_URL}
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:test"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "value": ">=2000000000000000000 && value <=4000000000000000000" // [!code focus]
```

You can use the `=` to filter on other aspects like the `from` or `to` address.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        # session_token is optional
        session_token: ${AWS_SESSION_TOKEN}
        # endpoint_url is optional
        endpoint_url: ${ENDPOINT_URL}
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:test"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662 || 0x0338ce5020c447f7e668dc2ef778025ce398266u" // [!code focus]
                - "value": ">=2000000000000000000 || value <=4000000000000000000" // [!code focus]
```

:::info
Note we advise you to filer any `indexed` fields in the contract details in the `rindexer.yaml` file.
As these can be filtered out on the request level and not filtered out in rindexer itself.
You can read more about it [here](/docs/start-building/yaml-config/contracts#indexed_1-indexed_2-indexed_3).
:::

If you have a tuple and you want to get that value you just use the object notation.

For example lets say we want to only get the events for `profileId` from the `quoteParams` tuple which equals `1`:

```json
{
     "anonymous": false,
     "inputs": [
       {
         "components": [
           {
             "internalType": "uint256",
             "name": "profileId", // [!code focus]
             "type": "uint256"
           },
           ...
         ],
         "indexed": false,
         "internalType": "struct Types.QuoteParams",
         "name": "quoteParams", // [!code focus]
         "type": "tuple"
       },
       ...
     ],
     "name": "QuoteCreated", // [!code focus]
     "type": "event"
}
```

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    sns: // [!code focus]
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
        # session_token is optional
        session_token: ${AWS_SESSION_TOKEN}
        # endpoint_url is optional
        endpoint_url: ${ENDPOINT_URL}
      topics: // [!code focus]
        - topic_arn: "arn:aws:sns:us-east-1:664643779377:test"
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: Transfer
              conditions: // [!code focus]
                - "quoteParams.profileId": "=1" // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/streams/webhooks.mdx">
# Webhooks

:::info
rindexer streams can be used without any other storage providers. It can also be used with storage providers.
:::

rindexer allows you to configure webhooks to fire based on your conditions to another API. This goes under
the [contracts](/docs/start-building/yaml-config/contracts) or [native_transfers](/docs/start-building/yaml-config/native-transfers)
section of the YAML configuration file.

## Configuration with rindexer

`webhooks` property accepts an array allowing you to split up the webhooks any way you wish.

## Example

:::code-group

```yaml [contract events]
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL // [!code focus]
        shared_secret: ${RINDEXER_WEBHOOK_SHARED_SECRET} // [!code focus]
        networks: // [!code focus]
          - ethereum // [!code focus]
        events: // [!code focus]
          - event_name: Transfer // [!code focus]
            alias: RocketPoolTransfer
```

```yaml [native transfers]
name: ETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL // [!code focus]
        shared_secret: ${RINDEXER_WEBHOOK_SHARED_SECRET} // [!code focus]
        networks: // [!code focus]
          - ethereum // [!code focus]
        events: // [!code focus]
          - event_name: NativeTransfer // [!code focus]
```

:::

## Response

The response sent to you is already decoded and parsed into a JSON object.

- `event_name` - The name of the event
- `event_signature_hash` - The event signature hash example the keccak256 hash of "Transfer(address,address,uint256)", this is topics[0] in the logs
- `event_data` - The event data which has all the event fields decoded and the transaction information which is under `transaction_information`
- `network` - The network the event was emitted on

For example a transfer event would look like:

```json
{
    "event_name": "Transfer",
    "event_signature_hash": "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
    "event_data": {
        "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
        "to": "0x0338ce5020c447f7e668dc2ef778025ce3982662",
        "value": "1000000000000000000",
        "transaction_information": {
            "address": "0xae78736cd615f374d3085123a210448e74fc6393",
            "block_hash": "0x8461da7a1d4b47190a01fa6eae219be40aacffab0dd64af7259b2d404572c3d9",
            "block_number": "18718011",
            "log_index": "0",
            "network": "ethereum",
            "transaction_hash": "0x145c6705ffbf461e85d08b4a7f5850d6b52a7364d93a057722ca1194034f3ba4",
            "transaction_index": "0"
        }
    },
    "network": "ethereum"
}
```

## endpoint

This is your webhook url.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL // [!code focus]
```

## shared_secret

This is the shared secret you want to use to authenticate the webhook so you know it ha came from rindexer.
This is always injected in the header as `x-rindexer-shared-secret`.

:::info
We advise you to put this in a environment variables.
:::

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL
        shared_secret: ${RINDEXER_WEBHOOK_SHARED_SECRET} // [!code focus]
```

This is an array of networks you want to stream to this webhook.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL
        shared_secret: ${RINDEXER_WEBHOOK_SHARED_SECRET}
        networks: // [!code focus]
          - ethereum // [!code focus]
```

## events

This is an array of events you want to stream to this webhook.

### event_name

This is the name of the event you want to stream to this webhook, must match the ABI event name.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL
        shared_secret: ${RINDEXER_WEBHOOK_SHARED_SECRET}
        networks:
          - ethereum
        events: // [!code focus]
          - event_name: Transfer // [!code focus]
```

#### alias

This is an optional `alias` you wish to assign to the event you want published to this Webhook.

It is paired with the event name and allows consumers to have unique discriminator keys in the event of
naming conflicts. E.g Transfer (ERC20) and Transfer (ERC721).

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL
        shared_secret: ${RINDEXER_WEBHOOK_SHARED_SECRET}
        networks:
          - ethereum
        events: // [!code focus]
          - event_name: Transfer // [!code focus]
            alias: RocketPoolTransfer // [!code focus]
```

### conditions

This accepts an array of conditions you want to apply to the event data before calling the webhook.

:::info
This is optional, if you do not provide any conditions all data will be streamed.
:::

You may want to filter on the stream based on the event data, if the event data has not got an index on the on the
solidity event you can not filter it over the logs. The `conditions` filter is here to help you with this,
based on your ABI you can filter on the event data.

rindexer has enabled a special syntax which allows you to define on your ABI fields what you want to filter on.

1. `>` - higher then (for numbers only)
2. `<` - lower then (for numbers only)
3. `=` - equals
4. `>=` - higher then or equals (for numbers only)
5. `<=` - lower then or equals (for numbers only)
6. `||` - or
7. `&&` - and

So lets look at an example lets say i only want to get transfer events which are higher then `2000000000000000000` RETH wei

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL
        shared_secret: ${RINDEXER_WEBHOOK_SHARED_SECRET}
        networks:
          - ethereum
        events: // [!code focus]
          - event_name: Transfer // [!code focus]
            conditions: // [!code focus]
              - "value": ">=2000000000000000000" // [!code focus]
```

We use the ABI input name `value` to filter on the value field, you can find these names in the ABI file.

```json
{
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value", // [!code focus]
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
}
```

You can use the `||` or `&&` to combine conditions.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL
        shared_secret: ${RINDEXER_WEBHOOK_SHARED_SECRET}
        networks:
          - ethereum
        events: // [!code focus]
          - event_name: Transfer
            conditions: // [!code focus]
              - "value": ">=2000000000000000000 && value <=4000000000000000000" // [!code focus]
```

You can use the `=` to filter on other aspects like the `from` or `to` address.

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL
        shared_secret: ${RINDEXER_WEBHOOK_SHARED_SECRET}
        networks:
          - ethereum
        events: // [!code focus]
          - event_name: Transfer
            conditions: // [!code focus]
              - "from": "0x0338ce5020c447f7e668dc2ef778025ce3982662 || 0x0338ce5020c447f7e668dc2ef778025ce398266u" // [!code focus]
              - "value": ">=2000000000000000000 || value <=4000000000000000000" // [!code focus]
```

:::info
Note we advise you to filer any `indexed` fields in the contract details in the `rindexer.yaml` file.
As these can be filtered out on the request level and not filtered out in rindexer itself.
You can read more about it [here](/docs/start-building/yaml-config/contracts#indexed_1-indexed_2-indexed_3).
:::

If you have a tuple and you want to get that value you just use the object notation.

For example lets say we want to only get the events for `profileId` from the `quoteParams` tuple which equals `1`:

```json
{
     "anonymous": false,
     "inputs": [
       {
         "components": [
           {
             "internalType": "uint256",
             "name": "profileId", // [!code focus]
             "type": "uint256"
           },
           ...
         ],
         "indexed": false,
         "internalType": "struct Types.QuoteParams",
         "name": "quoteParams", // [!code focus]
         "type": "tuple"
       },
       ...
     ],
     "name": "QuoteCreated", // [!code focus]
     "type": "event"
}
```

```yaml [rindexer.yaml]
...
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: "18600000"
    end_block: "18600181"
  abi: "./abis/RocketTokenRETH.abi.json"
  include_events:
  - Transfer
  streams: // [!code focus]
    webhooks: // [!code focus]
      - endpoint: YOUR_WEBHOOK_URL
        shared_secret: ${RINDEXER_WEBHOOK_SHARED_SECRET}
        networks:
          - ethereum
        events: // [!code focus]
          - event_name: Transfer
            conditions: // [!code focus]
              - "quoteParams.profileId": "=1" // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/yaml-config/config.mdx">
# Config

More advanced configuration options for fine-tuning memory usage, event throughput, and more. Most of the time you will
not need to adjust these values.

### Useful background

It is useful to become familiar with how rindexer controls concurrency across events and networks.

By nature of blockchain indexing, we must index events separately per network. However, we also try to
optimize throughput via `eth_getLogs` requests **per event**.

This means we ultimately run an indexing process per "network-event".

:::info
This "network-event" is where concurrency is controlled.
:::

## buffer

_Default: `4`_

This parameter controls "buffer" of events we will hold in memory, per "network-event". This is extremely useful for
limiting the upper memory-bound during large scale backfill operations for high-frequency events (like ERC20 transfers).

What happens if the handler does not release events as fast as they can queried? Well,a backlog of events we've indexed
would build up in memory and ultimately the process would OOM and be killed.

We avoid that by maintaining a bounded channel (buffer) of events. This way when the handler is ready it will pull the
next event, and it will trigger a new indexing fetch to fill the freed slot.

The default should be enough to balance memory use with high-throughput, however can be tweaked to constrain memory by
lowering the value. Or potentially increasing throughput by increasing the value.

:::info
This concept is known as "back-pressuring".
:::

```yaml [rindexer.yaml]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
config:
  buffer: 1 // [!code focus]
```

## callback_concurrency

_Default: `2`_

:::warn
When "index_event_in_order" is enabled for an event, it will override this setting with `1` to ensure FIFO ordering.
:::

This setting controls the "network-event" handler callback rate. This allows us to have `n` concurrent handlers being
called per "network event". This may or may not be desirable based on the use-case and code present in the handler callback function.

A case where this may not be desirable is if there is any kind of "per network-event global locking" which would mean that
trying to run 2 batches in parallel would simply result in one batch holding the other up.

Imagine setting this to some very high number, `999999`, representing unbounded concurrency. In this case
you can imagine that there is essentially no "back-pressure". This would work in the case where events are simply being
discarded, being maintained in memory, or some other hyper-efficient mechanism. But in reality, the most common case for
indexing will be to persist the events to a database, and in these cases there are factors such as data structure locks,
database connection pool limits, and resource constraints.

This means we cannot reasonably benefit from increasing this number too high, and on the contrary, can suffer a decrease
in throughput due to lock contention, and unwanted situations like connection pool exhaustion, deadlocks, and more.

You may benefit from increasing this above `2` for very simple workloads, generally a value of `1` or `2` is optimal.

```yaml [rindexer.yaml]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
config:
  callback_concurrency: 2 // [!code focus]
```

## timestamp_sample_rate

Optionally configure a sample rate to improve the efficiency of large-block-range requests by sampling those blocks.

If you do not want to sample either ignore the option or set it to `1.0`.

:::warn
Sampling is tradeoff and cannot guarantee 100% accuracy of timestamps, but is far more efficient whilst retaining high-accuracy.
Only enable sampling if you can accept small inaccuracies in timestamps.
:::

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
config:
  timestamp_sample_rate: 0.1 // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/yaml-config/contracts.mdx">
# Contracts

The list of contracts to index for this indexer.

:::info
You can have multiple contracts in an indexer.
:::

## name

This is the name of the contract to index, it will use this name on the database on tables it generates, alongside
on generated rust code if you are using the rust project.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
   - name: RocketPoolETH // [!code focus]
```

## details

The details for the contract mapping to the network and contract address.

### network

The network name to listen for events on, this should match the network name in the networks section of the YAML.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum // [!code focus]
```

### address

:::info
The address or addresses of the contract or contracts to listen for events on.
Only one of `address`, `filter` or `factory` can be provided for a given contract details.
:::

The contract address to listen for events on.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393" // [!code focus]
```

To listen to many contract addresses you can provide an array of addresses.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address:
          - "0xae78736cd615f374d3085123a210448e74fc6393" // [!code focus]
          - "0x2FD5c1659A82E87217DF254f3D4b71A22aE43eE1" // [!code focus]
```

### filter

:::info
Only one of `address`, `filter` or `factory` can be provided for a given contract details.
:::

If you wish to filter based on events only for example you want all transfer events from all contracts you can use the
filter.

:::warning
You currently cannot mix and match address and filter within the same contract definition.
This means if you want to index on a filter and with the same contract definition index on
an address you will need to do 2 different contract definitions in the yaml file.
:::

#### event_name

The event name to filter on, it must match the ABI event name.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: ERC20Transfer
    details:
      - network: ethereum
        filter: // [!code focus]
          event_name: Transfer // [!code focus]
```

#### Index more then 1 filter for the contract

You can just pass in an array of events names to index on the filter.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: ERC20 // [!code focus]
    details:
      - network: ethereum
        filter: // [!code focus]
          - event_name: Transfer // [!code focus]
          - event_name: Approval // [!code focus]
```

### factory

:::info
Only one of `address`, `filter` or `factory` can be provided for a given contract details.
:::

Some contracts are deployed through the factory contract (e.g. Uniswap V3). If you wish to track events only from factory-deployed addresses use `factory` filter.

:::warning
Factory filter requires to specify events which should be included when indexing through `include_events` property on the contract.
:::

#### name

The name of the factory contract to index.

#### address

The factory contract address to listen for events on. To listen to many factory contract addresses, you can provide an array of addresses.

#### abi

The ABI of the contract is pointing to the JSON file in the repository. It can be a relative path or a full path.

#### event_name

The event name to filter on, it must match the ABI event name.

#### input_name

The path to the factory-deployed contract address in the event inputs. Supports deep property access in case of complex event types: `pool.address`.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: UniswapV3Pool
    details:
      - network: ethereum
        factory: // [!code focus]
          name: UniswapV3Factory // [!code focus]
          address: 0x1F98431c8aD98523631AE4a59f267346ea31F984 // [!code focus]
          abi: ./abis/UniswapV3Factory.abi.json // [!code focus]
          event_name: PoolCreated // [!code focus]
          input_name: "pool" // [!code focus]
```

If a factory deploys more than one contract in a single event, an array of inputs can be provided to track multiple factory-deployed addresses.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: UniswapV3Pool
    details:
      - network: ethereum
        factory: // [!code focus]
          name: UniswapV3Factory // [!code focus]
          address: 0x1F98431c8aD98523631AE4a59f267346ea31F984 // [!code focus]
          abi: ./abis/UniswapV3Factory.abi.json // [!code focus]
          event_name: PoolCreated // [!code focus]
          input_name: // [!code focus]
            - "token0" // [!code focus]
            - "token1" // [!code focus]
```

:::info
For factories that deploy a high volume of contracts, consider optimizing event fetching logic to enhance indexing performance.
For detailed implementation guidance, refer to the [network configuration documentation](/docs/start-building/yaml-config/networks#get_logs_settings) settings.
:::

### indexed_1, indexed_2, indexed_3

:::info
This is optional and can be used on both address and filter.
:::

Indexed means that these values will be stored in the topics field rather than the data field when the event gets fired off
and you can filter these out on the JSONRPC side so you only get the events you want.

In EVM you can have up to 3 indexed fields to filter on. The indexed 1,2,3 are based on the order they emitted in the event.
So if you have 3 indexed fields in the event you can filter on all 3 or 2 or 1 of them in any direction. Indexed fields are arrays so
you can filter many values in the indexed fields, the arrays are `OR` not `AND` filtering.

example ABI:

```json
{
    "anonymous":false,
    "inputs":[
      {
        "indexed":true, // [!code focus]
        "internalType":"address",
        "name":"owner",
        "type":"address"
      },
      {
        "indexed":true, // [!code focus]
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "indexed":false, // [!code focus]
        "internalType":"uint256",
        "name":"value",
        "type":"uint256"
      }
    ],
    "name":"Approval", // [!code focus]
    "type":"event"
}
```

So this ABI says that the inputs `owner` and `spender` are indexed and can be filtered on. `value` is not
indexed so you can not filter on it.

For example if you wanted to get all the approvals for rETH for owner `0xd87b8e0db0cf9cbf9963c035a6ad72d614e37fd5`
and `0x0338ce5020c447f7e668dc2ef778025ce398266b` you could set the indexed filters like so:

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
   - name: RocketPoolETH
     details:
       - network: ethereum
         address: "0xae78736cd615f374d3085123a210448e74fc6393"
         indexed_filters: // [!code focus]
           - event_name: Approval // [!code focus]
             indexed_1:
               - "0xd87b8e0db0cf9cbf9963c035a6ad72d614e37fd5" // [!code focus]
               - "0x0338ce5020c447f7e668dc2ef778025ce398266b" // [!code focus]
```

Another example using filters is if you wanted to get all the approvals for any token for owner `0xd87b8e0db0cf9cbf9963c035a6ad72d614e37fd5`
and `0x0338ce5020c447f7e668dc2ef778025ce398266b` you could set the indexed filters like so:

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
   - name: RocketPoolETH
     details:
       - network: ethereum
         filter:
            event_name: Approval
         indexed_filters: // [!code focus]
           - event_name: Approval // [!code focus]
             indexed_1:
               - 0xd87b8e0db0cf9cbf9963c035a6ad72d614e37fd5 // [!code focus]
               - 0x0338ce5020c447f7e668dc2ef778025ce398266b // [!code focus]
```

### start_block

The block to start indexing from, you can use the deployed block if you wish to get everything.

:::info
This is optional but most people will want to use this, if you do not provide an start block it will index the data from now and then live index as new blocks
come in. Important to know this will NOT track last synced block and when you start and stop the indexer it will start from the latest block.
You can read more about this [here](/docs/start-building/live-indexing-and-historic).
:::


```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000 // [!code focus]
```

### end_block

:::info
This is optional, if you do not provide an end block it will index all the data and then live index as new blocks
come in. You can read more about this [here](/docs/start-building/live-indexing-and-historic).
:::

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056 // [!code focus]
```


### Multiple Networks

You can have multiple networks for the same contract, this is useful if you have a contract that is deployed on multiple
networks.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks: // [!code focus]
  - name: ethereum // [!code focus]
    chain_id: 1 // [!code focus]
    rpc: https://mainnet.gateway.tenderly.co // [!code focus]
  - name: base // [!code focus]
    chain_id: 8453 // [!code focus]
    rpc: https://base.gateway.tenderly.co // [!code focus]
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details: // [!code focus]
    - network: ethereum // [!code focus]
      address: "0xae78736cd615f374d3085123a210448e74fc6393" // [!code focus]
      start_block: 18600000 // [!code focus]
      end_block: 18718056 // [!code focus]
    - network: base // [!code focus]
      address: "0xba25348cd615f374d3085123a210448e74fa3333" // [!code focus]
      start_block: 18118056 // [!code focus]
      end_block: 18918056 // [!code focus]
```

## abi

The ABI of the contract pointing to the JSON file in the repository. It can be a relative path or a full path.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json // [!code focus]
```

### Many ABIs

If you need to use many ABIs in the single contract you can pass in an array
this is useful if you have a contract which has several different implementations ABIs.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi:
      - ./abis/RocketTokenRETH.abi.json // [!code focus]
      - ./abis/RocketTokenRETH2.abi.json // [!code focus]
```

## include_events

The events you wish to include in the indexer.

:::info
This is optional if you do not provide this it will include all events in the ABI.
:::

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:  // [!code focus]
      - Transfer // [!code focus]
      - Approval // [!code focus]
```

## index_event_in_order

rindexer was built to be as fast as it can so any blocking processes holds indexing up, the more concurrency the better.
Any events which you wish to index in the order the events were emitted on that event in the contract can be put in this list.

The more you put in here the slower the indexer will be as it will have to wait for the previous events to be indexed
before it can index the next events.

:::info
This is optional if you do not provide this it will assume speed is more important than order.
:::

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
    index_event_in_order: // [!code focus]
      - Transfer // [!code focus]
      - Approval // [!code focus]
```

## dependency_events

:::warning
if you are using defined dependency_events and using [relationships](/docs/start-building/yaml-config/storage#relationships)
you will need to make sure you define the relationship in the `dependency_events` manually as rindexer can not merge
the relationship with the dependency events if custom dependency_events are defined.
:::

:::warning
Also note any cross contracts relationships will not be applied automatically, you will need to define them manually in the YAML.
if you do not rindexer will panic and let you know that you have to define the [dependency_events](/docs/start-building/yaml-config/contracts#dependency_events).
:::

rindexer was built to be as fast as it can so any blocking processes holds indexing up, the more concurrency the better.
Any events which depend on each other can be put in the `dependency_events` list, this will mean that they will be processes
in the order they are in the list.

- `events` = process these events
- `then` = after you processed the `events` above process these events

If you do not put an event in the `dependency_events` events then it will be deemed a non-blocking event and will be processed
as soon as it can.

:::info
This is optional if you do not provide this it will assume speed is more important than order.
:::

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
    dependency_events: // [!code focus]
      events:
        - Transfer // [!code focus]
      then:
        events:
          - Approval // [!code focus]
```

### Cross Contract Dependency Events

You can also define dependency events blocking across contracts, this is useful if you have many contracts which emit
data but are dependent on each other.

:::info
WrappedRocketPoolETH example below does not exist on the chain, this is just an example of how you can use dependency events.
:::

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: WrappedRocketPoolETH // [!code focus]
    details:
      - network: ethereum
        address: 0x2FD5c1659A82E87217DF254f3D4b71A22aE43eE8
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/WrappedRocketTokenRETH.abi.json
    include_events: // [!code focus]
    - Approval // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer // [!code focus]
    dependency_events: // [!code focus]
      events:
        - Transfer // [!code focus]
      then:
        events:
          - contract_name: WrappedRocketPoolETH // [!code focus]
            event_name: Approval // [!code focus]
```

So now `WrappedRocketPoolETH` > `Approval` will not be processed until `RocketPoolETH` > `Transfer` is processed.


## reorg_safe_distance

Reorgs can happen on the chain, this is when a block is removed from the chain and replaced with another block.
This can cause issues with the indexer indexed state if you turn `reorg_safe_distance` on it will keep a safe distance from the live
latest block to avoid any reorg issues.

Note if you are doing live indexing you will need to handle more advanced reorgs, support for advanced reorgs
is in the backlog for rindexer.

:::info
This is optional if you do not provide this it will index the latest blocks instantly.
:::

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
    reorg_safe_distance: true // [!code focus]
```

## generate_csv

If you wish to generate a CSV file of the indexed data you can turn this on. This will be ignored if you do not have
the CSV storage enabled. By default if this is not supplied and the CSV storage is enabled it will generate a CSV file.

:::info
This is optional if you do not provide this it will generate a CSV file if the CSV storage is enabled.
:::

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts: // [!code focus]
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
    generate_csv: true // [!code focus]
```

## streams

You can configure streams to stream the data to other services, this is useful if you want to use other services
to index the data. You can read more about it [here](/docs/start-building/streams).

## chat

You can configure chat to send messages You can read more about it [here](/docs/start-building/chatbots).
</file>

<file path="documentation/docs/pages/docs/start-building/yaml-config/global.mdx">
# global

Global YAML.

## etherscan_api_key

:::info
This is optional and will use a shared fallback key if not provided. This can be rate limited as many people may be using it.
We advise if using `rindexer add` very often to provide your own key.
:::

We advise you to put the etherscan API key in an environment variable.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
    - Transfer
    - Approval
global: // [!code focus]
  etherscan_api_key: ${ETHERSCAN_API_KEY} // [!code focus]
```

## contracts

:::info
If you are building a no-code project you can skip this section. This is for rust projects only.
:::

The contracts section of the global YAML config allows you to define contracts which can be used in the indexers.
You can define many contracts in a single YAML file.

### name

The name of the contract, it should be unique to the YAML file.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
    - Transfer
    - Approval
global: // [!code focus]
  contracts: // [!code focus]
    - name: USDT // [!code focus]
```

### details

The details of the contract.

#### address

The address of the contract.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
    - Transfer
    - Approval
global:
  contracts:
    - name: USDT
      details: // [!code focus]
        - address: 0xdac17f958d2ee523a2206206994597c13d831ec7 // [!code focus]
```

#### network

The network the contract is on, this should match the network name in the networks section of the YAML file.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
    - Transfer
    - Approval
global:
  contracts:
    - name: USDT
      details:
        - address: 0xdac17f958d2ee523a2206206994597c13d831ec7
          network: ethereum // [!code focus]
```

### abi

The path to the ABI file for the contract. It can be a relative or full path.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
    - Transfer
    - Approval
global:
  contracts:
    - name: USDT
      details:
        - address: 0xdac17f958d2ee523a2206206994597c13d831ec7
          network: ethereum
      abi: ./abis/erc20.abi.json // [!code focus]
```

## Multiple Contracts

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
networks:
- name: ethereum // [!code focus]
  chain_id: 1 // [!code focus]
  rpc: https://mainnet.gateway.tenderly.co // [!code focus]
- name: base // [!code focus]
  chain_id: 8453 // [!code focus]
  rpc: https://mainnet.base.org // [!code focus]
storage:
  postgres:
    enabled: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: 18900000
    end_block: 19000000
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
    - Transfer
    - Approval
global: // [!code focus]
  contracts: // [!code focus]
    - name: USDT // [!code focus]
      details: // [!code focus]
        - address: 0xdac17f958d2ee523a2206206994597c13d831ec7 // [!code focus]
          network: ethereum // [!code focus]
        - address: 0xfde4C96c8593536E31F229EA8f37b2ADa2699bb2 // [!code focus]
          network: base // [!code focus]
      abi: ./abis/erc20.abi.json // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/yaml-config/graphql.mdx">
# graphql

To define some graphql settings you can use the `graphql` section of the YAML configuration file.

:::info
This is optional if you are happy with the default settings but worth knowing what you can configure.
:::

## port

You can use the `--port` flag when running to override the port number you want to use for the GraphQL server
but this yaml config allows you to set a default port number. By default if not set it will use port 3001.

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
graphql:
  port: 3001 // [!code focus]
```

## disable_advanced_filters

rindexer GraphQL supports [advanced filtering](/docs/accessing-data/graphql#filter) but these filters easily be abused and cause performance issues.
If you wish to disable advanced filtering you can set this to true. By default it is enabled so set as false.

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
graphql:
  disable_advanced_filters: true // [!code focus]
```

## filter_only_on_indexed_columns

When you end up having a database which has a lot of data querying that can become slow, indexes can help speed up the queries
and critical for the performance of the GraphQL server. By default rindexer lets you filter on any column even if it is
not indexed but this setting allows you to only exposed the ability to filter via GraphQL on the indexed columns.

You can define your own indexes in the [storage](/docs/start-building/yaml-config/storage#indexes) section of the YAML configuration file.

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: 18600000
        end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
graphql:
  filter_only_on_indexed_columns: true // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/yaml-config/index.mdx">
# Overview of the YAML Configuration File

The YAML configuration file is the heart of your rindexer project. It defines the project's name,
description, repository, and the contracts that will be used to index the data.
This file is used to set up the project and configure the indexing tasks that will be performed.

**YAML is case-sensitive, so make sure to use the correct case when defining the fields in the configuration file.**

:::info
YAML files can be mapped to environment variables to store sensitive information, such as RPC urls or other credentials.
The syntax for this in the YAML is `${ENV_VARIABLE_NAME}`.
:::

## YAML structure

- [Top level fields](/docs/start-building/yaml-config/top-level-fields) - The top-level fields of the YAML configuration file.
- [Networks](/docs/start-building/yaml-config/networks) - The networks to listen for events on are defined in the YAML configuration file.
- [Storage](/docs/start-building/yaml-config/storage) - The storage configuration is defined in the YAML configuration file.
- [Contracts](/docs/start-building/yaml-config/contracts) - The indexers of the project are defined in the YAML configuration file.
- [GraphQL](/docs/start-building/yaml-config/graphql) - The GraphQL configuration is defined in the YAML configuration file.
- [Global](/docs/start-building/yaml-config/global) - The global events to listen for are defined in the YAML configuration file.
- [Config](/docs/start-building/yaml-config/config) - The advanced configuration parameters for the indexer

### Environment Variables

YAML files can be mapped to environment variables to store sensitive information, such as RPC urls or other credentials.
Alongside different environments mappings, allowing you to store different values for different environments.
The syntax for this in the YAML is `${ENV_VARIABLE_NAME}`. This can be used in ANY field in the YAML file.

example:

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: ${RPC_URL} // [!code focus]
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
    - network: ethereum
      address: "0xae78736cd615f374d3085123a210448e74fc6393"
      start_block: 18600000
      end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
```

## Example YAML no-code configuration file

### For single contract address

Filter events for a specific address

#### Historic

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
    - network: ethereum
      address: "0xae78736cd615f374d3085123a210448e74fc6393"
      start_block: 18600000
      end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
```

#### Live

No start or end block will index from all new blocks as they are produced live.

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
    - network: ethereum
      address: "0xae78736cd615f374d3085123a210448e74fc6393"
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
```

#### Live and historic

No end block will index from the start block to the latest block then index all new blocks as they produced live.

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
    - network: ethereum
      address: "0xae78736cd615f374d3085123a210448e74fc6393"
      start_block: 18600000
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
```

### For many contract addresses

Filter events for many contract addresses

#### Historic

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
    - network: ethereum
      address:
        - "0xae78736cd615f374d3085123a210448e74fc6393" // [!code focus]
        - "0x2FD5c1659A82E87217DF254f3D4b71A22aE43eE1" // [!code focus]
      start_block: 18600000
      end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
```

### For address or addresses with indexed filter

Filter events for a specific address or array of addresses filtering on indexed fields.
You can read more about indexed fields [here](/docs/start-building/yaml-config/contracts#indexed_1-indexed_2-indexed_3).

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH
    details:
    - network: ethereum
      address: "0xae78736cd615f374d3085123a210448e74fc6393"
      indexed_filters: // [!code focus]
        - event_name: Transfer // [!code focus]
          indexed_1: // [!code focus]
            - 0xd87b8e0db0cf9cbf9963c035a6ad72d614e37fd5 // [!code focus]
      start_block: 18600000
      end_block: 18718056
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
      - Approval
```

### Filter for an event across all contracts

The historic, live and historic and live examples above can be used in every example. You can read more about
these terms [here](/docs/start-building/live-indexing-and-historic).

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: TransferEvents
    details:
    - network: ethereum
      filter:
        event_name: Transfer
      start_block: 18600000
      end_block: 18718056
    abi: ./abis/ERC20.abi.json
```

### Filter for an event across all contracts against indexed values

The historic, live and historic and live examples above can be used in every example. You can read more about
these terms [here](/docs/start-building/live-indexing-and-historic).

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: TransferEventForAddress
    details:
    - network: ethereum
      filter:
        event_name: Transfer
      indexed_filters:
         - event_name: Transfer
           indexed_1:
             - 0x4A1a2197f307222cD67A1762D9A352F64558d9Be
      start_block: 18600000
      end_block: 18718056
    abi: ./abis/ERC20.abi.json
```


### Filter for address or addresses deployed by factory contract

Filter events that are emitted from a known factory-deployed contract.
You can read more about indexed fields [here](/docs/start-building/yaml-config/contracts#factory).

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: UniswapV3Pool
    details:
      - network: ethereum
        factory:
          name: UniswapV3Factory
          address: 0x1F98431c8aD98523631AE4a59f267346ea31F984
          abi: ./abis/UniswapV3Factory.abi.json
          event_name: PoolCreated
          input_name: "pool"
```
</file>

<file path="documentation/docs/pages/docs/start-building/yaml-config/native-transfers.mdx">
# Native Transfers

A special opt-in configuration for indexing native token transfers such as "ETH", in the form of "ERC20"-like transfer events.

You can expect the event to be defined as if you were indexing an ERC20 Transfer event.

:::warning
This is **experimental** functionality which has not yet been extensively tested in production.
:::

Supported stream providers:

- [Simple](/docs/start-building/yaml-config/native-transfers#simple) - Simple opt-in (for csv, and postgres)
- [Complex](/docs/start-building/yaml-config/native-transfers#complex) - Complex indexing configuration with stream providers

# Simple

The "simple" opt-in is done via including the top level yaml `native_transfers: true`. This has a few special properties
and is designed to kickstart simple persistence based indexing of native transfers.

By default, this means:
- All networks defined in `networks` will be enabled for native transfer indexing
- All enabled `storage` options will be used
- Native transfers will be indexed in `live` mode, from the latest block onwards.

The event will be persisted to storage under the event name `NativeTransfer`.

```yaml [rindexer.yaml]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    # The rpc provider must support the `trace_block` rpc method in simple mode
    rpc: https://mainnet.gateway.tenderly.co  // [!code focus]
storage:
  postgres:
    enabled: true
native_transfers: true // [!code focus]
contracts: []
```

# Complex

The complex configuration is designed for more powerful configuration. Specifically if your use case is one of the following:
1. You want historical `native transfer` indexing
2. You want to use one of the `stream` or `chat` providers
3. You want to conditionally filter or alias the event name
4. You want to only opt-in to specific networks for `native transfer` events

If you provide any `networks` in the `native_transfers` config it is equivalent to setting `native_tranfers: true` and
you will be opted in to native transfer indexing for that network.

## networks

The network name to listen for events on, this should match the network name in the networks section of the YAML.

```yaml [rindexer.yaml]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks: // [!code focus]
    - network: ethereum // [!code focus]
contracts: []
```

### start_block

The block to start indexing from, you can use the deployed block if you wish to get everything.

:::info
This is optional but most people will want to use this, if you do not provide an start block it will index the data from now and then live index as new blocks
come in. Important to know this will NOT track last synced block and when you start and stop the indexer it will start from the latest block.
You can read more about this [here](/docs/start-building/live-indexing-and-historic).
:::

```yaml [rindexer.yaml]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks: // [!code focus]
    - network: ethereum
      start_block: 0 // [!code focus]
contracts: []
```

### end_block

:::info
This is optional, if you do not provide an end block it will index all the data and then live index as new blocks
come in. You can read more about this [here](/docs/start-building/live-indexing-and-historic).
:::

```yaml [rindexer.yaml]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks: // [!code focus]
    - network: ethereum
      start_block: 18600000
      end_block: 18718056 // [!code focus]
contracts: []
```

### method

:::info
This is optional, if you do not provide a method it will default to using `eth_getBlockByNumber` it is the most efficient,
well supported, and simple RPC method available.
:::

The method field is an advanced option, and typically does not need to be defined. By default it 
will use `eth_getBlockByNumber` and it is only recommended to manually override this in the event that your
RPC provider does not have adequate support or there is some reason you would prefer to use `trace_block` or `debug_traceBlockByNumber`.

Valid options are: `eth_getBlockByNumber` `debug_traceBlockByNumber` or `trace_block`.

:::code-group

```yaml [debug_traceBlockByNumber]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks: // [!code focus]
    - network: ethereum
      start_block: 18600000
      end_block: 18718056
      method: eth_getBlockByNumber
contracts: []
```

```yaml [debug_traceBlockByNumber]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks: // [!code focus]
    - network: ethereum
      start_block: 18600000
      end_block: 18718056 
      method: debug_traceBlockByNumber
contracts: []
```

```yaml [trace_block]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks: // [!code focus]
    - network: ethereum
      start_block: 18600000
      end_block: 18718056
      method: trace_block // [!code focus]
contracts: []
```

```yaml [default]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks: // [!code focus]
    - network: ethereum
      start_block: 18600000
      end_block: 18718056 
contracts: []
```

:::

### Multiple Networks

You can have multiple networks, this is useful if you must track native balances across a variety of networks.

```yaml [rindexer.yaml]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks: // [!code focus]
  - name: ethereum // [!code focus]
    chain_id: 1 // [!code focus]
    rpc: https://mainnet.gateway.tenderly.co // [!code focus]
  - name: base // [!code focus]
    chain_id: 8453 // [!code focus]
    rpc: https://base.gateway.tenderly.co // [!code focus]
storage:
  postgres:
    enabled: true
native_transfers: // [!code focus]
  networks: // [!code focus]
    - network: ethereum // [!code focus]
      start_block: 18600000
      end_block: 18718056
    - network: base // [!code focus]
      start_block: 18118056
      end_block: 18918056
```

## reorg_safe_distance

Reorgs can happen on the chain, this is when a block is removed from the chain and replaced with another block.
This can cause issues with the indexer indexed state if you turn `reorg_safe_distance` on it will keep a safe distance from the live
latest block to avoid any reorg issues.

Note if you are doing live indexing you will need to handle more advanced reorgs, support for advanced reorgs
is in the backlog for rindexer.

:::info
This is optional if you do not provide this it will index the latest blocks as soon as it is available for `debug`
indexing by the provider.
:::

```yaml [rindexer.yaml]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
  - name: base
    chain_id: 8453
    rpc: https://base.gateway.tenderly.co
native_transfers: // [!code focus]
  networks:
    - network: ethereum
    - network: base
  reorg_safe_distance: true // [!code focus]
contracts: []
```

## generate_csv

If you wish to generate a CSV file of the indexed data you can turn this on. This will be ignored if you do not have
the CSV storage enabled. By default if this is not supplied and the CSV storage is enabled it will generate a CSV file.

:::info
This is optional if you do not provide this it will generate a CSV file if the CSV storage is enabled.
:::

```yaml [rindexer.yaml]
name: rIndexer
description: My native transfers rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
native_transfers: // [!code focus]
  networks:
    - network: ethereum
  generate_csv: true // [!code focus]
contracts: []
```

## streams

The stream options for `native_transfers` is equivalent to contract event indexing with one exception.

All streams provided will have the `NativeTransfer` event enabled by default, so it does not need to be explicitly defined
unless special logic (e.g. aliasing event names) is desired.

:::info
You can configure streams to stream the data to other services, this is useful if you want to use other services
to index the data. You can read more about it [here](/docs/start-building/streams).
:::

## Simple stream definition

:::info
Notice we ___don't___ define `events` under the `topics` in this SNS stream example.
:::

That is because rindexer knows the single `NativeTransfer` event should be included by default.

```yaml [rindexer.yaml]
name: indexer
description: rindexer native transfers demo
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
      reorg_safe_distance: true
  streams:
    sns:
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
      topics: // [!code focus]
        - topic_arn: arn:aws:sns:us-east-1:000000000000:ethereum-transfers // [!code focus]
          networks: // [!code focus]
            - ethereum // [!code focus]
contracts: []
```

## Explicit `events` definition

In this case, we want to explicitly configure the stream processing for the event.

:::info
In order to add additional stream-event config, we **MUST** define the event with the name `NativeTransfer`.
:::

```yaml [rindexer.yaml]
name: indexer
description: rindexer native transfers demo
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
      reorg_safe_distance: true
  streams:
    sns:
      aws_config:
        region: us-east-1
        access_key: ${AWS_ACCESS_KEY_ID}
        secret_key: ${AWS_SECRET_ACCESS_KEY}
      topics:
        - topic_arn: arn:aws:sns:us-east-1:000000000000:ethereum-transfers
          networks:
            - ethereum
          events: // [!code focus]
            - event_name: NativeTransfer // [!code focus]
              alias: ETHTransfer // [!code focus]
contracts: []
```

## chat

You can configure chat to send messages You can read more about it [here](/docs/start-building/chatbots).
</file>

<file path="documentation/docs/pages/docs/start-building/yaml-config/networks.mdx">
# networks

Networks YAML config describes the networks you wish to enable.

:::info
You can have multiple networks in a single YAML file.
:::

## Fields

### name

The name of the network it should be unique to the YAML so you can not have 2 networks with the same name in the same
YAML file.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum // [!code focus]
```

### chain_id

The chainId of the network.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1 // [!code focus]
```

### rpc

The rpc url for the network.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co // [!code focus]
```

You can use [erpc](https://rindexer.xyz/docs/references/rpc-node-providers#rpc-proxy-and-caching) for load-balancing between multiple rpc endpoints (with failover, re-org aware caching, auto-batching, rate-limiters, auto-discovery of node providers, etc.)

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: http://erpc:4000/main/evm/1 // [!code focus]
```

We advise using environment variables for the rpc url to avoid checking in sensitive information.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: ${ETHEREUM_RPC} // [!code focus]
```

You can read more about environment variables in the [Environment Variables](/docs/start-building/yaml-config#environment-variables) section.

### max_block_range

:::info
This field is optional and will slow down indexing if applied, rindexer is fastest when you use a RPC provider who can predict the next block ranges when fetching logs.
You can read a bit more about RPC providers [here](/docs/references/rpc-node-providers#rpc-node-providers).
:::

Set the max block range for the network, this means when rindexer is fetching logs it will not fetch more than the max block range per request.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  max_block_range: 10000 // [!code focus]
```

### block_poll_frequency

:::info
This field is optional and may slow down indexing if applied, this is an advanced setting to be used with caution.
:::

Set the block poll frequency for the network, this allows making a trade-off between RPC use and live indexing speed. The
default setting will aggressively poll new blocks to ensure that we index as quickly as possible.

This is not always wanted, and you can choose configure to use an rpc "optimized" version, or manually define the millisecond
polling rate per network, or alternatively, manually define a factor of the polling rate.

:::code-group

```yaml [rapid (default)]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  block_poll_frequency: rapid // [!code focus] # This will rapid-poll, roughly every ~50ms.
```

```yaml [optimized]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  block_poll_frequency: optimized // [!code focus] # Reduce RPC call volume (at the cost of slightly slower indexing) whilst still aiming to be a non-human-noticeable indexing lag.
```

```yaml [division]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  block_poll_frequency: "/3" // [!code focus] # At a 12s blocktime, this will poll around every 4s, i.e. `12s / 3`.
```

```yaml [millis]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  block_poll_frequency: 1000 // [!code focus] # Poll every 1000ms for the network
```

:::

### compute_units_per_second

:::info
This field is optional
:::

The compute units per second for the network.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  compute_units_per_second: 660 // [!code focus]
```

### get_logs_settings

:::info
This field is optional. It is an advanced setting to be used with caution.
:::

Advanced configuration options that allow fine-grained control of event fetching logic.

#### address_filtering

Specifies how events that require address filtering (one that use either address filter or factory filter) are fetched from the network.

Can be one of:
- with `max_address_per_get_logs_request` configuration _(default behaviour)_ - events are fetched with addresses filter, log fetching happens in batches that consist of addresses chunks up to the specified value. Useful when events are often happening, but there is no huge number of addresses that are being watched for. The default value is 1000 addresses, which fits most of the RPC provider limits.
- `in-memory` - all matching events are fetched and then filtered in memory by addresses. Useful when events are not happening often, but there are a huge number of addresses that are being watched for.

:::code-group

```yaml [with max_address_per_get_logs_request]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  get_logs_settings:
    address_filtering:
      max_address_per_get_logs_request: 100000
```

```yaml [in-memory]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  get_logs_settings:
    address_filtering: "in-memory"
```

:::

### disable_logs_bloom_checks

:::warning
This field is optional and should only be turned on if you know what you are doing.

You should only enable this if you are using a chain which does not have support for logs blooms. Logs blooms
allow you to be able to skip calling `eth_getLogs` on blocks which do not contain the events you care about,
this is a huge performance gain for the indexer alongside a saving on the RPC bill.

If you are using a chain which does not support logs blooms you can enable this to skip the bloom checks.
:::

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  disable_logs_bloom_checks: true // [!code focus]
```

### reth

:::warning
Reth mode requires running a Reth archive node and is intended for advanced users. For more information on setting up Reth, visit [reth's official documentation](https://reth.rs/run/ethereum).
:::

Configure rindexer to use a local reth node for indexing. This provides direct connection to Reth with minimal latency and native reorg handling.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co  # Fallback RPC
  reth: // [!code focus]
    enabled: true // [!code focus]
    logging: true // [!code focus]  # Show Reth logs in stdout
    cli_args: // [!code focus]
      - "--datadir /data/reth" // [!code focus]
      - "--http" // [!code focus]
      - "--full false" // [!code focus]  # Archive mode
      - "--authrpc.jwtsecret /path/to/jwt.hex" // [!code focus]
```

#### enabled

Enable or disable the reth integration for this network.

#### logging

Show Reth logs in stdout (useful for debugging).

#### cli_args

Array of Reth CLI arguments in "flag value" format. Common arguments include:

- `--datadir`: Path to the reth data directory
- `--authrpc.jwtsecret`: Path to the JWT secret file for authenticated RPC
- `--authrpc.port`: The port for the auth RPC server (default: 8551)
- `--full`: Whether to run as a full node (use `false` for archive node)
- `--http`: Enable HTTP RPC server

## Multiple Networks

You can have as many networks as you want in the YAML file.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks: // [!code focus]
- name: ethereum // [!code focus]
  chain_id: 1 // [!code focus]
  rpc: https://mainnet.gateway.tenderly.co // [!code focus]
- name: base // [!code focus]
  chain_id: 8453 // [!code focus]
  rpc: https://mainnet.base.org // [!code focus]
```
</file>

<file path="documentation/docs/pages/docs/start-building/yaml-config/top-level-fields.mdx">
# Top level fields

The top-level fields of the YAML configuration file.

## name

The name of the project

```yaml [rindexer.yaml]
name: rETHIndexer // [!code focus]
```

## description

:::info
This field is optional
:::

The description of the project

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project // [!code focus]
```

## repository

:::info
This field is optional
:::

The repository of the project

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer // [!code focus]
```

## environment_path

By default rindexer will load the environment variables from the `.env` file in the root of the project.

You can override this by providing the path to the environment file you wish to use.

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
environment_path: "../../.env" // [!code focus]
```

## project_type

The rindexer project type

### no-code

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code // [!code focus]
```

OR

### rust

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust // [!code focus]
```

## config

More advanced opt-in configuration parameters. [See more details](/docs/start-building/yaml-config/config).


### no-code

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
config:
  buffer: 2 // [!code focus]
  callback_concurrency: 4 // [!code focus]
```

## timestamps

Enable block timestamps for all events on all networks. Timestamps are `disabled` by default.

Includes timestamps in all rindexer logs. Any logs with timestamps already included are ignored, and we
try to be as efficient as possible when fetching timestamps by first using fixed ranges, then precalculated, and
lastly fallback to RPC requests when required.

Timestamps can also be opted into for specific events. [See more details](/docs/start-building/yaml-config/contracts).

```yaml [rindexer.yaml]
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
timestamps: true
```
</file>

<file path="documentation/docs/pages/docs/start-building/add.mdx">
# Add

These commands allow you to through the CLI add elements to your YAML file.

## Contract

:::warning
You must have networks setup in the YAML to be able to use this command. You can see how to do that [here](/docs/start-building/yaml-config/networks).
:::

This allows you download the contracts metadata alongside the ABI from an supported network using Etherscan APIs.
This uses a shared Etherscan API key to try to download the ABIs, this means you will get rate limited if you use this too much
or if many people use this at the same time. You can add your own API key [here](/docs/start-building/yaml-config/global#etherscan_api_key).

You can see all the chains you can download ABIs from [here](https://docs.etherscan.io/contract-verification/supported-chains).

```bash
rindexer add contract
```

it then ask you 2 questions:

1. "Enter Network Name" - This is the network name in your YAML file.

```yaml
networks:
  - name: ethereum // [!code focus]
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
```

This will skip for you if you only have one network setup.

2. "Enter Contract Address" - This is the contract address you want to add to your YAML project

It will then download the ABI and put it in the `abis` folder and map it automatically in your YAML file.

Some things to know:
- If the contract is not verified on Etherscan it will not be able to download the ABI.
- If the contract is a proxy it try to download the ABI of the implementation contract.
</file>

<file path="documentation/docs/pages/docs/start-building/codegen.mdx">
# Codegen

You can generate a few different types of code when using rindexer codegen.

## GraphQL

You can generate .graphql prebuilt queries to get up and running in seconds.
These will be generated in a `queries` folder in the root of where the rindexer yaml is.

```bash
rindexer codegen graphql
```

By default it will point to `http://localhost:3001` graphql endpoint, you can change this by passing the endpoint flag:

```bash
rindexer codegen graphql --endpoint=YOUR_GRAPHQL_API_URL
```

### TypeScript

[graphql-codegen](https://the-guild.dev/graphql/codegen) is the best tool on the market to generate TypeScript typings for your GraphQL queries, mutations, and subscriptions.

learn about the `codegen.ts` config [here](https://the-guild.dev/graphql/codegen/docs/config-reference/codegen-config)

the graphql API url is the `schema` in the config, you can set this to your graphql endpoint like so:

```ts
import { CodegenConfig } from '@graphql-codegen/cli'
 
const config: CodegenConfig = {
  // this is YOUR_GRAPHQL_API_URL // [!code focus]
  schema: 'http://localhost:3001/graphql', // [!code focus]
  ...
}
 
export default config
```

then how you hook up the config with your tool of choice, below are some links to documentation:

- React Apollo - https://the-guild.dev/graphql/codegen/plugins/typescript/typescript-react-apollo#with-react-hooks
- React Query - https://the-guild.dev/graphql/codegen/plugins/typescript/typescript-react-query
- Node app - https://the-guild.dev/graphql/codegen/plugins/typescript/typescript-urql

### .NET, Dart, Java, Flow

codegen for other languages can be found [here](https://the-guild.dev/graphql/codegen)

## Typings

:::info
This feature is only available for Rust projects. Read the indepth guide [here](/docs/start-building/rust-project-deep-dive)
:::

When creating a new rust project with rindexer it will create you a typings folder, this has pretty advanced
typings for all your contracts, events and network information. This is generated from the ABIs you provide in the YAML configuration file.
This folder is not meant to be manually edited and should always be generated using codegen.

You can regenerate the typings folder by running the following command:

:::info
rindexer tries to be as smart as possible when it comes to updating the typings based on the `rindexer.yaml`,
it will resolves as much as it can without needing a regenerate but like any codegen tool if you
change certain aspects it does need to be regenerated.

if you change any of these properties in the `rindexer.yaml` file it will need to be regenerated:

- [indexer name](/docs/start-building/yaml-config/top-level-fields#name)
- anything in the [network](/docs/start-building/yaml-config/networks) section including adding and removing networks
- enabling or disabling a new [storage provider](/docs/start-building/yaml-config/storage)
- changing the [contract name](/docs/start-building/yaml-config/contracts#name)
- changing from [address](/docs/start-building/yaml-config/contracts#address)
contract indexing to [filter](/docs/start-building/yaml-config/contracts#filter) indexing or vice versa
- changing the contract [ABI](/docs/start-building/yaml-config/contracts#abi)
- anything in the [global](/docs/start-building/yaml-config/global) section

Also if you do regenerate your indexer files may need to be updated to match the new typings, you can manually migrate them or
generate them again using [indexer codegen command](/docs/start-building/codegen#indexers)
:::

```bash
rindexer codegen typings
```

## Indexers

:::info
This feature is only available for Rust projects. Read the indepth guide [here](/docs/start-building/rust-project-deep-dive)
:::

When creating a new rust project with rindexer it will create you a indexers folder, this is where you will write
your custom logic for the indexer. This is where you will do all your indexing logic, you can do anything you want
in here, you can do http requests, on chain lookups, custom logic, custom DBs, anything you can think of. rindexer gives you
the foundations and also baked in extendability. Rust enforces a strong type system, all logs will be streamed to you
just focus on the logic you want.

By default if you turn storage postgres on in the YAML configuration file it will also create you postgres tables,
also write SQL for you to use and expose you a postgres client. This is a great starting point for you to build on.

The tables creation can be skipped by using the [disable_create_tables](docs/start-building/yaml-config/storage#disable_create_tables)
in the YAML configuration file.

If you also enable the CSV storage it will also generate code in the handler to write to that CSV files.

You can regenerate the indexers folder by running the following command, please note this will overwrite
any custom logic you have written if you run it on an existing project.

```bash
rindexer codegen indexers
```
</file>

<file path="documentation/docs/pages/docs/start-building/delete.mdx">
# Delete

This allows you to delete data from the postgres database or csv files.
This is useful if you want to start fresh and start indexing again or if you updated an ABI and want to drop the tables and start over.

:::warning
Once confirmed this can not be undone.
:::

```bash
rindexer delete
```

## Example

```bash
rindexer delete

This will delete all data in the postgres database and csv files for the project at: /Users/jackedgson/Development/av..._demo_cli
This operation can not be reverted. Make sure you know what you are doing.

Are you sure you wish to delete the database data (it can not be reverted)? [yes, no]: yes 

Successfully deleted all data from the postgres database

Are you sure you wish to delete the csv data (it can not be reverted)? [yes, no]: yes

Successfully deleted all csv files.
```
</file>

<file path="documentation/docs/pages/docs/start-building/health-monitoring.mdx">
# Health Monitoring

Rindexer includes a comprehensive health monitoring system that provides real-time insights into the status of your indexing infrastructure. This built-in monitoring helps you ensure your indexers are running smoothly and quickly identify issues when they occur.

## Overview

The health monitoring system tracks the status of key components:

- **Database connectivity** - PostgreSQL connection health
- **Indexing status** - Whether the indexer is running and active task count
- **Sync status** - Data synchronization health across storage backends
- **Overall system health** - Aggregated status across all components

## Health Server

The health monitoring server runs automatically alongside your rindexer instance on a separate port. By default, it runs on port `8080`, but this can be configured.

### Starting the Health Server

The health server starts automatically when you run rindexer with indexing enabled. No additional configuration is required.

```bash
# Health server starts automatically with indexing
rindexer start indexer
rindexer start all
```

### Health Endpoints

#### GET /health

Returns the complete health status of your rindexer instance.

**Response Format:**

```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00Z",
  "services": {
    "database": "healthy",
    "indexing": "healthy",
    "sync": "healthy"
  },
  "indexing": {
    "active_tasks": 2,
    "is_running": true
  }
}
```

**HTTP Status Codes:**
- `200 OK` - System is healthy
- `503 Service Unavailable` - System has issues

## Health Status Types

The health endpoint returns different status types for each service:

| Status | Description |
|--------|-------------|
| `healthy` | Service is functioning normally |
| `unhealthy` | Service has encountered an error |
| `unknown` | Status cannot be determined |
| `not_configured` | Service is not set up |
| `disabled` | Service is intentionally disabled |
| `no_data` | Service is working but no data is available |
| `stopped` | Service is not running |

## Service Health Checks

### Database Health Check

The database health check verifies PostgreSQL connectivity and functionality:

- **`healthy`**: PostgreSQL is enabled and a simple `SELECT 1` query succeeds
- **`unhealthy`**: PostgreSQL is enabled but the connection fails or query errors occur
- **`not_configured`**: PostgreSQL is enabled but no database client is available
- **`disabled`**: PostgreSQL is not enabled in the configuration

**What it checks**: Basic database connectivity by executing `SELECT 1` against the PostgreSQL instance.

### Indexing Health Check

The indexing health check monitors the indexer process state:

- **`healthy`**: The indexer is currently running (system state flag is set)
- **`stopped`**: The indexer is not running (system state flag is not set)

**What it checks**: The global `IS_RUNNING` flag that tracks whether the indexer process is active.

### Sync Health Check

The sync health check verifies data synchronization status based on your storage configuration:

**For PostgreSQL storage:**
- **`healthy`**: Database has event tables (excluding system tables like `latest_block`, `*_last_known_*`, `*_last_run_*`)
- **`no_data`**: No event tables exist yet (acceptable for new deployments)
- **`unhealthy`**: Database query fails or connection issues
- **`not_configured`**: No database client available

**For CSV storage:**
- **`healthy`**: CSV directory exists and contains `.csv` files
- **`no_data`**: CSV directory doesn't exist or contains no `.csv` files
- **`unhealthy`**: CSV directory exists but cannot be read
- **`not_configured`**: CSV storage not configured

**What it checks**:
- **PostgreSQL**: Queries `information_schema.tables` to find user-created event tables
- **CSV**: Checks if the CSV directory exists and contains CSV files

### Overall Health Status

The overall health status is determined by combining all service checks:

- **`healthy`**: All critical services are healthy, or sync shows `no_data` (acceptable for new deployments)
- **`unhealthy`**: Any critical service is `unhealthy`, `not_configured`, or indexing is `stopped`

**Critical services**: Database, Indexing, and Sync (when enabled)

## Health Server Lifecycle

The health server's lifecycle depends on which services you start:

### `rindexer start indexer` (with end_block set)
- **Short-lived**: Health server starts with the indexer and **dies when indexing completes**
- **Use case**: Historical data indexing that has a defined end point
- **Health monitoring**: Only available during the indexing process

### `rindexer start indexer` (no end_block set)
- **Long-lived**: Health server starts with the indexer and **stays alive for live indexing**
- **Use case**: Continuous live indexing that runs indefinitely
- **Health monitoring**: Available continuously while the indexer is running

### `rindexer start graphql`
- **No health server**: Health server is **not started** in GraphQL-only mode
- **Use case**: Running only the GraphQL API without indexing
- **Health monitoring**: Not available (health server requires indexing to be enabled)

### `rindexer start all`
- **Long-lived**: Health server starts with the indexer and **follows the GraphQL server lifecycle**
- **Use case**: Running both indexing and GraphQL API together
- **Health monitoring**: Available as long as the GraphQL server is running

## Configuration

### Custom Health Port

You can configure the health server port using the `health_port` setting in your `rindexer.yaml` file:

```yaml
global:
  health_port: 8081
```

## Production Monitoring

### Load Balancer Health Checks

Configure your load balancer to use the health endpoint for health checks:

```
Health Check URL: http://your-rindexer-instance:8080/health
Expected Status: 200 OK
```

### Monitoring Tools

You can integrate with monitoring tools like Prometheus, Grafana, or DataDog to track health metrics and set up alerts based on HTTP status codes and response times.

## Troubleshooting

### Common Issues

- **Health server not starting**: Check if port is in use, verify YAML configuration
- **Database health failing**: Verify PostgreSQL connection and permissions
- **Sync health issues**: Check storage configuration and file permissions

### Debugging

Enable debug logging for detailed health information:

```bash
RUST_LOG=debug rindexer start indexer
```

## Best Practices

- Set up continuous monitoring of the health endpoint
- Configure appropriate alert thresholds
- Keep health check logs for troubleshooting
- Monitor multiple instances if running in a cluster

## API Reference

### Health Endpoint

- **URL**: `GET /health`
- **Response**: JSON with health status and service information
- **Status Codes**: 200 (healthy), 503 (unhealthy)
</file>

<file path="documentation/docs/pages/docs/start-building/live-indexing-and-historic.mdx">
# Historic indexing

If you want to index only historic data between block ranges just put in the [start_block](/docs/start-building/yaml-config/contracts#start_block)
and [end_block](/docs/start-building/yaml-config/contracts#end_block) in the YAML configuration file. This will index
only the data between those blocks.

:::info
rindexer will save the last synced block for each contract in the database so it can pick up where it left off if stopped and started again.
If you want to start fresh you can use the [delete](/docs/start-building/delete) command to drop all the data and start over.
You can also use the [drop_each_run](/docs/start-building/yaml-config/storage#drop_each_run) option in the YAML configuration file to drop all the data for the indexer before starting.
:::

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH // [!code focus]
    details: // [!code focus]
    - network: ethereum // [!code focus]
      address: "0xae78736cd615f374d3085123a210448e74fc6393" // [!code focus]
      start_block: 18600000 // [!code focus]
      end_block: 18718056 // [!code focus]
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
    - Transfer
    - Approval
```

# Live indexing

If you want to index live data you can just remove the [start_block](/docs/start-building/yaml-config/contracts#start_block)
and [end_block](/docs/start-building/yaml-config/contracts#end_block) from the YAML configuration file. This will index
from the latest block and then index all new blocks as they come in.

:::info
Important to know this will NOT track last synced block and when you start and stop the indexer it will start from the latest block.
:::

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH // [!code focus]
    details: // [!code focus]
    - network: ethereum // [!code focus]
      address: "0xae78736cd615f374d3085123a210448e74fc6393" // [!code focus]
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
    - Transfer
    - Approval
```

# Historic and live indexing

If you want to index historic data and then live data you can put in the [start_block](/docs/start-building/yaml-config/contracts#start_block)
you wish to index the data from and then remove the [end_block](/docs/start-building/yaml-config/contracts#end_block) from the YAML configuration file. This will index
from the block you specified and then index all new blocks as they come in.

:::info
rindexer will save the last synced block for each contract in the database so it can pick up where it left off if stopped and started again.
If you want to start fresh you can use the [delete](/docs/start-building/delete) command to drop all the data and start over.
You can also use the [drop_each_run](/docs/start-building/yaml-config/storage#drop_each_run) option in the YAML configuration file to drop all the data for the indexer before starting.
:::

```yaml
name: rETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: RocketPoolETH // [!code focus]
    details: // [!code focus]
    - network: ethereum // [!code focus]
      address: "0xae78736cd615f374d3085123a210448e74fc6393" // [!code focus]
      start_block: 18600000 // [!code focus]
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
    - Transfer
    - Approval
```
</file>

<file path="documentation/docs/pages/docs/start-building/phantom.mdx">
# Phantom events

Phantom events enable you to add custom events or modify existing events for any smart contract.
This feature allows you to extract any data you need from smart contracts without the constraints of on-chain modifications.

## What are Phantom Events?
Phantom events are gasless events logged in an off-chain execution environment that mirrors the mainnet state in real-time.
They provide a solution for obtaining additional information from a contract without incurring extra gas costs for users.

## Why Use Phantom Events?

Whether you don't control the contract or want to avoid additional gas costs, phantom events offer a flexible solution
for diverse data needs and use cases. Powered by [Shadow](https://www.shadow.xyz/) and [dyRPC](https://ui.dyrpc.network/),
they are part of the rindexer suite designed to simplify being able to use these powerful features.

## Getting Started with Phantom Events
rindexer abstracts away the complexity and offers first-party support for implementing phantom events.
It utilizes Etherscan APIs to download source code and ABIs for the contracts you want to index.
Note that the shared Etherscan API key may lead to rate limits if heavily used.
To avoid this, we recommend adding your own API key here [here](/docs/start-building/yaml-config/global#etherscan_api_key).

Right let's get started with phantom events.

## Providers

:::warning
To use phantom events you will need to have a provider, rindexer just offers first-party support for implementing phantom events.
:::

### [Shadow](https://www.shadow.xyz/)

Shadow enables you to modify a deployed contract's source code to add gasless custom event logs and view functions on a shadow fork that
is instrumented to mirror mainnet state in realtime.

#### Networks Supported

- Ethereum

### [dyRPC](https://ui.dyrpc.network/)

dyRPC is a tool built on top of overlay which can be ran on any erigon node and allows you to also modify the contract's source code
adding gasless custom event logs and view functions.

#### Networks Supported

- Ethereum

## Dependencies

### Installing Foundry

:::info
If you do not have `foundry` installed it will install it for you when you run the `init` command but we recommend you install it yourself.
:::

foundry is required to be installed to compile the contracts.

```bash
curl -L https://foundry.paradigm.xyz | bash
```

if you already have got foundry installed you can run `foundryup` to update it.

## Init

rindexer uses its CLI first approach for everything and phantom events behaves the same way. Each rindexer project by default
does not have phantom events enabled you have to set them up for each project.

To enable phantom events for your rindexer project you can run the following command:

```bash
rindexer phantom init
```

### Required information

- Shadow
    - API key (generate on the shadow portal)
    - Fork ID (generate on the shadow portal)
- dyRPC
    - API key (generate on the dyRPC portal or use "new" to generate a new one)

You will be asked to pick your provider and add your API key. It will save the API key
int the `.env` file under `RINDEXER_PHANTOM_API_KEY` in your project directory.
It will also add your phantom provider to the `rindexer.yaml` file.

## Clone

As the `rindexer.yaml` file is defined by you we use these contract names and network names to allow you to
easily understand what you are cloning.

:::info
Only verified contracts on Etherscan can be cloned, if you wish to use an unverified contract it will still work
but you will have to create the foundry project manually in the `phantom` folder.
:::

```bash
rindexer phantom clone --contract-name <CONTRACT_NAME> --network <NETWORK>
```

lets say we had a `rindexer.yaml` file like this:

```yaml
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
storage:
  postgres:
    enabled: true
    drop_each_run: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: '18600000'
    end_block: '18718056'
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
  - Transfer
```

to clone this contract you would run the following command

```bash
rindexer phantom clone --contract-name RocketPoolETH --network ethereum
```

This will create you a folder called `phantom` in the root of your rindexer project and also create the network name
to make it easier to find contracts you have cloned. for example in the above example it will create a folder called `phantom/ethereum/`
and inside it will have a folder called `RocketPoolETH/` which will have your solidity project files and the contract ABI.
This folder will contain all the phantom contracts you have cloned.

You can now go to the contracts folder and start making changes to the phantom contracts.

## Add your own event

Above we cloned `RocketPoolETH` on `ethereum` lets open up `RocketTokenRETH.sol` and add a phantom event on transfer hook.

```solidity
contract RocketTokenRETH is RocketBase, ERC20, RocketTokenRETHInterface {
    using SafeMath for uint;

    event EtherDeposited(address indexed from, uint256 amount, uint256 time);
    event TokensMinted(address indexed to, uint256 amount, uint256 ethAmount, uint256 time);
    event TokensBurned(address indexed from, uint256 amount, uint256 ethAmount, uint256 time);
    event PhantomTransferTime(address indexed from, uint256 time); // [!code focus]

    ...

    function _beforeTokenTransfer(address from, address, uint256) internal override {
        // emit your own event
        emit PhantomTransferTime(from, block.timestamp); // [!code focus]

        // Don't run check if this is a mint transaction
        if (from != address(0)) {
            // Check which block the user's last deposit was
            bytes32 key = keccak256(abi.encodePacked("user.deposit.block", from));
            uint256 lastDepositBlock = getUint(key);
            if (lastDepositBlock > 0) {
                // Ensure enough blocks have passed
                uint256 depositDelay = getUint(keccak256(abi.encodePacked(keccak256("dao.protocol.setting.network"), "network.reth.deposit.delay")));
                uint256 blocksPassed = block.number.sub(lastDepositBlock);
                require(blocksPassed > depositDelay, "Not enough time has passed since deposit");
                // Clear the state as it's no longer necessary to check this until another deposit is made
                deleteUint(key);
            }
        }
    }
```

That is it you can now compile and deploy your phantom contract which we will go over in the next section.

## Editing existing events

You can edit any event to whatever you want for example lets say we wanted to change `TokensMinted` to include
the new balance of the `to` address after minted.

```solidity
contract RocketTokenRETH is RocketBase, ERC20, RocketTokenRETHInterface {
    using SafeMath for uint;

    event EtherDeposited(address indexed from, uint256 amount, uint256 time);
    event TokensMinted(uint256 newBalance, address indexed to, uint256 amount, uint256 ethAmount, uint256 time); // [!code focus]
    event TokensBurned(address indexed from, uint256 amount, uint256 ethAmount, uint256 time);
    event PhantomTransferTime(address indexed from, uint256 time);

    ...

    function mint(uint256 _ethAmount, address _to) override external onlyLatestContract("rocketDepositPool", msg.sender) {
        // Get rETH amount
        uint256 rethAmount = getRethValue(_ethAmount);
        // Check rETH amount
        require(rethAmount > 0, "Invalid token mint amount");
        // Update balance & supply
        _mint(_to, rethAmount);
        // Emit tokens minted event
        emit TokensMinted(balanceOf(_to), _to, rethAmount, _ethAmount, block.timestamp); // [!code focus]
    }
```

That is it you can now compile and deploy your phantom contract which we will go over in the next section.

:::info
If editing different events on different networks (aka your indexing `RocketPoolETH` on ethereum as well as base)
your contract details should be separate for each network in the `rindexer.yaml` file,
as when you deploy the phantom contract it will remap the new ABI and if your events now do not match the types
it will error.
:::

## Compile

:::info
rindexer uses `foundry` to clone and compile the contracts.
:::

To compile the phantom contracts you can run the following command:

```bash
rindexer phantom compile --contract-name <CONTRACT_NAME> --network <NETWORK>
```

So using the same yaml example as above you would run the following command:

```bash
rindexer phantom compile --contract-name RocketPoolETH --network ethereum
```

This will show you the same compile errors as `foundry` would show you if you have made any mistakes.

## Deploy

Deploying your phantom contract is different to deploying a normal contract. rindexer will take care of uploading
the new phantom contract to the provider and all the mappings for you in the `rindexer.yaml` file.

```bash
rindexer phantom deploy --contract-name <CONTRACT_NAME> --network <NETWORK>
```

So using the same yaml example as above you would run the following command:

```bash
rindexer phantom deploy --contract-name RocketPoolETH --network ethereum
```

This will do a few things to your yaml file:

1. It will add the phantom network to the `rindexer.yaml` file this is always named `phantom_${NETWORK_NAME}_${CONTRACT_NAME}`

```yaml
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
- name: phantom_ethereum_RocketPoolETH // [!code focus]
  chain_id: 1 // [!code focus]
  rpc: PROVIDER_RPC // [!code focus]
...
```

2. It will change the `contracts` section to point the contract details to the phantom network.

```yaml
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
- name: phantom_ethereum_RocketPoolETH // [!code focus]
  chain_id: 1 // [!code focus]
  rpc: PROVIDER_RPC // [!code focus]
storage:
  postgres:
    enabled: true
    drop_each_run: true
contracts:
- name: RocketPoolETH
  details:
  - network: phantom_ethereum_RocketPoolETH // [!code focus]
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: '18600000'
    end_block: '18718056'
  abi: ./abis/phantom_ethereum_RocketPoolETH.abi.json
  include_events:
  - Transfer
...
```

3. It will upload the new ABI to your `abi` folder named the same as the network name but with the `.abi.json` extension.

```yaml
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
- name: phantom_ethereum_RocketPoolETH // [!code focus]
  chain_id: 1 // [!code focus]
  rpc: PROVIDER_RPC // [!code focus]
storage:
  postgres:
    enabled: true
    drop_each_run: true
contracts:
- name: RocketPoolETH
  details:
  - network: phantom_ethereum_RocketPoolETH // [!code focus]
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: '18600000'
    end_block: '18718056'
  abi: ./abis/phantom_ethereum_RocketPoolETH.abi.json // [!code focus]
  include_events:
  - Transfer
...
```

right now lets include the `PhantomTransferTime` event in our yaml file `include_events` array so we can index it.

```yaml
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
- name: phantom_ethereum_RocketPoolETH
  chain_id: 1
  rpc: PROVIDER_RPC
storage:
  postgres:
    enabled: true
    drop_each_run: true
contracts:
- name: RocketPoolETH
  details:
  - network: phantom_ethereum_RocketPoolETH
    address: "0xae78736cd615f374d3085123a210448e74fc6393"
    start_block: '18600000'
    end_block: '18718056'
  abi: ./abis/phantom_ethereum_RocketPoolETH.abi.json
  include_events:
  - PhantomTransferTime // [!code focus]
...
```

## Indexing

everything else is same as before so you can run `rindexer start all` the database tables will all be created,
indexer will start indexing and the GraphQL API will be available at http://localhost:3001/graphql.

:::info
Expect slower indexing as you will have to wait for the phantom provider to index the events.
All phantom providers at the moment use block ranges over optimal ranges so phantom events will be slower
than normal events.
:::

That is it you now have created phantom events with rindexer.
</file>

<file path="documentation/docs/pages/docs/start-building/running.mdx">
# Running

## No-code Project

You can run the no-code project really easily with the CLI toolset.

:::info
rindexer starts your postgres docker compose file up for you automatically if the DATABASE_URL can not connect to the database and docker-compose.yml is present in the parent directory.
You will need to make sure you have docker running on your machine before starting the project. If you have not got docker you can install it [here](https://docs.docker.com/get-docker/).
You can also run docker manually by using `docker compose up -d`.
:::

:::warn
graphql API can only be ran when you have a postgres storage setup in your YAML.
:::

:::code-group

```bash [indexer and graphql]
rindexer start all
```

```bash [indexer]
rindexer start indexer
```

```bash [graphql]
rindexer start graphql
```

:::

You can change the GraphQL port by doing --port [number] in both all and graphql commands above.

:::info
If you change your contract ABIs or want to start fresh you can use the [delete](/docs/start-building/delete) command to drop all the data and start over.
You can also use the [drop_each_run](/docs/start-building/yaml-config/storage#drop_each_run) option in the YAML configuration file to drop all the data for the indexer before starting.
:::

## Health Monitoring

When you start rindexer with indexing enabled, a health monitoring server automatically starts on port `8080`. This provides real-time insights into your indexing infrastructure status.

### Quick Health Check

Access the health endpoint at `http://localhost:8080/health` to get system status:

```json
{
  "status": "healthy",
  "services": {
    "database": "healthy",
    "indexing": "healthy", 
    "sync": "healthy"
  },
  "indexing": {
    "active_tasks": 2,
    "is_running": true
  }
}
```

### Health Server Lifecycle

- **`rindexer start indexer` (with end_block)**: Short-lived - dies when historical indexing completes
- **`rindexer start indexer` (no end_block)**: Long-lived - stays alive for live indexing  
- **`rindexer start graphql`**: No health server - health monitoring not available
- **`rindexer start all`**: Long-lived - follows GraphQL server lifecycle

:::info
For detailed health monitoring documentation, see the [Health Monitoring](/docs/start-building/health-monitoring) guide.
:::

## Rust Project

If you want to run this with docker support for the postgres first run:

```bash
docker compose up -d
```

Then to run the the rust project you can run the following command:

:::info
You are creating a rust rindexer project you should be wanting to change all of this logic to suit your needs.
Just like react create app exposes you to the boilerplate code to get you started, this is the same.
If you change the main.rs some of the arguments like --indexer and --graphql may not run with these
commands but as you would of changed it you will know how to run it.
:::

:::code-group

```bash [everything]
cargo run
```

```bash [indexer only]
cargo run -- --indexer
```

```bash [graphql only]
cargo run -- --graphql
```

:::

We also advise you in production to run your rust projects in release mode, you can run it in release mode using

```bash
cargo run --release
```

You can also do other fancy production builds with other frameworks like jemalloc and other flags, but we will leave that to you to explore.
</file>

<file path="documentation/docs/pages/docs/start-building/timestamps.mdx">
# Log block-timestamps

## What is the problem

A log result in the JSON-RPC spec does not always expose the block timestamp, which means it can require
another block lookup per each log to get the block timestamp. This is not efficient and can cause a big bottleneck in indexing.

:::info
But we want the best DX and so until all node implementation catch up, we have the following solutions.
:::

# How we handle it

## RPC Support for Log Timestamps

We want to be able to solve this effectively and have worked with node implementations like GETH and RETH to
include block timestamps in logs.

Providers and L2s will slowly begin rolling this out in their nodes over the coming months and years and soon this
problem will no longer exist.

## Delta run-length encoded

Delta run-length encoding is an effective way to support block-timestamps that are not necessarily sequential but
generally follow a pattern.

Most chains will have a roughly "fixed" block-time, and this can be used to encode the block-timestamps more efficiently
via "runs" of the delta between times.

This process requires more upfront-work and more storage/memory, but can be a great way to save on network requests and IO time.

We precompute chains and store the [highly compressed kB to MB scale binary files for hydration](https://github.com/joshstevens19/rindexer/tree/master/core/resources).

This is a manual process and is designed to optimize backfill operations but won't help for head-of-line indexing which
will still require a manual rpc call.

## Fixed timestamps chains

These are the simplest of the networks, it is the most extreme delta-run-length encoding and can therefore be
optimized even more.

Rather than storing "runs", we consider the whole chain to be a single "run" and can simply
calculate any timestamp for a block.

Due to the lack of any strong guarantee, we can only do this up to a "known" block number where
the fixed-timestamp consistency has been validated. If at any time a chain breaks this pattern
we must drop back to delta run length encoding.

## Sampled & Batched Lookups

if we don't have a precomputed or fixed-chain mapping here we will fall back to an optimised sampled
and batch RPC call per network.

We aim to minimize network round-trip time with optimal batch-sizes and concurrent requests for large block-ranges.

We also perform the lookup as soon as the logs are returned such that if there is any bottle-necking in the handler or
database write or stream processing, we will take advantage of that dead time.

**Loose time-ordering**

We also optionally allow users to configure a sample rate, this can additionally speed up worst-case scenario
network latency times by significantly reducing the data over the wire and RPC processing time. For example an RPC call
for 2 blocks is fast, but if we have a sparse event over 5,000 blocks in a single log response, we may have to make tens
or hundreds of concurrent and/or sequential calls to fetch them all.

Sampling helps minimize this by fetching 50 blocks at spaced intervals in a single batched RPC request and interpolating
the timestamps between those intervals. This can be a massive performance boost, at the cost of occasionally slight
inaccuracies in timestamps.

This should be opted-into based on your workload and requirements.

# Extending supported chains

To begin encoding a new chain's block-timestamps, you can run the following command:

```sh
cargo xtask encode-block-clock \
  --network 43114 \
  --rpc-url "https://avax-mainnet.g.alchemy.com/v2/API_KEY" \
  --batch-size 2000
```

This will encode and periodically flush data to the file `core/resources/blockclock/43113.blockclock` as per the above example.

Simply replace the network if and RPC url and run the command until it is complete. This will potentially consume a lot of
CU for your provider so be aware of this.
</file>

<file path="documentation/docs/pages/docs/changelog.mdx">
# Changelog

## Changes Not Deployed
-------------------------------------------------

### Features
-------------------------------------------------

- feat: `create_batch_clickhouse_operation` and `create_batch_postgres_operation` functions to do upsert/delete/insert tasks

### Bug fixes
-------------------------------------------------

### Breaking changes
-------------------------------------------------

## Releases
-------------------------------------------------

all release branches are deployed through `release/VERSION_NUMBER` branches


# 0.29.0-beta - 6th January 2026

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.29.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.29.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.29.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.29.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.29.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: upgrade to alloy 1.1.3
- feat: expose `RindexerEventStream` to allow subscribing to rindexer events

### Bug fixes
-------------------------------------------------
- fix: add missing --graphql and --indexer flags for 'rindexer start all
- fix: resolve the bad ANSI escape on the logger causing logs to render the ANSI escape incorrectly

### Breaking changes
-------------------------------------------------
- `IndexingDetails` now includes optional `event_stream` for subscribing to rindexer events

# 0.28.2-beta - 5th November 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.28.2

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.2/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.2/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.2/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.2/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: reorg safe range calculation for out-of-range an error message
- fix: Upgrade dependecy minor versions to get latest alloy fixes
- fix: gate kafka streams support behind `kafka` feature


# 0.28.1-beta - 21st October 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.28.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: allow https connections for clickhouse
- fix: resolve issue with "" on postgres insert


# 0.28.0-beta - 13th October 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.28.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.28.0/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: only log error when the current block lumber is lower than the last seen when range is outside chain reorg safe
- fix: unpin `tracing-subscriber` version

### Features
-------------------------------------------------
- feat: check if the RPC chain id is matching the configured chain id in the yaml config on startup
- feat: add support for `RINDEXER_LOG` environment variable to control the log level of rindexer
- feat: Add clickhouse integration to rindexer rust project and nocode

# 0.27.1-beta - 6th October 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.27.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.27.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.27.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.27.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.27.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: build break after alloy v2.10.0: remove NamedChain::PolygonZkEvm usage
- fix: resolved race condition in event dependencies indexing that caused issues on networks with high block production rates
- fix: support for the latest alloy


# 0.27.0-beta - 26th September 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.27.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.27.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.27.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.27.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.27.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: add health endpoint with comprehensive system status monitoring
- feat: add cloudflare queues to the streams

### Bug fixes
-------------------------------------------------
- fix: optimisations


# 0.26.0-beta - 12th September 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.26.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.26.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.26.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.26.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.26.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: expose PostgresClient::raw_connection so its easier to do transactions

### Bug fixes
-------------------------------------------------
- fix: start indexer and graphql when a rust project is started without any commands
- fix: take the correct default graphql endpoint when generating graphql files


# 0.25.3-beta - 8th September 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.25.3

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.3/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.3/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.3/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.3/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: `contract_address` Postgres column changed from `char(66)` to `char(42)`
- fix: `PostgresClient` now only exposes `insert_bulk` which handles internally whether to insert rows via INSERT or COPY
- fix: regenerated example projects to support latest changes
- fix: Index creation fails for filter contracts due to schema name mismatch

### Breaking changes
-------------------------------------------------
- `contract_address` Postgres column changed from `char(66)` to `char(42)`
- `PostgresClient` now only exposes `insert_bulk` which handles internally whether to insert rows via INSERT or COPY

# 0.25.2-beta - 30th August 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.25.2

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.2/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.2/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.2/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.2/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: inject block timestamp in the generate indexing code
- fix: resolve timestamp override mapping to contract breaking change
- fix: resolve bad logs


# 0.25.1-beta - 28th August 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.25.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: correct tuple wrapper slice indexing in map_ethereum_wrapper_to_json


# 0.25.0-beta - 27th August 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.25.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.25.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: Adds timestamp config to the yaml config, allowing users to opt-in to block timestamps in logs
- feat: Add chain_id to TxInformation struct
- feat: Add xtask with block timestamp encoding mechanism
- feat: Adds nocode postgres migration "versioning system"
- feat: Expose a rust project handler to process raw blocks with transactions if native indexing is enabled.
- feat: bump alloy to 1.0.27

### Bug fixes
-------------------------------------------------
- fix: compiler issue with solar-parse
- fix: Move rust_playground to examples
- fix: resolve telegram marketdownv2


# 0.24.1-beta - 20th August 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.24.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.24.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.24.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.24.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.24.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: graphql embedded binary

# 0.24.0-beta - 19th August 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.24.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.24.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.24.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.24.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.24.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------

- feat: support tuples and nested tuples as event inputs
- feat: bring graphql into the binary
- feat: support array in `input_name` factory filter configuration

### Bug fixes
-------------------------------------------------
- fix: requiring to install lsof when running the graphql

# 0.23.0-beta - 4th August 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.23.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.23.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.23.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.23.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.23.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: enhance filter conditions parsing and evaluation to handle complex expressions with logical operators
- feat: extend `is_known_zk_evm_compatible_chain` to some new chains

### Bug fixes
- fix: fix codegen for events with irregular width solidity integer types
- fix: logical operators precedence in filter conditions - https://github.com/joshstevens19/rindexer/issues/225

### Breaking changes
-------------------------------------------------
- `EthereumSqlTypeWrapper` `U64`, `U64Nullable` and `U64BigInt` are now a rust `u64` type
- `EthereumSqlTypeWrapper::VecU64` is now a rust `Vec<u64>` type
- `TxInformation` `block_number` and `transaction_index` are now a rust `u64` type


# 0.22.3-beta - 30th July 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.22.3

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.3/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.3/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.3/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.3/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: do not try to write streams file even when csv is off


# 0.22.2-beta - 30th July 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.22.2

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.2/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.2/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.2/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.2/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: remove custom panic handling as causing issues
- fix: too much noise on the last seen live logs


# 0.22.1-beta - 29th July 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.22.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: make rpc error logging only log out errors which are not already handled
- fix: generate proper snake case file names for factory contracts handlers
- fix: drop rindexer_internal.latest_block when drop_each_run is defined


# 0.22.0-beta - 25th July 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.22.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.22.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: add some extra logging extensions to see when rpcs are falling over
- feat: adding more logging when the node gets back since last seen block number
- feat: improve factory contract indexing by fully indexing factory contract event

### Bug fixes
-------------------------------------------------
- fix: Consolidate Async Runtimes - https://github.com/joshstevens19/rindexer/issues/271


# 0.21.2-beta - 16th July 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.21.2

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.2/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.2/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.2/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.2/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: resolve build issues with reth dependencies, and feature gate reth dependencies to avoid bloating the binary size


# 0.21.1-beta - 16th July 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.21.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.1/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: Add a `Nullable` Numeric256 SQL type, `Nullable` DateTime SQL type and Uuid SQL type

### Bug fixes
-------------------------------------------------
- fix: resolve issues with updating last seen block even if the event failed


# 0.21.0-beta - 15th July 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.21.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.21.0/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: resolve multi-network dependency issues


# 0.20.0-beta - 10th July 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.20.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.20.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.20.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.20.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.20.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: support reth natively in rindexer https://rindexer.xyz/docs/start-building/create-new-project/reth-mode + https://rindexer.xyz/docs/advanced/using-reth-exex

- Indexing with `eth_getBlockByNumber` for more efficiency (but still retain all debug/trace logic for future options)
- Add tx receipt endpoint as useful prep work for raw transaction indexing option
- Add an ever-so-slightly backpressured queue per "network-event" for super fair scheduling, decreased write contention on the database, and improved memory utilisation, can use 1/4 of memory with no throughput drop in some cases.

### Bug fixes
-------------------------------------------------
- fix: https://github.com/ethereum/go-ethereum/pull/31876 changed max address per logs to 1000 to be aligned with geth

- Use correct provider base which is `AnyProvider` that can handle optimism, rollups, and all evm chain style responses.
- Misc tweaks to optimise how this native transfer fetching is done with batching, and pass rpc provider to allow for better batching in other endpoints
- Remove the permits system entirely, it's had some serious problems with fair distribution (where some events would consistenyl take more permits than others)
- We weren't writing to the db if no events were found in a block range... this was a major problem for highly infrequent events as rindexer wouldn't write the "last seen block", fixed.
- Fixed a pretty major bug with the "optimal log parsing regex" (where the BlockNumber::from_str() wasn't actually parsing the hex, so it silently failed and used sub-optimal fallbacks)
- Fix indexer codegen to use correct names and types, as well as fix formatting problems where rustfmt couldn't parse the nesting we were doing
- Other memory optimisations

# 0.19.1-beta - 17th June 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.19.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.19.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.19.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.19.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.19.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: Fix code generation for complex event names (with `_` separator)

# 0.19.0-beta - 17th June 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.19.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.19.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.19.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.19.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.19.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: Numeric U256 Array EthereumSqlTypeWrapper
- feat: U64BigInt and I256Numeric

### Bug fixes
-------------------------------------------------
- fix: Fix dependency events handling after regression introduced with factory filtering

# 0.18.0-beta - 13th June 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.18.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.18.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.18.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.18.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.18.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------

- Add support for indexing events from a factory-deployed smart contract by introducing a `factory` filter option in `contract` configuration.

# 0.17.3-beta - 9th June 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.17.3

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.3/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.3/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.3/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.3/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------

- Improve RPC Efficiency
- Make Native Transfer indexing a little more gently on startup, and a bit slower on failure, to prevent overloading apis with concurrency
- Improve Startup speed significantly by reducing lots of redundant sequential rpc calls
- Respect native transfer `enabled: false` setting
- Add some more debug logging, spans, and change some log levels

### Breaking changes
-------------------------------------------------

- Add  `block_poll_frequency` to the yaml config to allow better control over block polling behavior. This will require re-running codegen for rust projects as it breaks the existing `create_client` interface.

## 0.17.2-beta - 30th May 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.17.2

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.2/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.2/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.2/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.2/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------

- fix: await register call in event handlers generated on rust projects
- fix: resolve foundry compiler mismatch version

## 0.17.1-beta - 27th May 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.17.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------

- Resolve Docker image crashes with "Illegal instruction (core dumped)" on various x86-64 processors

## 0.17.0-beta - 26th May 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.17.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.17.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------

- Alter full toolchain to stable, drop some nightly rustfmt options and re-run fmt.
- Improve logging experience by including more `errors` and `network` info where possible.
- Add a currently undocumented `CONTRACT_PERMITS` env var to control manually the concurrency via an env var.
- Add `HasTxInformation` trait to allow working with generics over Network Contract Events.

### Bug fixes
-------------------------------------------------

- Fix a bug with broken binary copy in the Postgres client. `finish` should be called manually on a bad write.
- Fix bug with breaking out of historical indexing on log fetch error.
- Fix native transfer indexing bug by adjusting the reorg safe condition to be correct
- Fix `fetch_logs` block range parsing to include fallback string if no Err variant found (fixes Lens indexing)

all release branches are deployed through `release/VERSION_NUMBER` branches

## 0.16.1-beta - 20th May 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.16.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.16.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.16.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.16.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.16.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: alloy stable + alloy dependency mismatch

## 0.16.0-beta - 6th May 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.16.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.16.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.16.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.16.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.16.0/rindexer_win32-amd64.zip

### Breaking changes
-------------------------------------------------
- alloy migration for rust projects - https://rindexer.xyz/docs/start-building/rust-project-deep-dive/ethers-alloy-migration

## 0.15.5-beta - 24th April 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.15.5

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.5/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.5/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.5/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.5/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: native trace patches for rust projects

## 0.15.4-beta - 8th April 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.15.4

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.4/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.4/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.4/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.4/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: issue if your yaml contracts or event names are too long - postgres max length is 63 char but it doesnt fail meaning last indexed block number is never snapshotted

## 0.15.3-beta - 27th March 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.15.3

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.3/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.3/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.3/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.3/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: rust project generation


## 0.15.2-beta - 27th March 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.15.2

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.2/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.2/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.2/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.2/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: resolve issue when event is on the last block indexed

## 0.15.1-beta - 26th March 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.15.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.1/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: allow manifest config to define the rpc method used for native-transfer indexing

### Bug fixes

- fix: resolve ubuntu to build on version 22.0.4 due to GLIBC_2.39 issues on 22.4

## 0.15.0-beta - 25th March 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.15.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.15.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: improve speed of indexing with a few optimisations
- feat: native transfer indexing

## 0.14.0-beta - 6th March 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.14.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.14.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.14.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.14.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.14.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: add support for alias in streams

## 0.13.0-beta - 20th Feb 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.13.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.13.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.13.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.13.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.13.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: add a `U64Nullable` type to handle zero values
- feat: support custom AWS endpoints
- feat: forward event signature for all events streamed

## 0.12.0-beta - 1st Jan 2025

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.12.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.12.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.12.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.12.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.12.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: Redis Streams

### Bug fixes
-------------------------------------------------
- fix: issue two's complement on large u256 values
- fix: resolve building the project on a PR branch
- fix: resolve numeric type parsing issues on out of range rust decimals

## 0.11.3-beta - 28th December 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.11.3

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.3/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.3/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.3/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.3/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: index throws on multiple relationships with same input name

## 0.11.2-beta - 19th December 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.11.2

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.2/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.2/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.2/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.2/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: cors when accessing rindexer graphql api from different host

### Breaking changes
-------------------------------------------------
- rindexerdown and then `curl -L https://rindexer.xyz/install.sh | bash` to reinstall rindexer

## 0.11.1-beta - 10th December 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.11.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: resolve bad typings generated for decoding if only 1 parameter in rust project

## 0.11.0-beta - 7th December 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.11.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.11.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: handle graceful shutdowns

### Bug fixes
-------------------------------------------------
- fix: resolve race condition of dependency blocking indexing
- fix: add new type for EthereumSqlTypeWrapper to handle VARCHAR strings
- fix: error on startup not +1 onto the next block causing duplicates logs sometimes
- fix: issue with it working on windows

## 0.10.0-beta - 15th October 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.10.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.10.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.10.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.10.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.10.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: expose an insert_bulk new postgres function to make inserting bulk data easier
- feat: expose new ethereum sql type wrappers for bytes types#
- feat: expose postgres ToSql trait
- feat: support with_transaction in postgres client
- feat: get the block timestamp from the RPC call (its an option as not all providers expose it)
- feat: allow you to override environment file path

### Bug fixes
-------------------------------------------------
- fix: dependency events not being applied to the correct contract
- fix: resolve defining environment variables in contract address fields in the yaml
- fix: resolve topic_id packing issues

## 0.9.0-beta - 19th September 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.9.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.9.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.9.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.9.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.9.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: allow some other attributes on the generated event typings

### Bug fixes
-------------------------------------------------
- fix: handle signed integers throughout rindexer
- fix: generating global types would repeat the same code on a regenerate causing issues

### Breaking changes
-------------------------------------------------
- breaking: rindexer had an parsing error meaning stuff like `UniswapV3Pool` would parse to uniswap_v_3_pool. This caused
some other issues with mapping to object names so now it has been fixed and in the example above it will be `uniswap_v3_pool`,
if you have any running indexers with these buggy names in your db you just need to rename them and same with the `rindexer.internal` tables which will
have these odd names as well.

## 0.8.0-beta - 17th September 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.8.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.8.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.8.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.8.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.8.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: info log if no new blocks are published in the last 20 seconds to avoid people thinking rindexer is stuck

### Bug fixes
-------------------------------------------------
- fix: pascal case still has some edge cases on parsing
- fix: allow #![allow(non_snake_case)] in indexer code
- fix: still generate internal tables for rindexer even if creating new event tables is disabled

## 0.7.1-beta - 17th September 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.7.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.7.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.7.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.7.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.7.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: throw error if contract names are not unique
- fix: allow non camel case types in generated code
- fix: pascal case not parsing capitals full words correctly

## 0.7.0-beta - 16th September 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.7.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.7.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.7.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.7.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.7.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------
- feat: support multiple abis in a single contract
- feat: allow array of filters in the same contract without repeating

### Bug fixes
-------------------------------------------------
- fix: running rust project should only start indexer or graphql passed on args passed
- fix: resolve issue of paths in generated typings
- fix: when running rindexer codegen typings csv folder created
- fix: underscores in events within a rust project maps it wrong in typings
- fix: share a postgres instance across rust project

## 0.6.2-beta - 24th August 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.6.2

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.2/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.2/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.2/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.2/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------
- fix: Use the prefix when generating abi name properties.

## 0.6.1-beta - 15th August 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.6.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------

- fix: resolve issue with conflicting event names on graphql meaning it would not load
- fix: resolve filter table names mapping to graphql meaning it would not expose the graphql queries

## 0.6.0-beta - 8th August 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.6.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.6.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------

- feat: add a disable_logs_bloom_checks field to the network section of the [YAML configuration file](https://rindexer.xyz/docs/start-building/yaml-config/networks#disable_logs_bloom_checks)


## 0.5.1-beta - 7th August 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.5.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.5.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.5.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.5.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.5.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------

- fix: resolve unhandled solidity types in solidity_type_to_ethereum_sql_type_wrapper

## 0.5.0-beta - 6th August 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.5.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.5.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.5.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.5.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.5.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------

- feat: support chatbots on telegram - https://rindexer.xyz/docs/start-building/chatbots/telegram
- feat: support chatbots on discord - https://rindexer.xyz/docs/start-building/chatbots/discord
- feat: support chatbots on slack - https://rindexer.xyz/docs/start-building/chatbots/slack
- feat: support streams with kafka - https://rindexer.xyz/docs/start-building/streams/kafka
- feat: support streams with rabbitmq - https://rindexer.xyz/docs/start-building/streams/rabbitmq
- feat: support streams with webhooks - https://rindexer.xyz/docs/start-building/streams/webhooks
- feat: support streams with sns/sqs - https://rindexer.xyz/docs/start-building/streams/sns
- feat: create .gitignore file for new projects

## 0.4.0-beta - 30th July 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.4.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.4.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.4.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.4.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.4.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------

- feat: create a docker image and github workflow for building it when pushed

## 0.3.1-beta - 30th July 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.3.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.3.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.3.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.3.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.3.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------

- fix: throw an error if trying to include a non-event type in the `include_events` array
- fix: postgres connection error issue seen on supabase
- fix: refactor postgres new to always try ssl first then retry without ssl to be inline with best practices

## 0.3.0-beta - 26th July 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.3.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.3.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.3.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.3.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.3.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------

- feat: support for phantom events - https://rindexer.xyz/docs/start-building/phantom

### Bug fixes
-------------------------------------------------

- fix: resolve issue with no inputs in events syntax error for postgres
- fix: better error message when etherscan is not supported for network


## 0.2.0-beta - 21th July 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.2.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.2.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.2.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.2.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.2.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------

- feat: add max_block_range to networks - https://github.com/joshstevens19/rindexer/issues/55
- feat: allow you to add your own etherscan api key - https://rindexer.xyz/docs/start-building/yaml-config/global#etherscan_api_key
- feat: improve logs bloom log message

### Bug fixes
-------------------------------------------------

- fix: resolve `substitute_env_variables` to use `${}` instead of `$<>` for env variables

## 0.1.4-beta - 20th July 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.1.4

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.4/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.4/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.4/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.4/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------

- fix: fixing the query of implemntation ABI for proxy contracts
- fix: add request timeouts to adapt to different verifier's rate limits
- fix: make chain_id u64 instead of u32 - https://github.com/joshstevens19/rindexer/issues/53
- fix: fix rust project not being able to run due to borrower check
- fix: fix typings generations to parse the object values correctly


## 0.1.3-beta - 19th July 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.1.3

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.3/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.3/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.3/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.3/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------

- fix: Remove package specifier from codegen Cargo.toml

## 0.1.2-beta - 18th July 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.1.2

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.2/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.2/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.2/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.2/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------

- fix: allow postgres tls connections to be used (?sslmode=require)

## 0.1.1-beta - 16th July 2024

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.1.1

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.1/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.1/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.1/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.1/rindexer_win32-amd64.zip

### Bug fixes
-------------------------------------------------

- fix: support all the int solidity types - https://github.com/joshstevens19/rindexer/issues/45

## 0.1.0-beta - 15th July 2024
-------------------------------------------------

github branch - https://github.com/joshstevens19/rindexer/tree/release/0.1.0

- linux binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.0/rindexer_linux-amd64.tar.gz
- mac apple silicon binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.0/rindexer_darwin-arm64.tar.gz
- mac apple intel binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.0/rindexer_darwin-amd64.tar.gz
- windows binary - https://github.com/joshstevens19/rindexer/releases/download/v0.1.0/rindexer_win32-amd64.zip

### Features
-------------------------------------------------

Release of rindexer
</file>

<file path="documentation/docs/pages/docs/shoutout.mdx">
# Shoutout

Big big thanks to [Avara](https://avara.xyz/) who allowed me to finish this off in work time.

Also big thanks to [Foundry's](https://github.com/foundry-rs/foundry) release github workflow
which we took inspiration from to build the cross platform rindexer binaries. Building the cross platform binaries
was harder then building the whole project lol.
</file>

<file path="documentation/docs/pages/index.mdx">
---
layout: landing
---

import { HomePage } from 'vocs/components'

<HomePage.Root>
  <HomePage.Logo />
  <HomePage.Tagline>built in rust</HomePage.Tagline>
```bash
curl -L https://rindexer.xyz/install.sh | bash
```

:::warning
If you’re on Windows, you will need to install and use Git BASH or WSL, as your terminal,
since rindexer installation does not support Powershell or Cmd.
:::
  <HomePage.Description>A no-code or framework to build blazing fast EVM indexers - built in rust.</HomePage.Description>
  <HomePage.Buttons>
    <HomePage.Button href="/docs/introduction/installation" variant="accent">Start building</HomePage.Button>
    <HomePage.Button href="https://github.com/joshstevens19/rindexer">GitHub</HomePage.Button>
  </HomePage.Buttons>
</HomePage.Root>
</file>

<file path="documentation/docs/public/install.sh">
#!/usr/bin/env bash
set -eo pipefail
BASE_DIR="${XDG_CONFIG_HOME:-$HOME}"
RINDEXER_DIR="${RINDEXER_DIR:-"$BASE_DIR/.rindexer"}"
RINDEXER_BIN_DIR="$RINDEXER_DIR/bin"
RINDEXERUP_PATH="$RINDEXER_BIN_DIR/rindexerup"
RINDEXERDOWN_PATH="$RINDEXER_BIN_DIR/rindexerdown"
OS_TYPE=$(uname)
ARCH_TYPE=$(uname -m)
# Parse command line arguments
VERSION=""
while [[ $# -gt 0 ]]; do
    case $1 in
        --version)
            VERSION="$2"
            shift 2
            ;;
        --local)
            LOCAL_INSTALL=true
            shift
            ;;
        --uninstall)
            UNINSTALL=true
            shift
            ;;
        *)
            shift
            ;;
    esac
done
if [[ "$OS_TYPE" == "Linux" ]]; then
    BIN_PATH="$RINDEXER_BIN_DIR/rindexer"
    PLATFORM="linux"
    ARCH_TYPE="amd64"
    EXT="tar.gz"
    if ! command -v unzip &> /dev/null; then
        sudo apt-get update && sudo apt-get install -y unzip
    fi
elif [[ "$OS_TYPE" == "Darwin" ]]; then
    BIN_PATH="$RINDEXER_BIN_DIR/rindexer"
    PLATFORM="darwin"
    # For Darwin, you're using `uname -m` for ARCH_TYPE.
    # If the binary is specifically for 'arm64' or 'amd64', ensure ARCH_TYPE is set correctly.
    # From your output: Downloading binary archive from ...rindexer_darwin-arm64.tar.gz
    # So, if uname -m gives 'arm64', this is fine. If it gives 'x86_64' for Intel Macs, it will be 'amd64'.
    # This seems to be handled correctly in the original script.
    if [[ "$ARCH_TYPE" == "x86_64" ]]; then
        ARCH_TYPE="amd64"
    fi
    EXT="tar.gz"
elif [[ "$OS_TYPE" == "MINGW"* ]] || [[ "$OS_TYPE" == "MSYS"* ]] || [[ "$OS_TYPE" == "CYGWIN"* ]]; then
    PLATFORM="win32"
    EXT="zip"
    BIN_PATH="$RINDEXER_BIN_DIR/rindexer.exe"
else
    echo "Unsupported OS: $OS_TYPE"
    exit 1
fi
# Function to get the latest version from GitHub API
get_latest_version() {
    local latest_version
    latest_version=$(curl -s "https://api.github.com/repos/joshstevens19/rindexer/releases/latest" | grep '"tag_name":' | sed -E 's/.*"([^"]+)".*/\1/' | sed 's/^v//')
    echo "$latest_version"
}
# Set download URLs
if [[ -n "$VERSION" ]]; then
    BIN_URL="https://github.com/joshstevens19/rindexer/releases/download/v${VERSION}/rindexer_${PLATFORM}-${ARCH_TYPE}.${EXT}"
else
    # Get latest version if no version specified
    VERSION=$(get_latest_version)
    BIN_URL="https://github.com/joshstevens19/rindexer/releases/download/v${VERSION}/rindexer_${PLATFORM}-${ARCH_TYPE}.${EXT}"
fi
log() {
   echo -e "\033[1;32m$1\033[0m"
}
error_log() {
    echo -e "\033[1;31m$1\033[0m"
}
spinner() {
    local text="$1"
    local pid=$!
    local delay=0.1
    local spinstr='|/-\'
    log "$text"
    # This spinner logic is still a bit problematic if not carefully managed with background processes
    # For now, it's bypassed in the critical download section by making curl foreground.
    while ps -p "$pid" &>/dev/null; do
        local temp=${spinstr#?}
        printf " [%c]  " "$spinstr"
        local spinstr=$temp${spinstr%"$temp"}
        sleep $delay
        printf "\b\b\b\b\b\b"
    done
    echo ""
}
# Install or uninstall based on the command line option
if [[ "$LOCAL_INSTALL" == true ]]; then
    log "Using local binary from $LOCAL_BIN_PATH..."
    cp "$LOCAL_BIN_PATH" "$BIN_PATH"
elif [[ "$UNINSTALL" == true ]]; then
    log "Uninstalling rindexer..."
    rm -f "$BIN_PATH" "$RINDEXERUP_PATH"
    rmdir "$RINDEXER_BIN_DIR" "$RINDEXER_DIR" 2> /dev/null
    if [[ "$OS_TYPE" == "Darwin" ]]; then
        sed -i '' '/rindexerup/d' "$PROFILE"
        sed -i '' '/rindexer/d' "$PROFILE"
    else
        sed -i '/rindexerup/d' "$PROFILE"
        sed -i '/rindexer/d' "$PROFILE"
    fi
    log "Uninstallation complete! Please restart your shell or source your profile to complete the process."
    exit 0
else
    if [[ -n "$VERSION" ]]; then
        log "Installing rindexer version $VERSION..."
    else
        log "Installing latest rindexer version ($VERSION)..."
    fi
    log "Preparing the installation..."
    mkdir -p "$RINDEXER_BIN_DIR"
    log "Downloading binary archive from $BIN_URL..."
    if ! curl -sSf -L "$BIN_URL" -o "$RINDEXER_DIR/rindexer.${EXT}"; then
        error_log "Failed to download rindexer version $VERSION. Please check if the version exists."
        error_log "Available releases: https://github.com/joshstevens19/rindexer/releases"
        exit 1
    fi
    log "Downloaded binary archive to $RINDEXER_DIR/rindexer.${EXT}"
    log "Extracting archive..."
    if [[ "$EXT" == "tar.gz" ]]; then
        tar -xzvf "$RINDEXER_DIR/rindexer.${EXT}" -C "$RINDEXER_DIR"
    else
        unzip -o "$RINDEXER_DIR/rindexer.${EXT}" -d "$RINDEXER_DIR"
    fi
    # Move the main binary to the bin directory, creating it if necessary
    mkdir -p "$RINDEXER_BIN_DIR"
    if [[ -f "$RINDEXER_DIR/rindexer_cli" ]]; then
        mv "$RINDEXER_DIR/rindexer_cli" "$BIN_PATH"
    elif [[ -f "$RINDEXER_DIR/rindexer" ]]; then
        mv "$RINDEXER_DIR/rindexer" "$BIN_PATH"
    elif [[ -f "$RINDEXER_DIR/rindexer_cli.exe" ]]; then
        mv "$RINDEXER_DIR/rindexer_cli.exe" "$BIN_PATH"
    elif [[ -f "$RINDEXER_DIR/rindexer.exe" ]]; then
        mv "$RINDEXER_DIR/rindexer.exe" "$BIN_PATH"
    fi
    log "Extracted files to $RINDEXER_DIR"
    rm "$RINDEXER_DIR/rindexer.${EXT}"
fi
# Ensure the binary exists before setting permissions
if [ -f "$BIN_PATH" ]; then
    chmod +x "$BIN_PATH"
    log "Binary found and permissions set at $BIN_PATH"
else
    error_log "Error: Binary not found at $BIN_PATH"
    exit 1
fi
# Update PATH in user's profile
PROFILE="${HOME}/.profile"  # Default to .profile
case $SHELL in
    */zsh) PROFILE="${ZDOTDIR:-"$HOME"}/.zshenv" ;;
    */bash) PROFILE="$HOME/.bashrc" ;;
    */fish) PROFILE="$HOME/.config/fish/config.fish" ;;
esac
if [[ ":$PATH:" != *":${RINDEXER_BIN_DIR}:"* ]]; then
    echo "export PATH=\"\$PATH:$RINDEXER_BIN_DIR\"" >> "$PROFILE"
    log "PATH updated in $PROFILE. Please log out and back in or source the profile file."
fi
# Add the rindexerup and rindexerdown commands to the profile
if ! grep -q "alias rindexerup" "$PROFILE"; then
    echo "" >> "$PROFILE"
    echo "# Adding rindexerup and rindexerdown commands" >> "$PROFILE"
    if [[ "$SHELL" == */fish ]]; then
        echo "alias rindexerup 'bash $RINDEXERUP_PATH \$argv'" >> "$PROFILE"
        echo "alias rindexerdown 'bash $RINDEXERDOWN_PATH'" >> "$PROFILE"
    else
        echo "alias rindexerup='bash $RINDEXERUP_PATH \$@'" >> "$PROFILE"
        echo "alias rindexerdown='bash $RINDEXERDOWN_PATH'" >> "$PROFILE"
    fi
fi
# Create or update the rindexerup script to check for updates
cat <<EOF > "$RINDEXERUP_PATH"
#!/usr/bin/env bash
set -eo pipefail
# Parse command line arguments for rindexerup
UPDATE_VERSION=""
while [[ \$# -gt 0 ]]; do
    case \$1 in
        --version)
            UPDATE_VERSION="\$2"
            shift 2
            ;;
        --local)
            LOCAL_UPDATE=true
            shift
            ;;
        *)
            shift
            ;;
    esac
done
echo "Updating rindexer..."
if [[ "\$LOCAL_UPDATE" == true ]]; then
    echo "Using local binary for update..."
    cp "$LOCAL_BIN_PATH" "$BIN_PATH"
else
    # Function to get the latest version from GitHub API
    get_latest_version() {
        local latest_version
        latest_version=\$(curl -s "https://api.github.com/repos/joshstevens19/rindexer/releases/latest" | grep '"tag_name":' | sed -E 's/.*"([^"]+)".*/\1/' | sed 's/^v//')
        echo "\$latest_version"
    }
    # Set version for update
    if [[ -n "\$UPDATE_VERSION" ]]; then
        echo "Updating to rindexer version \$UPDATE_VERSION..."
        DOWNLOAD_URL="https://github.com/joshstevens19/rindexer/releases/download/v\${UPDATE_VERSION}/rindexer_${PLATFORM}-${ARCH_TYPE}.${EXT}"
    else
        UPDATE_VERSION=\$(get_latest_version)
        echo "Updating to latest rindexer version (\$UPDATE_VERSION)..."
        DOWNLOAD_URL="https://github.com/joshstevens19/rindexer/releases/download/v\${UPDATE_VERSION}/rindexer_${PLATFORM}-${ARCH_TYPE}.${EXT}"
    fi
    echo "Downloading the binary from \$DOWNLOAD_URL..."
    if ! curl -sSf -L "\$DOWNLOAD_URL" -o "$RINDEXER_DIR/rindexer.${EXT}"; then
        echo "Failed to download rindexer version \$UPDATE_VERSION. Please check if the version exists."
        echo "Available releases: https://github.com/joshstevens19/rindexer/releases"
        exit 1
    fi
    echo "Extracting archive..."
    if [[ "$EXT" == "tar.gz" ]]; then
        tar -xzvf "$RINDEXER_DIR/rindexer.${EXT}" -C "$RINDEXER_DIR"
    else
        unzip -o "$RINDEXER_DIR/rindexer.${EXT}" -d "$RINDEXER_DIR"
    fi
    # Move the main binary to the bin directory
    if [[ -f "$RINDEXER_DIR/rindexer_cli" ]]; then
        mv "$RINDEXER_DIR/rindexer_cli" "$BIN_PATH"
    elif [[ -f "$RINDEXER_DIR/rindexer" ]]; then
        mv "$RINDEXER_DIR/rindexer" "$BIN_PATH"
    elif [[ -f "$RINDEXER_DIR/rindexer_cli.exe" ]]; then
        mv "$RINDEXER_DIR/rindexer_cli.exe" "$BIN_PATH"
    elif [[ -f "$RINDEXER_DIR/rindexer.exe" ]]; then
        mv "$RINDEXER_DIR/rindexer.exe" "$BIN_PATH"
    fi
    rm "$RINDEXER_DIR/rindexer.${EXT}"
fi
chmod +x "$BIN_PATH"
echo "rindexer has been updated successfully."
EOF
chmod +x "$RINDEXERUP_PATH"
# rindexerdown
cat <<EOF > "$RINDEXERDOWN_PATH"
#!/usr/bin/env bash
set -eo pipefail
echo "Uninstalling rindexer..."
rm -f "$BIN_PATH" "$RINDEXERUP_PATH"
rmdir "$RINDEXER_BIN_DIR" "$RINDEXER_DIR" 2> /dev/null
if [[ "$(uname)" == "Darwin" ]]; then
    sed -i '' '/rindexerup/d' "$PROFILE"
    sed -i '' '/rindexer/d' "$PROFILE"
else
    sed -i '/rindexerup/d' "$PROFILE"
    sed -i '/rindexer/d' "$PROFILE"
fi
echo "rindexer uninstallation complete!"
EOF
chmod +x "$RINDEXERDOWN_PATH"
log ""
log "rindexer has been installed successfully"
log ""
log "To update rindexer run 'rindexerup' (latest) or 'rindexerup --version X.X.X' (specific version)."
log ""
log "To uninstall rindexer run 'rindexerdown'."
log ""
log "Open a new terminal and run 'rindexer' to get started."
</file>

<file path="documentation/package.json">
{
  "name": "rindexer_docs",
  "version": "0.0.0",
  "engines": {
    "node": ">=20"
  },
  "type": "module",
  "scripts": {
    "dev": "vocs dev",
    "build": "vocs build",
    "preview": "vocs preview"
  },
  "dependencies": {
    "@types/react": "latest",
    "react": "latest",
    "react-dom": "latest",
    "typescript": "latest",
    "vocs": "^1.0.13"
  }
}
</file>

<file path="documentation/README.md">
# Documentation

This is the documentation for rindexer, powered by [Vocs](https://vocs.dev).

## Installing

```bash
npm i
```

## Running

```bash
npm run dev
```
</file>

<file path="documentation/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "useDefineForClassFields": true,
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true
  },
  "include": ["**/*.ts", "**/*.tsx"]
}
</file>

<file path="documentation/vocs.config.tsx">
import { defineConfig } from "vocs";
export default defineConfig({
  head: (
    <>
      <meta property="og:type" content="website" />
      <meta
        property="og:title"
        content="rindexer · A lighting-fast multi chain indexing solution written in Rust"
      />
      <meta property="og:image" content="https://rindexer.xyz/favicon.png" />
      <meta property="og:url" content="https://rindexer.xyz" />
      <meta
        property="og:description"
        content="Build scalable, efficient, and secure blockchain indexing solutions for modern decentralized applications."
      />
    </>
  ),
  title: "🦀 rindexer 🦀",
  iconUrl: "/favicon.png",
  ogImageUrl: "/favicon.png",
  description:
    "rindexer is a lighting-fast multi chain indexing solution written in Rust",
  topNav: [
    { text: "Docs", link: "/docs/introduction/installation", match: "/docs" },
    { text: "Changelog", link: "/docs/changelog", match: "/docs" },
  ],
  socials: [
    {
      icon: "github",
      link: "https://github.com/joshstevens19/rindexer",
    },
  ],
  sidebar: [
    {
      text: "Introduction",
      items: [
        {
          text: "What is rindexer?",
          link: "/docs/introduction/what-is-rindexer",
        },
        { text: "Why rindexer?", link: "/docs/introduction/why-rindexer" },
        { text: "Installation", link: "/docs/introduction/installation" },
        {
          text: "Other Indexing Tools",
          link: "/docs/introduction/other-indexing-tools",
        },
      ],
    },
    {
      text: "Start Building...",
      items: [
        {
          text: "Project Types",
          link: "/docs/start-building/project-types",
          items: [
            {
              text: "No-code",
              link: "/docs/start-building/project-types/no-code-project",
            },
            {
              text: "Rust",
              link: "/docs/start-building/project-types/rust-project",
            },
          ],
        },
        {
          text: "Create New Project",
          link: "/docs/start-building/create-new-project",
          items: [
            {
              text: "Standard Mode",
              link: "/docs/start-building/create-new-project/standard",
            },
            {
              text: "Reth Mode",
              link: "/docs/start-building/create-new-project/reth-mode",
            },
          ],
        },
        {
          text: "Live indexing vs Historic indexing",
          link: "/docs/start-building/live-indexing-and-historic",
        },
        {
          text: "YAML Config",
          link: "/docs/start-building/yaml-config",
          items: [
            {
              text: "Top-level Fields",
              link: "/docs/start-building/yaml-config/top-level-fields",
            },
            {
              text: "Config",
              link: "/docs/start-building/yaml-config/config",
            },
            {
              text: "Networks",
              link: "/docs/start-building/yaml-config/networks",
            },
            {
              text: "Storage",
              link: "/docs/start-building/yaml-config/storage",
            },
            {
              text: "Native Transfers",
              link: "/docs/start-building/yaml-config/native-transfers",
            },
            {
              text: "Contracts",
              link: "/docs/start-building/yaml-config/contracts",
            },
            {
              text: "GraphQL",
              link: "/docs/start-building/yaml-config/graphql",
            },
            { text: "Global", link: "/docs/start-building/yaml-config/global" },
          ],
        },
        {
          text: "Add",
          link: "/docs/start-building/add",
        },
        {
          text: "Codegen",
          link: "/docs/start-building/codegen",
        },
        {
          text: "Running",
          link: "/docs/start-building/running",
        },
        {
          text: "Delete",
          link: "/docs/start-building/delete",
        },
        {
          text: "Chatbots",
          link: "/docs/start-building/chatbots",
          items: [
            {
              text: "Telegram",
              link: "/docs/start-building/chatbots/telegram",
            },
            { text: "Discord", link: "/docs/start-building/chatbots/discord" },
            { text: "Slack", link: "/docs/start-building/chatbots/slack" },
          ],
        },
        {
          text: "Streams",
          link: "/docs/start-building/streams",
          items: [
            { text: "Webhooks", link: "/docs/start-building/streams/webhooks" },
            { text: "Kafka", link: "/docs/start-building/streams/kafka" },
            { text: "Rabbitmq", link: "/docs/start-building/streams/rabbitmq" },
            { text: "SNS/SQS", link: "/docs/start-building/streams/sns" },
            { text: "Redis", link: "/docs/start-building/streams/redis" },
            { text: "Cloudflare Queues", link: "/docs/start-building/streams/cloudflare-queues" },
          ],
        },
        {
          text: "Timestamps",
          link: "/docs/start-building/timestamps",
        },
        {
          text: "Health Monitoring",
          link: "/docs/start-building/health-monitoring",
        },
        {
          text: "Phantom Events",
          link: "/docs/start-building/phantom",
        },
        {
          text: "Rust Project Deep Dive",
          link: "/docs/start-building/rust-project-deep-dive",
          items: [
            {
              text: "Typings",
              link: "/docs/start-building/rust-project-deep-dive/typings",
            },
            {
              text: "Indexers",
              link: "/docs/start-building/rust-project-deep-dive/indexers",
            },
            { text: "Building Own DB Schema - coming soon" },
            { text: "Ethers to Alloy Migration", link: "/docs/start-building/rust-project-deep-dive/ethers-alloy-migration" },
          ],
        },
      ],
    },
    {
      text: "Accessing Data",
      items: [
        { text: "GraphQL", link: "/docs/accessing-data/graphql" },
        { text: "Direct SQL", link: "/docs/accessing-data/direct-sql" },
      ],
    },
    {
      text: "Deploying",
      items: [
        { text: "Railway", link: "/docs/deploying/railway" },
        { text: "AWS", link: "/docs/deploying/aws" },
        { text: "GCP", link: "/docs/deploying/gcp" },
      ],
    },
    {
      text: "References",
      items: [
        { text: "CLI ", link: "/docs/references/cli" },
        {
          text: "RPC Node Providers ",
          link: "/docs/references/rpc-node-providers",
        },
      ],
    },
    {
      text: "Advanced",
      items: [
        { text: "From A Foundry Project - coming soon" },
        { text: "From A Hardhat Project - coming soon" },
        { text: "Using Reth ExExes", link: "/docs/advanced/using-reth-exex" },
      ],
    },
    { text: "Changelog", link: "/docs/changelog" },
    { text: "Shoutout", link: "/docs/shoutout" },
  ],
});
</file>

<file path="examples/clickhouse_factory_indexing/abis/erc20-abi.json">
[
  {
    "constant": true,
    "inputs": [],
    "name": "name",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_spender",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "approve",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "totalSupply",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_from",
        "type": "address"
      },
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transferFrom",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "decimals",
    "outputs": [
      {
        "name": "",
        "type": "uint8"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "balanceOf",
    "outputs": [
      {
        "name": "balance",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "symbol",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transfer",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      },
      {
        "name": "_spender",
        "type": "address"
      }
    ],
    "name": "allowance",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "payable": true,
    "stateMutability": "payable",
    "type": "fallback"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "spender",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Approval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "from",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Transfer",
    "type": "event"
  }
]
</file>

<file path="examples/clickhouse_factory_indexing/abis/uniswap-v3-factory-abi.json">
[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "FeeAmountEnabled",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "oldOwner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newOwner",
        "type": "address"
      }
    ],
    "name": "OwnerChanged",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "name": "PoolCreated",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "tokenA",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "tokenB",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      }
    ],
    "name": "createPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "enableFeeAmount",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "feeAmountTickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "getPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "owner",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "parameters",
    "outputs": [
      {
        "internalType": "address",
        "name": "factory",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "setOwner",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  }
]
</file>

<file path="examples/clickhouse_factory_indexing/abis/uniswap-v3-pool-abi.json">
[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Burn",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "Collect",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "CollectProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid1",
        "type": "uint256"
      }
    ],
    "name": "Flash",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextOld",
        "type": "uint16"
      },
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextNew",
        "type": "uint16"
      }
    ],
    "name": "IncreaseObservationCardinalityNext",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Initialize",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Mint",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0New",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1New",
        "type": "uint8"
      }
    ],
    "name": "SetFeeProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "liquidity",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Swap",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      }
    ],
    "name": "burn",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collect",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collectProtocol",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "factory",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "fee",
    "outputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal0X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal1X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "flash",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      }
    ],
    "name": "increaseObservationCardinalityNext",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      }
    ],
    "name": "initialize",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "liquidity",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "maxLiquidityPerTick",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "mint",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint256",
        "name": "index",
        "type": "uint256"
      }
    ],
    "name": "observations",
    "outputs": [
      {
        "internalType": "uint32",
        "name": "blockTimestamp",
        "type": "uint32"
      },
      {
        "internalType": "int56",
        "name": "tickCumulative",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityCumulativeX128",
        "type": "uint160"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint32[]",
        "name": "secondsAgos",
        "type": "uint32[]"
      }
    ],
    "name": "observe",
    "outputs": [
      {
        "internalType": "int56[]",
        "name": "tickCumulatives",
        "type": "int56[]"
      },
      {
        "internalType": "uint160[]",
        "name": "secondsPerLiquidityCumulativeX128s",
        "type": "uint160[]"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "bytes32",
        "name": "key",
        "type": "bytes32"
      }
    ],
    "name": "positions",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "_liquidity",
        "type": "uint128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside0LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside1LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "protocolFees",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "token0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "token1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint8",
        "name": "feeProtocol0",
        "type": "uint8"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol1",
        "type": "uint8"
      }
    ],
    "name": "setFeeProtocol",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "slot0",
    "outputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      },
      {
        "internalType": "uint16",
        "name": "observationIndex",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinality",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol",
        "type": "uint8"
      },
      {
        "internalType": "bool",
        "name": "unlocked",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      }
    ],
    "name": "snapshotCumulativesInside",
    "outputs": [
      {
        "internalType": "int56",
        "name": "tickCumulativeInside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityInsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsInside",
        "type": "uint32"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "bool",
        "name": "zeroForOne",
        "type": "bool"
      },
      {
        "internalType": "int256",
        "name": "amountSpecified",
        "type": "int256"
      },
      {
        "internalType": "uint160",
        "name": "sqrtPriceLimitX96",
        "type": "uint160"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "swap",
    "outputs": [
      {
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int16",
        "name": "wordPosition",
        "type": "int16"
      }
    ],
    "name": "tickBitmap",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "tickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "ticks",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "liquidityGross",
        "type": "uint128"
      },
      {
        "internalType": "int128",
        "name": "liquidityNet",
        "type": "int128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside0X128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside1X128",
        "type": "uint256"
      },
      {
        "internalType": "int56",
        "name": "tickCumulativeOutside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityOutsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsOutside",
        "type": "uint32"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token0",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token1",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  }
]
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/mod.rs">
#![allow(dead_code, unused)]
pub mod uniswap_v3_factory;
pub mod uniswap_v3_factory_pool_created_pool;
pub mod uniswap_v3_factory_pool_created_token_0_token_1;
pub mod uniswap_v3_pool;
pub mod uniswap_v3_pool_token;
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/uniswap_v3_factory_pool_created_pool.rs">
#![allow(non_snake_case)]
use rindexer::{
                event::callback_registry::EventCallbackRegistry,
                EthereumSqlTypeWrapper, PgType, RindexerColorize, rindexer_error, rindexer_info
            };
        use std::sync::Arc;
use std::path::PathBuf;
        use alloy::primitives::{U64, U256, I256};
        use super::super::super::typings::rindexer_factory_contract::events::uniswap_v3_factory_pool_created_pool::{no_extensions, UniswapV3FactoryPoolCreatedPoolEventType,PoolCreatedEvent};
async fn pool_created_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = PoolCreatedEvent::handler(|results, context| async move {
                                if results.is_empty() {
                                    return Ok(());
                                }
                    let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
                    for result in results.iter() {
                        let data = vec![
EthereumSqlTypeWrapper::Address(result.tx_information.address),
EthereumSqlTypeWrapper::Address(result.event_data.token0),
EthereumSqlTypeWrapper::Address(result.event_data.token1),
EthereumSqlTypeWrapper::U32(result.event_data.fee.to()),
EthereumSqlTypeWrapper::I32(result.event_data.tickSpacing.unchecked_into()),
EthereumSqlTypeWrapper::Address(result.event_data.pool),
EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
EthereumSqlTypeWrapper::DateTimeNullable(result.tx_information.block_timestamp_to_datetime()),
EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
EthereumSqlTypeWrapper::U256(result.tx_information.log_index)
];
                        postgres_bulk_data.push(data);
                    }
                    if postgres_bulk_data.is_empty() {
                        return Ok(());
                    }
                    let rows = ["contract_address".to_string(), "token_0".to_string(), "token_1".to_string(), "fee".to_string(), "tick_spacing".to_string(), "pool".to_string(), "tx_hash".to_string(), "block_number".to_string(), "block_timestamp".to_string(), "block_hash".to_string(), "network".to_string(), "tx_index".to_string(), "log_index".to_string()];
                    let result = context
                        .database
                        .insert_bulk(
                            "rindexer_factory_contract_uniswap_v3_factory_pool_created_pool.pool_created",
                            &rows,
                            &postgres_bulk_data,
                        )
                        .await;
                    if let Err(e) = result {
                        rindexer_error!("UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated inserting bulk data: {:?}", e);
                        return Err(e.to_string());
                    }
                                rindexer_info!(
                                    "UniswapV3FactoryPoolCreatedPool::PoolCreated - {} - {} events",
                                    "INDEXED".green(),
                                    results.len(),
                                );
                                Ok(())
                            },
                            no_extensions(),
                          )
                          .await;
    UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated(handler)
        .register(manifest_path, registry)
        .await;
}
pub async fn uniswap_v3_factory_pool_created_pool_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    pool_created_handler(manifest_path, registry).await;
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/uniswap_v3_factory_pool_created_token_0_token_1.rs">
#![allow(non_snake_case)]
use rindexer::{
                event::callback_registry::EventCallbackRegistry,
                EthereumSqlTypeWrapper, PgType, RindexerColorize, rindexer_error, rindexer_info
            };
        use std::sync::Arc;
use std::path::PathBuf;
        use alloy::primitives::{U64, U256, I256};
        use super::super::super::typings::rindexer_factory_contract::events::uniswap_v3_factory_pool_created_token_0_token_1::{no_extensions, UniswapV3FactoryPoolCreatedToken0Token1EventType,PoolCreatedEvent};
async fn pool_created_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = PoolCreatedEvent::handler(|results, context| async move {
                                if results.is_empty() {
                                    return Ok(());
                                }
                    let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
                    for result in results.iter() {
                        let data = vec![
EthereumSqlTypeWrapper::Address(result.tx_information.address),
EthereumSqlTypeWrapper::Address(result.event_data.token0),
EthereumSqlTypeWrapper::Address(result.event_data.token1),
EthereumSqlTypeWrapper::U32(result.event_data.fee.to()),
EthereumSqlTypeWrapper::I32(result.event_data.tickSpacing.unchecked_into()),
EthereumSqlTypeWrapper::Address(result.event_data.pool),
EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
EthereumSqlTypeWrapper::DateTimeNullable(result.tx_information.block_timestamp_to_datetime()),
EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
EthereumSqlTypeWrapper::U256(result.tx_information.log_index)
];
                        postgres_bulk_data.push(data);
                    }
                    if postgres_bulk_data.is_empty() {
                        return Ok(());
                    }
                    let rows = ["contract_address".to_string(), "token_0".to_string(), "token_1".to_string(), "fee".to_string(), "tick_spacing".to_string(), "pool".to_string(), "tx_hash".to_string(), "block_number".to_string(), "block_timestamp".to_string(), "block_hash".to_string(), "network".to_string(), "tx_index".to_string(), "log_index".to_string()];
                    let result = context
                        .database
                        .insert_bulk(
                            "rindexer_factory_contract_uniswap_v3_factory_pool_created_token_0_token_1.pool_created",
                            &rows,
                            &postgres_bulk_data,
                        )
                        .await;
                    if let Err(e) = result {
                        rindexer_error!("UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated inserting bulk data: {:?}", e);
                        return Err(e.to_string());
                    }
                                rindexer_info!(
                                    "UniswapV3FactoryPoolCreatedToken0Token1::PoolCreated - {} - {} events",
                                    "INDEXED".green(),
                                    results.len(),
                                );
                                Ok(())
                            },
                            no_extensions(),
                          )
                          .await;
    UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated(handler)
        .register(manifest_path, registry)
        .await;
}
pub async fn uniswap_v3_factory_pool_created_token_0_token_1_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    pool_created_handler(manifest_path, registry).await;
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/uniswap_v3_factory.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_factory_contract::events::uniswap_v3_factory::{
    no_extensions, OwnerChangedEvent, UniswapV3FactoryEventType,
};
use alloy::primitives::{I256, U256, U64};
use rindexer::{
    event::callback_registry::EventCallbackRegistry, rindexer_error, rindexer_info,
    EthereumSqlTypeWrapper, PgType, RindexerColorize,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn owner_changed_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = OwnerChangedEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            for result in results.iter() {
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.oldOwner),
                    EthereumSqlTypeWrapper::Address(result.event_data.newOwner),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "old_owner".to_string(),
                "new_owner".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_factory_contract_uniswap_v3_factory.owner_changed",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!(
                    "UniswapV3FactoryEventType::OwnerChanged inserting bulk data: {:?}",
                    e
                );
                return Err(e.to_string());
            }
            rindexer_info!(
                "UniswapV3Factory::OwnerChanged - {} - {} events",
                "INDEXED".green(),
                results.len(),
            );
            Ok(())
        },
        no_extensions(),
    )
    .await;
    UniswapV3FactoryEventType::OwnerChanged(handler).register(manifest_path, registry).await;
}
pub async fn uniswap_v3_factory_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    owner_changed_handler(manifest_path, registry).await;
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/uniswap_v3_pool_token.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_factory_contract::events::uniswap_v3_pool_token::{
    no_extensions, TransferEvent, UniswapV3PoolTokenEventType,
};
use alloy::primitives::{I256, U256, U64};
use rindexer::{
    event::callback_registry::EventCallbackRegistry, rindexer_error, rindexer_info,
    EthereumSqlTypeWrapper, PgType, RindexerColorize,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn transfer_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = TransferEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            for result in results.iter() {
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.from),
                    EthereumSqlTypeWrapper::Address(result.event_data.to),
                    EthereumSqlTypeWrapper::U256(U256::from(result.event_data.value)),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "from".to_string(),
                "to".to_string(),
                "value".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_factory_contract_uniswap_v3_pool_token.transfer",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!(
                    "UniswapV3PoolTokenEventType::Transfer inserting bulk data: {:?}",
                    e
                );
                return Err(e.to_string());
            }
            rindexer_info!(
                "UniswapV3PoolToken::Transfer - {} - {} events",
                "INDEXED".green(),
                results.len(),
            );
            Ok(())
        },
        no_extensions(),
    )
    .await;
    UniswapV3PoolTokenEventType::Transfer(handler).register(manifest_path, registry).await;
}
pub async fn uniswap_v3_pool_token_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    transfer_handler(manifest_path, registry).await;
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/uniswap_v3_pool.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_factory_contract::events::uniswap_v3_pool::{
    no_extensions, SwapEvent, UniswapV3PoolEventType,
};
use alloy::primitives::{I256, U256, U64};
use rindexer::{
    event::callback_registry::EventCallbackRegistry, rindexer_error, rindexer_info,
    EthereumSqlTypeWrapper, PgType, RindexerColorize,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn swap_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = SwapEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            for result in results.iter() {
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.sender),
                    EthereumSqlTypeWrapper::Address(result.event_data.recipient),
                    EthereumSqlTypeWrapper::I256(I256::from(result.event_data.amount0)),
                    EthereumSqlTypeWrapper::I256(I256::from(result.event_data.amount1)),
                    EthereumSqlTypeWrapper::U256(U256::from(result.event_data.sqrtPriceX96)),
                    EthereumSqlTypeWrapper::U128(result.event_data.liquidity),
                    EthereumSqlTypeWrapper::I32(result.event_data.tick.unchecked_into()),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "sender".to_string(),
                "recipient".to_string(),
                "amount_0".to_string(),
                "amount_1".to_string(),
                "sqrt_price_x96".to_string(),
                "liquidity".to_string(),
                "tick".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_factory_contract_uniswap_v3_pool.swap",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!("UniswapV3PoolEventType::Swap inserting bulk data: {:?}", e);
                return Err(e.to_string());
            }
            rindexer_info!(
                "UniswapV3Pool::Swap - {} - {} events",
                "INDEXED".green(),
                results.len(),
            );
            Ok(())
        },
        no_extensions(),
    )
    .await;
    UniswapV3PoolEventType::Swap(handler).register(manifest_path, registry).await;
}
pub async fn uniswap_v3_pool_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    swap_handler(manifest_path, registry).await;
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/indexers/all_handlers.rs">
use super::rindexer_factory_contract::uniswap_v3_factory::uniswap_v3_factory_handlers;
use super::rindexer_factory_contract::uniswap_v3_factory_pool_created_pool::uniswap_v3_factory_pool_created_pool_handlers;
use super::rindexer_factory_contract::uniswap_v3_factory_pool_created_token_0_token_1::uniswap_v3_factory_pool_created_token_0_token_1_handlers;
use super::rindexer_factory_contract::uniswap_v3_pool::uniswap_v3_pool_handlers;
use super::rindexer_factory_contract::uniswap_v3_pool_token::uniswap_v3_pool_token_handlers;
use rindexer::event::callback_registry::EventCallbackRegistry;
use std::path::PathBuf;
pub async fn register_all_handlers(manifest_path: &PathBuf) -> EventCallbackRegistry {
    let mut registry = EventCallbackRegistry::new();
    uniswap_v3_factory_handlers(manifest_path, &mut registry).await;
    uniswap_v3_factory_pool_created_pool_handlers(manifest_path, &mut registry).await;
    uniswap_v3_pool_handlers(manifest_path, &mut registry).await;
    uniswap_v3_factory_pool_created_token_0_token_1_handlers(manifest_path, &mut registry).await;
    uniswap_v3_pool_token_handlers(manifest_path, &mut registry).await;
    registry
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/indexers/mod.rs">
#![allow(dead_code, unused)]
pub mod all_handlers;
pub mod rindexer_factory_contract;
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/mod.rs">
#![allow(dead_code, unused)]
pub mod uniswap_v3_factory;
mod uniswap_v3_factory_abi_gen;
pub mod uniswap_v3_factory_pool_created_pool;
mod uniswap_v3_factory_pool_created_pool_abi_gen;
pub mod uniswap_v3_factory_pool_created_token_0_token_1;
mod uniswap_v3_factory_pool_created_token_0_token_1_abi_gen;
pub mod uniswap_v3_pool;
mod uniswap_v3_pool_abi_gen;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer
/// Any manual changes to this file will be overwritten.
pub mod uniswap_v3_pool_token;
mod uniswap_v3_pool_token_abi_gen;
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3FactoryGen,
    r#"[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "FeeAmountEnabled",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "oldOwner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newOwner",
        "type": "address"
      }
    ],
    "name": "OwnerChanged",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "name": "PoolCreated",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "tokenA",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "tokenB",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      }
    ],
    "name": "createPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "enableFeeAmount",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "feeAmountTickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "getPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "owner",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "parameters",
    "outputs": [
      {
        "internalType": "address",
        "name": "factory",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "setOwner",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  }
]
"#
);
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory_pool_created_pool_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3FactoryPoolCreatedPoolGen,
    r#"[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "FeeAmountEnabled",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "oldOwner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newOwner",
        "type": "address"
      }
    ],
    "name": "OwnerChanged",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "name": "PoolCreated",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "tokenA",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "tokenB",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      }
    ],
    "name": "createPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "enableFeeAmount",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "feeAmountTickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "getPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "owner",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "parameters",
    "outputs": [
      {
        "internalType": "address",
        "name": "factory",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "setOwner",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  }
]
"#
);
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory_pool_created_pool.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
        ///
        /// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
        /// Any manual changes to this file will be overwritten.
        use super::uniswap_v3_factory_pool_created_pool_abi_gen::{
            RindexerUniswapV3FactoryPoolCreatedPoolGen::{self, RindexerUniswapV3FactoryPoolCreatedPoolGenInstance, RindexerUniswapV3FactoryPoolCreatedPoolGenEvents}
        };
        use std::{any::Any, sync::Arc};
        use std::error::Error;
        use std::future::Future;
        use std::collections::HashMap;
        use std::pin::Pin;
        use std::path::{Path, PathBuf};
        use alloy::network::AnyNetwork;
        use alloy::primitives::{Address, Bytes, B256};
        use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
        use rindexer::{
            async_trait,
            generate_random_id,
            FutureExt,
            blockclock::BlockClock,
            event::{
                callback_registry::{
                    EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
                    EventResult, TxInformation, HasTxInformation
                },
                contract_setup::{ContractInformation, NetworkContract},
            },
            manifest::{
                contract::{Contract, ContractDetails},
                yaml::read_manifest,
            },
            provider::{JsonRpcCachedProvider, RindexerProvider},
            ClickhouseClient,
        };
        use super::super::super::super::typings::networks::get_provider_cache_for_network;
        use super::super::super::super::typings::database::get_or_init_clickhouse_client;
pub type FeeAmountEnabledData = RindexerUniswapV3FactoryPoolCreatedPoolGen::FeeAmountEnabled;
#[derive(Debug, Clone)]
pub struct FeeAmountEnabledResult {
    pub event_data: FeeAmountEnabledData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for FeeAmountEnabledResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type OwnerChangedData = RindexerUniswapV3FactoryPoolCreatedPoolGen::OwnerChanged;
#[derive(Debug, Clone)]
pub struct OwnerChangedResult {
    pub event_data: OwnerChangedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for OwnerChangedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type PoolCreatedData = RindexerUniswapV3FactoryPoolCreatedPoolGen::PoolCreated;
#[derive(Debug, Clone)]
pub struct PoolCreatedResult {
    pub event_data: PoolCreatedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for PoolCreatedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<ClickhouseClient>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn poolcreated_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> PoolCreatedEventCallbackType<TExtensions>
where
    PoolCreatedResult: Clone + 'static,
    F: for<'a> Fn(Vec<PoolCreatedResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type PoolCreatedEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<PoolCreatedResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: PoolCreatedEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        PoolCreatedResult: Clone + 'static,
        F: for<'a> Fn(Vec<PoolCreatedResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        Self {
            callback: poolcreated_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_clickhouse_client().await,
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<PoolCreatedResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<PoolCreatedData>().ok().map(|arc| PoolCreatedResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("PoolCreatedEvent: Unexpected data type - expected: PoolCreatedData")
        }
    }
}
pub enum UniswapV3FactoryPoolCreatedPoolEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    PoolCreated(PoolCreatedEvent<TExtensions>),
}
pub async fn uniswap_v3_factory_pool_created_pool_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3FactoryPoolCreatedPoolGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3FactoryPoolCreatedPoolGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3FactoryPoolCreatedPoolGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3FactoryPoolCreatedPoolGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else if network == "ethereum" {
        RindexerUniswapV3FactoryPoolCreatedPoolGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3FactoryPoolCreatedPoolEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated(_) => {
                "0x783cca1c0412dd0d695e784568c96da2e9c22ff989357a2e8b1d9b2b4e6b7118"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated(_) => "PoolCreated",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3FactoryPoolCreatedPool".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated(_) => Arc::new(
                move |topics: Vec<B256>, data: Bytes| match PoolCreatedData::decode_raw_log(
                    topics,
                    &data[0..],
                ) {
                    Ok(event) => {
                        let result: PoolCreatedData = event;
                        Arc::new(result) as Arc<dyn Any + Send + Sync>
                    }
                    Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                },
            ),
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .map_or(false, |vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .map_or(false, |n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerFactoryContract".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory_pool_created_token_0_token_1_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen,
    r#"[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "FeeAmountEnabled",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "oldOwner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newOwner",
        "type": "address"
      }
    ],
    "name": "OwnerChanged",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "name": "PoolCreated",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "tokenA",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "tokenB",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      }
    ],
    "name": "createPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "enableFeeAmount",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "feeAmountTickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "getPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "owner",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "parameters",
    "outputs": [
      {
        "internalType": "address",
        "name": "factory",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "setOwner",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  }
]
"#
);
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory_pool_created_token_0_token_1.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
        ///
        /// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
        /// Any manual changes to this file will be overwritten.
        use super::uniswap_v3_factory_pool_created_token_0_token_1_abi_gen::{
            RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::{self, RindexerUniswapV3FactoryPoolCreatedToken0Token1GenInstance, RindexerUniswapV3FactoryPoolCreatedToken0Token1GenEvents}
        };
        use std::{any::Any, sync::Arc};
        use std::error::Error;
        use std::future::Future;
        use std::collections::HashMap;
        use std::pin::Pin;
        use std::path::{Path, PathBuf};
        use alloy::network::AnyNetwork;
        use alloy::primitives::{Address, Bytes, B256};
        use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
        use rindexer::{
            async_trait,
            generate_random_id,
            FutureExt,
            blockclock::BlockClock,
            event::{
                callback_registry::{
                    EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
                    EventResult, TxInformation, HasTxInformation
                },
                contract_setup::{ContractInformation, NetworkContract},
            },
            manifest::{
                contract::{Contract, ContractDetails},
                yaml::read_manifest,
            },
            provider::{JsonRpcCachedProvider, RindexerProvider},
            ClickhouseClient,
        };
        use super::super::super::super::typings::networks::get_provider_cache_for_network;
        use super::super::super::super::typings::database::get_or_init_clickhouse_client;
pub type FeeAmountEnabledData =
    RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::FeeAmountEnabled;
#[derive(Debug, Clone)]
pub struct FeeAmountEnabledResult {
    pub event_data: FeeAmountEnabledData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for FeeAmountEnabledResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type OwnerChangedData = RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::OwnerChanged;
#[derive(Debug, Clone)]
pub struct OwnerChangedResult {
    pub event_data: OwnerChangedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for OwnerChangedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type PoolCreatedData = RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::PoolCreated;
#[derive(Debug, Clone)]
pub struct PoolCreatedResult {
    pub event_data: PoolCreatedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for PoolCreatedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<ClickhouseClient>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn poolcreated_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> PoolCreatedEventCallbackType<TExtensions>
where
    PoolCreatedResult: Clone + 'static,
    F: for<'a> Fn(Vec<PoolCreatedResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type PoolCreatedEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<PoolCreatedResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: PoolCreatedEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        PoolCreatedResult: Clone + 'static,
        F: for<'a> Fn(Vec<PoolCreatedResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        Self {
            callback: poolcreated_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_clickhouse_client().await,
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<PoolCreatedResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<PoolCreatedData>().ok().map(|arc| PoolCreatedResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("PoolCreatedEvent: Unexpected data type - expected: PoolCreatedData")
        }
    }
}
pub enum UniswapV3FactoryPoolCreatedToken0Token1EventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    PoolCreated(PoolCreatedEvent<TExtensions>),
}
pub async fn uniswap_v3_factory_pool_created_token_0_token_1_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3FactoryPoolCreatedToken0Token1GenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3FactoryPoolCreatedToken0Token1GenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else if network == "ethereum" {
        RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3FactoryPoolCreatedToken0Token1EventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated(_) => {
                "0x783cca1c0412dd0d695e784568c96da2e9c22ff989357a2e8b1d9b2b4e6b7118"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated(_) => "PoolCreated",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3FactoryPoolCreatedToken0Token1".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated(_) => Arc::new(
                move |topics: Vec<B256>, data: Bytes| match PoolCreatedData::decode_raw_log(
                    topics,
                    &data[0..],
                ) {
                    Ok(event) => {
                        let result: PoolCreatedData = event;
                        Arc::new(result) as Arc<dyn Any + Send + Sync>
                    }
                    Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                },
            ),
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .map_or(false, |vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .map_or(false, |n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerFactoryContract".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_clickhouse_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::uniswap_v3_factory_abi_gen::RindexerUniswapV3FactoryGen::{
    self, RindexerUniswapV3FactoryGenEvents, RindexerUniswapV3FactoryGenInstance,
};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, Bytes, B256};
use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
use rindexer::{
    async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation,
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::read_manifest,
    },
    provider::{JsonRpcCachedProvider, RindexerProvider},
    ClickhouseClient, FutureExt,
};
use std::collections::HashMap;
use std::error::Error;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type FeeAmountEnabledData = RindexerUniswapV3FactoryGen::FeeAmountEnabled;
#[derive(Debug, Clone)]
pub struct FeeAmountEnabledResult {
    pub event_data: FeeAmountEnabledData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for FeeAmountEnabledResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type OwnerChangedData = RindexerUniswapV3FactoryGen::OwnerChanged;
#[derive(Debug, Clone)]
pub struct OwnerChangedResult {
    pub event_data: OwnerChangedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for OwnerChangedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type PoolCreatedData = RindexerUniswapV3FactoryGen::PoolCreated;
#[derive(Debug, Clone)]
pub struct PoolCreatedResult {
    pub event_data: PoolCreatedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for PoolCreatedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<ClickhouseClient>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn ownerchanged_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> OwnerChangedEventCallbackType<TExtensions>
where
    OwnerChangedResult: Clone + 'static,
    F: for<'a> Fn(Vec<OwnerChangedResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type OwnerChangedEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<OwnerChangedResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct OwnerChangedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: OwnerChangedEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> OwnerChangedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        OwnerChangedResult: Clone + 'static,
        F: for<'a> Fn(Vec<OwnerChangedResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        Self {
            callback: ownerchanged_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_clickhouse_client().await,
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for OwnerChangedEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<OwnerChangedResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<OwnerChangedData>().ok().map(|arc| {
                    OwnerChangedResult {
                        event_data: (*arc).clone(),
                        tx_information: item.tx_information,
                    }
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("OwnerChangedEvent: Unexpected data type - expected: OwnerChangedData")
        }
    }
}
pub enum UniswapV3FactoryEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    OwnerChanged(OwnerChangedEvent<TExtensions>),
}
pub async fn uniswap_v3_factory_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3FactoryGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3FactoryGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3FactoryGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3FactoryGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else if network == "ethereum" {
        RindexerUniswapV3FactoryGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3FactoryEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3FactoryEventType::OwnerChanged(_) => {
                "0xb532073b38c83145e3e5135377a08bf9aab55bc0fd7c1179cd4fb995d2a5159c"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3FactoryEventType::OwnerChanged(_) => "OwnerChanged",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3Factory".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3FactoryEventType::OwnerChanged(_) => {
                Arc::new(move |topics: Vec<B256>, data: Bytes| {
                    match OwnerChangedData::decode_raw_log(topics, &data[0..]) {
                        Ok(event) => {
                            let result: OwnerChangedData = event;
                            Arc::new(result) as Arc<dyn Any + Send + Sync>
                        }
                        Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                    }
                })
            }
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .map_or(false, |vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .map_or(false, |n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3FactoryEventType::OwnerChanged(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerFactoryContract".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_pool_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3PoolGen,
    r#"[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Burn",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "Collect",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "CollectProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid1",
        "type": "uint256"
      }
    ],
    "name": "Flash",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextOld",
        "type": "uint16"
      },
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextNew",
        "type": "uint16"
      }
    ],
    "name": "IncreaseObservationCardinalityNext",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Initialize",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Mint",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0New",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1New",
        "type": "uint8"
      }
    ],
    "name": "SetFeeProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "liquidity",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Swap",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      }
    ],
    "name": "burn",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collect",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collectProtocol",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "factory",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "fee",
    "outputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal0X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal1X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "flash",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      }
    ],
    "name": "increaseObservationCardinalityNext",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      }
    ],
    "name": "initialize",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "liquidity",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "maxLiquidityPerTick",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "mint",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint256",
        "name": "index",
        "type": "uint256"
      }
    ],
    "name": "observations",
    "outputs": [
      {
        "internalType": "uint32",
        "name": "blockTimestamp",
        "type": "uint32"
      },
      {
        "internalType": "int56",
        "name": "tickCumulative",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityCumulativeX128",
        "type": "uint160"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint32[]",
        "name": "secondsAgos",
        "type": "uint32[]"
      }
    ],
    "name": "observe",
    "outputs": [
      {
        "internalType": "int56[]",
        "name": "tickCumulatives",
        "type": "int56[]"
      },
      {
        "internalType": "uint160[]",
        "name": "secondsPerLiquidityCumulativeX128s",
        "type": "uint160[]"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "bytes32",
        "name": "key",
        "type": "bytes32"
      }
    ],
    "name": "positions",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "_liquidity",
        "type": "uint128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside0LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside1LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "protocolFees",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "token0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "token1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint8",
        "name": "feeProtocol0",
        "type": "uint8"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol1",
        "type": "uint8"
      }
    ],
    "name": "setFeeProtocol",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "slot0",
    "outputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      },
      {
        "internalType": "uint16",
        "name": "observationIndex",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinality",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol",
        "type": "uint8"
      },
      {
        "internalType": "bool",
        "name": "unlocked",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      }
    ],
    "name": "snapshotCumulativesInside",
    "outputs": [
      {
        "internalType": "int56",
        "name": "tickCumulativeInside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityInsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsInside",
        "type": "uint32"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "bool",
        "name": "zeroForOne",
        "type": "bool"
      },
      {
        "internalType": "int256",
        "name": "amountSpecified",
        "type": "int256"
      },
      {
        "internalType": "uint160",
        "name": "sqrtPriceLimitX96",
        "type": "uint160"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "swap",
    "outputs": [
      {
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int16",
        "name": "wordPosition",
        "type": "int16"
      }
    ],
    "name": "tickBitmap",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "tickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "ticks",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "liquidityGross",
        "type": "uint128"
      },
      {
        "internalType": "int128",
        "name": "liquidityNet",
        "type": "int128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside0X128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside1X128",
        "type": "uint256"
      },
      {
        "internalType": "int56",
        "name": "tickCumulativeOutside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityOutsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsOutside",
        "type": "uint32"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token0",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token1",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  }
]"#
);
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_pool_token_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3PoolTokenGen,
    r#"[
  {
    "constant": true,
    "inputs": [],
    "name": "name",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_spender",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "approve",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "totalSupply",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_from",
        "type": "address"
      },
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transferFrom",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "decimals",
    "outputs": [
      {
        "name": "",
        "type": "uint8"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "balanceOf",
    "outputs": [
      {
        "name": "balance",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "symbol",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transfer",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      },
      {
        "name": "_spender",
        "type": "address"
      }
    ],
    "name": "allowance",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "payable": true,
    "stateMutability": "payable",
    "type": "fallback"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "spender",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Approval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "from",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Transfer",
    "type": "event"
  }
]
"#
);
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_pool_token.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_clickhouse_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::uniswap_v3_pool_token_abi_gen::RindexerUniswapV3PoolTokenGen::{
    self, RindexerUniswapV3PoolTokenGenEvents, RindexerUniswapV3PoolTokenGenInstance,
};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, Bytes, B256};
use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
use rindexer::{
    async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation,
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::read_manifest,
    },
    provider::{JsonRpcCachedProvider, RindexerProvider},
    ClickhouseClient, FutureExt,
};
use std::collections::HashMap;
use std::error::Error;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type ApprovalData = RindexerUniswapV3PoolTokenGen::Approval;
#[derive(Debug, Clone)]
pub struct ApprovalResult {
    pub event_data: ApprovalData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for ApprovalResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type TransferData = RindexerUniswapV3PoolTokenGen::Transfer;
#[derive(Debug, Clone)]
pub struct TransferResult {
    pub event_data: TransferData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for TransferResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<ClickhouseClient>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn transfer_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> TransferEventCallbackType<TExtensions>
where
    TransferResult: Clone + 'static,
    F: for<'a> Fn(Vec<TransferResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type TransferEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<TransferResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct TransferEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: TransferEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> TransferEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        TransferResult: Clone + 'static,
        F: for<'a> Fn(Vec<TransferResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        Self {
            callback: transfer_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_clickhouse_client().await,
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for TransferEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<TransferResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<TransferData>().ok().map(|arc| TransferResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("TransferEvent: Unexpected data type - expected: TransferData")
        }
    }
}
pub enum UniswapV3PoolTokenEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    Transfer(TransferEvent<TExtensions>),
}
pub async fn uniswap_v3_pool_token_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3PoolTokenGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3PoolTokenGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3PoolTokenGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3PoolTokenGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else if network == "ethereum" {
        RindexerUniswapV3PoolTokenGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3PoolTokenEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3PoolTokenEventType::Transfer(_) => {
                "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3PoolTokenEventType::Transfer(_) => "Transfer",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3PoolToken".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3PoolTokenEventType::Transfer(_) => {
                Arc::new(move |topics: Vec<B256>, data: Bytes| {
                    match TransferData::decode_raw_log(topics, &data[0..]) {
                        Ok(event) => {
                            let result: TransferData = event;
                            Arc::new(result) as Arc<dyn Any + Send + Sync>
                        }
                        Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                    }
                })
            }
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .map_or(false, |vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .map_or(false, |n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3PoolTokenEventType::Transfer(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerFactoryContract".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_pool.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_clickhouse_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::uniswap_v3_pool_abi_gen::RindexerUniswapV3PoolGen::{
    self, RindexerUniswapV3PoolGenEvents, RindexerUniswapV3PoolGenInstance,
};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, Bytes, B256};
use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
use rindexer::{
    async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation,
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::read_manifest,
    },
    provider::{JsonRpcCachedProvider, RindexerProvider},
    ClickhouseClient, FutureExt,
};
use std::collections::HashMap;
use std::error::Error;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type BurnData = RindexerUniswapV3PoolGen::Burn;
#[derive(Debug, Clone)]
pub struct BurnResult {
    pub event_data: BurnData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for BurnResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type CollectData = RindexerUniswapV3PoolGen::Collect;
#[derive(Debug, Clone)]
pub struct CollectResult {
    pub event_data: CollectData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for CollectResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type CollectProtocolData = RindexerUniswapV3PoolGen::CollectProtocol;
#[derive(Debug, Clone)]
pub struct CollectProtocolResult {
    pub event_data: CollectProtocolData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for CollectProtocolResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type FlashData = RindexerUniswapV3PoolGen::Flash;
#[derive(Debug, Clone)]
pub struct FlashResult {
    pub event_data: FlashData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for FlashResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type IncreaseObservationCardinalityNextData =
    RindexerUniswapV3PoolGen::IncreaseObservationCardinalityNext;
#[derive(Debug, Clone)]
pub struct IncreaseObservationCardinalityNextResult {
    pub event_data: IncreaseObservationCardinalityNextData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for IncreaseObservationCardinalityNextResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type InitializeData = RindexerUniswapV3PoolGen::Initialize;
#[derive(Debug, Clone)]
pub struct InitializeResult {
    pub event_data: InitializeData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for InitializeResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type MintData = RindexerUniswapV3PoolGen::Mint;
#[derive(Debug, Clone)]
pub struct MintResult {
    pub event_data: MintData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for MintResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type SetFeeProtocolData = RindexerUniswapV3PoolGen::SetFeeProtocol;
#[derive(Debug, Clone)]
pub struct SetFeeProtocolResult {
    pub event_data: SetFeeProtocolData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for SetFeeProtocolResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type SwapData = RindexerUniswapV3PoolGen::Swap;
#[derive(Debug, Clone)]
pub struct SwapResult {
    pub event_data: SwapData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for SwapResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<ClickhouseClient>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn swap_handler<TExtensions, F, Fut>(custom_logic: F) -> SwapEventCallbackType<TExtensions>
where
    SwapResult: Clone + 'static,
    F: for<'a> Fn(Vec<SwapResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type SwapEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<SwapResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct SwapEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: SwapEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> SwapEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        SwapResult: Clone + 'static,
        F: for<'a> Fn(Vec<SwapResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        Self {
            callback: swap_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_clickhouse_client().await,
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for SwapEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<SwapResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<SwapData>().ok().map(|arc| SwapResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("SwapEvent: Unexpected data type - expected: SwapData")
        }
    }
}
pub enum UniswapV3PoolEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    Swap(SwapEvent<TExtensions>),
}
pub async fn uniswap_v3_pool_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3PoolGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3PoolGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3PoolGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3PoolGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else if network == "ethereum" {
        RindexerUniswapV3PoolGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3PoolEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3PoolEventType::Swap(_) => {
                "0xc42079f94a6350d7e6235f29174924f928cc2ac818eb64fed8004e115fbcca67"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3PoolEventType::Swap(_) => "Swap",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3Pool".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3PoolEventType::Swap(_) => Arc::new(move |topics: Vec<B256>, data: Bytes| {
                match SwapData::decode_raw_log(topics, &data[0..]) {
                    Ok(event) => {
                        let result: SwapData = event;
                        Arc::new(result) as Arc<dyn Any + Send + Sync>
                    }
                    Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                }
            }),
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .map_or(false, |vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .map_or(false, |n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3PoolEventType::Swap(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerFactoryContract".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/mod.rs">
#![allow(dead_code, unused)]
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer
/// Any manual changes to this file will be overwritten.
pub mod events;
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/database.rs">
use rindexer::ClickhouseClient;
use std::sync::Arc;
use tokio::sync::OnceCell;
static CLICKHOUSE_CLIENT: OnceCell<Arc<ClickhouseClient>> = OnceCell::const_new();
pub async fn get_or_init_clickhouse_client() -> Arc<ClickhouseClient> {
    CLICKHOUSE_CLIENT
        .get_or_init(|| async {
            Arc::new(ClickhouseClient::new().await.expect("Failed to connect to Clickhouse"))
        })
        .await
        .clone()
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/mod.rs">
#![allow(dead_code, unused)]
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer
/// Any manual changes to this file will be overwritten.
pub mod database;
pub mod networks;
pub mod rindexer_factory_contract;
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/typings/networks.rs">
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use alloy::{primitives::U64, transports::http::reqwest::header::HeaderMap};
use rindexer::{
    lazy_static,
    manifest::network::{AddressFiltering, BlockPollFrequency},
    notifications::ChainStateNotification,
    provider::{create_client, JsonRpcCachedProvider, RetryClientError, RindexerProvider},
    public_read_env_value,
};
use std::sync::Arc;
use tokio::sync::broadcast::Sender;
use tokio::sync::OnceCell;
#[allow(dead_code)]
async fn create_shadow_client(
    rpc_url: &str,
    chain_id: u64,
    compute_units_per_second: Option<u64>,
    block_poll_frequency: Option<BlockPollFrequency>,
    max_block_range: Option<U64>,
    address_filtering: Option<AddressFiltering>,
    chain_state_notification: Option<Sender<ChainStateNotification>>,
) -> Result<Arc<JsonRpcCachedProvider>, RetryClientError> {
    let mut header = HeaderMap::new();
    header.insert(
        "X-SHADOW-API-KEY",
        public_read_env_value("RINDEXER_PHANTOM_API_KEY").unwrap().parse().unwrap(),
    );
    create_client(
        rpc_url,
        chain_id,
        compute_units_per_second,
        max_block_range,
        block_poll_frequency,
        header,
        address_filtering,
        chain_state_notification,
    )
    .await
}
static ETHEREUM_PROVIDER: OnceCell<Arc<JsonRpcCachedProvider>> = OnceCell::const_new();
static BASE_PROVIDER: OnceCell<Arc<JsonRpcCachedProvider>> = OnceCell::const_new();
pub async fn get_ethereum_provider_cache() -> Arc<JsonRpcCachedProvider> {
    ETHEREUM_PROVIDER
        .get_or_init(|| async {
            let chain_state_notification = None;
            create_client(
                &public_read_env_value(
                    "https://eth-mainnet.g.alchemy.com/v2/8MK-cIOhcdlVH6z13k0RG928Iieg71fl",
                )
                .unwrap_or(
                    "https://eth-mainnet.g.alchemy.com/v2/8MK-cIOhcdlVH6z13k0RG928Iieg71fl"
                        .to_string(),
                ),
                1,
                None,
                None,
                None,
                HeaderMap::new(),
                None,
                chain_state_notification,
            )
            .await
            .expect("Error creating provider")
        })
        .await
        .clone()
}
pub async fn get_ethereum_provider() -> Arc<RindexerProvider> {
    get_ethereum_provider_cache().await.get_inner_provider()
}
pub async fn get_base_provider_cache() -> Arc<JsonRpcCachedProvider> {
    BASE_PROVIDER
        .get_or_init(|| async {
            let chain_state_notification = None;
            create_client(
                &public_read_env_value(
                    "https://base-mainnet.g.alchemy.com/v2/8MK-cIOhcdlVH6z13k0RG928Iieg71fl",
                )
                .unwrap_or(
                    "https://base-mainnet.g.alchemy.com/v2/8MK-cIOhcdlVH6z13k0RG928Iieg71fl"
                        .to_string(),
                ),
                8453,
                None,
                None,
                None,
                HeaderMap::new(),
                None,
                chain_state_notification,
            )
            .await
            .expect("Error creating provider")
        })
        .await
        .clone()
}
pub async fn get_base_provider() -> Arc<RindexerProvider> {
    get_base_provider_cache().await.get_inner_provider()
}
pub async fn get_provider_cache_for_network(network: &str) -> Arc<JsonRpcCachedProvider> {
    if network == "ethereum" {
        return get_ethereum_provider_cache().await;
    }
    if network == "base" {
        return get_base_provider_cache().await;
    }
    panic!("Network not supported")
}
</file>

<file path="examples/clickhouse_factory_indexing/src/rindexer_lib/mod.rs">
#![allow(dead_code, unused)]
pub mod indexers;
pub mod typings;
</file>

<file path="examples/clickhouse_factory_indexing/src/main.rs">
use self::rindexer_lib::indexers::all_handlers::register_all_handlers;
use rindexer::{
    event::callback_registry::TraceCallbackRegistry, start_rindexer, GraphqlOverrideSettings,
    IndexingDetails, StartDetails,
};
use std::env;
#[allow(clippy::all)]
mod rindexer_lib;
#[tokio::main]
async fn main() {
    let args: Vec<String> = env::args().collect();
    let mut enable_graphql = false;
    let mut enable_indexer = false;
    let mut graphql_port: Option<u16> = None;
    let args = args.iter();
    if args.len() == 1 {
        enable_graphql = true;
        enable_indexer = true;
    }
    for arg in args {
        match arg.as_str() {
            "--graphql" => enable_graphql = true,
            "--indexer" => enable_indexer = true,
            _ if arg.starts_with("--port=") || arg.starts_with("--p") => {
                if let Some(value) = arg.split('=').nth(1) {
                    let overridden_port = value.parse::<u16>();
                    match overridden_port {
                        Ok(overridden_port) => graphql_port = Some(overridden_port),
                        Err(_) => {
                            println!("Invalid port number");
                            return;
                        }
                    }
                }
            }
            _ => {}
        }
    }
    let path = env::current_dir();
    match path {
        Ok(path) => {
            let manifest_path = path.join("rindexer.yaml");
            let result = start_rindexer(StartDetails {
                manifest_path: &manifest_path,
                indexing_details: if enable_indexer {
                    Some(IndexingDetails {
                        registry: register_all_handlers(&manifest_path).await,
                        trace_registry: TraceCallbackRegistry { events: vec![] },
                        event_stream: None,
                    })
                } else {
                    None
                },
                graphql_details: GraphqlOverrideSettings {
                    enabled: enable_graphql,
                    override_port: graphql_port,
                },
            })
            .await;
            match result {
                Ok(_) => {}
                Err(e) => {
                    println!("Error starting rindexer: {:?}", e);
                }
            }
        }
        Err(e) => {
            println!("Error getting current directory: {:?}", e);
        }
    }
}
</file>

<file path="examples/clickhouse_factory_indexing/.env.example">
CLICKHOUSE_URL="http://localhost:8123"
CLICKHOUSE_DB="default"
CLICKHOUSE_USER="default"
CLICKHOUSE_PASSWORD="default"
CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT="1"
</file>

<file path="examples/clickhouse_factory_indexing/.gitignore">
generated_csv
</file>

<file path="examples/clickhouse_factory_indexing/Cargo.toml">
[package]
name = "clickhouse_factory_indexing"
version = "0.1.0"
edition = "2021"

[dependencies]
# internal dependencies
rindexer = { path = "../../core" }

# external dependencies
tokio = { version = "1", features = ["full"] }
alloy = { version = "1.1.3", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
</file>

<file path="examples/clickhouse_factory_indexing/docker-compose.yml">
volumes:
  clickhouse_data:
    driver: local
services:
  clickhouse:
    image: clickhouse/clickhouse-server:25.7
    container_name: nocode_clickhouse
    restart: unless-stopped
    shm_size: 1g
    env_file:
      - ./.env
    volumes:
      - clickhouse_data:/var
    healthcheck:
      test: [ "CMD-SHELL", "clickhouse-client --query='SELECT 1' || exit 1" ]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s
    ports:
      - "8123:8123" # HTTP interface
      - "9000:9000" # Native TCP interface
      - "9009:9009" # Interserver communication
</file>

<file path="examples/clickhouse_factory_indexing/rindexer.yaml">
name: RindexerFactoryContract
project_type: rust
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
  - name: base
    chain_id: 8453
    rpc: https://base.llamarpc.com
storage:
  clickhouse:
    enabled: true
    drop_each_run: true
contracts:
  - name: UniswapV3Factory
    details:
      - network: base
        start_block: 33000000
        end_block: 33000500
        address: 0x33128a8fC17869897dcE68Ed026d694621f6FDfD
      - network: ethereum
        start_block: 22000000
        end_block: 22000500
        address: 0x1F98431c8aD98523631AE4a59f267346ea31F984
    abi: ./abis/uniswap-v3-factory-abi.json
    include_events:
      - OwnerChanged
  - name: UniswapV3Pool
    details:
      - network: base
        start_block: 33000000
        end_block: 33000500
        factory:
          name: UniswapV3Factory
          address: 0x33128a8fC17869897dcE68Ed026d694621f6FDfD
          abi: ./abis/uniswap-v3-factory-abi.json
          event_name: PoolCreated
          input_name: "pool"
      - network: ethereum
        start_block: 22000000
        end_block: 22000500
        factory:
          name: UniswapV3Factory
          address: 0x1F98431c8aD98523631AE4a59f267346ea31F984
          abi: ./abis/uniswap-v3-factory-abi.json
          event_name: PoolCreated
          input_name: "pool"
    abi: ./abis/uniswap-v3-pool-abi.json
    include_events:
      - Swap
  - name: UniswapV3PoolToken
    details:
      - network: base
        start_block: 33000000
        end_block: 33000500
        factory:
          name: UniswapV3Factory
          address: 0x33128a8fC17869897dcE68Ed026d694621f6FDfD
          abi: ./abis/uniswap-v3-factory-abi.json
          event_name: PoolCreated
          input_name:
            - "token0"
            - "token1"
      - network: ethereum
        start_block: 22000000
        end_block: 22000500
        factory:
          name: UniswapV3Factory
          address: 0x1F98431c8aD98523631AE4a59f267346ea31F984
          abi: ./abis/uniswap-v3-factory-abi.json
          event_name: PoolCreated
          input_name:
            - "token0"
            - "token1"
    abi: ./abis/erc20-abi.json
    include_events:
      - Transfer
</file>

<file path="examples/nocode_clickhouse/abis/RocketTokenRETH.abi.json">
[
  {
    "inputs":[
      {
        "internalType":"contract RocketStorageInterface",
        "name":"_rocketStorageAddress",
        "type":"address"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"constructor"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"owner",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value",
        "type":"uint256"
      }
    ],
    "name":"Approval",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"EtherDeposited",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"ethAmount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"TokensBurned",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"ethAmount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"TokensMinted",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value",
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"owner",
        "type":"address"
      },
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      }
    ],
    "name":"allowance",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"approve",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"account",
        "type":"address"
      }
    ],
    "name":"balanceOf",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_rethAmount",
        "type":"uint256"
      }
    ],
    "name":"burn",
    "outputs":[

    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"decimals",
    "outputs":[
      {
        "internalType":"uint8",
        "name":"",
        "type":"uint8"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"subtractedValue",
        "type":"uint256"
      }
    ],
    "name":"decreaseAllowance",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"depositExcess",
    "outputs":[

    ],
    "stateMutability":"payable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"depositExcessCollateral",
    "outputs":[

    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"getCollateralRate",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_rethAmount",
        "type":"uint256"
      }
    ],
    "name":"getEthValue",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"getExchangeRate",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_ethAmount",
        "type":"uint256"
      }
    ],
    "name":"getRethValue",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"getTotalCollateral",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"addedValue",
        "type":"uint256"
      }
    ],
    "name":"increaseAllowance",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_ethAmount",
        "type":"uint256"
      },
      {
        "internalType":"address",
        "name":"_to",
        "type":"address"
      }
    ],
    "name":"mint",
    "outputs":[

    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"name",
    "outputs":[
      {
        "internalType":"string",
        "name":"",
        "type":"string"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"symbol",
    "outputs":[
      {
        "internalType":"string",
        "name":"",
        "type":"string"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"totalSupply",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"recipient",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"transfer",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"sender",
        "type":"address"
      },
      {
        "internalType":"address",
        "name":"recipient",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"transferFrom",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"version",
    "outputs":[
      {
        "internalType":"uint8",
        "name":"",
        "type":"uint8"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "stateMutability":"payable",
    "type":"receive"
  }
]
</file>

<file path="examples/nocode_clickhouse/.env.example">
CLICKHOUSE_URL="http://localhost:8123"
CLICKHOUSE_DB="default"
CLICKHOUSE_USER="default"
CLICKHOUSE_PASSWORD="default"
CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT="1"
</file>

<file path="examples/nocode_clickhouse/.gitignore">
.rindexer
generated_csv/**/*.txt
</file>

<file path="examples/nocode_clickhouse/docker-compose.yml">
volumes:
  clickhouse_data:
    driver: local
services:
  clickhouse:
    image: clickhouse/clickhouse-server:25.7
    container_name: nocode_clickhouse
    restart: unless-stopped
    shm_size: 1g
    env_file:
      - ./.env
    volumes:
      - clickhouse_data:/var
    healthcheck:
      test: [ "CMD-SHELL", "clickhouse-client --query='SELECT 1' || exit 1" ]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s
    ports:
      - "8123:8123" # HTTP interface
      - "9000:9000" # Native TCP interface
      - "9009:9009" # Interserver communication
</file>

<file path="examples/nocode_clickhouse/rindexer.yaml">
name: ClickhouseIndexer
description: My first clickhouse rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
timestamps: true
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  block_poll_frequency: rapid
storage:
  clickhouse:
    enabled: true
    drop_each_run: true
contracts:
- name: RocketPool
  details:
  - network: ethereum
    address: 0xae78736cd615f374d3085123a210448e74fc6393
    start_block: '18600000'
    end_block: '18718056'
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
  - Transfer
</file>

<file path="examples/rindexer_demo_cli/abis/RocketTokenRETH.abi.json">
[
  {
    "inputs":[
      {
        "internalType":"contract RocketStorageInterface",
        "name":"_rocketStorageAddress",
        "type":"address"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"constructor"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"owner",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value",
        "type":"uint256"
      }
    ],
    "name":"Approval",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"EtherDeposited",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"ethAmount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"TokensBurned",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"ethAmount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"TokensMinted",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value",
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"owner",
        "type":"address"
      },
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      }
    ],
    "name":"allowance",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"approve",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"account",
        "type":"address"
      }
    ],
    "name":"balanceOf",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_rethAmount",
        "type":"uint256"
      }
    ],
    "name":"burn",
    "outputs":[

    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"decimals",
    "outputs":[
      {
        "internalType":"uint8",
        "name":"",
        "type":"uint8"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"subtractedValue",
        "type":"uint256"
      }
    ],
    "name":"decreaseAllowance",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"depositExcess",
    "outputs":[

    ],
    "stateMutability":"payable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"depositExcessCollateral",
    "outputs":[

    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"getCollateralRate",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_rethAmount",
        "type":"uint256"
      }
    ],
    "name":"getEthValue",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"getExchangeRate",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_ethAmount",
        "type":"uint256"
      }
    ],
    "name":"getRethValue",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"getTotalCollateral",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"addedValue",
        "type":"uint256"
      }
    ],
    "name":"increaseAllowance",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_ethAmount",
        "type":"uint256"
      },
      {
        "internalType":"address",
        "name":"_to",
        "type":"address"
      }
    ],
    "name":"mint",
    "outputs":[

    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"name",
    "outputs":[
      {
        "internalType":"string",
        "name":"",
        "type":"string"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"symbol",
    "outputs":[
      {
        "internalType":"string",
        "name":"",
        "type":"string"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"totalSupply",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"recipient",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"transfer",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"sender",
        "type":"address"
      },
      {
        "internalType":"address",
        "name":"recipient",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"transferFrom",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"version",
    "outputs":[
      {
        "internalType":"uint8",
        "name":"",
        "type":"uint8"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "stateMutability":"payable",
    "type":"receive"
  }
]
</file>

<file path="examples/rindexer_demo_cli/.gitignore">
.rindexer
generated_csv/**/*.txt
</file>

<file path="examples/rindexer_demo_cli/docker-compose.yml">
volumes:
  postgres_data:
    driver: local
services:
  postgresql:
    image: postgres:16
    shm_size: 1g
    restart: always
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - 5440:5432
    env_file:
      - ./.env
</file>

<file path="examples/rindexer_demo_cli/rindexer.yaml">
name: RocketPoolETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
timestamps: true
networks:
- name: ethereum
  chain_id: 1
  rpc: https://mainnet.gateway.tenderly.co
  block_poll_frequency: rapid
storage:
  postgres:
    enabled: true
    drop_each_run: true
contracts:
- name: RocketPoolETH
  details:
  - network: ethereum
    address: 0xae78736cd615f374d3085123a210448e74fc6393
    start_block: '18600000'
    end_block: '18718056'
  abi: ./abis/RocketTokenRETH.abi.json
  include_events:
  - Transfer
</file>

<file path="examples/rindexer_factory_indexing/abis/erc20-abi.json">
[
  {
    "constant": true,
    "inputs": [],
    "name": "name",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_spender",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "approve",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "totalSupply",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_from",
        "type": "address"
      },
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transferFrom",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "decimals",
    "outputs": [
      {
        "name": "",
        "type": "uint8"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "balanceOf",
    "outputs": [
      {
        "name": "balance",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "symbol",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transfer",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      },
      {
        "name": "_spender",
        "type": "address"
      }
    ],
    "name": "allowance",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "payable": true,
    "stateMutability": "payable",
    "type": "fallback"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "spender",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Approval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "from",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Transfer",
    "type": "event"
  }
]
</file>

<file path="examples/rindexer_factory_indexing/abis/uniswap-v3-factory-abi.json">
[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "FeeAmountEnabled",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "oldOwner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newOwner",
        "type": "address"
      }
    ],
    "name": "OwnerChanged",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "name": "PoolCreated",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "tokenA",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "tokenB",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      }
    ],
    "name": "createPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "enableFeeAmount",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "feeAmountTickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "getPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "owner",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "parameters",
    "outputs": [
      {
        "internalType": "address",
        "name": "factory",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "setOwner",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  }
]
</file>

<file path="examples/rindexer_factory_indexing/abis/uniswap-v3-pool-abi.json">
[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Burn",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "Collect",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "CollectProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid1",
        "type": "uint256"
      }
    ],
    "name": "Flash",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextOld",
        "type": "uint16"
      },
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextNew",
        "type": "uint16"
      }
    ],
    "name": "IncreaseObservationCardinalityNext",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Initialize",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Mint",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0New",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1New",
        "type": "uint8"
      }
    ],
    "name": "SetFeeProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "liquidity",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Swap",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      }
    ],
    "name": "burn",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collect",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collectProtocol",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "factory",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "fee",
    "outputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal0X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal1X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "flash",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      }
    ],
    "name": "increaseObservationCardinalityNext",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      }
    ],
    "name": "initialize",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "liquidity",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "maxLiquidityPerTick",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "mint",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint256",
        "name": "index",
        "type": "uint256"
      }
    ],
    "name": "observations",
    "outputs": [
      {
        "internalType": "uint32",
        "name": "blockTimestamp",
        "type": "uint32"
      },
      {
        "internalType": "int56",
        "name": "tickCumulative",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityCumulativeX128",
        "type": "uint160"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint32[]",
        "name": "secondsAgos",
        "type": "uint32[]"
      }
    ],
    "name": "observe",
    "outputs": [
      {
        "internalType": "int56[]",
        "name": "tickCumulatives",
        "type": "int56[]"
      },
      {
        "internalType": "uint160[]",
        "name": "secondsPerLiquidityCumulativeX128s",
        "type": "uint160[]"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "bytes32",
        "name": "key",
        "type": "bytes32"
      }
    ],
    "name": "positions",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "_liquidity",
        "type": "uint128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside0LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside1LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "protocolFees",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "token0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "token1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint8",
        "name": "feeProtocol0",
        "type": "uint8"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol1",
        "type": "uint8"
      }
    ],
    "name": "setFeeProtocol",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "slot0",
    "outputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      },
      {
        "internalType": "uint16",
        "name": "observationIndex",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinality",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol",
        "type": "uint8"
      },
      {
        "internalType": "bool",
        "name": "unlocked",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      }
    ],
    "name": "snapshotCumulativesInside",
    "outputs": [
      {
        "internalType": "int56",
        "name": "tickCumulativeInside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityInsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsInside",
        "type": "uint32"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "bool",
        "name": "zeroForOne",
        "type": "bool"
      },
      {
        "internalType": "int256",
        "name": "amountSpecified",
        "type": "int256"
      },
      {
        "internalType": "uint160",
        "name": "sqrtPriceLimitX96",
        "type": "uint160"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "swap",
    "outputs": [
      {
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int16",
        "name": "wordPosition",
        "type": "int16"
      }
    ],
    "name": "tickBitmap",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "tickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "ticks",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "liquidityGross",
        "type": "uint128"
      },
      {
        "internalType": "int128",
        "name": "liquidityNet",
        "type": "int128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside0X128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside1X128",
        "type": "uint256"
      },
      {
        "internalType": "int56",
        "name": "tickCumulativeOutside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityOutsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsOutside",
        "type": "uint32"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token0",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token1",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  }
]
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/mod.rs">
#![allow(dead_code, unused)]
pub mod uniswap_v3_factory;
pub mod uniswap_v3_factory_pool_created_pool;
pub mod uniswap_v3_factory_pool_created_token_0_token_1;
pub mod uniswap_v3_pool;
pub mod uniswap_v3_pool_token;
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/uniswap_v3_factory_pool_created_pool.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_factory_contract::events::uniswap_v3_factory_pool_created_pool::{
    no_extensions, PoolCreatedEvent, UniswapV3FactoryPoolCreatedPoolEventType,
};
use alloy::primitives::{I256, U256, U64};
use rindexer::{
    event::callback_registry::EventCallbackRegistry, rindexer_error, rindexer_info,
    EthereumSqlTypeWrapper, PgType, RindexerColorize,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn pool_created_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = PoolCreatedEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = Vec::new();
            let mut csv_bulk_data: Vec<Vec<String>> = Vec::new();
            for result in results.iter() {
                csv_bulk_data.push(vec![
                    result.tx_information.address.to_string(),
                    result.event_data.token0.to_string(),
                    result.event_data.token1.to_string(),
                    result.event_data.fee.to_string(),
                    result.event_data.tickSpacing.to_string(),
                    result.event_data.pool.to_string(),
                    result.tx_information.transaction_hash.to_string(),
                    result.tx_information.block_number.to_string(),
                    result.tx_information.block_hash.to_string(),
                    result.tx_information.network.to_string(),
                    result.tx_information.transaction_index.to_string(),
                    result.tx_information.log_index.to_string(),
                ]);
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.token0),
                    EthereumSqlTypeWrapper::Address(result.event_data.token1),
                    EthereumSqlTypeWrapper::U32(result.event_data.fee.to()),
                    EthereumSqlTypeWrapper::I32(result.event_data.tickSpacing.unchecked_into()),
                    EthereumSqlTypeWrapper::Address(result.event_data.pool),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if !csv_bulk_data.is_empty() {
                if let Err(e) = context.csv.append_bulk(csv_bulk_data).await {
                    rindexer_error!(
                        "UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated inserting csv data: {:?}",
                        e
                    );
                    return Err(e.to_string());
                }
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "token_0".to_string(),
                "token_1".to_string(),
                "fee".to_string(),
                "tick_spacing".to_string(),
                "pool".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_factory_contract_uniswap_v3_factory_pool_created_pool.pool_created",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!(
                    "UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated inserting bulk data: {:?}",
                    e
                );
                return Err(e.to_string());
            }
            rindexer_info!(
                "UniswapV3FactoryPoolCreatedPool::PoolCreated - {} - {} events",
                "INDEXED".green(),
                results.len(),
            );
            Ok(())
        },
        no_extensions(),
    )
    .await;
    UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated(handler)
        .register(manifest_path, registry)
        .await;
}
pub async fn uniswap_v3_factory_pool_created_pool_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    pool_created_handler(manifest_path, registry).await;
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/uniswap_v3_factory_pool_created_token_0_token_1.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_factory_contract::events::uniswap_v3_factory_pool_created_token_0_token_1::{
    no_extensions, PoolCreatedEvent, UniswapV3FactoryPoolCreatedToken0Token1EventType,
};
use alloy::primitives::{I256, U256, U64};
use rindexer::{
    event::callback_registry::EventCallbackRegistry, rindexer_error, rindexer_info,
    EthereumSqlTypeWrapper, PgType, RindexerColorize,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn pool_created_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = PoolCreatedEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = Vec::new();
            let mut csv_bulk_data: Vec<Vec<String>> = Vec::new();
            for result in results.iter() {
                csv_bulk_data.push(vec![
                    result.tx_information.address.to_string(),
                    result.event_data.token0.to_string(),
                    result.event_data.token1.to_string(),
                    result.event_data.fee.to_string(),
                    result.event_data.tickSpacing.to_string(),
                    result.event_data.pool.to_string(),
                    result.tx_information.transaction_hash.to_string(),
                    result.tx_information.block_number.to_string(),
                    result.tx_information.block_hash.to_string(),
                    result.tx_information.network.to_string(),
                    result.tx_information.transaction_index.to_string(),
                    result.tx_information.log_index.to_string(),
                ]);
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.token0),
                    EthereumSqlTypeWrapper::Address(result.event_data.token1),
                    EthereumSqlTypeWrapper::U32(result.event_data.fee.to()),
                    EthereumSqlTypeWrapper::I32(result.event_data.tickSpacing.unchecked_into()),
                    EthereumSqlTypeWrapper::Address(result.event_data.pool),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if !csv_bulk_data.is_empty() {
                if let Err(e) = context.csv.append_bulk(csv_bulk_data).await {
                    rindexer_error!(
                        "UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated inserting csv data: {:?}",
                        e
                    );
                    return Err(e.to_string());
                }
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "token_0".to_string(),
                "token_1".to_string(),
                "fee".to_string(),
                "tick_spacing".to_string(),
                "pool".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_factory_contract_uniswap_v3_factory_pool_created_token_0_token_1.pool_created",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!(
                    "UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated inserting bulk data: {:?}",
                    e
                );
                return Err(e.to_string());
            }
            rindexer_info!(
                "UniswapV3FactoryPoolCreatedToken0Token1::PoolCreated - {} - {} events",
                "INDEXED".green(),
                results.len(),
            );
            Ok(())
        },
        no_extensions(),
    )
    .await;
    UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated(handler)
        .register(manifest_path, registry)
        .await;
}
pub async fn uniswap_v3_factory_pool_created_token_0_token_1_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    pool_created_handler(manifest_path, registry).await;
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/uniswap_v3_factory.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_factory_contract::events::uniswap_v3_factory::{
    no_extensions, OwnerChangedEvent, UniswapV3FactoryEventType,
};
use alloy::primitives::{I256, U256, U64};
use rindexer::{
    event::callback_registry::EventCallbackRegistry, rindexer_error, rindexer_info,
    EthereumSqlTypeWrapper, PgType, RindexerColorize,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn owner_changed_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = OwnerChangedEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            let mut csv_bulk_data: Vec<Vec<String>> = vec![];
            for result in results.iter() {
                csv_bulk_data.push(vec![
                    result.tx_information.address.to_string(),
                    result.event_data.oldOwner.to_string(),
                    result.event_data.newOwner.to_string(),
                    result.tx_information.transaction_hash.to_string(),
                    result.tx_information.block_number.to_string(),
                    result.tx_information.block_hash.to_string(),
                    result.tx_information.network.to_string(),
                    result.tx_information.transaction_index.to_string(),
                    result.tx_information.log_index.to_string(),
                ]);
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.oldOwner),
                    EthereumSqlTypeWrapper::Address(result.event_data.newOwner),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if !csv_bulk_data.is_empty() {
                let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                if let Err(e) = csv_result {
                    rindexer_error!(
                        "UniswapV3FactoryEventType::OwnerChanged inserting csv data: {:?}",
                        e
                    );
                    return Err(e.to_string());
                }
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "old_owner".to_string(),
                "new_owner".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_factory_contract_uniswap_v3_factory.owner_changed",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!(
                    "UniswapV3FactoryEventType::OwnerChanged inserting bulk data: {:?}",
                    e
                );
                return Err(e.to_string());
            }
            rindexer_info!(
                "UniswapV3Factory::OwnerChanged - {} - {} events",
                "INDEXED".green(),
                results.len(),
            );
            Ok(())
        },
        no_extensions(),
    )
    .await;
    UniswapV3FactoryEventType::OwnerChanged(handler).register(manifest_path, registry).await;
}
pub async fn uniswap_v3_factory_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    owner_changed_handler(manifest_path, registry).await;
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/uniswap_v3_pool_token.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_factory_contract::events::uniswap_v3_pool_token::{
    no_extensions, TransferEvent, UniswapV3PoolTokenEventType,
};
use alloy::primitives::{I256, U256, U64};
use rindexer::{
    event::callback_registry::EventCallbackRegistry, rindexer_error, rindexer_info,
    EthereumSqlTypeWrapper, PgType, RindexerColorize,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn transfer_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = TransferEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            let mut csv_bulk_data: Vec<Vec<String>> = vec![];
            for result in results.iter() {
                csv_bulk_data.push(vec![
                    result.tx_information.address.to_string(),
                    result.event_data.from.to_string(),
                    result.event_data.to.to_string(),
                    result.event_data.value.to_string(),
                    result.tx_information.transaction_hash.to_string(),
                    result.tx_information.block_number.to_string(),
                    result.tx_information.block_hash.to_string(),
                    result.tx_information.network.to_string(),
                    result.tx_information.transaction_index.to_string(),
                    result.tx_information.log_index.to_string(),
                ]);
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.from),
                    EthereumSqlTypeWrapper::Address(result.event_data.to),
                    EthereumSqlTypeWrapper::U256(U256::from(result.event_data.value)),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if !csv_bulk_data.is_empty() {
                let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                if let Err(e) = csv_result {
                    rindexer_error!(
                        "UniswapV3PoolTokenEventType::Transfer inserting csv data: {:?}",
                        e
                    );
                    return Err(e.to_string());
                }
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "from".to_string(),
                "to".to_string(),
                "value".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_factory_contract_uniswap_v3_pool_token.transfer",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!(
                    "UniswapV3PoolTokenEventType::Transfer inserting bulk data: {:?}",
                    e
                );
                return Err(e.to_string());
            }
            rindexer_info!(
                "UniswapV3PoolToken::Transfer - {} - {} events",
                "INDEXED".green(),
                results.len(),
            );
            Ok(())
        },
        no_extensions(),
    )
    .await;
    UniswapV3PoolTokenEventType::Transfer(handler).register(manifest_path, registry).await;
}
pub async fn uniswap_v3_pool_token_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    transfer_handler(manifest_path, registry).await;
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/indexers/rindexer_factory_contract/uniswap_v3_pool.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_factory_contract::events::uniswap_v3_pool::{
    no_extensions, SwapEvent, UniswapV3PoolEventType,
};
use alloy::primitives::{I256, U256, U64};
use rindexer::{
    event::callback_registry::EventCallbackRegistry, rindexer_error, rindexer_info,
    EthereumSqlTypeWrapper, PgType, RindexerColorize,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn swap_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = SwapEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            let mut csv_bulk_data: Vec<Vec<String>> = vec![];
            for result in results.iter() {
                csv_bulk_data.push(vec![
                    result.tx_information.address.to_string(),
                    result.event_data.sender.to_string(),
                    result.event_data.recipient.to_string(),
                    result.event_data.amount0.to_string(),
                    result.event_data.amount1.to_string(),
                    result.event_data.sqrtPriceX96.to_string(),
                    result.event_data.liquidity.to_string(),
                    result.event_data.tick.to_string(),
                    result.tx_information.transaction_hash.to_string(),
                    result.tx_information.block_number.to_string(),
                    result.tx_information.block_hash.to_string(),
                    result.tx_information.network.to_string(),
                    result.tx_information.transaction_index.to_string(),
                    result.tx_information.log_index.to_string(),
                ]);
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.sender),
                    EthereumSqlTypeWrapper::Address(result.event_data.recipient),
                    EthereumSqlTypeWrapper::I256(I256::from(result.event_data.amount0)),
                    EthereumSqlTypeWrapper::I256(I256::from(result.event_data.amount1)),
                    EthereumSqlTypeWrapper::U256(U256::from(result.event_data.sqrtPriceX96)),
                    EthereumSqlTypeWrapper::U128(result.event_data.liquidity),
                    EthereumSqlTypeWrapper::I32(result.event_data.tick.unchecked_into()),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if !csv_bulk_data.is_empty() {
                let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                if let Err(e) = csv_result {
                    rindexer_error!("UniswapV3PoolEventType::Swap inserting csv data: {:?}", e);
                    return Err(e.to_string());
                }
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "sender".to_string(),
                "recipient".to_string(),
                "amount_0".to_string(),
                "amount_1".to_string(),
                "sqrt_price_x96".to_string(),
                "liquidity".to_string(),
                "tick".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_factory_contract_uniswap_v3_pool.swap",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!("UniswapV3PoolEventType::Swap inserting bulk data: {:?}", e);
                return Err(e.to_string());
            }
            rindexer_info!(
                "UniswapV3Pool::Swap - {} - {} events",
                "INDEXED".green(),
                results.len(),
            );
            Ok(())
        },
        no_extensions(),
    )
    .await;
    UniswapV3PoolEventType::Swap(handler).register(manifest_path, registry).await;
}
pub async fn uniswap_v3_pool_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    swap_handler(manifest_path, registry).await;
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/indexers/all_handlers.rs">
use super::rindexer_factory_contract::uniswap_v3_factory::uniswap_v3_factory_handlers;
use super::rindexer_factory_contract::uniswap_v3_factory_pool_created_pool::uniswap_v3_factory_pool_created_pool_handlers;
use super::rindexer_factory_contract::uniswap_v3_factory_pool_created_token_0_token_1::uniswap_v3_factory_pool_created_token_0_token_1_handlers;
use super::rindexer_factory_contract::uniswap_v3_pool::uniswap_v3_pool_handlers;
use super::rindexer_factory_contract::uniswap_v3_pool_token::uniswap_v3_pool_token_handlers;
use rindexer::event::callback_registry::EventCallbackRegistry;
use std::path::PathBuf;
pub async fn register_all_handlers(manifest_path: &PathBuf) -> EventCallbackRegistry {
    let mut registry = EventCallbackRegistry::new();
    uniswap_v3_factory_handlers(manifest_path, &mut registry).await;
    uniswap_v3_factory_pool_created_pool_handlers(manifest_path, &mut registry).await;
    uniswap_v3_pool_handlers(manifest_path, &mut registry).await;
    uniswap_v3_factory_pool_created_token_0_token_1_handlers(manifest_path, &mut registry).await;
    uniswap_v3_pool_token_handlers(manifest_path, &mut registry).await;
    registry
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/indexers/mod.rs">
#![allow(dead_code, unused)]
pub mod all_handlers;
pub mod rindexer_factory_contract;
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/mod.rs">
#![allow(dead_code, unused)]
pub mod uniswap_v3_factory;
mod uniswap_v3_factory_abi_gen;
pub mod uniswap_v3_factory_pool_created_pool;
mod uniswap_v3_factory_pool_created_pool_abi_gen;
pub mod uniswap_v3_factory_pool_created_token_0_token_1;
mod uniswap_v3_factory_pool_created_token_0_token_1_abi_gen;
pub mod uniswap_v3_pool;
mod uniswap_v3_pool_abi_gen;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer
/// Any manual changes to this file will be overwritten.
pub mod uniswap_v3_pool_token;
mod uniswap_v3_pool_token_abi_gen;
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3FactoryGen,
    r#"[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "FeeAmountEnabled",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "oldOwner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newOwner",
        "type": "address"
      }
    ],
    "name": "OwnerChanged",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "name": "PoolCreated",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "tokenA",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "tokenB",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      }
    ],
    "name": "createPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "enableFeeAmount",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "feeAmountTickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "getPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "owner",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "parameters",
    "outputs": [
      {
        "internalType": "address",
        "name": "factory",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "setOwner",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  }
]
"#
);
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory_pool_created_pool_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3FactoryPoolCreatedPoolGen,
    r#"[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "FeeAmountEnabled",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "oldOwner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newOwner",
        "type": "address"
      }
    ],
    "name": "OwnerChanged",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "name": "PoolCreated",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "tokenA",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "tokenB",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      }
    ],
    "name": "createPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "enableFeeAmount",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "feeAmountTickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "getPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "owner",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "parameters",
    "outputs": [
      {
        "internalType": "address",
        "name": "factory",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "setOwner",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  }
]
"#
);
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory_pool_created_pool.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
        ///
        /// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
        /// Any manual changes to this file will be overwritten.
        use super::uniswap_v3_factory_pool_created_pool_abi_gen::{
            RindexerUniswapV3FactoryPoolCreatedPoolGen::{self, RindexerUniswapV3FactoryPoolCreatedPoolGenInstance, RindexerUniswapV3FactoryPoolCreatedPoolGenEvents}
        };
        use std::{any::Any, sync::Arc};
        use std::error::Error;
        use std::future::Future;
        use std::collections::HashMap;
        use std::pin::Pin;
        use std::path::{Path, PathBuf};
        use alloy::network::AnyNetwork;
        use alloy::primitives::{Address, Bytes, B256};
        use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
        use rindexer::{
            async_trait,
            AsyncCsvAppender,
            generate_random_id,
            FutureExt,
            blockclock::BlockClock,
            event::{
                callback_registry::{
                    EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
                    EventResult, TxInformation, HasTxInformation
                },
                contract_setup::{ContractInformation, NetworkContract},
            },
            manifest::{
                contract::{Contract, ContractDetails},
                yaml::read_manifest,
            },
            provider::{JsonRpcCachedProvider, RindexerProvider},
            PostgresClient,
        };
        use super::super::super::super::typings::networks::get_provider_cache_for_network;
        use super::super::super::super::typings::database::get_or_init_postgres_client;
pub type FeeAmountEnabledData = RindexerUniswapV3FactoryPoolCreatedPoolGen::FeeAmountEnabled;
#[derive(Debug, Clone)]
pub struct FeeAmountEnabledResult {
    pub event_data: FeeAmountEnabledData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for FeeAmountEnabledResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type OwnerChangedData = RindexerUniswapV3FactoryPoolCreatedPoolGen::OwnerChanged;
#[derive(Debug, Clone)]
pub struct OwnerChangedResult {
    pub event_data: OwnerChangedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for OwnerChangedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type PoolCreatedData = RindexerUniswapV3FactoryPoolCreatedPoolGen::PoolCreated;
#[derive(Debug, Clone)]
pub struct PoolCreatedResult {
    pub event_data: PoolCreatedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for PoolCreatedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<PostgresClient>,
    pub csv: Arc<AsyncCsvAppender>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn poolcreated_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> PoolCreatedEventCallbackType<TExtensions>
where
    PoolCreatedResult: Clone + 'static,
    F: for<'a> Fn(Vec<PoolCreatedResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type PoolCreatedEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<PoolCreatedResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: PoolCreatedEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        PoolCreatedResult: Clone + 'static,
        F: for<'a> Fn(Vec<PoolCreatedResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        let csv = AsyncCsvAppender::new(
            r"/Users/pawellula/RustroverProjects/rindexer/cli/../examples/rindexer_factory_indexing/generated_csv/UniswapV3FactoryPoolCreatedP...-poolcreated.csv",
        );
        if !Path::new(r"/Users/pawellula/RustroverProjects/rindexer/cli/../examples/rindexer_factory_indexing/generated_csv/UniswapV3FactoryPoolCreatedP...-poolcreated.csv").exists() {
            csv.append_header(vec!["contract_address".into(), "token_0".into(), "token_1".into(), "fee".into(), "tick_spacing".into(), "pool".into(), "tx_hash".into(), "block_number".into(), "block_hash".into(), "network".into(), "tx_index".into(), "log_index".into()])
                .await
                .expect("Failed to write CSV header");
        }
        Self {
            callback: poolcreated_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_postgres_client().await,
                csv: Arc::new(csv),
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<PoolCreatedResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<PoolCreatedData>().ok().map(|arc| PoolCreatedResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("PoolCreatedEvent: Unexpected data type - expected: PoolCreatedData")
        }
    }
}
pub enum UniswapV3FactoryPoolCreatedPoolEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    PoolCreated(PoolCreatedEvent<TExtensions>),
}
pub async fn uniswap_v3_factory_pool_created_pool_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3FactoryPoolCreatedPoolGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3FactoryPoolCreatedPoolGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3FactoryPoolCreatedPoolGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3FactoryPoolCreatedPoolGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else if network == "ethereum" {
        RindexerUniswapV3FactoryPoolCreatedPoolGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3FactoryPoolCreatedPoolEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated(_) => {
                "0x783cca1c0412dd0d695e784568c96da2e9c22ff989357a2e8b1d9b2b4e6b7118"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated(_) => "PoolCreated",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3FactoryPoolCreatedPool".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated(_) => Arc::new(
                move |topics: Vec<B256>, data: Bytes| match PoolCreatedData::decode_raw_log(
                    topics,
                    &data[0..],
                ) {
                    Ok(event) => {
                        let result: PoolCreatedData = event;
                        Arc::new(result) as Arc<dyn Any + Send + Sync>
                    }
                    Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                },
            ),
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .is_some_and(|vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .is_some_and(|n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3FactoryPoolCreatedPoolEventType::PoolCreated(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerFactoryContract".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory_pool_created_token_0_token_1_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen,
    r#"[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "FeeAmountEnabled",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "oldOwner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newOwner",
        "type": "address"
      }
    ],
    "name": "OwnerChanged",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "name": "PoolCreated",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "tokenA",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "tokenB",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      }
    ],
    "name": "createPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "pool",
        "type": "address"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "name": "enableFeeAmount",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "feeAmountTickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "name": "getPool",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "owner",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "parameters",
    "outputs": [
      {
        "internalType": "address",
        "name": "factory",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token0",
        "type": "address"
      },
      {
        "internalType": "address",
        "name": "token1",
        "type": "address"
      },
      {
        "internalType": "uint24",
        "name": "fee",
        "type": "uint24"
      },
      {
        "internalType": "int24",
        "name": "tickSpacing",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "setOwner",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  }
]
"#
);
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory_pool_created_token_0_token_1.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_postgres_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
        ///
        /// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
        /// Any manual changes to this file will be overwritten.
        use super::uniswap_v3_factory_pool_created_token_0_token_1_abi_gen::RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::{self, RindexerUniswapV3FactoryPoolCreatedToken0Token1GenInstance};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, Bytes, B256};
use alloy::sol_types::SolEvent;
use rindexer::{
    async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::yaml::read_manifest,
    provider::{JsonRpcCachedProvider, RindexerProvider},
    AsyncCsvAppender,
    FutureExt,
    PostgresClient,
};
use std::collections::HashMap;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type FeeAmountEnabledData =
    RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::FeeAmountEnabled;
#[derive(Debug, Clone)]
pub struct FeeAmountEnabledResult {
    pub event_data: FeeAmountEnabledData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for FeeAmountEnabledResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type OwnerChangedData = RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::OwnerChanged;
#[derive(Debug, Clone)]
pub struct OwnerChangedResult {
    pub event_data: OwnerChangedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for OwnerChangedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type PoolCreatedData = RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::PoolCreated;
#[derive(Debug, Clone)]
pub struct PoolCreatedResult {
    pub event_data: PoolCreatedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for PoolCreatedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<PostgresClient>,
    pub csv: Arc<AsyncCsvAppender>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn poolcreated_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> PoolCreatedEventCallbackType<TExtensions>
where
    PoolCreatedResult: Clone + 'static,
    F: for<'a> Fn(Vec<PoolCreatedResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type PoolCreatedEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<PoolCreatedResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: PoolCreatedEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        PoolCreatedResult: Clone + 'static,
        F: for<'a> Fn(Vec<PoolCreatedResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        let csv = AsyncCsvAppender::new(
            r"/Users/pawellula/RustroverProjects/rindexer/cli/../examples/rindexer_factory_indexing/generated_csv/UniswapV3FactoryPoolCreatedT...-poolcreated.csv",
        );
        if !Path::new(r"/Users/pawellula/RustroverProjects/rindexer/cli/../examples/rindexer_factory_indexing/generated_csv/UniswapV3FactoryPoolCreatedT...-poolcreated.csv").exists() {
            csv.append_header(vec!["contract_address".into(), "token_0".into(), "token_1".into(), "fee".into(), "tick_spacing".into(), "pool".into(), "tx_hash".into(), "block_number".into(), "block_hash".into(), "network".into(), "tx_index".into(), "log_index".into()])
                .await
                .expect("Failed to write CSV header");
        }
        Self {
            callback: poolcreated_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_postgres_client().await,
                csv: Arc::new(csv),
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for PoolCreatedEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<PoolCreatedResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<PoolCreatedData>().ok().map(|arc| PoolCreatedResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("PoolCreatedEvent: Unexpected data type - expected: PoolCreatedData")
        }
    }
}
pub enum UniswapV3FactoryPoolCreatedToken0Token1EventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    PoolCreated(PoolCreatedEvent<TExtensions>),
}
pub async fn uniswap_v3_factory_pool_created_token_0_token_1_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3FactoryPoolCreatedToken0Token1GenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3FactoryPoolCreatedToken0Token1GenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else if network == "ethereum" {
        RindexerUniswapV3FactoryPoolCreatedToken0Token1Gen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3FactoryPoolCreatedToken0Token1EventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated(_) => {
                "0x783cca1c0412dd0d695e784568c96da2e9c22ff989357a2e8b1d9b2b4e6b7118"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated(_) => "PoolCreated",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3FactoryPoolCreatedToken0Token1".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated(_) => Arc::new(
                move |topics: Vec<B256>, data: Bytes| match PoolCreatedData::decode_raw_log(
                    topics,
                    &data[0..],
                ) {
                    Ok(event) => {
                        let result: PoolCreatedData = event;
                        Arc::new(result) as Arc<dyn Any + Send + Sync>
                    }
                    Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                },
            ),
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .is_some_and(|vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .is_some_and(|n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3FactoryPoolCreatedToken0Token1EventType::PoolCreated(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerFactoryContract".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_factory.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_postgres_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::uniswap_v3_factory_abi_gen::RindexerUniswapV3FactoryGen::{
    self, RindexerUniswapV3FactoryGenEvents, RindexerUniswapV3FactoryGenInstance,
};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, Bytes, B256};
use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
use rindexer::{
    async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation,
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::read_manifest,
    },
    provider::{JsonRpcCachedProvider, RindexerProvider},
    AsyncCsvAppender, FutureExt, PostgresClient,
};
use std::collections::HashMap;
use std::error::Error;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type FeeAmountEnabledData = RindexerUniswapV3FactoryGen::FeeAmountEnabled;
#[derive(Debug, Clone)]
pub struct FeeAmountEnabledResult {
    pub event_data: FeeAmountEnabledData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for FeeAmountEnabledResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type OwnerChangedData = RindexerUniswapV3FactoryGen::OwnerChanged;
#[derive(Debug, Clone)]
pub struct OwnerChangedResult {
    pub event_data: OwnerChangedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for OwnerChangedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type PoolCreatedData = RindexerUniswapV3FactoryGen::PoolCreated;
#[derive(Debug, Clone)]
pub struct PoolCreatedResult {
    pub event_data: PoolCreatedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for PoolCreatedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<PostgresClient>,
    pub csv: Arc<AsyncCsvAppender>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn ownerchanged_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> OwnerChangedEventCallbackType<TExtensions>
where
    OwnerChangedResult: Clone + 'static,
    F: for<'a> Fn(Vec<OwnerChangedResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type OwnerChangedEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<OwnerChangedResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct OwnerChangedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: OwnerChangedEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> OwnerChangedEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        OwnerChangedResult: Clone + 'static,
        F: for<'a> Fn(Vec<OwnerChangedResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        let csv = AsyncCsvAppender::new(
            r"/Users/pawellula/RustroverProjects/rindexer/cli/../examples/rindexer_factory_indexing/generated_csv/UniswapV3Factory/uniswapv3factory-ownerchanged.csv",
        );
        if !Path::new(r"/Users/pawellula/RustroverProjects/rindexer/cli/../examples/rindexer_factory_indexing/generated_csv/UniswapV3Factory/uniswapv3factory-ownerchanged.csv").exists() {
            csv.append_header(vec!["contract_address".into(), "old_owner".into(), "new_owner".into(), "tx_hash".into(), "block_number".into(), "block_hash".into(), "network".into(), "tx_index".into(), "log_index".into()])
                .await
                .expect("Failed to write CSV header");
        }
        Self {
            callback: ownerchanged_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_postgres_client().await,
                csv: Arc::new(csv),
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for OwnerChangedEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<OwnerChangedResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<OwnerChangedData>().ok().map(|arc| {
                    OwnerChangedResult {
                        event_data: (*arc).clone(),
                        tx_information: item.tx_information,
                    }
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("OwnerChangedEvent: Unexpected data type - expected: OwnerChangedData")
        }
    }
}
pub enum UniswapV3FactoryEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    OwnerChanged(OwnerChangedEvent<TExtensions>),
}
pub async fn uniswap_v3_factory_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3FactoryGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3FactoryGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3FactoryGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3FactoryGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else if network == "ethereum" {
        RindexerUniswapV3FactoryGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3FactoryEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3FactoryEventType::OwnerChanged(_) => {
                "0xb532073b38c83145e3e5135377a08bf9aab55bc0fd7c1179cd4fb995d2a5159c"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3FactoryEventType::OwnerChanged(_) => "OwnerChanged",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3Factory".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3FactoryEventType::OwnerChanged(_) => {
                Arc::new(move |topics: Vec<B256>, data: Bytes| {
                    match OwnerChangedData::decode_raw_log(topics, &data[0..]) {
                        Ok(event) => {
                            let result: OwnerChangedData = event;
                            Arc::new(result) as Arc<dyn Any + Send + Sync>
                        }
                        Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                    }
                })
            }
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .is_some_and(|vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .is_some_and(|n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3FactoryEventType::OwnerChanged(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerFactoryContract".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_pool_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3PoolGen,
    r#"[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Burn",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "Collect",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "CollectProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid1",
        "type": "uint256"
      }
    ],
    "name": "Flash",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextOld",
        "type": "uint16"
      },
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextNew",
        "type": "uint16"
      }
    ],
    "name": "IncreaseObservationCardinalityNext",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Initialize",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Mint",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0New",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1New",
        "type": "uint8"
      }
    ],
    "name": "SetFeeProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "liquidity",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Swap",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      }
    ],
    "name": "burn",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collect",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collectProtocol",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "factory",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "fee",
    "outputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal0X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal1X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "flash",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      }
    ],
    "name": "increaseObservationCardinalityNext",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      }
    ],
    "name": "initialize",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "liquidity",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "maxLiquidityPerTick",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "mint",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint256",
        "name": "index",
        "type": "uint256"
      }
    ],
    "name": "observations",
    "outputs": [
      {
        "internalType": "uint32",
        "name": "blockTimestamp",
        "type": "uint32"
      },
      {
        "internalType": "int56",
        "name": "tickCumulative",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityCumulativeX128",
        "type": "uint160"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint32[]",
        "name": "secondsAgos",
        "type": "uint32[]"
      }
    ],
    "name": "observe",
    "outputs": [
      {
        "internalType": "int56[]",
        "name": "tickCumulatives",
        "type": "int56[]"
      },
      {
        "internalType": "uint160[]",
        "name": "secondsPerLiquidityCumulativeX128s",
        "type": "uint160[]"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "bytes32",
        "name": "key",
        "type": "bytes32"
      }
    ],
    "name": "positions",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "_liquidity",
        "type": "uint128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside0LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside1LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "protocolFees",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "token0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "token1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint8",
        "name": "feeProtocol0",
        "type": "uint8"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol1",
        "type": "uint8"
      }
    ],
    "name": "setFeeProtocol",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "slot0",
    "outputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      },
      {
        "internalType": "uint16",
        "name": "observationIndex",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinality",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol",
        "type": "uint8"
      },
      {
        "internalType": "bool",
        "name": "unlocked",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      }
    ],
    "name": "snapshotCumulativesInside",
    "outputs": [
      {
        "internalType": "int56",
        "name": "tickCumulativeInside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityInsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsInside",
        "type": "uint32"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "bool",
        "name": "zeroForOne",
        "type": "bool"
      },
      {
        "internalType": "int256",
        "name": "amountSpecified",
        "type": "int256"
      },
      {
        "internalType": "uint160",
        "name": "sqrtPriceLimitX96",
        "type": "uint160"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "swap",
    "outputs": [
      {
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int16",
        "name": "wordPosition",
        "type": "int16"
      }
    ],
    "name": "tickBitmap",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "tickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "ticks",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "liquidityGross",
        "type": "uint128"
      },
      {
        "internalType": "int128",
        "name": "liquidityNet",
        "type": "int128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside0X128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside1X128",
        "type": "uint256"
      },
      {
        "internalType": "int56",
        "name": "tickCumulativeOutside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityOutsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsOutside",
        "type": "uint32"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token0",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token1",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  }
]"#
);
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_pool_token_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3PoolTokenGen,
    r#"[
  {
    "constant": true,
    "inputs": [],
    "name": "name",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_spender",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "approve",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "totalSupply",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_from",
        "type": "address"
      },
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transferFrom",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "decimals",
    "outputs": [
      {
        "name": "",
        "type": "uint8"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "balanceOf",
    "outputs": [
      {
        "name": "balance",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "symbol",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transfer",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      },
      {
        "name": "_spender",
        "type": "address"
      }
    ],
    "name": "allowance",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "payable": true,
    "stateMutability": "payable",
    "type": "fallback"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "spender",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Approval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "from",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Transfer",
    "type": "event"
  }
]
"#
);
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_pool_token.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_postgres_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::uniswap_v3_pool_token_abi_gen::RindexerUniswapV3PoolTokenGen::{
    self, RindexerUniswapV3PoolTokenGenEvents, RindexerUniswapV3PoolTokenGenInstance,
};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, Bytes, B256};
use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
use rindexer::{
    async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation,
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::read_manifest,
    },
    provider::{JsonRpcCachedProvider, RindexerProvider},
    AsyncCsvAppender, FutureExt, PostgresClient,
};
use std::collections::HashMap;
use std::error::Error;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type ApprovalData = RindexerUniswapV3PoolTokenGen::Approval;
#[derive(Debug, Clone)]
pub struct ApprovalResult {
    pub event_data: ApprovalData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for ApprovalResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type TransferData = RindexerUniswapV3PoolTokenGen::Transfer;
#[derive(Debug, Clone)]
pub struct TransferResult {
    pub event_data: TransferData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for TransferResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<PostgresClient>,
    pub csv: Arc<AsyncCsvAppender>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn transfer_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> TransferEventCallbackType<TExtensions>
where
    TransferResult: Clone + 'static,
    F: for<'a> Fn(Vec<TransferResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type TransferEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<TransferResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct TransferEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: TransferEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> TransferEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        TransferResult: Clone + 'static,
        F: for<'a> Fn(Vec<TransferResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        let csv = AsyncCsvAppender::new(
            r"/Users/pawellula/RustroverProjects/rindexer/cli/../examples/rindexer_factory_indexing/generated_csv/UniswapV3PoolToken/uniswapv3pooltoken-transfer.csv",
        );
        if !Path::new(r"/Users/pawellula/RustroverProjects/rindexer/cli/../examples/rindexer_factory_indexing/generated_csv/UniswapV3PoolToken/uniswapv3pooltoken-transfer.csv").exists() {
            csv.append_header(vec!["contract_address".into(), "from".into(), "to".into(), "value".into(), "tx_hash".into(), "block_number".into(), "block_hash".into(), "network".into(), "tx_index".into(), "log_index".into()])
                .await
                .expect("Failed to write CSV header");
        }
        Self {
            callback: transfer_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_postgres_client().await,
                csv: Arc::new(csv),
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for TransferEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<TransferResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<TransferData>().ok().map(|arc| TransferResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("TransferEvent: Unexpected data type - expected: TransferData")
        }
    }
}
pub enum UniswapV3PoolTokenEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    Transfer(TransferEvent<TExtensions>),
}
pub async fn uniswap_v3_pool_token_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3PoolTokenGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3PoolTokenGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3PoolTokenGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3PoolTokenGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else if network == "ethereum" {
        RindexerUniswapV3PoolTokenGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3PoolTokenEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3PoolTokenEventType::Transfer(_) => {
                "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3PoolTokenEventType::Transfer(_) => "Transfer",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3PoolToken".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3PoolTokenEventType::Transfer(_) => {
                Arc::new(move |topics: Vec<B256>, data: Bytes| {
                    match TransferData::decode_raw_log(topics, &data[0..]) {
                        Ok(event) => {
                            let result: TransferData = event;
                            Arc::new(result) as Arc<dyn Any + Send + Sync>
                        }
                        Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                    }
                })
            }
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .is_some_and(|vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .is_some_and(|n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3PoolTokenEventType::Transfer(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerFactoryContract".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/events/uniswap_v3_pool.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_postgres_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::uniswap_v3_pool_abi_gen::RindexerUniswapV3PoolGen::{
    self, RindexerUniswapV3PoolGenEvents, RindexerUniswapV3PoolGenInstance,
};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, Bytes, B256};
use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
use rindexer::{
    async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation,
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::read_manifest,
    },
    provider::{JsonRpcCachedProvider, RindexerProvider},
    AsyncCsvAppender, FutureExt, PostgresClient,
};
use std::collections::HashMap;
use std::error::Error;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type BurnData = RindexerUniswapV3PoolGen::Burn;
#[derive(Debug, Clone)]
pub struct BurnResult {
    pub event_data: BurnData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for BurnResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type CollectData = RindexerUniswapV3PoolGen::Collect;
#[derive(Debug, Clone)]
pub struct CollectResult {
    pub event_data: CollectData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for CollectResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type CollectProtocolData = RindexerUniswapV3PoolGen::CollectProtocol;
#[derive(Debug, Clone)]
pub struct CollectProtocolResult {
    pub event_data: CollectProtocolData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for CollectProtocolResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type FlashData = RindexerUniswapV3PoolGen::Flash;
#[derive(Debug, Clone)]
pub struct FlashResult {
    pub event_data: FlashData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for FlashResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type IncreaseObservationCardinalityNextData =
    RindexerUniswapV3PoolGen::IncreaseObservationCardinalityNext;
#[derive(Debug, Clone)]
pub struct IncreaseObservationCardinalityNextResult {
    pub event_data: IncreaseObservationCardinalityNextData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for IncreaseObservationCardinalityNextResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type InitializeData = RindexerUniswapV3PoolGen::Initialize;
#[derive(Debug, Clone)]
pub struct InitializeResult {
    pub event_data: InitializeData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for InitializeResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type MintData = RindexerUniswapV3PoolGen::Mint;
#[derive(Debug, Clone)]
pub struct MintResult {
    pub event_data: MintData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for MintResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type SetFeeProtocolData = RindexerUniswapV3PoolGen::SetFeeProtocol;
#[derive(Debug, Clone)]
pub struct SetFeeProtocolResult {
    pub event_data: SetFeeProtocolData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for SetFeeProtocolResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type SwapData = RindexerUniswapV3PoolGen::Swap;
#[derive(Debug, Clone)]
pub struct SwapResult {
    pub event_data: SwapData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for SwapResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<PostgresClient>,
    pub csv: Arc<AsyncCsvAppender>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn swap_handler<TExtensions, F, Fut>(custom_logic: F) -> SwapEventCallbackType<TExtensions>
where
    SwapResult: Clone + 'static,
    F: for<'a> Fn(Vec<SwapResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type SwapEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<SwapResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct SwapEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: SwapEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> SwapEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        SwapResult: Clone + 'static,
        F: for<'a> Fn(Vec<SwapResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        let csv = AsyncCsvAppender::new(
            r"/Users/pawellula/RustroverProjects/rindexer/cli/../examples/rindexer_factory_indexing/generated_csv/UniswapV3Pool/uniswapv3pool-swap.csv",
        );
        if !Path::new(r"/Users/pawellula/RustroverProjects/rindexer/cli/../examples/rindexer_factory_indexing/generated_csv/UniswapV3Pool/uniswapv3pool-swap.csv").exists() {
            csv.append_header(vec!["contract_address".into(), "sender".into(), "recipient".into(), "amount_0".into(), "amount_1".into(), "sqrt_price_x96".into(), "liquidity".into(), "tick".into(), "tx_hash".into(), "block_number".into(), "block_hash".into(), "network".into(), "tx_index".into(), "log_index".into()])
                .await
                .expect("Failed to write CSV header");
        }
        Self {
            callback: swap_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_postgres_client().await,
                csv: Arc::new(csv),
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for SwapEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<SwapResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<SwapData>().ok().map(|arc| SwapResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("SwapEvent: Unexpected data type - expected: SwapData")
        }
    }
}
pub enum UniswapV3PoolEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    Swap(SwapEvent<TExtensions>),
}
pub async fn uniswap_v3_pool_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3PoolGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3PoolGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3PoolGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3PoolGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else if network == "ethereum" {
        RindexerUniswapV3PoolGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3PoolEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3PoolEventType::Swap(_) => {
                "0xc42079f94a6350d7e6235f29174924f928cc2ac818eb64fed8004e115fbcca67"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3PoolEventType::Swap(_) => "Swap",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3Pool".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3PoolEventType::Swap(_) => Arc::new(move |topics: Vec<B256>, data: Bytes| {
                match SwapData::decode_raw_log(topics, &data[0..]) {
                    Ok(event) => {
                        let result: SwapData = event;
                        Arc::new(result) as Arc<dyn Any + Send + Sync>
                    }
                    Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                }
            }),
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .is_some_and(|vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .is_some_and(|n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3PoolEventType::Swap(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerFactoryContract".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/rindexer_factory_contract/mod.rs">
#![allow(dead_code, unused)]
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer
/// Any manual changes to this file will be overwritten.
pub mod events;
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/database.rs">
use rindexer::PostgresClient;
use std::sync::Arc;
use tokio::sync::OnceCell;
static POSTGRES_CLIENT: OnceCell<Arc<PostgresClient>> = OnceCell::const_new();
pub async fn get_or_init_postgres_client() -> Arc<PostgresClient> {
    POSTGRES_CLIENT
        .get_or_init(|| async {
            Arc::new(PostgresClient::new().await.expect("Failed to connect to Postgres"))
        })
        .await
        .clone()
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/mod.rs">
#![allow(dead_code, unused)]
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer
/// Any manual changes to this file will be overwritten.
pub mod database;
pub mod networks;
pub mod rindexer_factory_contract;
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/typings/networks.rs">
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use alloy::{primitives::U64, transports::http::reqwest::header::HeaderMap};
use rindexer::{
    lazy_static,
    manifest::network::{AddressFiltering, BlockPollFrequency},
    notifications::ChainStateNotification,
    provider::{create_client, JsonRpcCachedProvider, RetryClientError, RindexerProvider},
    public_read_env_value,
};
use std::sync::Arc;
use tokio::sync::broadcast::Sender;
use tokio::sync::OnceCell;
#[allow(dead_code)]
async fn create_shadow_client(
    rpc_url: &str,
    chain_id: u64,
    compute_units_per_second: Option<u64>,
    block_poll_frequency: Option<BlockPollFrequency>,
    max_block_range: Option<U64>,
    address_filtering: Option<AddressFiltering>,
    chain_state_notification: Option<Sender<ChainStateNotification>>,
) -> Result<Arc<JsonRpcCachedProvider>, RetryClientError> {
    let mut header = HeaderMap::new();
    header.insert(
        "X-SHADOW-API-KEY",
        public_read_env_value("RINDEXER_PHANTOM_API_KEY").unwrap().parse().unwrap(),
    );
    create_client(
        rpc_url,
        chain_id,
        compute_units_per_second,
        max_block_range,
        block_poll_frequency,
        header,
        address_filtering,
        chain_state_notification,
    )
    .await
}
static ETHEREUM_PROVIDER: OnceCell<Arc<JsonRpcCachedProvider>> = OnceCell::const_new();
static BASE_PROVIDER: OnceCell<Arc<JsonRpcCachedProvider>> = OnceCell::const_new();
pub async fn get_ethereum_provider_cache() -> Arc<JsonRpcCachedProvider> {
    ETHEREUM_PROVIDER
        .get_or_init(|| async {
            let chain_state_notification = None;
            create_client(
                &public_read_env_value("https://mainnet.gateway.tenderly.co")
                    .unwrap_or("https://mainnet.gateway.tenderly.co".to_string()),
                1,
                None,
                None,
                None,
                HeaderMap::new(),
                None,
                chain_state_notification,
            )
            .await
            .expect("Error creating provider")
        })
        .await
        .clone()
}
pub async fn get_ethereum_provider() -> Arc<RindexerProvider> {
    get_ethereum_provider_cache().await.get_inner_provider()
}
pub async fn get_base_provider_cache() -> Arc<JsonRpcCachedProvider> {
    BASE_PROVIDER
        .get_or_init(|| async {
            let chain_state_notification = None;
            create_client(
                &public_read_env_value("https://mainnet.base.org")
                    .unwrap_or("https://mainnet.base.org".to_string()),
                8453,
                None,
                None,
                None,
                HeaderMap::new(),
                None,
                chain_state_notification,
            )
            .await
            .expect("Error creating provider")
        })
        .await
        .clone()
}
pub async fn get_base_provider() -> Arc<RindexerProvider> {
    get_base_provider_cache().await.get_inner_provider()
}
pub async fn get_provider_cache_for_network(network: &str) -> Arc<JsonRpcCachedProvider> {
    if network == "ethereum" {
        return get_ethereum_provider_cache().await;
    }
    if network == "base" {
        return get_base_provider_cache().await;
    }
    panic!("Network not supported")
}
</file>

<file path="examples/rindexer_factory_indexing/src/rindexer_lib/mod.rs">
#![allow(dead_code, unused)]
pub mod indexers;
pub mod typings;
</file>

<file path="examples/rindexer_factory_indexing/src/main.rs">
use std::env;
use self::rindexer_lib::indexers::all_handlers::register_all_handlers;
use rindexer::{
    event::callback_registry::TraceCallbackRegistry, start_rindexer, GraphqlOverrideSettings,
    IndexingDetails, StartDetails,
};
#[allow(clippy::all)]
mod rindexer_lib;
#[tokio::main]
async fn main() {
    let args: Vec<String> = env::args().collect();
    let mut enable_graphql = false;
    let mut enable_indexer = false;
    let mut graphql_port: Option<u16> = None;
    let args = args.iter();
    if args.len() == 1 {
        enable_graphql = true;
        enable_indexer = true;
    }
    for arg in args {
        match arg.as_str() {
            "--graphql" => enable_graphql = true,
            "--indexer" => enable_indexer = true,
            _ if arg.starts_with("--port=") || arg.starts_with("--p") => {
                if let Some(value) = arg.split('=').nth(1) {
                    let overridden_port = value.parse::<u16>();
                    match overridden_port {
                        Ok(overridden_port) => graphql_port = Some(overridden_port),
                        Err(_) => {
                            println!("Invalid port number");
                            return;
                        }
                    }
                }
            }
            _ => {}
        }
    }
    let path = env::current_dir();
    match path {
        Ok(path) => {
            let manifest_path = path.join("rindexer.yaml");
            let result = start_rindexer(StartDetails {
                manifest_path: &manifest_path,
                indexing_details: if enable_indexer {
                    Some(IndexingDetails {
                        registry: register_all_handlers(&manifest_path).await,
                        trace_registry: TraceCallbackRegistry { events: vec![] },
                        event_stream: None,
                    })
                } else {
                    None
                },
                graphql_details: GraphqlOverrideSettings {
                    enabled: enable_graphql,
                    override_port: graphql_port,
                },
            })
            .await;
            match result {
                Ok(_) => {}
                Err(e) => {
                    println!("Error starting rindexer: {:?}", e);
                }
            }
        }
        Err(e) => {
            println!("Error getting current directory: {:?}", e);
        }
    }
}
</file>

<file path="examples/rindexer_factory_indexing/.gitignore">
generated_csv
</file>

<file path="examples/rindexer_factory_indexing/Cargo.toml">
[package]
name = "rindexer_factory_contract"
version = "0.1.0"
edition = "2021"

[dependencies]
# internal dependencies
rindexer = { path = "../../core" }

# external dependencies
tokio = { version = "1", features = ["full"] }
alloy = { version = "1.1.3", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
</file>

<file path="examples/rindexer_factory_indexing/docker-compose.yml">
volumes:
  postgres_data:
    driver: local
services:
  postgresql:
    image: postgres:16
    shm_size: 1g
    restart: always
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - 5440:5432
    env_file:
      - ./.env
</file>

<file path="examples/rindexer_factory_indexing/rindexer.yaml">
name: RindexerFactoryContract
project_type: rust
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
  - name: base
    chain_id: 8453
    rpc: https://mainnet.base.org
storage:
  postgres:
    enabled: true
  csv:
    enabled: true
    path: ./generated_csv
native_transfers:
  enabled: false
contracts:
  - name: UniswapV3Factory
    details:
      - network: base
        start_block: 33000000
        address: 0x33128a8fC17869897dcE68Ed026d694621f6FDfD
      - network: ethereum
        start_block: 22000000
        address: 0x1F98431c8aD98523631AE4a59f267346ea31F984
    abi: ./abis/uniswap-v3-factory-abi.json
    include_events:
      - OwnerChanged
  - name: UniswapV3Pool
    details:
      - network: base
        start_block: 33000000
        factory:
          name: UniswapV3Factory
          address: 0x33128a8fC17869897dcE68Ed026d694621f6FDfD
          abi: ./abis/uniswap-v3-factory-abi.json
          event_name: PoolCreated
          input_name: "pool"
      - network: ethereum
        start_block: 22000000
        factory:
          name: UniswapV3Factory
          address: 0x1F98431c8aD98523631AE4a59f267346ea31F984
          abi: ./abis/uniswap-v3-factory-abi.json
          event_name: PoolCreated
          input_name: "pool"
    abi: ./abis/uniswap-v3-pool-abi.json
    include_events:
      - Swap
  - name: UniswapV3PoolToken
    details:
      - network: base
        start_block: 33000000
        factory:
          name: UniswapV3Factory
          address: 0x33128a8fC17869897dcE68Ed026d694621f6FDfD
          abi: ./abis/uniswap-v3-factory-abi.json
          event_name: PoolCreated
          input_name:
            - "token0"
            - "token1"
      - network: ethereum
        start_block: 22000000
        factory:
          name: UniswapV3Factory
          address: 0x1F98431c8aD98523631AE4a59f267346ea31F984
          abi: ./abis/uniswap-v3-factory-abi.json
          event_name: PoolCreated
          input_name:
            - "token0"
            - "token1"
    abi: ./abis/erc20-abi.json
    include_events:
      - Transfer
</file>

<file path="examples/rindexer_native_transfers/abis/ERC20.abi.json">
[
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "from",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Transfer",
    "type": "event"
  }
]
</file>

<file path="examples/rindexer_native_transfers/rindexer.yaml">
name: rindexer-native-transfers
description: index all erc20 transfer events and all ethereum native token transfer events
project_type: no-code
networks:
  - name: ethereum
    chain_id: 1
    # Not all providers support `trace_block` functionality across all chains.
    # Please ensure that this method is available.
    #
    # This endpoint will be HIGHLY rate-limited, so not recommended to test this example with it.
    rpc: https://mainnet.gateway.tenderly.co
native_transfers:
  networks:
    - network: ethereum
  streams:
    sns:
      aws_config:
        region: us-east-1
        access_key: test
        secret_key: test
        endpoint_url: ${ENDPOINT_URL}
      topics:
        - topic_arn: arn:aws:sns:us-east-1:000000000000:erc20-transfers
          networks:
            - ethereum
          # The underlying event name is `NativeTransfer`. This events section can be omitted, but if you want
          # to alias the stream event name, you can do so by referencing the event `NativeTransfer`.
          #
          # This can be useful in cases where you don't want to make a distinction between, say an Erc20 "Transfer" and
          # a native token transfer, they can be mapped to the same event name.
          #
          # Otherwise, this whole `events` section can be omitted.
          events:
            - event_name: NativeTransfer
              alias: Transfer
contracts:
  - name: ERC20Transfer
    abi: ./abis/ERC20.abi.json
    details:
      - network: ethereum
        filter:
          - event_name: Transfer
    include_events:
      - Transfer
    streams:
      sns:
        aws_config:
          region: us-east-1
          access_key: test
          secret_key: test
          endpoint_url: ${ENDPOINT_URL}
        topics:
          - topic_arn: arn:aws:sns:us-east-1:000000000000:erc20-transfers
            networks:
              - ethereum
            events:
              - event_name: Transfer
</file>

<file path="examples/rindexer_rust_playground/abis/erc20-abi.json">
[
  {
    "constant": true,
    "inputs": [],
    "name": "name",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_spender",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "approve",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "totalSupply",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_from",
        "type": "address"
      },
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transferFrom",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "decimals",
    "outputs": [
      {
        "name": "",
        "type": "uint8"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "balanceOf",
    "outputs": [
      {
        "name": "balance",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "symbol",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transfer",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      },
      {
        "name": "_spender",
        "type": "address"
      }
    ],
    "name": "allowance",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "payable": true,
    "stateMutability": "payable",
    "type": "fallback"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "spender",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Approval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "from",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Transfer",
    "type": "event"
  }
]
</file>

<file path="examples/rindexer_rust_playground/abis/erc721-abi.json">
[
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "approved",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "tokenId",
        "type": "uint256"
      }
    ],
    "name": "Approval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "from",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "to",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "tokenId",
        "type": "uint256"
      }
    ],
    "name": "Transfer",
    "type": "event"
  }
]
</file>

<file path="examples/rindexer_rust_playground/abis/lens-hub-events-abi.json">
[
  {
    "anonymous": false,
    "inputs": [
      {
        "components": [
          {
            "internalType": "uint256",
            "name": "publicationActedProfileId",
            "type": "uint256"
          },
          {
            "internalType": "uint256",
            "name": "publicationActedId",
            "type": "uint256"
          },
          {
            "internalType": "uint256",
            "name": "actorProfileId",
            "type": "uint256"
          },
          {
            "internalType": "uint256[]",
            "name": "referrerProfileIds",
            "type": "uint256[]"
          },
          {
            "internalType": "uint256[]",
            "name": "referrerPubIds",
            "type": "uint256[]"
          },
          {
            "internalType": "address",
            "name": "actionModuleAddress",
            "type": "address"
          },
          {
            "internalType": "bytes",
            "name": "actionModuleData",
            "type": "bytes"
          }
        ],
        "indexed": false,
        "internalType": "struct Types.PublicationActionParams",
        "name": "publicationActionParams",
        "type": "tuple"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "actionModuleReturnData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "Acted",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "actionModule",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "id",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "bool",
        "name": "whitelisted",
        "type": "bool"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "ActionModuleWhitelisted",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "string",
        "name": "name",
        "type": "string"
      },
      {
        "indexed": false,
        "internalType": "string",
        "name": "symbol",
        "type": "string"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "BaseInitialized",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "byProfileId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "idOfProfileBlocked",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "Blocked",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "profileId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "pubId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "collectNFT",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "CollectNFTDeployed",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "profileId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "pubId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "collectNFTId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "from",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "CollectNFTTransferred",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "collectedProfileId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "collectedPubId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "collectorProfileId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "nftRecipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "collectActionData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "collectActionResult",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "collectNFT",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "tokenId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "Collected",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "components": [
          {
            "internalType": "uint256",
            "name": "profileId",
            "type": "uint256"
          },
          {
            "internalType": "string",
            "name": "contentURI",
            "type": "string"
          },
          {
            "internalType": "uint256",
            "name": "pointedProfileId",
            "type": "uint256"
          },
          {
            "internalType": "uint256",
            "name": "pointedPubId",
            "type": "uint256"
          },
          {
            "internalType": "uint256[]",
            "name": "referrerProfileIds",
            "type": "uint256[]"
          },
          {
            "internalType": "uint256[]",
            "name": "referrerPubIds",
            "type": "uint256[]"
          },
          {
            "internalType": "bytes",
            "name": "referenceModuleData",
            "type": "bytes"
          },
          {
            "internalType": "address[]",
            "name": "actionModules",
            "type": "address[]"
          },
          {
            "internalType": "bytes[]",
            "name": "actionModulesInitDatas",
            "type": "bytes[]"
          },
          {
            "internalType": "address",
            "name": "referenceModule",
            "type": "address"
          },
          {
            "internalType": "bytes",
            "name": "referenceModuleInitData",
            "type": "bytes"
          }
        ],
        "indexed": false,
        "internalType": "struct Types.CommentParams",
        "name": "commentParams",
        "type": "tuple"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "pubId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "referenceModuleReturnData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "bytes[]",
        "name": "actionModulesInitReturnDatas",
        "type": "bytes[]"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "referenceModuleInitReturnData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "CommentCreated",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "delegatorProfileId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "configNumber",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "DelegatedExecutorsConfigApplied",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "delegatorProfileId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "configNumber",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "address[]",
        "name": "delegatedExecutors",
        "type": "address[]"
      },
      {
        "indexed": false,
        "internalType": "bool[]",
        "name": "approvals",
        "type": "bool[]"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "DelegatedExecutorsConfigChanged",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "caller",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "oldEmergencyAdmin",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newEmergencyAdmin",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "EmergencyAdminSet",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "profileId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "followModule",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "followModuleInitData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "followModuleReturnData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "FollowModuleSet",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "followModule",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "bool",
        "name": "whitelisted",
        "type": "bool"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "FollowModuleWhitelisted",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "profileId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "followNFT",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "FollowNFTDeployed",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "followerProfileId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "idOfProfileFollowed",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "followTokenIdAssigned",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "followModuleData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "processFollowModuleReturnData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "Followed",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "caller",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "prevGovernance",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newGovernance",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "GovernanceSet",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "components": [
          {
            "internalType": "uint256",
            "name": "profileId",
            "type": "uint256"
          },
          {
            "internalType": "string",
            "name": "metadataURI",
            "type": "string"
          },
          {
            "internalType": "uint256",
            "name": "pointedProfileId",
            "type": "uint256"
          },
          {
            "internalType": "uint256",
            "name": "pointedPubId",
            "type": "uint256"
          },
          {
            "internalType": "uint256[]",
            "name": "referrerProfileIds",
            "type": "uint256[]"
          },
          {
            "internalType": "uint256[]",
            "name": "referrerPubIds",
            "type": "uint256[]"
          },
          {
            "internalType": "bytes",
            "name": "referenceModuleData",
            "type": "bytes"
          }
        ],
        "indexed": false,
        "internalType": "struct Types.MirrorParams",
        "name": "mirrorParams",
        "type": "tuple"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "pubId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "referenceModuleReturnData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "MirrorCreated",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "currency",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "bool",
        "name": "prevWhitelisted",
        "type": "bool"
      },
      {
        "indexed": true,
        "internalType": "bool",
        "name": "whitelisted",
        "type": "bool"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "ModuleGlobalsCurrencyWhitelisted",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "prevGovernance",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newGovernance",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "ModuleGlobalsGovernanceSet",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint16",
        "name": "prevTreasuryFee",
        "type": "uint16"
      },
      {
        "indexed": true,
        "internalType": "uint16",
        "name": "newTreasuryFee",
        "type": "uint16"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "ModuleGlobalsTreasuryFeeSet",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "prevTreasury",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "newTreasury",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "ModuleGlobalsTreasurySet",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "components": [
          {
            "internalType": "uint256",
            "name": "profileId",
            "type": "uint256"
          },
          {
            "internalType": "string",
            "name": "contentURI",
            "type": "string"
          },
          {
            "internalType": "address[]",
            "name": "actionModules",
            "type": "address[]"
          },
          {
            "internalType": "bytes[]",
            "name": "actionModulesInitDatas",
            "type": "bytes[]"
          },
          {
            "internalType": "address",
            "name": "referenceModule",
            "type": "address"
          },
          {
            "internalType": "bytes",
            "name": "referenceModuleInitData",
            "type": "bytes"
          }
        ],
        "indexed": false,
        "internalType": "struct Types.PostParams",
        "name": "postParams",
        "type": "tuple"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "pubId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "bytes[]",
        "name": "actionModulesInitReturnDatas",
        "type": "bytes[]"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "referenceModuleInitReturnData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "PostCreated",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "profileId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "creator",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "ProfileCreated",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "profileCreator",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "bool",
        "name": "whitelisted",
        "type": "bool"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "ProfileCreatorWhitelisted",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "profileId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "string",
        "name": "metadata",
        "type": "string"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "ProfileMetadataSet",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "components": [
          {
            "internalType": "uint256",
            "name": "profileId",
            "type": "uint256"
          },
          {
            "internalType": "string",
            "name": "contentURI",
            "type": "string"
          },
          {
            "internalType": "uint256",
            "name": "pointedProfileId",
            "type": "uint256"
          },
          {
            "internalType": "uint256",
            "name": "pointedPubId",
            "type": "uint256"
          },
          {
            "internalType": "uint256[]",
            "name": "referrerProfileIds",
            "type": "uint256[]"
          },
          {
            "internalType": "uint256[]",
            "name": "referrerPubIds",
            "type": "uint256[]"
          },
          {
            "internalType": "bytes",
            "name": "referenceModuleData",
            "type": "bytes"
          },
          {
            "internalType": "address[]",
            "name": "actionModules",
            "type": "address[]"
          },
          {
            "internalType": "bytes[]",
            "name": "actionModulesInitDatas",
            "type": "bytes[]"
          },
          {
            "internalType": "address",
            "name": "referenceModule",
            "type": "address"
          },
          {
            "internalType": "bytes",
            "name": "referenceModuleInitData",
            "type": "bytes"
          }
        ],
        "indexed": false,
        "internalType": "struct Types.QuoteParams",
        "name": "quoteParams",
        "type": "tuple"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "pubId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "referenceModuleReturnData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "bytes[]",
        "name": "actionModulesInitReturnDatas",
        "type": "bytes[]"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "referenceModuleInitReturnData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "QuoteCreated",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "referenceModule",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "bool",
        "name": "whitelisted",
        "type": "bool"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "ReferenceModuleWhitelisted",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "caller",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "enum Types.ProtocolState",
        "name": "prevState",
        "type": "uint8"
      },
      {
        "indexed": true,
        "internalType": "enum Types.ProtocolState",
        "name": "newState",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "StateSet",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "wallet",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "bool",
        "name": "enabled",
        "type": "bool"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "tokenGuardianDisablingTimestamp",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "TokenGuardianStateChanged",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "byProfileId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "idOfProfileUnblocked",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "Unblocked",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "unfollowerProfileId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "idOfProfileUnfollowed",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "Unfollowed",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "signer",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "nonce",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "NonceUpdated",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "address",
        "name": "implementation",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "string",
        "name": "version",
        "type": "string"
      },
      {
        "indexed": false,
        "internalType": "bytes20",
        "name": "gitCommit",
        "type": "bytes20"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "LensUpgradeVersion",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "publicationCollectedProfileId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "publicationCollectedId",
        "type": "uint256"
      },
      {
        "indexed": true,
        "internalType": "uint256",
        "name": "collectorProfileId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "referrerProfileId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "referrerPubId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "collectModule",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "bytes",
        "name": "collectModuleData",
        "type": "bytes"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "tokenId",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "CollectedLegacy",
    "type": "event"
  }
]
</file>

<file path="examples/rindexer_rust_playground/abis/lens-registry-events-abi.json">
[
  {
    "anonymous": false,
    "inputs": [
      {
        "components": [
          {
            "internalType": "uint256",
            "name": "id",
            "type": "uint256"
          },
          {
            "internalType": "address",
            "name": "collection",
            "type": "address"
          }
        ],
        "indexed": false,
        "internalType": "struct RegistryTypes.Handle",
        "name": "handle",
        "type": "tuple"
      },
      {
        "components": [
          {
            "internalType": "uint256",
            "name": "id",
            "type": "uint256"
          },
          {
            "internalType": "address",
            "name": "collection",
            "type": "address"
          }
        ],
        "indexed": false,
        "internalType": "struct RegistryTypes.Token",
        "name": "token",
        "type": "tuple"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "HandleLinked",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "components": [
          {
            "internalType": "uint256",
            "name": "id",
            "type": "uint256"
          },
          {
            "internalType": "address",
            "name": "collection",
            "type": "address"
          }
        ],
        "indexed": false,
        "internalType": "struct RegistryTypes.Handle",
        "name": "handle",
        "type": "tuple"
      },
      {
        "components": [
          {
            "internalType": "uint256",
            "name": "id",
            "type": "uint256"
          },
          {
            "internalType": "address",
            "name": "collection",
            "type": "address"
          }
        ],
        "indexed": false,
        "internalType": "struct RegistryTypes.Token",
        "name": "token",
        "type": "tuple"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "transactionExecutor",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "HandleUnlinked",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "signer",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "nonce",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "timestamp",
        "type": "uint256"
      }
    ],
    "name": "NonceUpdated",
    "type": "event"
  }
]
</file>

<file path="examples/rindexer_rust_playground/abis/playground-types-abi.json">
[
  {
    "anonymous": false,
    "name": "BasicTypes",
    "type": "event",
    "inputs": [
      {
        "name": "aBool",
        "type": "bool",
        "indexed": false
      },
      {
        "name": "simpleAddress",
        "type": "address",
        "indexed": false
      },
      {
        "name": "simpleString",
        "type": "string",
        "indexed": false
      }
    ]
  },
  {
    "anonymous": false,
    "name": "TupleTypes",
    "type": "event",
    "inputs": [
      {
        "name": "array",
        "type": "tuple[]",
        "internalType": "struct SimpleStruct[]",
        "indexed": false,
        "components": [
          {
            "name": "address",
            "type": "address"
          },
          {
            "name": "string",
            "type": "string"
          },
          {
            "name": "fixedBytes",
            "type": "bytes32"
          },
          {
            "name": "dynamicBytes",
            "type": "bytes"
          }
        ]
      },
      {
        "name": "nestedArray",
        "type": "tuple[]",
        "internalType": "struct ComplexStruct[]",
        "indexed": false,
        "components": [
          {
            "name": "ruleParam",
            "type": "tuple",
            "internalType": "struct KeyValueArray",
            "components": [
              {
                "name": "key",
                "type": "bytes32"
              },
              {
                "name": "value",
                "type": "tuple[]",
                "internalType": "struct KeyValue[]",
                "components": [
                  {
                    "name": "key",
                    "type": "bytes32"
                  },
                  {
                    "name": "value",
                    "type": "bytes"
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "anonymous": false,
    "name": "BytesTypes",
    "type": "event",
    "inputs": [
      {
        "name": "aByte1",
        "type": "bytes1",
        "indexed": false
      },
      {
        "name": "aByte4",
        "type": "bytes4",
        "indexed": false
      },
      {
        "name": "aByte8",
        "type": "bytes8",
        "indexed": false
      },
      {
        "name": "aByte16",
        "type": "bytes16",
        "indexed": false
      },
      {
        "name": "aByte32",
        "type": "bytes32",
        "indexed": false
      },
      {
        "name": "dynamicBytes",
        "type": "bytes",
        "indexed": false
      }
    ]
  },
  {
    "anonymous": false,
    "name": "RegularWidthSignedIntegers",
    "type": "event",
    "inputs": [
      {
        "name": "i8",
        "type": "int8",
        "indexed": false
      },
      {
        "name": "i16",
        "type": "int16",
        "indexed": false
      },
      {
        "name": "i32",
        "type": "int32",
        "indexed": false
      },
      {
        "name": "i64",
        "type": "int64",
        "indexed": false
      },
      {
        "name": "i128",
        "type": "int128",
        "indexed": false
      },
      {
        "name": "i256",
        "type": "int256",
        "indexed": false
      }
    ]
  },
  {
    "anonymous": false,
    "name": "RegularWidthUnsignedIntegers",
    "type": "event",
    "inputs": [
      {
        "name": "u8",
        "type": "uint8",
        "indexed": false
      },
      {
        "name": "u16",
        "type": "uint16",
        "indexed": false
      },
      {
        "name": "u32",
        "type": "uint32",
        "indexed": false
      },
      {
        "name": "u64",
        "type": "uint64",
        "indexed": false
      },
      {
        "name": "u128",
        "type": "uint128",
        "indexed": false
      },
      {
        "name": "u256",
        "type": "uint256",
        "indexed": false
      }
    ]
  },
  {
    "anonymous": false,
    "name": "IrregularWidthSignedIntegers",
    "type": "event",
    "inputs": [
      {
        "name": "i24",
        "type": "int24",
        "indexed": false
      },
      {
        "name": "i40",
        "type": "int40",
        "indexed": false
      },
      {
        "name": "i48",
        "type": "int48",
        "indexed": false
      },
      {
        "name": "i56",
        "type": "int56",
        "indexed": false
      },
      {
        "name": "i72",
        "type": "int72",
        "indexed": false
      },
      {
        "name": "i80",
        "type": "int80",
        "indexed": false
      },
      {
        "name": "i88",
        "type": "int88",
        "indexed": false
      },
      {
        "name": "i96",
        "type": "int96",
        "indexed": false
      },
      {
        "name": "i104",
        "type": "int104",
        "indexed": false
      },
      {
        "name": "i112",
        "type": "int112",
        "indexed": false
      },
      {
        "name": "i120",
        "type": "int120",
        "indexed": false
      },
      {
        "name": "i136",
        "type": "int136",
        "indexed": false
      },
      {
        "name": "i144",
        "type": "int144",
        "indexed": false
      },
      {
        "name": "i152",
        "type": "int152",
        "indexed": false
      },
      {
        "name": "i160",
        "type": "int160",
        "indexed": false
      },
      {
        "name": "i168",
        "type": "int168",
        "indexed": false
      },
      {
        "name": "i176",
        "type": "int176",
        "indexed": false
      },
      {
        "name": "i184",
        "type": "int184",
        "indexed": false
      },
      {
        "name": "i192",
        "type": "int192",
        "indexed": false
      },
      {
        "name": "i200",
        "type": "int200",
        "indexed": false
      },
      {
        "name": "i208",
        "type": "int208",
        "indexed": false
      },
      {
        "name": "i216",
        "type": "int216",
        "indexed": false
      },
      {
        "name": "i224",
        "type": "int224",
        "indexed": false
      },
      {
        "name": "i232",
        "type": "int232",
        "indexed": false
      },
      {
        "name": "i240",
        "type": "int240",
        "indexed": false
      },
      {
        "name": "i248",
        "type": "int248",
        "indexed": false
      }
    ]
  },
  {
    "anonymous": false,
    "name": "IrregularWidthUnsignedIntegers",
    "type": "event",
    "inputs": [
      {
        "name": "u24",
        "type": "uint24",
        "indexed": false
      },
      {
        "name": "u40",
        "type": "uint40",
        "indexed": false
      },
      {
        "name": "u48",
        "type": "uint48",
        "indexed": false
      },
      {
        "name": "u56",
        "type": "uint56",
        "indexed": false
      },
      {
        "name": "u72",
        "type": "uint72",
        "indexed": false
      },
      {
        "name": "u80",
        "type": "uint80",
        "indexed": false
      },
      {
        "name": "u88",
        "type": "uint88",
        "indexed": false
      },
      {
        "name": "u96",
        "type": "uint96",
        "indexed": false
      },
      {
        "name": "u104",
        "type": "uint104",
        "indexed": false
      },
      {
        "name": "u112",
        "type": "uint112",
        "indexed": false
      },
      {
        "name": "u120",
        "type": "uint120",
        "indexed": false
      },
      {
        "name": "u136",
        "type": "uint136",
        "indexed": false
      },
      {
        "name": "u144",
        "type": "uint144",
        "indexed": false
      },
      {
        "name": "u152",
        "type": "uint152",
        "indexed": false
      },
      {
        "name": "u160",
        "type": "uint160",
        "indexed": false
      },
      {
        "name": "u168",
        "type": "uint168",
        "indexed": false
      },
      {
        "name": "u176",
        "type": "uint176",
        "indexed": false
      },
      {
        "name": "u184",
        "type": "uint184",
        "indexed": false
      },
      {
        "name": "u192",
        "type": "uint192",
        "indexed": false
      },
      {
        "name": "u200",
        "type": "uint200",
        "indexed": false
      },
      {
        "name": "u208",
        "type": "uint208",
        "indexed": false
      },
      {
        "name": "u216",
        "type": "uint216",
        "indexed": false
      },
      {
        "name": "u224",
        "type": "uint224",
        "indexed": false
      },
      {
        "name": "u232",
        "type": "uint232",
        "indexed": false
      },
      {
        "name": "u240",
        "type": "uint240",
        "indexed": false
      },
      {
        "name": "u248",
        "type": "uint248",
        "indexed": false
      }
    ]
  },
  {
    "type": "event",
    "name": "Under_Score",
    "inputs": [
      {
        "name": "foo",
        "type": "address",
        "indexed": true,
        "internalType": "address"
      }
    ],
    "anonymous": false
  },
  {
    "type": "event",
    "name": "CAPITALIZED",
    "inputs": [
      {
        "name": "foo",
        "type": "address",
        "indexed": true,
        "internalType": "address"
      }
    ],
    "anonymous": false
  }
]
</file>

<file path="examples/rindexer_rust_playground/abis/uniswap-v3-pool-abi.json">
[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Burn",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "Collect",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "CollectProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid1",
        "type": "uint256"
      }
    ],
    "name": "Flash",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextOld",
        "type": "uint16"
      },
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextNew",
        "type": "uint16"
      }
    ],
    "name": "IncreaseObservationCardinalityNext",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Initialize",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Mint",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0New",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1New",
        "type": "uint8"
      }
    ],
    "name": "SetFeeProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "liquidity",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Swap",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      }
    ],
    "name": "burn",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collect",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collectProtocol",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "factory",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "fee",
    "outputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal0X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal1X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "flash",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      }
    ],
    "name": "increaseObservationCardinalityNext",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      }
    ],
    "name": "initialize",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "liquidity",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "maxLiquidityPerTick",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "mint",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint256",
        "name": "index",
        "type": "uint256"
      }
    ],
    "name": "observations",
    "outputs": [
      {
        "internalType": "uint32",
        "name": "blockTimestamp",
        "type": "uint32"
      },
      {
        "internalType": "int56",
        "name": "tickCumulative",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityCumulativeX128",
        "type": "uint160"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint32[]",
        "name": "secondsAgos",
        "type": "uint32[]"
      }
    ],
    "name": "observe",
    "outputs": [
      {
        "internalType": "int56[]",
        "name": "tickCumulatives",
        "type": "int56[]"
      },
      {
        "internalType": "uint160[]",
        "name": "secondsPerLiquidityCumulativeX128s",
        "type": "uint160[]"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "bytes32",
        "name": "key",
        "type": "bytes32"
      }
    ],
    "name": "positions",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "_liquidity",
        "type": "uint128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside0LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside1LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "protocolFees",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "token0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "token1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint8",
        "name": "feeProtocol0",
        "type": "uint8"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol1",
        "type": "uint8"
      }
    ],
    "name": "setFeeProtocol",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "slot0",
    "outputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      },
      {
        "internalType": "uint16",
        "name": "observationIndex",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinality",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol",
        "type": "uint8"
      },
      {
        "internalType": "bool",
        "name": "unlocked",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      }
    ],
    "name": "snapshotCumulativesInside",
    "outputs": [
      {
        "internalType": "int56",
        "name": "tickCumulativeInside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityInsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsInside",
        "type": "uint32"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "bool",
        "name": "zeroForOne",
        "type": "bool"
      },
      {
        "internalType": "int256",
        "name": "amountSpecified",
        "type": "int256"
      },
      {
        "internalType": "uint160",
        "name": "sqrtPriceLimitX96",
        "type": "uint160"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "swap",
    "outputs": [
      {
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int16",
        "name": "wordPosition",
        "type": "int16"
      }
    ],
    "name": "tickBitmap",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "tickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "ticks",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "liquidityGross",
        "type": "uint128"
      },
      {
        "internalType": "int128",
        "name": "liquidityNet",
        "type": "int128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside0X128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside1X128",
        "type": "uint256"
      },
      {
        "internalType": "int56",
        "name": "tickCumulativeOutside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityOutsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsOutside",
        "type": "uint32"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token0",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token1",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  }
]
</file>

<file path="examples/rindexer_rust_playground/abis/world.abi.json">
[
  {
    "type": "event",
    "name": "ComponentValueSet",
    "inputs": [
      {
        "name": "arg0",
        "type": "uint256",
        "indexed": true,
        "internalType": "uint256"
      },
      {
        "name": "arg1",
        "type": "address",
        "indexed": true,
        "internalType": "address"
      },
      {
        "name": "arg2",
        "type": "uint256",
        "indexed": true,
        "internalType": "uint256"
      },
      {
        "name": "arg3",
        "type": "bytes",
        "indexed": false,
        "internalType": "bytes"
      }
    ],
    "anonymous": false
  }
]
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/indexers/rindexer_playground/erc_20_filter.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_playground::events::erc_20_filter::{
    ApprovalEvent, ERC20FilterEventType, TransferEvent, no_extensions,
};
use alloy::primitives::{I256, U64, U256};
use rindexer::{
    EthereumSqlTypeWrapper, PgType, event::callback_registry::EventCallbackRegistry,
    rindexer_error, rindexer_info,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn approval_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = ApprovalEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            let mut csv_bulk_data: Vec<Vec<String>> = vec![];
            for result in results.iter() {
                csv_bulk_data.push(vec![
                    result.tx_information.address.to_string(),
                    result.event_data.owner.to_string(),
                    result.event_data.spender.to_string(),
                    result.event_data.value.to_string(),
                    result.tx_information.transaction_hash.to_string(),
                    result.tx_information.block_number.to_string(),
                    result.tx_information.block_hash.to_string(),
                    result.tx_information.network.to_string(),
                    result.tx_information.transaction_index.to_string(),
                    result.tx_information.log_index.to_string(),
                ]);
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.owner),
                    EthereumSqlTypeWrapper::Address(result.event_data.spender),
                    EthereumSqlTypeWrapper::U256(U256::from(result.event_data.value)),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if !csv_bulk_data.is_empty() {
                let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                if let Err(e) = csv_result {
                    rindexer_error!("ERC20FilterEventType::Approval inserting csv data: {:?}", e);
                    return Err(e.to_string());
                }
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "owner".to_string(),
                "spender".to_string(),
                "value".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_playground_erc_20_filter.approval",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!("ERC20FilterEventType::Approval inserting bulk data: {:?}", e);
                return Err(e.to_string());
            }
            rindexer_info!("ERC20Filter::Approval - INDEXED - {} events", results.len(),);
            Ok(())
        },
        no_extensions(),
    )
    .await;
    ERC20FilterEventType::Approval(handler).register(manifest_path, registry).await;
}
async fn transfer_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = TransferEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            let mut csv_bulk_data: Vec<Vec<String>> = vec![];
            for result in results.iter() {
                csv_bulk_data.push(vec![
                    result.tx_information.address.to_string(),
                    result.event_data.from.to_string(),
                    result.event_data.to.to_string(),
                    result.event_data.value.to_string(),
                    result.tx_information.transaction_hash.to_string(),
                    result.tx_information.block_number.to_string(),
                    result.tx_information.block_hash.to_string(),
                    result.tx_information.network.to_string(),
                    result.tx_information.transaction_index.to_string(),
                    result.tx_information.log_index.to_string(),
                ]);
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.from),
                    EthereumSqlTypeWrapper::Address(result.event_data.to),
                    EthereumSqlTypeWrapper::U256(U256::from(result.event_data.value)),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if !csv_bulk_data.is_empty() {
                let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                if let Err(e) = csv_result {
                    rindexer_error!("ERC20FilterEventType::Transfer inserting csv data: {:?}", e);
                    return Err(e.to_string());
                }
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "from".to_string(),
                "to".to_string(),
                "value".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_playground_erc_20_filter.transfer",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!("ERC20FilterEventType::Transfer inserting bulk data: {:?}", e);
                return Err(e.to_string());
            }
            rindexer_info!("ERC20Filter::Transfer - INDEXED - {} events", results.len(),);
            Ok(())
        },
        no_extensions(),
    )
    .await;
    ERC20FilterEventType::Transfer(handler).register(manifest_path, registry).await;
}
pub async fn erc_20_filter_handlers(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    approval_handler(manifest_path, registry).await;
    transfer_handler(manifest_path, registry).await;
}
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/indexers/rindexer_playground/mod.rs">
#![allow(dead_code, unused)]
pub mod erc_20_filter;
pub mod rocket_pool_eth;
pub mod uniswap_v3_pool_filter;
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/indexers/rindexer_playground/rocket_pool_eth.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_playground::events::rocket_pool_eth::{
    ApprovalEvent, RocketPoolETHEventType, TransferEvent, no_extensions,
};
use alloy::primitives::{I256, U64, U256};
use rindexer::{
    EthereumSqlTypeWrapper, PgType, event::callback_registry::EventCallbackRegistry,
    rindexer_error, rindexer_info,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn approval_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = ApprovalEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            let mut csv_bulk_data: Vec<Vec<String>> = vec![];
            for result in results.iter() {
                csv_bulk_data.push(vec![
                    result.tx_information.address.to_string(),
                    result.event_data.owner.to_string(),
                    result.event_data.spender.to_string(),
                    result.event_data.value.to_string(),
                    result.tx_information.transaction_hash.to_string(),
                    result.tx_information.block_number.to_string(),
                    result.tx_information.block_hash.to_string(),
                    result.tx_information.network.to_string(),
                    result.tx_information.transaction_index.to_string(),
                    result.tx_information.log_index.to_string(),
                ]);
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.owner),
                    EthereumSqlTypeWrapper::Address(result.event_data.spender),
                    EthereumSqlTypeWrapper::U256(U256::from(result.event_data.value)),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if !csv_bulk_data.is_empty() {
                let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                if let Err(e) = csv_result {
                    rindexer_error!("RocketPoolETHEventType::Approval inserting csv data: {:?}", e);
                    return Err(e.to_string());
                }
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "owner".to_string(),
                "spender".to_string(),
                "value".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_playground_rocket_pool_eth.approval",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!("RocketPoolETHEventType::Approval inserting bulk data: {:?}", e);
                return Err(e.to_string());
            }
            rindexer_info!("RocketPoolETH::Approval - INDEXED - {} events", results.len(),);
            Ok(())
        },
        no_extensions(),
    )
    .await;
    RocketPoolETHEventType::Approval(handler).register(manifest_path, registry).await;
}
async fn transfer_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = TransferEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            let mut csv_bulk_data: Vec<Vec<String>> = vec![];
            for result in results.iter() {
                csv_bulk_data.push(vec![
                    result.tx_information.address.to_string(),
                    result.event_data.from.to_string(),
                    result.event_data.to.to_string(),
                    result.event_data.value.to_string(),
                    result.tx_information.transaction_hash.to_string(),
                    result.tx_information.block_number.to_string(),
                    result.tx_information.block_hash.to_string(),
                    result.tx_information.network.to_string(),
                    result.tx_information.transaction_index.to_string(),
                    result.tx_information.log_index.to_string(),
                ]);
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.from),
                    EthereumSqlTypeWrapper::Address(result.event_data.to),
                    EthereumSqlTypeWrapper::U256(U256::from(result.event_data.value)),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if !csv_bulk_data.is_empty() {
                let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                if let Err(e) = csv_result {
                    rindexer_error!("RocketPoolETHEventType::Transfer inserting csv data: {:?}", e);
                    return Err(e.to_string());
                }
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "from".to_string(),
                "to".to_string(),
                "value".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_playground_rocket_pool_eth.transfer",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!("RocketPoolETHEventType::Transfer inserting bulk data: {:?}", e);
                return Err(e.to_string());
            }
            rindexer_info!("RocketPoolETH::Transfer - INDEXED - {} events", results.len(),);
            Ok(())
        },
        no_extensions(),
    )
    .await;
    RocketPoolETHEventType::Transfer(handler).register(manifest_path, registry).await;
}
pub async fn rocket_pool_eth_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    approval_handler(manifest_path, registry).await;
    transfer_handler(manifest_path, registry).await;
}
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/indexers/rindexer_playground/uniswap_v3_pool_filter.rs">
#![allow(non_snake_case)]
use super::super::super::typings::rindexer_playground::events::uniswap_v3_pool_filter::{
    SwapEvent, UniswapV3PoolFilterEventType, no_extensions,
};
use alloy::primitives::{I256, U64, U256};
use rindexer::{
    EthereumSqlTypeWrapper, PgType, event::callback_registry::EventCallbackRegistry,
    rindexer_error, rindexer_info,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn swap_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = SwapEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            let mut csv_bulk_data: Vec<Vec<String>> = vec![];
            for result in results.iter() {
                csv_bulk_data.push(vec![
                    result.tx_information.address.to_string(),
                    result.event_data.sender.to_string(),
                    result.event_data.recipient.to_string(),
                    result.event_data.amount0.to_string(),
                    result.event_data.amount1.to_string(),
                    result.event_data.sqrtPriceX96.to_string(),
                    result.event_data.liquidity.to_string(),
                    result.event_data.tick.to_string(),
                    result.tx_information.transaction_hash.to_string(),
                    result.tx_information.block_number.to_string(),
                    result.tx_information.block_hash.to_string(),
                    result.tx_information.network.to_string(),
                    result.tx_information.transaction_index.to_string(),
                    result.tx_information.log_index.to_string(),
                ]);
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.sender),
                    EthereumSqlTypeWrapper::Address(result.event_data.recipient),
                    EthereumSqlTypeWrapper::I256(I256::from(result.event_data.amount0)),
                    EthereumSqlTypeWrapper::I256(I256::from(result.event_data.amount1)),
                    EthereumSqlTypeWrapper::U256(U256::from(result.event_data.sqrtPriceX96)),
                    EthereumSqlTypeWrapper::U128(result.event_data.liquidity),
                    EthereumSqlTypeWrapper::I32(result.event_data.tick.unchecked_into()),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if !csv_bulk_data.is_empty() {
                let csv_result = context.csv.append_bulk(csv_bulk_data).await;
                if let Err(e) = csv_result {
                    rindexer_error!(
                        "UniswapV3PoolFilterEventType::Swap inserting csv data: {:?}",
                        e
                    );
                    return Err(e.to_string());
                }
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "sender".to_string(),
                "recipient".to_string(),
                "amount_0".to_string(),
                "amount_1".to_string(),
                "sqrt_price_x96".to_string(),
                "liquidity".to_string(),
                "tick".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk(
                    "rindexer_playground_uniswap_v3_pool_filter.swap",
                    &rows,
                    &postgres_bulk_data,
                )
                .await;
            if let Err(e) = result {
                rindexer_error!("UniswapV3PoolFilterEventType::Swap inserting bulk data: {:?}", e);
                return Err(e.to_string());
            }
            rindexer_info!("UniswapV3PoolFilter::Swap - INDEXED - {} events", results.len(),);
            Ok(())
        },
        no_extensions(),
    )
    .await;
    UniswapV3PoolFilterEventType::Swap(handler).register(manifest_path, registry).await;
}
pub async fn uniswap_v3_pool_filter_handlers(
    manifest_path: &PathBuf,
    registry: &mut EventCallbackRegistry,
) {
    swap_handler(manifest_path, registry).await;
}
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/indexers/all_handlers.rs">
use super::rindexer_playground::erc_20_filter::erc_20_filter_handlers;
use super::rindexer_playground::rocket_pool_eth::rocket_pool_eth_handlers;
use super::rindexer_playground::uniswap_v3_pool_filter::uniswap_v3_pool_filter_handlers;
use rindexer::event::callback_registry::EventCallbackRegistry;
use std::path::PathBuf;
pub async fn register_all_handlers(manifest_path: &PathBuf) -> EventCallbackRegistry {
    let mut registry = EventCallbackRegistry::new();
    rocket_pool_eth_handlers(manifest_path, &mut registry).await;
    erc_20_filter_handlers(manifest_path, &mut registry).await;
    uniswap_v3_pool_filter_handlers(manifest_path, &mut registry).await;
    registry
}
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/indexers/mod.rs">
#![allow(dead_code, unused)]
pub mod all_handlers;
pub mod rindexer_playground;
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/rindexer_playground/events/erc_20_filter_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerERC20FilterGen,
    r#"[
  {
    "constant": true,
    "inputs": [],
    "name": "name",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_spender",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "approve",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "totalSupply",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_from",
        "type": "address"
      },
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transferFrom",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "decimals",
    "outputs": [
      {
        "name": "",
        "type": "uint8"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "balanceOf",
    "outputs": [
      {
        "name": "balance",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "symbol",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transfer",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      },
      {
        "name": "_spender",
        "type": "address"
      }
    ],
    "name": "allowance",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "payable": true,
    "stateMutability": "payable",
    "type": "fallback"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "spender",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Approval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "from",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Transfer",
    "type": "event"
  }
]
"#
);
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/rindexer_playground/events/erc_20_filter.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_postgres_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::erc_20_filter_abi_gen::RindexerERC20FilterGen::{
    self, RindexerERC20FilterGenEvents, RindexerERC20FilterGenInstance,
};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, B256, Bytes};
use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
use rindexer::{
    AsyncCsvAppender, FutureExt, PostgresClient, async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation,
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::read_manifest,
    },
    provider::{JsonRpcCachedProvider, RindexerProvider},
};
use std::collections::HashMap;
use std::error::Error;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type ApprovalData = RindexerERC20FilterGen::Approval;
#[derive(Debug, Clone)]
pub struct ApprovalResult {
    pub event_data: ApprovalData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for ApprovalResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type TransferData = RindexerERC20FilterGen::Transfer;
#[derive(Debug, Clone)]
pub struct TransferResult {
    pub event_data: TransferData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for TransferResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<PostgresClient>,
    pub csv: Arc<AsyncCsvAppender>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn approval_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> ApprovalEventCallbackType<TExtensions>
where
    ApprovalResult: Clone + 'static,
    F: for<'a> Fn(Vec<ApprovalResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type ApprovalEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<ApprovalResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct ApprovalEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: ApprovalEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> ApprovalEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        ApprovalResult: Clone + 'static,
        F: for<'a> Fn(Vec<ApprovalResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        let csv = AsyncCsvAppender::new(
            r"/Users/josh/code/rindexer/examples/rindexer_rust_playground/generated_csv/ERC20Filter/erc20filter-approval.csv",
        );
        if !Path::new(r"/Users/josh/code/rindexer/examples/rindexer_rust_playground/generated_csv/ERC20Filter/erc20filter-approval.csv").exists() {
            csv.append_header(vec!["contract_address".into(), "owner".into(), "spender".into(), "value".into(), "tx_hash".into(), "block_number".into(), "block_hash".into(), "network".into(), "tx_index".into(), "log_index".into()].into())
                .await
                .expect("Failed to write CSV header");
        }
        Self {
            callback: approval_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_postgres_client().await,
                csv: Arc::new(csv),
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for ApprovalEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<ApprovalResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<ApprovalData>().ok().map(|arc| ApprovalResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        (self.callback)(&result, Arc::clone(&self.context)).await
    }
}
pub fn transfer_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> TransferEventCallbackType<TExtensions>
where
    TransferResult: Clone + 'static,
    F: for<'a> Fn(Vec<TransferResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type TransferEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<TransferResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct TransferEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: TransferEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> TransferEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        TransferResult: Clone + 'static,
        F: for<'a> Fn(Vec<TransferResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        let csv = AsyncCsvAppender::new(
            r"/Users/josh/code/rindexer/examples/rindexer_rust_playground/generated_csv/ERC20Filter/erc20filter-transfer.csv",
        );
        if !Path::new(r"/Users/josh/code/rindexer/examples/rindexer_rust_playground/generated_csv/ERC20Filter/erc20filter-transfer.csv").exists() {
            csv.append_header(vec!["contract_address".into(), "from".into(), "to".into(), "value".into(), "tx_hash".into(), "block_number".into(), "block_hash".into(), "network".into(), "tx_index".into(), "log_index".into()].into())
                .await
                .expect("Failed to write CSV header");
        }
        Self {
            callback: transfer_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_postgres_client().await,
                csv: Arc::new(csv),
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for TransferEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<TransferResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<TransferData>().ok().map(|arc| TransferResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        (self.callback)(&result, Arc::clone(&self.context)).await
    }
}
pub enum ERC20FilterEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    Approval(ApprovalEvent<TExtensions>),
    Transfer(TransferEvent<TExtensions>),
}
pub async fn erc_20_filter_contract(
    network: &str,
    address: Address,
) -> RindexerERC20FilterGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerERC20FilterGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerERC20FilterGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "ethereum" {
        RindexerERC20FilterGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> ERC20FilterEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            ERC20FilterEventType::Approval(_) => {
                "0x8c5be1e5ebec7d5bd14f71427d1e84f3dd0314c0f7b2291e5b200ac8c7c3b925"
            }
            ERC20FilterEventType::Transfer(_) => {
                "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            ERC20FilterEventType::Approval(_) => "Approval",
            ERC20FilterEventType::Transfer(_) => "Transfer",
        }
    }
    pub fn contract_name(&self) -> String {
        "ERC20".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            ERC20FilterEventType::Approval(_) => Arc::new(move |topics: Vec<B256>, data: Bytes| {
                match ApprovalData::decode_raw_log(topics, &data[0..]) {
                    Ok(event) => {
                        let result: ApprovalData = event;
                        Arc::new(result) as Arc<dyn Any + Send + Sync>
                    }
                    Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                }
            }),
            ERC20FilterEventType::Transfer(_) => Arc::new(move |topics: Vec<B256>, data: Bytes| {
                match TransferData::decode_raw_log(topics, &data[0..]) {
                    Ok(event) => {
                        let result: TransferData = event;
                        Arc::new(result) as Arc<dyn Any + Send + Sync>
                    }
                    Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                }
            }),
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .map_or(false, |vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .map_or(false, |n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            ERC20FilterEventType::Approval(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
            ERC20FilterEventType::Transfer(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerPlayground".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/rindexer_playground/events/mod.rs">
#![allow(dead_code, unused)]
pub mod erc_20_filter;
mod erc_20_filter_abi_gen;
pub mod rocket_pool_eth;
mod rocket_pool_eth_abi_gen;
pub mod uniswap_v3_pool_filter;
mod uniswap_v3_pool_filter_abi_gen;
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/rindexer_playground/events/rocket_pool_eth_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerRocketPoolETHGen,
    r#"[
  {
    "constant": true,
    "inputs": [],
    "name": "name",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_spender",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "approve",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "totalSupply",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_from",
        "type": "address"
      },
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transferFrom",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "decimals",
    "outputs": [
      {
        "name": "",
        "type": "uint8"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      }
    ],
    "name": "balanceOf",
    "outputs": [
      {
        "name": "balance",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "symbol",
    "outputs": [
      {
        "name": "",
        "type": "string"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      {
        "name": "_to",
        "type": "address"
      },
      {
        "name": "_value",
        "type": "uint256"
      }
    ],
    "name": "transfer",
    "outputs": [
      {
        "name": "",
        "type": "bool"
      }
    ],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      {
        "name": "_owner",
        "type": "address"
      },
      {
        "name": "_spender",
        "type": "address"
      }
    ],
    "name": "allowance",
    "outputs": [
      {
        "name": "",
        "type": "uint256"
      }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "payable": true,
    "stateMutability": "payable",
    "type": "fallback"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "spender",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Approval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "name": "from",
        "type": "address"
      },
      {
        "indexed": true,
        "name": "to",
        "type": "address"
      },
      {
        "indexed": false,
        "name": "value",
        "type": "uint256"
      }
    ],
    "name": "Transfer",
    "type": "event"
  }
]
"#
);
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/rindexer_playground/events/rocket_pool_eth.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_postgres_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::rocket_pool_eth_abi_gen::RindexerRocketPoolETHGen::{
    self, RindexerRocketPoolETHGenEvents, RindexerRocketPoolETHGenInstance,
};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, B256, Bytes};
use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
use rindexer::{
    AsyncCsvAppender, FutureExt, PostgresClient, async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation,
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::read_manifest,
    },
    provider::{JsonRpcCachedProvider, RindexerProvider},
};
use std::collections::HashMap;
use std::error::Error;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type ApprovalData = RindexerRocketPoolETHGen::Approval;
#[derive(Debug, Clone)]
pub struct ApprovalResult {
    pub event_data: ApprovalData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for ApprovalResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type TransferData = RindexerRocketPoolETHGen::Transfer;
#[derive(Debug, Clone)]
pub struct TransferResult {
    pub event_data: TransferData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for TransferResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<PostgresClient>,
    pub csv: Arc<AsyncCsvAppender>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn approval_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> ApprovalEventCallbackType<TExtensions>
where
    ApprovalResult: Clone + 'static,
    F: for<'a> Fn(Vec<ApprovalResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type ApprovalEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<ApprovalResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct ApprovalEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: ApprovalEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> ApprovalEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        ApprovalResult: Clone + 'static,
        F: for<'a> Fn(Vec<ApprovalResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        let csv = AsyncCsvAppender::new(
            r"/Users/josh/code/rindexer/examples/rindexer_rust_playground/generated_csv/RocketPoolETH/rocketpooleth-approval.csv",
        );
        if !Path::new(r"/Users/josh/code/rindexer/examples/rindexer_rust_playground/generated_csv/RocketPoolETH/rocketpooleth-approval.csv").exists() {
            csv.append_header(vec!["contract_address".into(), "owner".into(), "spender".into(), "value".into(), "tx_hash".into(), "block_number".into(), "block_hash".into(), "network".into(), "tx_index".into(), "log_index".into()].into())
                .await
                .expect("Failed to write CSV header");
        }
        Self {
            callback: approval_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_postgres_client().await,
                csv: Arc::new(csv),
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for ApprovalEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<ApprovalResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<ApprovalData>().ok().map(|arc| ApprovalResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("ApprovalEvent: Unexpected data type - expected: ApprovalData")
        }
    }
}
pub fn transfer_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> TransferEventCallbackType<TExtensions>
where
    TransferResult: Clone + 'static,
    F: for<'a> Fn(Vec<TransferResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type TransferEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<TransferResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct TransferEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: TransferEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> TransferEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        TransferResult: Clone + 'static,
        F: for<'a> Fn(Vec<TransferResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        let csv = AsyncCsvAppender::new(
            r"/Users/josh/code/rindexer/examples/rindexer_rust_playground/generated_csv/RocketPoolETH/rocketpooleth-transfer.csv",
        );
        if !Path::new(r"/Users/josh/code/rindexer/examples/rindexer_rust_playground/generated_csv/RocketPoolETH/rocketpooleth-transfer.csv").exists() {
            csv.append_header(vec!["contract_address".into(), "from".into(), "to".into(), "value".into(), "tx_hash".into(), "block_number".into(), "block_hash".into(), "network".into(), "tx_index".into(), "log_index".into()].into())
                .await
                .expect("Failed to write CSV header");
        }
        Self {
            callback: transfer_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_postgres_client().await,
                csv: Arc::new(csv),
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for TransferEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<TransferResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<TransferData>().ok().map(|arc| TransferResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("TransferEvent: Unexpected data type - expected: TransferData")
        }
    }
}
pub enum RocketPoolETHEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    Approval(ApprovalEvent<TExtensions>),
    Transfer(TransferEvent<TExtensions>),
}
pub async fn rocket_pool_eth_contract(
    network: &str,
) -> RindexerRocketPoolETHGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    let address: Address =
        "0xae78736cd615f374d3085123a210448e74fc6393".parse().expect("Invalid address");
    RindexerRocketPoolETHGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerRocketPoolETHGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "ethereum" {
        RindexerRocketPoolETHGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> RocketPoolETHEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            RocketPoolETHEventType::Approval(_) => {
                "0x8c5be1e5ebec7d5bd14f71427d1e84f3dd0314c0f7b2291e5b200ac8c7c3b925"
            }
            RocketPoolETHEventType::Transfer(_) => {
                "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            RocketPoolETHEventType::Approval(_) => "Approval",
            RocketPoolETHEventType::Transfer(_) => "Transfer",
        }
    }
    pub fn contract_name(&self) -> String {
        "RocketPoolETH".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            RocketPoolETHEventType::Approval(_) => {
                Arc::new(move |topics: Vec<B256>, data: Bytes| {
                    match ApprovalData::decode_raw_log(topics, &data[0..]) {
                        Ok(event) => {
                            let result: ApprovalData = event;
                            Arc::new(result) as Arc<dyn Any + Send + Sync>
                        }
                        Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                    }
                })
            }
            RocketPoolETHEventType::Transfer(_) => {
                Arc::new(move |topics: Vec<B256>, data: Bytes| {
                    match TransferData::decode_raw_log(topics, &data[0..]) {
                        Ok(event) => {
                            let result: TransferData = event;
                            Arc::new(result) as Arc<dyn Any + Send + Sync>
                        }
                        Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                    }
                })
            }
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .map_or(false, |vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .map_or(false, |n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            RocketPoolETHEventType::Approval(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
            RocketPoolETHEventType::Transfer(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerPlayground".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/rindexer_playground/events/uniswap_v3_pool_filter_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerUniswapV3PoolFilterGen,
    r#"[
  {
    "inputs": [],
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Burn",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "Collect",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "name": "CollectProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "paid1",
        "type": "uint256"
      }
    ],
    "name": "Flash",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextOld",
        "type": "uint16"
      },
      {
        "indexed": false,
        "internalType": "uint16",
        "name": "observationCardinalityNextNew",
        "type": "uint16"
      }
    ],
    "name": "IncreaseObservationCardinalityNext",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Initialize",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "owner",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "indexed": true,
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "indexed": false,
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "name": "Mint",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1Old",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol0New",
        "type": "uint8"
      },
      {
        "indexed": false,
        "internalType": "uint8",
        "name": "feeProtocol1New",
        "type": "uint8"
      }
    ],
    "name": "SetFeeProtocol",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      {
        "indexed": true,
        "internalType": "address",
        "name": "sender",
        "type": "address"
      },
      {
        "indexed": true,
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      },
      {
        "indexed": false,
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "indexed": false,
        "internalType": "uint128",
        "name": "liquidity",
        "type": "uint128"
      },
      {
        "indexed": false,
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "Swap",
    "type": "event"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      }
    ],
    "name": "burn",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collect",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint128",
        "name": "amount0Requested",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1Requested",
        "type": "uint128"
      }
    ],
    "name": "collectProtocol",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "amount0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "amount1",
        "type": "uint128"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "factory",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "fee",
    "outputs": [
      {
        "internalType": "uint24",
        "name": "",
        "type": "uint24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal0X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "feeGrowthGlobal1X128",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "flash",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      }
    ],
    "name": "increaseObservationCardinalityNext",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      }
    ],
    "name": "initialize",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "liquidity",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "maxLiquidityPerTick",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      },
      {
        "internalType": "uint128",
        "name": "amount",
        "type": "uint128"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "mint",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "amount0",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "amount1",
        "type": "uint256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint256",
        "name": "index",
        "type": "uint256"
      }
    ],
    "name": "observations",
    "outputs": [
      {
        "internalType": "uint32",
        "name": "blockTimestamp",
        "type": "uint32"
      },
      {
        "internalType": "int56",
        "name": "tickCumulative",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityCumulativeX128",
        "type": "uint160"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint32[]",
        "name": "secondsAgos",
        "type": "uint32[]"
      }
    ],
    "name": "observe",
    "outputs": [
      {
        "internalType": "int56[]",
        "name": "tickCumulatives",
        "type": "int56[]"
      },
      {
        "internalType": "uint160[]",
        "name": "secondsPerLiquidityCumulativeX128s",
        "type": "uint160[]"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "bytes32",
        "name": "key",
        "type": "bytes32"
      }
    ],
    "name": "positions",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "_liquidity",
        "type": "uint128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside0LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthInside1LastX128",
        "type": "uint256"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "tokensOwed1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "protocolFees",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "token0",
        "type": "uint128"
      },
      {
        "internalType": "uint128",
        "name": "token1",
        "type": "uint128"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "uint8",
        "name": "feeProtocol0",
        "type": "uint8"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol1",
        "type": "uint8"
      }
    ],
    "name": "setFeeProtocol",
    "outputs": [],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "slot0",
    "outputs": [
      {
        "internalType": "uint160",
        "name": "sqrtPriceX96",
        "type": "uint160"
      },
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      },
      {
        "internalType": "uint16",
        "name": "observationIndex",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinality",
        "type": "uint16"
      },
      {
        "internalType": "uint16",
        "name": "observationCardinalityNext",
        "type": "uint16"
      },
      {
        "internalType": "uint8",
        "name": "feeProtocol",
        "type": "uint8"
      },
      {
        "internalType": "bool",
        "name": "unlocked",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tickLower",
        "type": "int24"
      },
      {
        "internalType": "int24",
        "name": "tickUpper",
        "type": "int24"
      }
    ],
    "name": "snapshotCumulativesInside",
    "outputs": [
      {
        "internalType": "int56",
        "name": "tickCumulativeInside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityInsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsInside",
        "type": "uint32"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "address",
        "name": "recipient",
        "type": "address"
      },
      {
        "internalType": "bool",
        "name": "zeroForOne",
        "type": "bool"
      },
      {
        "internalType": "int256",
        "name": "amountSpecified",
        "type": "int256"
      },
      {
        "internalType": "uint160",
        "name": "sqrtPriceLimitX96",
        "type": "uint160"
      },
      {
        "internalType": "bytes",
        "name": "data",
        "type": "bytes"
      }
    ],
    "name": "swap",
    "outputs": [
      {
        "internalType": "int256",
        "name": "amount0",
        "type": "int256"
      },
      {
        "internalType": "int256",
        "name": "amount1",
        "type": "int256"
      }
    ],
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int16",
        "name": "wordPosition",
        "type": "int16"
      }
    ],
    "name": "tickBitmap",
    "outputs": [
      {
        "internalType": "uint256",
        "name": "",
        "type": "uint256"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "tickSpacing",
    "outputs": [
      {
        "internalType": "int24",
        "name": "",
        "type": "int24"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [
      {
        "internalType": "int24",
        "name": "tick",
        "type": "int24"
      }
    ],
    "name": "ticks",
    "outputs": [
      {
        "internalType": "uint128",
        "name": "liquidityGross",
        "type": "uint128"
      },
      {
        "internalType": "int128",
        "name": "liquidityNet",
        "type": "int128"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside0X128",
        "type": "uint256"
      },
      {
        "internalType": "uint256",
        "name": "feeGrowthOutside1X128",
        "type": "uint256"
      },
      {
        "internalType": "int56",
        "name": "tickCumulativeOutside",
        "type": "int56"
      },
      {
        "internalType": "uint160",
        "name": "secondsPerLiquidityOutsideX128",
        "type": "uint160"
      },
      {
        "internalType": "uint32",
        "name": "secondsOutside",
        "type": "uint32"
      },
      {
        "internalType": "bool",
        "name": "initialized",
        "type": "bool"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token0",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  },
  {
    "inputs": [],
    "name": "token1",
    "outputs": [
      {
        "internalType": "address",
        "name": "",
        "type": "address"
      }
    ],
    "stateMutability": "view",
    "type": "function"
  }
]"#
);
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/rindexer_playground/events/uniswap_v3_pool_filter.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_postgres_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::uniswap_v3_pool_filter_abi_gen::RindexerUniswapV3PoolFilterGen::{
    self, RindexerUniswapV3PoolFilterGenEvents, RindexerUniswapV3PoolFilterGenInstance,
};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, B256, Bytes};
use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
use rindexer::{
    AsyncCsvAppender, FutureExt, PostgresClient, async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation,
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::read_manifest,
    },
    provider::{JsonRpcCachedProvider, RindexerProvider},
};
use std::collections::HashMap;
use std::error::Error;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type BurnData = RindexerUniswapV3PoolFilterGen::Burn;
#[derive(Debug, Clone)]
pub struct BurnResult {
    pub event_data: BurnData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for BurnResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type CollectData = RindexerUniswapV3PoolFilterGen::Collect;
#[derive(Debug, Clone)]
pub struct CollectResult {
    pub event_data: CollectData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for CollectResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type CollectProtocolData = RindexerUniswapV3PoolFilterGen::CollectProtocol;
#[derive(Debug, Clone)]
pub struct CollectProtocolResult {
    pub event_data: CollectProtocolData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for CollectProtocolResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type FlashData = RindexerUniswapV3PoolFilterGen::Flash;
#[derive(Debug, Clone)]
pub struct FlashResult {
    pub event_data: FlashData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for FlashResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type IncreaseObservationCardinalityNextData =
    RindexerUniswapV3PoolFilterGen::IncreaseObservationCardinalityNext;
#[derive(Debug, Clone)]
pub struct IncreaseObservationCardinalityNextResult {
    pub event_data: IncreaseObservationCardinalityNextData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for IncreaseObservationCardinalityNextResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type InitializeData = RindexerUniswapV3PoolFilterGen::Initialize;
#[derive(Debug, Clone)]
pub struct InitializeResult {
    pub event_data: InitializeData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for InitializeResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type MintData = RindexerUniswapV3PoolFilterGen::Mint;
#[derive(Debug, Clone)]
pub struct MintResult {
    pub event_data: MintData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for MintResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type SetFeeProtocolData = RindexerUniswapV3PoolFilterGen::SetFeeProtocol;
#[derive(Debug, Clone)]
pub struct SetFeeProtocolResult {
    pub event_data: SetFeeProtocolData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for SetFeeProtocolResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type SwapData = RindexerUniswapV3PoolFilterGen::Swap;
#[derive(Debug, Clone)]
pub struct SwapResult {
    pub event_data: SwapData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for SwapResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<PostgresClient>,
    pub csv: Arc<AsyncCsvAppender>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn swap_handler<TExtensions, F, Fut>(custom_logic: F) -> SwapEventCallbackType<TExtensions>
where
    SwapResult: Clone + 'static,
    F: for<'a> Fn(Vec<SwapResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type SwapEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<SwapResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct SwapEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: SwapEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> SwapEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        SwapResult: Clone + 'static,
        F: for<'a> Fn(Vec<SwapResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        let csv = AsyncCsvAppender::new(
            r"/Users/josh/code/rindexer/examples/rindexer_rust_playground/generated_csv/UniswapV3PoolFilter/uniswapv3poolfilter-swap.csv",
        );
        if !Path::new(r"/Users/josh/code/rindexer/examples/rindexer_rust_playground/generated_csv/UniswapV3PoolFilter/uniswapv3poolfilter-swap.csv").exists() {
            csv.append_header(vec!["contract_address".into(), "sender".into(), "recipient".into(), "amount_0".into(), "amount_1".into(), "sqrt_price_x96".into(), "liquidity".into(), "tick".into(), "tx_hash".into(), "block_number".into(), "block_hash".into(), "network".into(), "tx_index".into(), "log_index".into()].into())
                .await
                .expect("Failed to write CSV header");
        }
        Self {
            callback: swap_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_postgres_client().await,
                csv: Arc::new(csv),
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for SwapEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<SwapResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<SwapData>().ok().map(|arc| SwapResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        (self.callback)(&result, Arc::clone(&self.context)).await
    }
}
pub enum UniswapV3PoolFilterEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    Swap(SwapEvent<TExtensions>),
}
pub async fn uniswap_v3_pool_filter_contract(
    network: &str,
    address: Address,
) -> RindexerUniswapV3PoolFilterGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    RindexerUniswapV3PoolFilterGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerUniswapV3PoolFilterGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "base" {
        RindexerUniswapV3PoolFilterGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> UniswapV3PoolFilterEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            UniswapV3PoolFilterEventType::Swap(_) => {
                "0xc42079f94a6350d7e6235f29174924f928cc2ac818eb64fed8004e115fbcca67"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            UniswapV3PoolFilterEventType::Swap(_) => "Swap",
        }
    }
    pub fn contract_name(&self) -> String {
        "UniswapV3Pool".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            UniswapV3PoolFilterEventType::Swap(_) => {
                Arc::new(move |topics: Vec<B256>, data: Bytes| {
                    match SwapData::decode_raw_log(topics, &data[0..]) {
                        Ok(event) => {
                            let result: SwapData = event;
                            Arc::new(result) as Arc<dyn Any + Send + Sync>
                        }
                        Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                    }
                })
            }
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .map_or(false, |vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .map_or(false, |n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            UniswapV3PoolFilterEventType::Swap(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "RindexerPlayground".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/rindexer_playground/mod.rs">
#![allow(dead_code, unused)]
pub mod events;
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/database.rs">
use rindexer::PostgresClient;
use std::sync::Arc;
use tokio::sync::OnceCell;
static POSTGRES_CLIENT: OnceCell<Arc<PostgresClient>> = OnceCell::const_new();
pub async fn get_or_init_postgres_client() -> Arc<PostgresClient> {
    POSTGRES_CLIENT
        .get_or_init(|| async {
            Arc::new(PostgresClient::new().await.expect("Failed to connect to Postgres"))
        })
        .await
        .clone()
}
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/global_contracts.rs">
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::networks::{get_base_provider, get_ethereum_provider};
use alloy::network::AnyNetwork;
use alloy::primitives::Address;
use alloy::sol;
use rindexer::provider::RindexerProvider;
use std::sync::Arc;
sol!(
    #[sol(rpc, all_derives)]
    USDTEthereum,
    "./abis/erc20-abi.json"
);
pub async fn usdt_get_ethereum_provider_contract()
-> USDTEthereum::USDTEthereumInstance<Arc<RindexerProvider>, AnyNetwork> {
    let address: Address =
        "0xdac17f958d2ee523a2206206994597c13d831ec7".parse().expect("Invalid address");
    USDTEthereum::new(address, get_ethereum_provider().await.clone())
}
sol!(
    #[sol(rpc, all_derives)]
    USDTBase,
    "./abis/erc20-abi.json"
);
pub async fn usdt_get_base_provider_contract()
-> USDTBase::USDTBaseInstance<Arc<RindexerProvider>, AnyNetwork> {
    let address: Address =
        "0xfde4c96c8593536e31f229ea8f37b2ada2699bb2".parse().expect("Invalid address");
    USDTBase::new(address, get_base_provider().await.clone())
}
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/mod.rs">
#![allow(dead_code, unused)]
pub mod database;
pub mod global_contracts;
pub mod networks;
pub mod rindexer_playground;
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/typings/networks.rs">
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use alloy::{primitives::U64, transports::http::reqwest::header::HeaderMap};
use rindexer::{
    lazy_static,
    manifest::network::{AddressFiltering, BlockPollFrequency},
    notifications::ChainStateNotification,
    provider::{JsonRpcCachedProvider, RetryClientError, RindexerProvider, create_client},
    public_read_env_value,
};
use std::sync::Arc;
use tokio::sync::OnceCell;
use tokio::sync::broadcast::Sender;
#[allow(dead_code)]
async fn create_shadow_client(
    rpc_url: &str,
    chain_id: u64,
    compute_units_per_second: Option<u64>,
    block_poll_frequency: Option<BlockPollFrequency>,
    max_block_range: Option<U64>,
    address_filtering: Option<AddressFiltering>,
    chain_state_notification: Option<Sender<ChainStateNotification>>,
) -> Result<Arc<JsonRpcCachedProvider>, RetryClientError> {
    let mut header = HeaderMap::new();
    header.insert(
        "X-SHADOW-API-KEY",
        public_read_env_value("RINDEXER_PHANTOM_API_KEY").unwrap().parse().unwrap(),
    );
    create_client(
        rpc_url,
        chain_id,
        compute_units_per_second,
        max_block_range,
        block_poll_frequency,
        header,
        address_filtering,
        chain_state_notification,
    )
    .await
}
static ETHEREUM_PROVIDER: OnceCell<Arc<JsonRpcCachedProvider>> = OnceCell::const_new();
static BASE_PROVIDER: OnceCell<Arc<JsonRpcCachedProvider>> = OnceCell::const_new();
pub async fn get_ethereum_provider_cache() -> Arc<JsonRpcCachedProvider> {
    ETHEREUM_PROVIDER
        .get_or_init(|| async {
            let chain_state_notification = None;
            create_client(
                &public_read_env_value("https://mainnet.gateway.tenderly.co")
                    .unwrap_or("https://mainnet.gateway.tenderly.co".to_string()),
                1,
                None,
                None,
                Some(BlockPollFrequency::Division { divisor: 4 }),
                HeaderMap::new(),
                None,
                chain_state_notification,
            )
            .await
            .expect("Error creating provider")
        })
        .await
        .clone()
}
pub async fn get_ethereum_provider() -> Arc<RindexerProvider> {
    get_ethereum_provider_cache().await.get_inner_provider()
}
pub async fn get_base_provider_cache() -> Arc<JsonRpcCachedProvider> {
    BASE_PROVIDER
        .get_or_init(|| async {
            let chain_state_notification = None;
            create_client(
                &public_read_env_value("https://mainnet.base.org")
                    .unwrap_or("https://mainnet.base.org".to_string()),
                8453,
                None,
                None,
                None,
                HeaderMap::new(),
                None,
                chain_state_notification,
            )
            .await
            .expect("Error creating provider")
        })
        .await
        .clone()
}
pub async fn get_base_provider() -> Arc<RindexerProvider> {
    get_base_provider_cache().await.get_inner_provider()
}
pub async fn get_provider_cache_for_network(network: &str) -> Arc<JsonRpcCachedProvider> {
    if network == "ethereum" {
        return get_ethereum_provider_cache().await;
    }
    if network == "base" {
        return get_base_provider_cache().await;
    }
    panic!("Network not supported")
}
</file>

<file path="examples/rindexer_rust_playground/src/rindexer_lib/mod.rs">
#![allow(dead_code, unused)]
pub mod indexers;
pub mod typings;
</file>

<file path="examples/rindexer_rust_playground/src/main.rs">
use std::{env, path::PathBuf};
use rindexer::{
    GraphqlOverrideSettings, IndexingDetails, StartDetails,
    event::callback_registry::TraceCallbackRegistry, manifest::yaml::read_manifest, start_rindexer,
};
use self::rindexer_lib::indexers::all_handlers::register_all_handlers;
#[allow(clippy::all)]
mod rindexer_lib;
#[tokio::main]
async fn main() {
    let args: Vec<String> = env::args().collect();
    let mut enable_graphql = false;
    let mut enable_indexer = false;
    let mut port: Option<u16> = None;
    let args = args.iter();
    if args.len() == 1 {
        enable_graphql = true;
        enable_indexer = true;
    }
    for arg in args {
        match arg.as_str() {
            "--graphql" => enable_graphql = true,
            "--indexer" => enable_indexer = true,
            _ if arg.starts_with("--port=") || arg.starts_with("--p") => {
                if let Some(value) = arg.split('=').nth(1) {
                    let overridden_port = value.parse::<u16>();
                    match overridden_port {
                        Ok(overridden_port) => port = Some(overridden_port),
                        Err(_) => {
                            println!("Invalid port number");
                            return;
                        }
                    }
                }
            }
            _ => {}
        }
    }
    println!("Starting rindexer rust project - graphql {enable_graphql} indexer {enable_indexer}");
    let path = env::current_dir();
    match path {
        Ok(path) => {
            let manifest_path = path.join("rindexer.yaml");
            let result = start_rindexer(StartDetails {
                manifest_path: &manifest_path,
                indexing_details: if enable_indexer {
                    Some(IndexingDetails {
                        registry: register_all_handlers(&manifest_path).await,
                        trace_registry: TraceCallbackRegistry { events: vec![] },
                        event_stream: None,
                    })
                } else {
                    None
                },
                graphql_details: GraphqlOverrideSettings {
                    enabled: enable_graphql,
                    override_port: port,
                },
            })
            .await;
            match result {
                Ok(_) => {}
                Err(e) => {
                    println!("Error starting rindexer: {e:?}");
                }
            }
        }
        Err(e) => {
            println!("Error getting current directory: {e:?}");
        }
    }
}
#[allow(dead_code)]
fn generate() {
    let manifest_dir = env::var("CARGO_MANIFEST_DIR").expect("CARGO_MANIFEST_DIR not set");
    let path = PathBuf::from(manifest_dir).join("rindexer.yaml");
    let manifest = read_manifest(&path).expect("Failed to read manifest");
    rindexer::generator::build::generate_rindexer_typings(&manifest, &path, true)
        .expect("Failed to generate typings");
}
#[allow(dead_code)]
fn generate_code_test() {
    let manifest_dir = env::var("CARGO_MANIFEST_DIR").expect("CARGO_MANIFEST_DIR not set");
    let path = PathBuf::from(manifest_dir).join("rindexer.yaml");
    let manifest = read_manifest(&path).expect("Failed to read manifest");
    rindexer::generator::build::generate_rindexer_handlers(manifest, &path, true)
        .expect("Failed to generate handlers");
}
#[allow(dead_code)]
fn generate_all() {
    let manifest_dir = env::var("CARGO_MANIFEST_DIR").expect("CARGO_MANIFEST_DIR not set");
    let path = PathBuf::from(manifest_dir).join("rindexer.yaml");
    rindexer::generator::build::generate_rindexer_typings_and_handlers(&path)
        .expect("Failed to generate typings and handlers");
}
#[cfg(test)]
mod tests {
    use super::*;
    #[test]
    fn test_generate() {
        generate();
    }
    #[test]
    fn test_code_generate() {
        generate_code_test();
    }
    #[test]
    fn test_generate_all() {
        generate_all();
    }
}
</file>

<file path="examples/rindexer_rust_playground/src/mod.rs">
#![allow(dead_code, unused)]
pub mod main;
</file>

<file path="examples/rindexer_rust_playground/.gitignore">
generated_csv
</file>

<file path="examples/rindexer_rust_playground/Cargo.toml">
[package]
name = "rindexer_rust_playground"
version = "0.1.0"
edition = "2024"

[dependencies]
# internal dependencies
rindexer = { path = "../../core" }

# external dependencies
tokio = { version = "1", features = ["full"] }
alloy = { version = "1.1.3", features = ["full"] }

[features]
reth = ["rindexer/reth"]
</file>

<file path="examples/rindexer_rust_playground/docker-compose.yml">
volumes:
  postgres_data:
    driver: local
services:
  postgresql:
    image: postgres:16
    shm_size: 1g
    restart: always
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - 5440:5432
    env_file:
      - .env
    healthcheck:
      test:
        ['CMD-SHELL', 'pg_isready -U $${DATABASE_USER} -d $${DATABASE_NAME} -q']
      interval: 5s
      timeout: 10s
      retries: 10c
</file>

<file path="examples/rindexer_rust_playground/rindexer.yaml">
name: RindexerPlayground
project_type: rust
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
    block_poll_frequency: "/4"
  - name: base
    chain_id: 8453
    rpc: https://mainnet.base.org
storage:
  postgres:
    enabled: true
  csv:
    enabled: true
    path: ./generated_csv
contracts:
  - name: RocketPoolETH
    details:
      - network: ethereum
        address: "0xae78736cd615f374d3085123a210448e74fc6393"
        start_block: '18900000'
        end_block: '19100000'
    abi: ./abis/erc20-abi.json
    include_events:
      - Transfer
      - Approval
  - name: ERC20
    details:
      - filter:
          - event_name: Transfer
          - event_name: Approval
        network: ethereum
    abi: ./abis/erc20-abi.json
    generate_csv: true
  - name: UniswapV3Pool
    details:
      - network: base
        filter:
          event_name: Swap
        start_block: 19953475
        end_block: 19963475
    abi: ./abis/uniswap-v3-pool-abi.json
    include_events:
      - Swap
global:
  contracts:
    - name: USDT
      details:
        - address: 0xdac17f958d2ee523a2206206994597c13d831ec7
          network: ethereum
        - address: 0xfde4C96c8593536E31F229EA8f37b2ADa2699bb2
          network: base
      abi: ./abis/erc20-abi.json
</file>

<file path="examples/rust_clickhouse/abis/RocketTokenRETH.abi.json">
[
  {
    "inputs":[
      {
        "internalType":"contract RocketStorageInterface",
        "name":"_rocketStorageAddress",
        "type":"address"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"constructor"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"owner",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value",
        "type":"uint256"
      }
    ],
    "name":"Approval",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"EtherDeposited",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"ethAmount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"TokensBurned",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"ethAmount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"TokensMinted",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value",
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"owner",
        "type":"address"
      },
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      }
    ],
    "name":"allowance",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"approve",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"account",
        "type":"address"
      }
    ],
    "name":"balanceOf",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_rethAmount",
        "type":"uint256"
      }
    ],
    "name":"burn",
    "outputs":[

    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"decimals",
    "outputs":[
      {
        "internalType":"uint8",
        "name":"",
        "type":"uint8"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"subtractedValue",
        "type":"uint256"
      }
    ],
    "name":"decreaseAllowance",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"depositExcess",
    "outputs":[

    ],
    "stateMutability":"payable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"depositExcessCollateral",
    "outputs":[

    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"getCollateralRate",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_rethAmount",
        "type":"uint256"
      }
    ],
    "name":"getEthValue",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"getExchangeRate",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_ethAmount",
        "type":"uint256"
      }
    ],
    "name":"getRethValue",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"getTotalCollateral",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"addedValue",
        "type":"uint256"
      }
    ],
    "name":"increaseAllowance",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_ethAmount",
        "type":"uint256"
      },
      {
        "internalType":"address",
        "name":"_to",
        "type":"address"
      }
    ],
    "name":"mint",
    "outputs":[

    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"name",
    "outputs":[
      {
        "internalType":"string",
        "name":"",
        "type":"string"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"symbol",
    "outputs":[
      {
        "internalType":"string",
        "name":"",
        "type":"string"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"totalSupply",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"recipient",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"transfer",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"sender",
        "type":"address"
      },
      {
        "internalType":"address",
        "name":"recipient",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"transferFrom",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[

    ],
    "name":"version",
    "outputs":[
      {
        "internalType":"uint8",
        "name":"",
        "type":"uint8"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "stateMutability":"payable",
    "type":"receive"
  }
]
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/indexers/clickhouse_indexer/mod.rs">
#![allow(dead_code, unused)]
pub mod rocket_pool;
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/indexers/clickhouse_indexer/rocket_pool.rs">
#![allow(non_snake_case)]
use super::super::super::typings::clickhouse_indexer::events::rocket_pool::{
    RocketPoolEventType, TransferEvent, no_extensions,
};
use alloy::primitives::{I256, U64, U256};
use rindexer::{
    EthereumSqlTypeWrapper, PgType, event::callback_registry::EventCallbackRegistry,
    rindexer_error, rindexer_info,
};
use std::path::PathBuf;
use std::sync::Arc;
async fn transfer_handler(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    let handler = TransferEvent::handler(
        |results, context| async move {
            if results.is_empty() {
                return Ok(());
            }
            let mut postgres_bulk_data: Vec<Vec<EthereumSqlTypeWrapper>> = vec![];
            for result in results.iter() {
                let data = vec![
                    EthereumSqlTypeWrapper::Address(result.tx_information.address),
                    EthereumSqlTypeWrapper::Address(result.event_data.from),
                    EthereumSqlTypeWrapper::Address(result.event_data.to),
                    EthereumSqlTypeWrapper::U256(U256::from(result.event_data.value)),
                    EthereumSqlTypeWrapper::B256(result.tx_information.transaction_hash),
                    EthereumSqlTypeWrapper::U64(result.tx_information.block_number),
                    EthereumSqlTypeWrapper::DateTimeNullable(
                        result.tx_information.block_timestamp_to_datetime(),
                    ),
                    EthereumSqlTypeWrapper::B256(result.tx_information.block_hash),
                    EthereumSqlTypeWrapper::String(result.tx_information.network.to_string()),
                    EthereumSqlTypeWrapper::U64(result.tx_information.transaction_index),
                    EthereumSqlTypeWrapper::U256(result.tx_information.log_index),
                ];
                postgres_bulk_data.push(data);
            }
            if postgres_bulk_data.is_empty() {
                return Ok(());
            }
            let rows = [
                "contract_address".to_string(),
                "from".to_string(),
                "to".to_string(),
                "value".to_string(),
                "tx_hash".to_string(),
                "block_number".to_string(),
                "block_timestamp".to_string(),
                "block_hash".to_string(),
                "network".to_string(),
                "tx_index".to_string(),
                "log_index".to_string(),
            ];
            let result = context
                .database
                .insert_bulk("clickhouse_indexer_rocket_pool.transfer", &rows, &postgres_bulk_data)
                .await;
            if let Err(e) = result {
                rindexer_error!("RocketPoolEventType::Transfer inserting bulk data: {:?}", e);
                return Err(e.to_string());
            }
            rindexer_info!("RocketPool::Transfer - INDEXED - {} events", results.len(),);
            Ok(())
        },
        no_extensions(),
    )
    .await;
    RocketPoolEventType::Transfer(handler).register(manifest_path, registry).await;
}
pub async fn rocket_pool_handlers(manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
    transfer_handler(manifest_path, registry).await;
}
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/indexers/all_handlers.rs">
use super::clickhouse_indexer::rocket_pool::rocket_pool_handlers;
use rindexer::event::callback_registry::EventCallbackRegistry;
use std::path::PathBuf;
pub async fn register_all_handlers(manifest_path: &PathBuf) -> EventCallbackRegistry {
    let mut registry = EventCallbackRegistry::new();
    rocket_pool_handlers(manifest_path, &mut registry).await;
    registry
}
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/indexers/mod.rs">
#![allow(dead_code, unused)]
pub mod all_handlers;
pub mod clickhouse_indexer;
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/typings/clickhouse_indexer/events/mod.rs">
#![allow(dead_code, unused)]
pub mod rocket_pool;
mod rocket_pool_abi_gen;
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/typings/clickhouse_indexer/events/rocket_pool_abi_gen.rs">
use alloy::sol;
sol!(
    #[sol(rpc, all_derives)]
    RindexerRocketPoolGen,
    r#"[
  {
    "inputs":[
      {
        "internalType":"contract RocketStorageInterface",
        "name":"_rocketStorageAddress",
        "type":"address"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"constructor"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"owner",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value",
        "type":"uint256"
      }
    ],
    "name":"Approval",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"EtherDeposited",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"ethAmount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"TokensBurned",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"ethAmount",
        "type":"uint256"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"time",
        "type":"uint256"
      }
    ],
    "name":"TokensMinted",
    "type":"event"
  },
  {
    "anonymous":false,
    "inputs":[
      {
        "indexed":true,
        "internalType":"address",
        "name":"from",
        "type":"address"
      },
      {
        "indexed":true,
        "internalType":"address",
        "name":"to",
        "type":"address"
      },
      {
        "indexed":false,
        "internalType":"uint256",
        "name":"value",
        "type":"uint256"
      }
    ],
    "name":"Transfer",
    "type":"event"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"owner",
        "type":"address"
      },
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      }
    ],
    "name":"allowance",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"approve",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"account",
        "type":"address"
      }
    ],
    "name":"balanceOf",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_rethAmount",
        "type":"uint256"
      }
    ],
    "name":"burn",
    "outputs":[
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
    ],
    "name":"decimals",
    "outputs":[
      {
        "internalType":"uint8",
        "name":"",
        "type":"uint8"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"subtractedValue",
        "type":"uint256"
      }
    ],
    "name":"decreaseAllowance",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
    ],
    "name":"depositExcess",
    "outputs":[
    ],
    "stateMutability":"payable",
    "type":"function"
  },
  {
    "inputs":[
    ],
    "name":"depositExcessCollateral",
    "outputs":[
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
    ],
    "name":"getCollateralRate",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_rethAmount",
        "type":"uint256"
      }
    ],
    "name":"getEthValue",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
    ],
    "name":"getExchangeRate",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_ethAmount",
        "type":"uint256"
      }
    ],
    "name":"getRethValue",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
    ],
    "name":"getTotalCollateral",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"spender",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"addedValue",
        "type":"uint256"
      }
    ],
    "name":"increaseAllowance",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"uint256",
        "name":"_ethAmount",
        "type":"uint256"
      },
      {
        "internalType":"address",
        "name":"_to",
        "type":"address"
      }
    ],
    "name":"mint",
    "outputs":[
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
    ],
    "name":"name",
    "outputs":[
      {
        "internalType":"string",
        "name":"",
        "type":"string"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
    ],
    "name":"symbol",
    "outputs":[
      {
        "internalType":"string",
        "name":"",
        "type":"string"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
    ],
    "name":"totalSupply",
    "outputs":[
      {
        "internalType":"uint256",
        "name":"",
        "type":"uint256"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"recipient",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"transfer",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
      {
        "internalType":"address",
        "name":"sender",
        "type":"address"
      },
      {
        "internalType":"address",
        "name":"recipient",
        "type":"address"
      },
      {
        "internalType":"uint256",
        "name":"amount",
        "type":"uint256"
      }
    ],
    "name":"transferFrom",
    "outputs":[
      {
        "internalType":"bool",
        "name":"",
        "type":"bool"
      }
    ],
    "stateMutability":"nonpayable",
    "type":"function"
  },
  {
    "inputs":[
    ],
    "name":"version",
    "outputs":[
      {
        "internalType":"uint8",
        "name":"",
        "type":"uint8"
      }
    ],
    "stateMutability":"view",
    "type":"function"
  },
  {
    "stateMutability":"payable",
    "type":"receive"
  }
]"#
);
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/typings/clickhouse_indexer/events/rocket_pool.rs">
#![allow(
    non_camel_case_types,
    clippy::enum_variant_names,
    clippy::too_many_arguments,
    clippy::upper_case_acronyms,
    clippy::type_complexity,
    dead_code
)]
use super::super::super::super::typings::database::get_or_init_clickhouse_client;
use super::super::super::super::typings::networks::get_provider_cache_for_network;
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use super::rocket_pool_abi_gen::RindexerRocketPoolGen::{
    self, RindexerRocketPoolGenEvents, RindexerRocketPoolGenInstance,
};
use alloy::network::AnyNetwork;
use alloy::primitives::{Address, B256, Bytes};
use alloy::sol_types::{SolEvent, SolEventInterface, SolType};
use rindexer::{
    ClickhouseClient, FutureExt, async_trait,
    blockclock::BlockClock,
    event::{
        callback_registry::{
            EventCallbackRegistry, EventCallbackRegistryInformation, EventCallbackResult,
            EventResult, HasTxInformation, TxInformation,
        },
        contract_setup::{ContractInformation, NetworkContract},
    },
    generate_random_id,
    manifest::{
        contract::{Contract, ContractDetails},
        yaml::read_manifest,
    },
    provider::{JsonRpcCachedProvider, RindexerProvider},
};
use std::collections::HashMap;
use std::error::Error;
use std::future::Future;
use std::path::{Path, PathBuf};
use std::pin::Pin;
use std::{any::Any, sync::Arc};
pub type ApprovalData = RindexerRocketPoolGen::Approval;
#[derive(Debug, Clone)]
pub struct ApprovalResult {
    pub event_data: ApprovalData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for ApprovalResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type EtherDepositedData = RindexerRocketPoolGen::EtherDeposited;
#[derive(Debug, Clone)]
pub struct EtherDepositedResult {
    pub event_data: EtherDepositedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for EtherDepositedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type TokensBurnedData = RindexerRocketPoolGen::TokensBurned;
#[derive(Debug, Clone)]
pub struct TokensBurnedResult {
    pub event_data: TokensBurnedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for TokensBurnedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type TokensMintedData = RindexerRocketPoolGen::TokensMinted;
#[derive(Debug, Clone)]
pub struct TokensMintedResult {
    pub event_data: TokensMintedData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for TokensMintedResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
pub type TransferData = RindexerRocketPoolGen::Transfer;
#[derive(Debug, Clone)]
pub struct TransferResult {
    pub event_data: TransferData,
    pub tx_information: TxInformation,
}
impl HasTxInformation for TransferResult {
    fn tx_information(&self) -> &TxInformation {
        &self.tx_information
    }
}
type BoxFuture<'a, T> = Pin<Box<dyn Future<Output = T> + Send + 'a>>;
#[async_trait]
trait EventCallback {
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()>;
}
pub struct EventContext<TExtensions>
where
    TExtensions: Send + Sync,
{
    pub database: Arc<ClickhouseClient>,
    pub extensions: Arc<TExtensions>,
}
// didn't want to use option or none made harder DX
// so a blank struct makes interface nice
pub struct NoExtensions {}
pub fn no_extensions() -> NoExtensions {
    NoExtensions {}
}
pub fn transfer_handler<TExtensions, F, Fut>(
    custom_logic: F,
) -> TransferEventCallbackType<TExtensions>
where
    TransferResult: Clone + 'static,
    F: for<'a> Fn(Vec<TransferResult>, Arc<EventContext<TExtensions>>) -> Fut
        + Send
        + Sync
        + 'static
        + Clone,
    Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    TExtensions: Send + Sync + 'static,
{
    Arc::new(move |results, context| {
        let custom_logic = custom_logic.clone();
        let results = results.clone();
        let context = Arc::clone(&context);
        async move { (custom_logic)(results, context).await }.boxed()
    })
}
type TransferEventCallbackType<TExtensions> = Arc<
    dyn for<'a> Fn(
            &'a Vec<TransferResult>,
            Arc<EventContext<TExtensions>>,
        ) -> BoxFuture<'a, EventCallbackResult<()>>
        + Send
        + Sync,
>;
pub struct TransferEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    callback: TransferEventCallbackType<TExtensions>,
    context: Arc<EventContext<TExtensions>>,
}
impl<TExtensions> TransferEvent<TExtensions>
where
    TExtensions: Send + Sync + 'static,
{
    pub async fn handler<F, Fut>(closure: F, extensions: TExtensions) -> Self
    where
        TransferResult: Clone + 'static,
        F: for<'a> Fn(Vec<TransferResult>, Arc<EventContext<TExtensions>>) -> Fut
            + Send
            + Sync
            + 'static
            + Clone,
        Fut: Future<Output = EventCallbackResult<()>> + Send + 'static,
    {
        Self {
            callback: transfer_handler(closure),
            context: Arc::new(EventContext {
                database: get_or_init_clickhouse_client().await,
                extensions: Arc::new(extensions),
            }),
        }
    }
}
#[async_trait]
impl<TExtensions> EventCallback for TransferEvent<TExtensions>
where
    TExtensions: Send + Sync,
{
    async fn call(&self, events: Vec<EventResult>) -> EventCallbackResult<()> {
        let events_len = events.len();
        // note some can not downcast because it cant decode
        // this happens on events which failed decoding due to
        // not having the right abi for example
        // transfer events with 2 indexed topics cant decode
        // transfer events with 3 indexed topics
        let result: Vec<TransferResult> = events
            .into_iter()
            .filter_map(|item| {
                item.decoded_data.downcast::<TransferData>().ok().map(|arc| TransferResult {
                    event_data: (*arc).clone(),
                    tx_information: item.tx_information,
                })
            })
            .collect();
        if result.len() == events_len {
            (self.callback)(&result, Arc::clone(&self.context)).await
        } else {
            panic!("TransferEvent: Unexpected data type - expected: TransferData")
        }
    }
}
pub enum RocketPoolEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    Transfer(TransferEvent<TExtensions>),
}
pub async fn rocket_pool_contract(
    network: &str,
) -> RindexerRocketPoolGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    let address: Address =
        "0xae78736cd615f374d3085123a210448e74fc6393".parse().expect("Invalid address");
    RindexerRocketPoolGen::new(
        address,
        get_provider_cache_for_network(network).await.get_inner_provider(),
    )
}
pub async fn decoder_contract(
    network: &str,
) -> RindexerRocketPoolGenInstance<Arc<RindexerProvider>, AnyNetwork> {
    if network == "ethereum" {
        RindexerRocketPoolGen::new(
            // do not care about address here its decoding makes it easier to handle ValueOrArray
            Address::ZERO,
            get_provider_cache_for_network(network).await.get_inner_provider(),
        )
    } else {
        panic!("Network not supported");
    }
}
impl<TExtensions> RocketPoolEventType<TExtensions>
where
    TExtensions: 'static + Send + Sync,
{
    pub fn topic_id(&self) -> &'static str {
        match self {
            RocketPoolEventType::Transfer(_) => {
                "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef"
            }
        }
    }
    pub fn event_name(&self) -> &'static str {
        match self {
            RocketPoolEventType::Transfer(_) => "Transfer",
        }
    }
    pub fn contract_name(&self) -> String {
        "RocketPool".to_string()
    }
    async fn get_provider(&self, network: &str) -> Arc<JsonRpcCachedProvider> {
        get_provider_cache_for_network(network).await
    }
    fn decoder(
        &self,
        network: &str,
    ) -> Arc<dyn Fn(Vec<B256>, Bytes) -> Arc<dyn Any + Send + Sync> + Send + Sync> {
        let decoder_contract = decoder_contract(network);
        match self {
            RocketPoolEventType::Transfer(_) => Arc::new(move |topics: Vec<B256>, data: Bytes| {
                match TransferData::decode_raw_log(topics, &data[0..]) {
                    Ok(event) => {
                        let result: TransferData = event;
                        Arc::new(result) as Arc<dyn Any + Send + Sync>
                    }
                    Err(error) => Arc::new(error) as Arc<dyn Any + Send + Sync>,
                }
            }),
        }
    }
    pub async fn register(self, manifest_path: &PathBuf, registry: &mut EventCallbackRegistry) {
        let rindexer_yaml = read_manifest(manifest_path).expect("Failed to read rindexer.yaml");
        let topic_id = self.topic_id();
        let contract_name = self.contract_name();
        let event_name = self.event_name();
        let contract_details = rindexer_yaml
            .all_contracts()
            .iter()
            .find(|c| c.name == contract_name)
            .unwrap_or_else(|| {
                panic!(
                    "Contract {} not found please make sure its defined in the rindexer.yaml",
                    contract_name
                )
            })
            .clone();
        let index_event_in_order = contract_details
            .index_event_in_order
            .as_ref()
            .map_or(false, |vec| vec.contains(&event_name.to_string()));
        // Expect providers to have been initialized, but it's an async init so this should
        // be fast but for correctness we must await each future.
        let mut providers = HashMap::new();
        for n in contract_details.details.iter() {
            let provider = self.get_provider(&n.network).await;
            providers.insert(n.network.clone(), provider);
        }
        let contract = ContractInformation {
            name: contract_details.before_modify_name_if_filter_readonly().into_owned(),
            details: contract_details
                .details
                .iter()
                .map(|c| {
                    let provider = providers.get(&c.network).expect("must have a provider").clone();
                    NetworkContract {
                        id: generate_random_id(10),
                        network: c.network.clone(),
                        cached_provider: provider.clone(),
                        block_clock: BlockClock::new(
                            rindexer_yaml.timestamps,
                            rindexer_yaml.config.timestamp_sample_rate,
                            provider.clone(),
                        ),
                        decoder: self.decoder(&c.network),
                        indexing_contract_setup: c.indexing_contract_setup(manifest_path),
                        start_block: c.start_block,
                        end_block: c.end_block,
                        disable_logs_bloom_checks: rindexer_yaml
                            .networks
                            .iter()
                            .find(|n| n.name == c.network)
                            .map_or(false, |n| n.disable_logs_bloom_checks.unwrap_or_default()),
                    }
                })
                .collect(),
            abi: contract_details.abi,
            reorg_safe_distance: contract_details.reorg_safe_distance.unwrap_or_default(),
        };
        let callback: Arc<
            dyn Fn(Vec<EventResult>) -> BoxFuture<'static, EventCallbackResult<()>> + Send + Sync,
        > = match self {
            RocketPoolEventType::Transfer(event) => {
                let event = Arc::new(event);
                Arc::new(move |result| {
                    let event = Arc::clone(&event);
                    async move { event.call(result).await }.boxed()
                })
            }
        };
        registry.register_event(EventCallbackRegistryInformation {
            id: generate_random_id(10),
            indexer_name: "ClickhouseIndexer".to_string(),
            event_name: event_name.to_string(),
            index_event_in_order,
            topic_id: topic_id.parse::<B256>().unwrap(),
            contract,
            callback,
        });
    }
}
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/typings/clickhouse_indexer/mod.rs">
#![allow(dead_code, unused)]
pub mod events;
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/typings/database.rs">
use rindexer::ClickhouseClient;
use std::sync::Arc;
use tokio::sync::OnceCell;
static CLICKHOUSE_CLIENT: OnceCell<Arc<ClickhouseClient>> = OnceCell::const_new();
pub async fn get_or_init_clickhouse_client() -> Arc<ClickhouseClient> {
    CLICKHOUSE_CLIENT
        .get_or_init(|| async {
            Arc::new(ClickhouseClient::new().await.expect("Failed to connect to Clickhouse"))
        })
        .await
        .clone()
}
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/typings/mod.rs">
#![allow(dead_code, unused)]
pub mod clickhouse_indexer;
pub mod database;
pub mod networks;
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/typings/networks.rs">
/// THIS IS A GENERATED FILE. DO NOT MODIFY MANUALLY.
///
/// This file was auto generated by rindexer - https://github.com/joshstevens19/rindexer.
/// Any manual changes to this file will be overwritten.
use alloy::{primitives::U64, transports::http::reqwest::header::HeaderMap};
use rindexer::{
    lazy_static,
    manifest::network::{AddressFiltering, BlockPollFrequency},
    notifications::ChainStateNotification,
    provider::{JsonRpcCachedProvider, RetryClientError, RindexerProvider, create_client},
    public_read_env_value,
};
use std::sync::Arc;
use tokio::sync::OnceCell;
use tokio::sync::broadcast::Sender;
#[allow(dead_code)]
async fn create_shadow_client(
    rpc_url: &str,
    chain_id: u64,
    compute_units_per_second: Option<u64>,
    block_poll_frequency: Option<BlockPollFrequency>,
    max_block_range: Option<U64>,
    address_filtering: Option<AddressFiltering>,
    chain_state_notification: Option<Sender<ChainStateNotification>>,
) -> Result<Arc<JsonRpcCachedProvider>, RetryClientError> {
    let mut header = HeaderMap::new();
    header.insert(
        "X-SHADOW-API-KEY",
        public_read_env_value("RINDEXER_PHANTOM_API_KEY").unwrap().parse().unwrap(),
    );
    create_client(
        rpc_url,
        chain_id,
        compute_units_per_second,
        max_block_range,
        block_poll_frequency,
        header,
        address_filtering,
        chain_state_notification,
    )
    .await
}
static ETHEREUM_PROVIDER: OnceCell<Arc<JsonRpcCachedProvider>> = OnceCell::const_new();
pub async fn get_ethereum_provider_cache() -> Arc<JsonRpcCachedProvider> {
    ETHEREUM_PROVIDER
        .get_or_init(|| async {
            let chain_state_notification = None;
            create_client(
                &public_read_env_value("https://mainnet.gateway.tenderly.co")
                    .unwrap_or("https://mainnet.gateway.tenderly.co".to_string()),
                1,
                None,
                None,
                Some(BlockPollFrequency::Rapid),
                HeaderMap::new(),
                None,
                chain_state_notification,
            )
            .await
            .expect("Error creating provider")
        })
        .await
        .clone()
}
pub async fn get_ethereum_provider() -> Arc<RindexerProvider> {
    get_ethereum_provider_cache().await.get_inner_provider()
}
pub async fn get_provider_cache_for_network(network: &str) -> Arc<JsonRpcCachedProvider> {
    if network == "ethereum" {
        return get_ethereum_provider_cache().await;
    }
    panic!("Network not supported")
}
</file>

<file path="examples/rust_clickhouse/src/rindexer_lib/mod.rs">
#![allow(dead_code, unused)]
pub mod indexers;
pub mod typings;
</file>

<file path="examples/rust_clickhouse/src/main.rs">
use self::rindexer_lib::indexers::all_handlers::register_all_handlers;
use rindexer::{
    GraphqlOverrideSettings, IndexingDetails, StartDetails,
    event::callback_registry::TraceCallbackRegistry, manifest::yaml::read_manifest, start_rindexer,
};
use std::{env, path::PathBuf};
#[allow(clippy::all)]
mod rindexer_lib;
#[tokio::main]
async fn main() {
    let args: Vec<String> = env::args().collect();
    let mut enable_graphql = false;
    let mut enable_indexer = false;
    let mut port: Option<u16> = None;
    let args = args.iter();
    if args.len() == 1 {
        enable_graphql = true;
        enable_indexer = true;
    }
    for arg in args {
        match arg.as_str() {
            "--graphql" => enable_graphql = true,
            "--indexer" => enable_indexer = true,
            _ if arg.starts_with("--port=") || arg.starts_with("--p") => {
                if let Some(value) = arg.split('=').nth(1) {
                    let overridden_port = value.parse::<u16>();
                    match overridden_port {
                        Ok(overridden_port) => port = Some(overridden_port),
                        Err(_) => {
                            println!("Invalid port number");
                            return;
                        }
                    }
                }
            }
            _ => {}
        }
    }
    println!("Starting rindexer rust project - graphql {enable_graphql} indexer {enable_indexer}");
    let path = env::current_dir();
    match path {
        Ok(path) => {
            let manifest_path = path.join("rindexer.yaml");
            let result = start_rindexer(StartDetails {
                manifest_path: &manifest_path,
                indexing_details: if enable_indexer {
                    Some(IndexingDetails {
                        registry: register_all_handlers(&manifest_path).await,
                        trace_registry: TraceCallbackRegistry { events: vec![] },
                        event_stream: None,
                    })
                } else {
                    None
                },
                graphql_details: GraphqlOverrideSettings {
                    enabled: enable_graphql,
                    override_port: port,
                },
            })
            .await;
            match result {
                Ok(_) => {}
                Err(e) => {
                    println!("Error starting rindexer: {e:?}");
                }
            }
        }
        Err(e) => {
            println!("Error getting current directory: {e:?}");
        }
    }
}
#[allow(dead_code)]
fn generate() {
    let manifest_dir = env::var("CARGO_MANIFEST_DIR").expect("CARGO_MANIFEST_DIR not set");
    let path = PathBuf::from(manifest_dir).join("rindexer.yaml");
    let manifest = read_manifest(&path).expect("Failed to read manifest");
    rindexer::generator::build::generate_rindexer_typings(&manifest, &path, true)
        .expect("Failed to generate typings");
}
#[allow(dead_code)]
fn generate_code_test() {
    let manifest_dir = env::var("CARGO_MANIFEST_DIR").expect("CARGO_MANIFEST_DIR not set");
    let path = PathBuf::from(manifest_dir).join("rindexer.yaml");
    let manifest = read_manifest(&path).expect("Failed to read manifest");
    rindexer::generator::build::generate_rindexer_handlers(manifest, &path, true)
        .expect("Failed to generate handlers");
}
#[allow(dead_code)]
fn generate_all() {
    let manifest_dir = env::var("CARGO_MANIFEST_DIR").expect("CARGO_MANIFEST_DIR not set");
    let path = PathBuf::from(manifest_dir).join("rindexer.yaml");
    rindexer::generator::build::generate_rindexer_typings_and_handlers(&path)
        .expect("Failed to generate typings and handlers");
}
#[cfg(test)]
mod tests {
    use super::*;
    #[test]
    fn test_generate() {
        generate();
    }
    #[test]
    fn test_code_generate() {
        generate_code_test();
    }
    #[test]
    fn test_generate_all() {
        generate_all();
    }
}
</file>

<file path="examples/rust_clickhouse/src/mod.rs">
#![allow(dead_code, unused)]
pub mod main;
</file>

<file path="examples/rust_clickhouse/.env.example">
CLICKHOUSE_URL="http://localhost:8123"
CLICKHOUSE_DB="default"
CLICKHOUSE_USER="default"
CLICKHOUSE_PASSWORD="default"
CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT="1"
</file>

<file path="examples/rust_clickhouse/.gitignore">
generated_csv
</file>

<file path="examples/rust_clickhouse/Cargo.toml">
[package]
name = "rust_clickhouse"
version = "0.1.0"
edition = "2024"

[dependencies]
rindexer = { path = "../../core" }
tokio = { version = "1", features = ["full"] }
alloy = { version = "1.0.41", features = ["full"] }

[features]
reth = ["rindexer/reth"]
</file>

<file path="examples/rust_clickhouse/docker-compose.yml">
volumes:
  clickhouse_data:
    driver: local
services:
  clickhouse:
    image: clickhouse/clickhouse-server:25.7
    container_name: nocode_clickhouse
    restart: unless-stopped
    shm_size: 1g
    env_file:
      - ./.env
    volumes:
      - clickhouse_data:/var
    healthcheck:
      test: [ "CMD-SHELL", "clickhouse-client --query='SELECT 1' || exit 1" ]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s
    ports:
      - "8123:8123" # HTTP interface
      - "9000:9000" # Native TCP interface
      - "9009:9009" # Interserver communication
</file>

<file path="examples/rust_clickhouse/rindexer.yaml">
name: ClickhouseIndexer
description: My first clickhouse rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: rust
timestamps: true
networks:
  - name: ethereum
    chain_id: 1
    rpc: https://mainnet.gateway.tenderly.co
    block_poll_frequency: rapid
storage:
  clickhouse:
    enabled: true
    drop_each_run: true
contracts:
  - name: RocketPool
    details:
      - network: ethereum
        address: 0xae78736cd615f374d3085123a210448e74fc6393
        start_block: '18600000'
        end_block: '18718056'
    abi: ./abis/RocketTokenRETH.abi.json
    include_events:
      - Transfer
</file>

<file path="graphql/index.js">
const express = require('express')
const cors = require('cors')
const { postgraphile } = require("postgraphile");
const { makeWrapResolversPlugin } = require("graphile-utils");
const PgSimplifyInflectorPlugin = require("@graphile-contrib/pg-simplify-inflector");
const ConnectionFilterPlugin = require("postgraphile-plugin-connection-filter");
const args = process.argv.slice(2);
if (args.length < 5) {
    console.error("Usage: postgraphile <connectionString> <schemas> <port> <page_limit> <timeout> <filterOnlyOnIndexedColumns> <disableAdvancedFilters>");
    process.exit(1);
}
const connectionString = args[0];
const schemas = args[1].split(",");
const port = parseInt(args[2]);
let graphqlPageLimit = parseInt(args[3]);
let graphqlTimeout = parseInt(args[4]);
let filterOnlyOnIndexedColumns = args[5] === "true";
let disableAdvancedFilters = args[6] === "true";
const byteaToHex = makeWrapResolversPlugin(
    (context) => {
        return context;
    },
    ({ scope }) =>
        async (resolver, user, args, context, _resolveInfo) => {
            if (typeof args === "object") {
                const first = args['first'];
                const last = args['last'];
                const firstValue = first ? parseInt(first) : 0;
                const lastValue = last ? parseInt(last) : 0;
                if (firstValue > graphqlPageLimit || lastValue > graphqlPageLimit) {
                    throw new Error(`Pagination limit exceeded. Maximum allowed is ${graphqlPageLimit}.`)
                }
            }
            // always add a limit on the amount you can bring back if last is defined
            // then let the resolver handle it as limits has been handled above already
            if (args['last'] === undefined) {
                args['first'] = args['first'] ? args['first'] : graphqlPageLimit;
            }
            let result = await resolver();
            if (result && typeof result === "string") {
                // it is a bytea need to turn back to a hex
                result = result.startsWith('\\x') ? result.replace('\\x', '0x') : result;
            }
            return result;
        },
);
let appendPlugins = [byteaToHex, PgSimplifyInflectorPlugin];
if (!disableAdvancedFilters) {
    appendPlugins.push(ConnectionFilterPlugin);
}
const options = {
    watchPg: true,
    host: "localhost",
    disableDefaultMutations: true,
    dynamicJson: true,
    cors: true,
    retryOnInitFail: true,
    enableQueryBatching: true,
    sortExport: true,
    ignoreIndexes: !filterOnlyOnIndexedColumns,
    enhanceGraphiql: false,
    graphiql: false,
    disableQueryLog: true,
    pgSettings: {
        statement_timeout: graphqlTimeout,
    },
    simpleCollections: 'omit',
    appendPlugins,
    graphileBuildOptions: {
        pgOmitListSuffix: false,
        pgSimplifyAllRows: false,
        pgShortPk: false,
        connectionFilterAllowedOperators: [
            "isNull",
            "equalTo",
            "notEqualTo",
            "distinctFrom",
            "notDistinctFrom",
            "lessThan",
            "lessThanOrEqualTo",
            "greaterThan",
            "greaterThanOrEqualTo",
            "in",
            "notIn",
        ],
    },
};
const htmlContent = (endpoint) => `
    <div style="width: 100%; height: 100%;" id='embedded-sandbox'></div>
    <script src="https://embeddable-sandbox.cdn.apollographql.com/_latest/embeddable-sandbox.umd.production.min.js"></script> 
    <script>
      new window.EmbeddedSandbox({
        target: '#embedded-sandbox',
        initialEndpoint: '${endpoint}',
      });
    </script>
`;
const app = express()
app.use(cors())
app.use(express.json())
app.use(postgraphile(connectionString, schemas, options))
app.get('/playground', (req, res) => {
    res.send(htmlContent(`http://localhost:${port}/graphql`));
});
app.listen(port, "0.0.0.0", () => {
    console.log(`GraphQL endpoint: http://localhost:${port}/graphql`);
    console.log(
        `GraphiQL Playground endpoint: http://localhost:${port}/playground`
    );
});
</file>

<file path="graphql/package.json">
{
  "name": "graphql",
  "version": "1.0.0",
  "main": "index.js",
  "bin": "index.js",
  "scripts": {
    "build": "pkg ."
  },
  "dependencies": {
    "@graphile-contrib/pg-simplify-inflector": "^6.1.0",
    "body-parser": "^1.20.2",
    "cors": "^2.8.5",
    "express": "^4.19.2",
    "graphile-utils": "^4.13.0",
    "knex": "^3.1.0",
    "knex-migrate": "^1.7.4",
    "pg": "^8.12.0",
    "postgraphile": "^4.13.0",
    "postgraphile-plugin-connection-filter": "^2.3.0"
  },
  "devDependencies": {
    "dotenv": "^16.4.5",
    "nodemon": "^3.1.4",
    "@yao-pkg/pkg": "^6.6.0"
  }
}
</file>

<file path="graphql/README.md">
# GraphQL Server

GraphQL server for rindexer using PostGraphile. This gets automatically built into a binary during the Rust build process.

## Development

To run the GraphQL server locally for development:

```bash
npm install
npm start
```

## Build

The GraphQL binary is automatically built when you compile the main Rust project. No manual steps required.

## Requirements

- Node.js (LTS version recommended)
- npm
</file>

<file path="helm/rindexer/files/abis/RocketTokenRETH.abi.json">
[{"inputs":[{"internalType":"contract RocketStorageInterface","name":"_rocketStorageAddress","type":"address"}],"stateMutability":"nonpayable","type":"constructor"},{"anonymous":false,"inputs":[{"indexed":true,"internalType":"address","name":"owner","type":"address"},{"indexed":true,"internalType":"address","name":"spender","type":"address"},{"indexed":false,"internalType":"uint256","name":"value","type":"uint256"}],"name":"Approval","type":"event"},{"anonymous":false,"inputs":[{"indexed":true,"internalType":"address","name":"from","type":"address"},{"indexed":false,"internalType":"uint256","name":"amount","type":"uint256"},{"indexed":false,"internalType":"uint256","name":"time","type":"uint256"}],"name":"EtherDeposited","type":"event"},{"anonymous":false,"inputs":[{"indexed":true,"internalType":"address","name":"from","type":"address"},{"indexed":false,"internalType":"uint256","name":"amount","type":"uint256"},{"indexed":false,"internalType":"uint256","name":"ethAmount","type":"uint256"},{"indexed":false,"internalType":"uint256","name":"time","type":"uint256"}],"name":"TokensBurned","type":"event"},{"anonymous":false,"inputs":[{"indexed":true,"internalType":"address","name":"to","type":"address"},{"indexed":false,"internalType":"uint256","name":"amount","type":"uint256"},{"indexed":false,"internalType":"uint256","name":"ethAmount","type":"uint256"},{"indexed":false,"internalType":"uint256","name":"time","type":"uint256"}],"name":"TokensMinted","type":"event"},{"anonymous":false,"inputs":[{"indexed":true,"internalType":"address","name":"from","type":"address"},{"indexed":true,"internalType":"address","name":"to","type":"address"},{"indexed":false,"internalType":"uint256","name":"value","type":"uint256"}],"name":"Transfer","type":"event"},{"inputs":[{"internalType":"address","name":"owner","type":"address"},{"internalType":"address","name":"spender","type":"address"}],"name":"allowance","outputs":[{"internalType":"uint256","name":"","type":"uint256"}],"stateMutability":"view","type":"function"},{"inputs":[{"internalType":"address","name":"spender","type":"address"},{"internalType":"uint256","name":"amount","type":"uint256"}],"name":"approve","outputs":[{"internalType":"bool","name":"","type":"bool"}],"stateMutability":"nonpayable","type":"function"},{"inputs":[{"internalType":"address","name":"account","type":"address"}],"name":"balanceOf","outputs":[{"internalType":"uint256","name":"","type":"uint256"}],"stateMutability":"view","type":"function"},{"inputs":[{"internalType":"uint256","name":"_rethAmount","type":"uint256"}],"name":"burn","outputs":[],"stateMutability":"nonpayable","type":"function"},{"inputs":[],"name":"decimals","outputs":[{"internalType":"uint8","name":"","type":"uint8"}],"stateMutability":"view","type":"function"},{"inputs":[{"internalType":"address","name":"spender","type":"address"},{"internalType":"uint256","name":"subtractedValue","type":"uint256"}],"name":"decreaseAllowance","outputs":[{"internalType":"bool","name":"","type":"bool"}],"stateMutability":"nonpayable","type":"function"},{"inputs":[],"name":"depositExcess","outputs":[],"stateMutability":"payable","type":"function"},{"inputs":[],"name":"depositExcessCollateral","outputs":[],"stateMutability":"nonpayable","type":"function"},{"inputs":[],"name":"getCollateralRate","outputs":[{"internalType":"uint256","name":"","type":"uint256"}],"stateMutability":"view","type":"function"},{"inputs":[{"internalType":"uint256","name":"_rethAmount","type":"uint256"}],"name":"getEthValue","outputs":[{"internalType":"uint256","name":"","type":"uint256"}],"stateMutability":"view","type":"function"},{"inputs":[],"name":"getExchangeRate","outputs":[{"internalType":"uint256","name":"","type":"uint256"}],"stateMutability":"view","type":"function"},{"inputs":[{"internalType":"uint256","name":"_ethAmount","type":"uint256"}],"name":"getRethValue","outputs":[{"internalType":"uint256","name":"","type":"uint256"}],"stateMutability":"view","type":"function"},{"inputs":[],"name":"getTotalCollateral","outputs":[{"internalType":"uint256","name":"","type":"uint256"}],"stateMutability":"view","type":"function"},{"inputs":[{"internalType":"address","name":"spender","type":"address"},{"internalType":"uint256","name":"addedValue","type":"uint256"}],"name":"increaseAllowance","outputs":[{"internalType":"bool","name":"","type":"bool"}],"stateMutability":"nonpayable","type":"function"},{"inputs":[{"internalType":"uint256","name":"_ethAmount","type":"uint256"},{"internalType":"address","name":"_to","type":"address"}],"name":"mint","outputs":[],"stateMutability":"nonpayable","type":"function"},{"inputs":[],"name":"name","outputs":[{"internalType":"string","name":"","type":"string"}],"stateMutability":"view","type":"function"},{"inputs":[],"name":"symbol","outputs":[{"internalType":"string","name":"","type":"string"}],"stateMutability":"view","type":"function"},{"inputs":[],"name":"totalSupply","outputs":[{"internalType":"uint256","name":"","type":"uint256"}],"stateMutability":"view","type":"function"},{"inputs":[{"internalType":"address","name":"recipient","type":"address"},{"internalType":"uint256","name":"amount","type":"uint256"}],"name":"transfer","outputs":[{"internalType":"bool","name":"","type":"bool"}],"stateMutability":"nonpayable","type":"function"},{"inputs":[{"internalType":"address","name":"sender","type":"address"},{"internalType":"address","name":"recipient","type":"address"},{"internalType":"uint256","name":"amount","type":"uint256"}],"name":"transferFrom","outputs":[{"internalType":"bool","name":"","type":"bool"}],"stateMutability":"nonpayable","type":"function"},{"inputs":[],"name":"version","outputs":[{"internalType":"uint8","name":"","type":"uint8"}],"stateMutability":"view","type":"function"},{"stateMutability":"payable","type":"receive"}]
</file>

<file path="helm/rindexer/templates/_helpers.tpl">
{{- define "rindexer.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" -}}
{{- end -}}

{{- define "rindexer.fullname" -}}
{{- if .Values.fullnameOverride -}}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" -}}
{{- else -}}
{{- $name := default .Chart.Name .Values.nameOverride -}}
{{- printf "%s-%s" $name .Release.Name | trunc 63 | trimSuffix "-" -}}
{{- end -}}
{{- end -}}

{{- define "rindexer.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" -}}
{{- end -}}

{{- define "rindexer.labels" -}}
app.kubernetes.io/name: {{ include "rindexer.name" . }}
helm.sh/chart: {{ include "rindexer.chart" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- with .Values.podLabels }}
{{ toYaml . | indent 4 }}
{{- end }}
{{- end -}}

{{- define "rindexer.selectorLabels" -}}
app.kubernetes.io/name: {{ include "rindexer.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end -}}
</file>

<file path="helm/rindexer/templates/configmap-abis.yaml">
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "rindexer.fullname" . }}-abis
  labels:
    {{- include "rindexer.labels" . | nindent 4 }}
data:
  RocketTokenRETH.abi.json: |
    {{ .Files.Get "files/abis/RocketTokenRETH.abi.json" | indent 4 }}
</file>

<file path="helm/rindexer/templates/configmap.yaml">
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "rindexer.fullname" . }}-config
  labels:
    {{- include "rindexer.labels" . | nindent 4 }}
data:
  rindexer.yaml: |
    name: franky
    project_type: no-code
    networks:
    - name: ethereum
      chain_id: 1
      rpc: https://mainnet.gateway.tenderly.co
    storage:
      postgres:
        enabled: {{ .Values.postgresql.enabled }}
    contracts:
    - name: RocketPoolETH
      details:
      - network: ethereum
        address: 0xae78736cd615f374d3085123a210448e74fc6393
        start_block: '18900000'
        end_block: '19000000'
      abi: ./abis/RocketTokenRETH.abi.json
      include_events:
      - Transfer
      - Approval
</file>

<file path="helm/rindexer/templates/deployment.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "rindexer.fullname" . }}
  labels:
    {{- include "rindexer.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "rindexer.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "rindexer.selectorLabels" . | nindent 8 }}
    spec:
      {{- if .Values.securityContext.enabled }}
      securityContext:
        runAsUser: {{ .Values.securityContext.runAsUser }}
        runAsGroup: {{ .Values.securityContext.runAsGroup }}
        fsGroup: {{ .Values.securityContext.fsGroup }}
      {{- end }}
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          command:
            {{- if and .Values.postgresql.enabled }}
            - "/app/rindexer"
            - "start"
            - "-p"
            - "{{ .Values.projectPath }}"
            - "all"
            {{- else }}
            - "/app/rindexer"
            - "start"
            - "-p"
            - "{{ .Values.projectPath }}"
            - "indexer"
            {{- end }}
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          env:
            - name: PROJECT_PATH
              value: {{ .Values.projectPath }}
            {{- if .Values.postgresql.enabled }}
            - name: POSTGRES_PASSWORD
              {{- if .Values.externalSecret }}
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.externalSecret }}
                  key: POSTGRES_PASSWORD
              {{- else }}
              value: {{ .Values.postgresql.auth.password | quote }}
              {{- end }}
            - name: DATABASE_URL
              {{- if .Values.externalSecret }}
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.externalSecret }}
                  key: DATABASE_URL
              {{- else }}
              value: {{ printf "postgres://%s:%s@%s:%s/%s" .Values.postgresql.auth.username .Values.postgresql.auth.password .Values.postgresql.auth.host .Values.postgresql.auth.port .Values.postgresql.auth.database | quote }}
              {{- end }}
            {{- end }}
          volumeMounts:
            - name: rindexer-config
              mountPath: {{ .Values.projectPath }}/rindexer.yaml
              subPath: rindexer.yaml
            - name: rindexer-abis
              mountPath: {{ .Values.projectPath }}/abis
              readOnly: true
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      volumes:
        - name: rindexer-config
          configMap:
            name: {{ include "rindexer.fullname" . }}-config
        - name: rindexer-abis
          configMap:
            name: {{ include "rindexer.fullname" . }}-abis
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
</file>

<file path="helm/rindexer/templates/ingress.yaml">
{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{ include "rindexer.fullname" . }}
  labels:
    {{- include "rindexer.labels" . | nindent 4 }}
  annotations:
    {{- toYaml .Values.ingress.annotations | nindent 4 }}
spec:
  rules:
    {{- range .Values.ingress.hosts }}
    - host: {{ .host | quote }}
      http:
        paths:
          {{- range .paths }}
          - path: {{ . | quote }}
            pathType: ImplementationSpecific
            backend:
              service:
                name: {{ include "rindexer.fullname" $ }}
                port:
                  number: {{ $.Values.service.port }}
          {{- end }}
    {{- end }}
  {{- if .Values.ingress.tls }}
  tls:
    {{- range .Values.ingress.tls }}
    - hosts:
        {{- range .hosts }}
        - {{ . | quote }}
        {{- end }}
      secretName: {{ .secretName }}
    {{- end }}
  {{- end }}
{{- end }}
</file>

<file path="helm/rindexer/templates/NOTES.txt">
1. Get the application URL by running these commands:
{{- if .Values.ingress.enabled }}
{{- range $host := .Values.ingress.hosts }}
  {{- range .paths }}
  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}{{ .path }}
  {{- end }}
{{- end }}
{{- else if contains "NodePort" .Values.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "rindexer.fullname" . }})
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" .Values.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch its status by running 'kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include "rindexer.fullname" . }}'
  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include "rindexer.fullname" . }} --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
  echo http://$SERVICE_IP:{{ .Values.service.port }}
{{- else if contains "ClusterIP" .Values.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "rindexer.name" . }},app.kubernetes.io/instance={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT
{{- end }}
</file>

<file path="helm/rindexer/templates/service.yaml">
apiVersion: v1
kind: Service
metadata:
  name: {{ include "rindexer.fullname" . }}
  labels:
    {{- include "rindexer.labels" . | nindent 4 }}
spec:
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "rindexer.selectorLabels" . | nindent 4 }}
  type: {{ .Values.service.type }}
</file>

<file path="helm/rindexer/templates/serviceaccount.yaml">
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ include "rindexer.fullname" . }}
  labels:
    {{- include "rindexer.labels" . | nindent 4 }}
</file>

<file path="helm/rindexer/.helmignore">
# Patterns to ignore when building packages.
# This supports shell glob matching, relative path matching, and
# negation (prefixed with !). Only one pattern per line.
.DS_Store
# Common VCS dirs
.git/
.gitignore
.bzr/
.bzrignore
.hg/
.hgignore
.svn/
# Common backup files
*.swp
*.bak
*.tmp
*.orig
*~
# Various IDEs
.project
.idea/
*.tmproj
.vscode/
</file>

<file path="helm/rindexer/Chart.yaml">
apiVersion: v2
name: rindexer
description: A Helm chart for deploying the rindexer application. See the [rindexer GitHub repository](https://github.com/joshstevens19/rindexer) for more details.
type: application
version: 0.1.0
appVersion: "1.0.0"
</file>

<file path="helm/rindexer/values.yaml">
replicaCount: 1
image:
  repository: ghcr.io/joshstevens19/rindexer
  tag: "latest"
  pullPolicy: IfNotPresent
service:
  type: ClusterIP
  port: 3001
ingress:
  enabled: false
  annotations: {}
  hosts:
    - host: chart-example.local
      paths: []
  tls: []
resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 100m
    memory: 128Mi
postgresql:
  enabled: false  # Enable or disable PostgreSQL integration
  auth:           # Optional: Define PostgreSQL credentials directly in values.yaml
    username: "rindexer"
    password: "yourpassword"
    database: "rindexerdb"
    host: "rindexerhost"
    port: "5432"
externalSecret: ""  # Specify the external secret for environment variables
securityContext:
  enabled: true
  runAsUser: 1000
  runAsGroup: 3000
  fsGroup: 2000
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  dropCapabilities:
    - ALL
projectPath: "/app/project"  # Define the base path for project files
nodeSelector: {}
tolerations: []
affinity: {}
</file>

<file path="providers/aws/.gitkeep">

</file>

<file path="providers/railway/example-app/erc20.abi.json">
[
	{
		"constant": true,
		"inputs": [],
		"name": "name",
		"outputs": [{ "name": "", "type": "string" }],
		"payable": false,
		"stateMutability": "view",
		"type": "function"
	},
	{
		"constant": false,
		"inputs": [
			{ "name": "guy", "type": "address" },
			{ "name": "wad", "type": "uint256" }
		],
		"name": "approve",
		"outputs": [{ "name": "", "type": "bool" }],
		"payable": false,
		"stateMutability": "nonpayable",
		"type": "function"
	},
	{
		"constant": true,
		"inputs": [],
		"name": "totalSupply",
		"outputs": [{ "name": "", "type": "uint256" }],
		"payable": false,
		"stateMutability": "view",
		"type": "function"
	},
	{
		"constant": false,
		"inputs": [
			{ "name": "src", "type": "address" },
			{ "name": "dst", "type": "address" },
			{ "name": "wad", "type": "uint256" }
		],
		"name": "transferFrom",
		"outputs": [{ "name": "", "type": "bool" }],
		"payable": false,
		"stateMutability": "nonpayable",
		"type": "function"
	},
	{
		"constant": false,
		"inputs": [{ "name": "wad", "type": "uint256" }],
		"name": "withdraw",
		"outputs": [],
		"payable": false,
		"stateMutability": "nonpayable",
		"type": "function"
	},
	{
		"constant": true,
		"inputs": [],
		"name": "decimals",
		"outputs": [{ "name": "", "type": "uint8" }],
		"payable": false,
		"stateMutability": "view",
		"type": "function"
	},
	{
		"constant": true,
		"inputs": [{ "name": "", "type": "address" }],
		"name": "balanceOf",
		"outputs": [{ "name": "", "type": "uint256" }],
		"payable": false,
		"stateMutability": "view",
		"type": "function"
	},
	{
		"constant": true,
		"inputs": [],
		"name": "symbol",
		"outputs": [{ "name": "", "type": "string" }],
		"payable": false,
		"stateMutability": "view",
		"type": "function"
	},
	{
		"constant": false,
		"inputs": [
			{ "name": "dst", "type": "address" },
			{ "name": "wad", "type": "uint256" }
		],
		"name": "transfer",
		"outputs": [{ "name": "", "type": "bool" }],
		"payable": false,
		"stateMutability": "nonpayable",
		"type": "function"
	},
	{
		"constant": false,
		"inputs": [],
		"name": "deposit",
		"outputs": [],
		"payable": true,
		"stateMutability": "payable",
		"type": "function"
	},
	{
		"constant": true,
		"inputs": [
			{ "name": "", "type": "address" },
			{ "name": "", "type": "address" }
		],
		"name": "allowance",
		"outputs": [{ "name": "", "type": "uint256" }],
		"payable": false,
		"stateMutability": "view",
		"type": "function"
	},
	{ "payable": true, "stateMutability": "payable", "type": "fallback" },
	{
		"anonymous": false,
		"inputs": [
			{ "indexed": true, "name": "src", "type": "address" },
			{ "indexed": true, "name": "guy", "type": "address" },
			{ "indexed": false, "name": "wad", "type": "uint256" }
		],
		"name": "Approval",
		"type": "event"
	},
	{
		"anonymous": false,
		"inputs": [
			{ "indexed": true, "name": "src", "type": "address" },
			{ "indexed": true, "name": "dst", "type": "address" },
			{ "indexed": false, "name": "wad", "type": "uint256" }
		],
		"name": "Transfer",
		"type": "event"
	},
	{
		"anonymous": false,
		"inputs": [
			{ "indexed": true, "name": "dst", "type": "address" },
			{ "indexed": false, "name": "wad", "type": "uint256" }
		],
		"name": "Deposit",
		"type": "event"
	},
	{
		"anonymous": false,
		"inputs": [
			{ "indexed": true, "name": "src", "type": "address" },
			{ "indexed": false, "name": "wad", "type": "uint256" }
		],
		"name": "Withdrawal",
		"type": "event"
	}
]
</file>

<file path="providers/railway/example-app/rindexer.yaml">
name: wETHIndexer
description: My first rindexer project
repository: https://github.com/joshstevens19/rindexer
project_type: no-code
networks:
  - name: sepolia
    chain_id: 11155111
    rpc: https://sepolia.gateway.tenderly.co
storage:
  postgres:
    enabled: true
contracts:
  - name: SepoliaWETH
    details:
      - network: sepolia
        address: 0x097D90c9d3E0B50Ca60e1ae45F6A81010f9FB534
    abi: ./erc20.abi.json
    include_events:
      - Deposit
      - Approval
      - Transfer
      - Withdrawal
</file>

<file path="providers/railway/.dockerignore">
*

!Dockerfile
!.dockerignore
!example-app
</file>

<file path="providers/railway/Dockerfile">
# syntax = docker/dockerfile:1.6
FROM ghcr.io/joshstevens19/rindexer:latest AS rindexer

ARG PROJECT_PATH="./example-app"

WORKDIR /usr/src/app

COPY ${PROJECT_PATH} /usr/src/app/rindexer-app

ARG PORT="3001"
ENV PORT=${PORT}

EXPOSE ${PORT}

ENTRYPOINT ["/app/rindexer"]
CMD ["start", "-p", "./rindexer-app", "all"]
</file>

<file path="providers/railway/railway.toml">
#: schema https://railway.app/railway.schema.json
[build]
builder = "DOCKERFILE"
buildEnvironment = "V2"
dockerfilePath = "./Dockerfile"

[deploy]
runtime = "V2"
numReplicas = 1
sleepApplication = false
restartPolicyMaxRetries = 10
restartPolicyType = "ON_FAILURE"
</file>

<file path="providers/railway/README.md">
# Railway

## One-click Deploy Example

[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/Rqrlcf?referralCode=eD4laT)

## Deploy an example project

<https://github.com/joshstevens19/rindexer/tree/master/providers/railway>

1. Clone the relevant directory

  ```bash
  # this will clone the railway directory
  mkdir rindexer-railway && cd rindexer-railway
  git clone \
    --depth=1 \
    --no-checkout \
    --filter=tree:0 \
    https://github.com/joshstevens19/rindexer .
  git sparse-checkout set --no-cone providers/railway .
  git checkout && cp -r providers/railway/* . && rm -rf providers
  ```

2. Initialize a new Railway project

  ```bash
  railway init --name rindexer-example
  ```

3. Create a service and link it to the project
  
  ```bash
  railway up --detach
  railway link --name rindexer-example --enviroment production
  ```

4. Create a Postgres database

  ```bash
  railway add --database postgre-sql
  ```

5. Configure environment variables

  ```bash
  railway open
  ```

- then open the service "Variables" tab and click on "Add Variable Reference" and select `DATABASE_URL`,
- postfix `?sslmode=diable` to the end of the value. It should look like this: `${{Postgres.DATABASE_URL}}?sslmode=disable`,
- hit "Deploy" or Shift+Enter.

6. Create a domain to access GraphQL Playground

  ```bash
  railway domain
  ```

7. Redeploy the service

  ```bash
  railway up
  ```
</file>

<file path="xtask/src/main.rs">
use anyhow::Context;
use clap::{Parser, Subcommand};
use rindexer::blockclock::DeltaEncoder;
use std::path::PathBuf;
use tracing_subscriber::filter::LevelFilter;
use tracing_subscriber::layer::SubscriberExt;
use tracing_subscriber::util::SubscriberInitExt;
use tracing_subscriber::{EnvFilter, fmt};
#[derive(Parser)]
#[command(name = "xtask", version, about = "Backend scripts")]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}
#[derive(Subcommand)]
enum Commands {
    /// RunLength Encode the timestamps for block on a specific network.
    ///
    /// # Example
    ///
    /// ```sh
    /// cargo xtask encode-block-clock \
    ///   --network 137 \
    ///   --rpc-url "https://polygon-mainnet.g.alchemy.com/v2/API_KEY" \
    ///   --batch-size 1000
    /// ```
    EncodeBlockClock {
        /// Name of the continuous aggregate (without _base suffix)
        #[arg(long)]
        network: u32,
        /// RPC Url for the Network.
        #[arg(long)]
        rpc_url: String,
        /// Batch size for block requests.
        #[arg(long)]
        batch_size: Option<u64>,
    },
    /// Update the timestamps encodings for blocks on all already supported networks.
    ///
    /// # Example
    ///
    /// ```sh
    /// cargo xtask update-block-clocks \
    ///   --alchemy-api-key "API_KEY" \
    ///   --batch-size 1000
    /// ```
    UpdateBlockClocks {
        /// RPC Url for the Network.
        #[arg(long)]
        alchemy_api_key: String,
        /// Batch size for block requests.
        #[arg(long)]
        batch_size: Option<u64>,
    },
}
#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let cli = Cli::parse();
    let filter =
        EnvFilter::builder().with_default_directive(LevelFilter::INFO.into()).from_env_lossy();
    let fmt = fmt::layer().with_timer(fmt::time::UtcTime::rfc_3339()).with_target(false);
    tracing_subscriber::registry().with(fmt).with(filter).init();
    let res: anyhow::Result<()> = match cli.command {
        Commands::EncodeBlockClock { network, rpc_url, batch_size } => {
            let base = std::env::var("CARGO_MANIFEST_DIR").context("missing CARGO_MANIFEST_DIR")?;
            let path = PathBuf::from(base);
            let path = path
                .parent()
                .context("missing parent directory")?
                .join("core")
                .join("resources")
                .join("blockclock");
            let path = path.join(format!("{}.blockclock", network));
            tracing::info!("Writing to directory: {}", path.to_str().context("path to string")?);
            let mut encoder = DeltaEncoder::from_file_inner(network, Some(&rpc_url), &path)?;
            encoder.poll_encode_loop(batch_size.unwrap_or(100)).await?;
            Ok(())
        }
        Commands::UpdateBlockClocks { alchemy_api_key, batch_size } => {
            let base = std::env::var("CARGO_MANIFEST_DIR").context("missing CARGO_MANIFEST_DIR")?;
            let path = PathBuf::from(base);
            let blockclock_dir = path
                .parent()
                .context("missing parent directory")?
                .join("core")
                .join("resources")
                .join("blockclock");
            for entry in
                std::fs::read_dir(&blockclock_dir).context("reading blockclock directory")?
            {
                let entry = entry?;
                let path = entry.path();
                if path.extension().and_then(|e| e.to_str()) != Some("blockclock") {
                    continue;
                }
                let stem =
                    path.file_stem().and_then(|s| s.to_str()).context("invalid file name")?;
                let network: u32 = stem
                    .parse()
                    .with_context(|| format!("invalid network id in file: {}", stem))?;
                tracing::info!("--------------------------------------------------");
                tracing::info!(
                    "Updating blockclock for network {} from file {}",
                    network,
                    path.display()
                );
                tracing::info!("--------------------------------------------------");
                let subdomain = match network {
                    1 => "eth-mainnet",
                    137 => "polygon-mainnet",
                    10 => "opt-mainnet",
                    42161 => "arb-mainnet",
                    56 => "bnb-mainnet",
                    324 => "zksync-mainnet",
                    8453 => "base-mainnet",
                    43114 => "avax-mainnet",
                    534352 => "scroll-mainnet",
                    _ => anyhow::bail!("unsupported network {network}"),
                };
                let rpc_url = format!("https://{}.g.alchemy.com/v2/{}", subdomain, alchemy_api_key);
                let mut encoder = DeltaEncoder::from_file_inner(network, Some(&rpc_url), &path)
                    .with_context(|| format!("failed to create encoder for network {}", network))?;
                encoder.poll_encode_loop(batch_size.unwrap_or(100)).await?;
            }
            tracing::info!(
                "Finished updating all supported block clocks: {}",
                path.to_str().context("path to string")?
            );
            Ok(())
        }
    };
    if let Err(e) = res {
        tracing::error!("Error running xtask: {}", e);
    };
    Ok(())
}
</file>

<file path="xtask/Cargo.toml">
[package]
name = "xtask"
version = "0.1.0"
edition = "2024"

[dependencies]
clap = { version = "4.5.50", features = ["derive", "env"] }
anyhow = { version = "1.0.100" }
tokio = { version = "1.48.0", features = ["full"] }
tracing = { version = "0.1.41" }
tracing-subscriber = { version = "0.3.20", features = [
    "env-filter",
    "fmt"
] }
rindexer = { path = "../core" }

[features]
default = []
debug-json = []
</file>

<file path=".dockerignore">
.git
Dockerfile
.env
</file>

<file path=".gitattributes">
*.tar.gz filter=lfs diff=lfs merge=lfs -text
documentation/docs/public/releases/resources.zip filter=lfs diff=lfs merge=lfs -text
</file>

<file path=".gitignore">
# Generated by Cargo
# will have compiled files and executables
debug/
target/

.idea
src/.DS_Store
.DS_Store

# These are backup files generated by rustfmt
**/*.rs.bk

# MSVC Windows builds of rustc generate these, which store debugging information
*.pdb

# App ignores
cli/rindexer.yaml
cli/.env
examples/*
!examples/rindexer_demo_cli
!examples/nocode_clickhouse
!examples/rindexer_native_transfers
!examples/rindexer_factory_indexing
!examples/rindexer_rust_playground
!examples/rust_clickhouse
!examples/clickhouse_factory_indexing
documentation/node_modules
documentation/dist
documentation/docs/dist
documentation/vocs.config.ts.*
graphql/node_modules
rindexer_rust_playground/generated_csv/*/*.csv
.env
!.env.example
node_modules
core/resources/rindexer-graphql-*
</file>

<file path="Cargo.toml">
[workspace]
resolver = "2"
members = [
    "core",
    "cli",
    "xtask",
    "examples/rindexer_rust_playground",
    "examples/rust_clickhouse",
    "examples/clickhouse_factory_indexing",
    "examples/rindexer_factory_indexing"
]
</file>

<file path="docker-compose.yml">
networks:
  default:
    name: rindexer_default
services:
  rindexer:
    image: ghcr.io/joshstevens19/rindexer
    platform: linux/amd64
    command: |
      start -p /app/project_path all
    environment:
      - PROJECT_PATH
      - DATABASE_URL
    volumes:
      - ${PROJECT_PATH}:/app/project_path
    ports:
      - 3001:3001
    restart: always
</file>

<file path="Dockerfile">
FROM --platform=linux/amd64 rust:1.88.0-bookworm as builder
ENV CARGO_NET_GIT_FETCH_WITH_CLI=true

RUN curl -fsSL https://deb.nodesource.com/setup_lts.x | bash - && \
    apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    clang \
    libclang-dev \
    nodejs \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app
COPY . .

# Build for standard Linux (glibc) instead of musl
RUN RUSTFLAGS='-C target-cpu=x86-64-v2' cargo build --release --features jemalloc,reth --workspace --exclude rindexer_rust_playground

FROM --platform=linux/amd64 debian:bookworm-slim
RUN apt-get update && apt-get install -y \
    libssl3 \
    ca-certificates \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

RUN curl -L https://foundry.paradigm.xyz | bash
RUN /root/.foundry/bin/foundryup

COPY --from=builder /app/target/release/rindexer_cli /app/rindexer
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/

ENTRYPOINT ["/app/rindexer"]
</file>

<file path="funding.json">
{
  "opRetro": {
    "projectId": "0x97525ec91080ddd917eaccabf9d383cf70a2f78839ab21eeabe1687e312f2132"
  }
}
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 Josh Stevens

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="README.md">
# 🦀 rindexer 🦀 

Note rindexer is brand new and actively under development, things will change and bugs will exist - if you find any bugs or have any
feature requests please open an issue on [github](https://github.com/joshstevens19/rindexer/issues).

rindexer is an opensource powerful, high-speed indexing toolset developed in Rust, designed for compatibility with any EVM chain.
This tool allows you to index chain events using a simple YAML file, requiring no additional coding.
For more advanced needs, the rindexer provides foundations and advanced capabilities to build whatever you want.
It's highly extendable, enabling you to construct indexing pipelines with ease and focus exclusively on the logic.
rindexer out the box also gives you a GraphQL API to query the data you have indexed instantly.

You can get to the full rindexer [documentation](https://rindexer.xyz/).

## Install 

```bash
curl -L https://rindexer.xyz/install.sh | bash
```

If you’re on Windows, you will need to install and use Git BASH or WSL, as your terminal,
since rindexer installation does not support Powershell or Cmd.

## Use rindexer

Once installed you can run `rindexer --help` in your terminal to see all the commands available to you.

```bash
rindexer --help
```

```bash
Blazing fast EVM indexing tool built in rust

Usage: rindexer [COMMAND]

Commands:
  new           Creates a new rindexer no-code project or rust project
  start         Start various services like indexers, GraphQL APIs or both together
  add           Add elements such as contracts to the rindexer.yaml file
  codegen       Generates rust code based on rindexer.yaml or graphql queries
  delete        Delete data from the postgres database or csv files
  phantom       Use phantom events to add your own events to contracts
  help          Print this message or the help of the given subcommand(s)

Options:
  -h, --help     Print help
  -V, --version  Print version
```

We have full documentation https://rindexer.xyz/docs/introduction/installation which goes into more detail on how to use 
rindexer and all the commands available to you.

## Docker

There's a pre-built docker image which can be used to run `rindexer` inside your dockerized infra:

- Docker image: [`ghcr.io/joshstevens19/rindexer`](https://github.com/users/joshstevens19/packages/container/package/rindexer)

### Create new project
To create a new `no-code` project in your current directory, you can run the following:

`docker run -it -v $PWD:/app/project_path ghcr.io/joshstevens19/rindexer new -p /app/project_path no-code`

### Use with existing project
To use it with an existing project and a running postgres instance you can simply invoke:

```
export PROJECT_PATH=/path/to/your/project
export DATABASE_URL="postgresql://user:pass@postgres/db"

docker-compose up -d
```

This will start all local indexing and if you have enabled the graphql endpoint, it will become exposed under:

http://localhost:3001

## Helm Chart

We also provide a Helm chart for deploying `rindexer` in Kubernetes environments. The Helm chart simplifies the deployment process and allows for easy customization of the deployment parameters.

You can find the Helm chart in the following directory:

- **[rindexer Helm Chart](https://github.com/joshstevens19/rindexer/tree/master/helm/rindexer)**

To use the Helm chart, follow the instructions in the [Helm Chart README](https://github.com/joshstevens19/rindexer/tree/master/helm/rindexer/README.md) to deploy `rindexer` to your Kubernetes cluster.

## What can I use rindexer for?

- Hackathons: spin up a quick indexer to index events for your dApp with an API without any code needed
- Data reporting
- Building advanced indexers
- Building a custom indexer for your project
- Fast prototyping and MVP developments
- Quick proof-of-concept projects
- Enterprise standard indexing solutions for projects
- Much more... 

## Crate.io

rindexer rust project building is available on crate.io but we strongly recommend using the git repository to install it
and use it in your project. To use the CLI please install it using the above instructions.

https://crates.io/crates/rindexer

## What networks do you support?

rindexer supports any EVM chain out of the box. If you have a custom chain, you can easily add support for it by
adding the chain's RPC URL to the YAML configuration file and defining the chain ID. No code changes are required.

## Code structure

### core

This is the core of rindexer, it contains all the logic for indexing and where most the code lives.

### cli

This
is the cli for rindexer, it contains all the logic for the cli and is how users interact with rindexer.

### graphql

This is the express project which leverages postgraphile rindexer GraphQL, it is automatically built into a binary during the Rust build process using `pkg`.

**Build Process:**
- Automatically builds during `cargo build`
- Detects target architecture (macOS, Linux, Windows) 
- Smart rebuilding - only rebuilds when source files change
- Requires Node.js and npm for development/building

**Development:**
```bash
cd graphql
npm install
npm start
```

The binary is embedded into the Rust application and started automatically when GraphQL functionality is enabled.

### documentation

This is the documentation for rindexer, it is built using [voc](https://vocs.dev/) which is an incredible
tool to build documentation. Big shout out to `wevm` team for all the work they have done on `vocs`, `viem` and `wagmi`.

### examples

This just holds some no-code examples for rindexer which is referenced in the docs or used for new users to see
how a project is setup.

## Building

### Requirements

- Rust (latest stable)
- Node.js and npm (for GraphQL server build)

### Locally 

To build locally you can just run `cargo build` in the root of the project. This will build everything for you as this is a workspace, including the GraphQL server binary.

**Note:** The first build may take longer as it needs to:
1. Install npm dependencies for the GraphQL server
2. Build the GraphQL binary for your target platform
3. Compile all Rust code

Subsequent builds use smart caching and will only rebuild components that have changed.

### Prod

To build for prod you can run `make prod_build` this will build everything for you and optimise it for production.

## Formatting

you can run `cargo fmt` to format the code, rules have been mapped in the `rustfmt.toml` file.

## Contributing

Anyone is welcome to contribute to rindexer, feel free to look over the issues or open a new one if you have
any new ideas or bugs you have found.

### Playing around with the CLI locally

You can use the `make` commands to run the CLI commands locally, this is useful for testing and developing.
These are located in the `cli` folder > `Makefile`. It uses `CURDIR` to resolve the paths for you, so they should work
out of the box. The examples repo has a `rindexer_demo_cli` folder which you can modify (please do not commit any changes though) 
or spin up a new no-code project using the make commands.

## Release

To release a new rindexer:

1. Checkout `release/x.x.x` branch depending on the next version number
2. Push the branch to GitHub which will queue a build on the CI
3. Once build is successful, a PR will be automatically created with updated changelog and version
4. Review and merge the auto-generated PR - this will auto-deploy the release with binaries built from the release branch
</file>

<file path="rust-toolchain.toml">
[toolchain]
channel = "stable"
</file>

<file path="rustfmt.toml">
reorder_imports = true
use_small_heuristics = "Max"
use_field_init_shorthand = true
</file>

</files>
